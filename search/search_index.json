{"config":{"lang":["fr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome into my Arctic Vault !","text":"<p>Mon id\u00e9e ici est de regrouper tout ce qui peut m'\u00eatre utile, et de formaliser mes pens\u00e9es, sur le deep learning, le DevOps, le MLOps, etc.</p> <ul> <li>Status of GPU-Determinism in TensorFlow</li> <li>Status of GPU-Determinism in PyTorch</li> </ul>"},{"location":"#ressources-generales","title":"Ressources g\u00e9n\u00e9rales","text":"<ul> <li>The Matrix Calculus You Need For Deep Learning</li> <li>Statisticians say the darndest things</li> <li>Distill</li> <li>CS231n: Convolutional Neural Networks for Visual Recognition</li> <li>CS229: Machine Learning</li> <li>Stanford Welcome to the Deep Learning Tutorial!</li> <li>Mathematics for Machine Learning</li> <li>Neural Networks and Deep Learning</li> <li>Learn TensorFlow and deep learning, without a Ph.D.</li> <li>The Twelve-Factor App</li> <li>Yann Le Cun Deep Learning printemps 2020</li> <li>The Missing Semester of Your CS Education</li> <li>Great Practical Ideas in Computer Science</li> <li>Code. Simply. Clearly. Calmly.</li> <li>Gestion s\u00e9mantique de version</li> <li>Use Bash Strict Mode (Unless You Love Debugging)</li> <li>Understanding the Fourier Transform by example</li> <li>ML Notebooks</li> <li>srsly: Modern high-performance serialization utilities for Python</li> <li>Radically efficient machine teaching. An annotation tool powered by active learning.</li> <li>A refreshing functional take on deep learning, compatible with your favorite libraries.</li> </ul>"},{"location":"#redaction-documentation-python","title":"R\u00e9daction documentation Python","text":"<ul> <li>MkDocs</li> <li>MkDocs-Material</li> <li>mkdocstrings</li> <li>Code Tells You How, Comments Tell You Why</li> <li>Diagram as Code</li> </ul>"},{"location":"#formating-linting-type-hinting","title":"Formating, Linting, Type Hinting","text":"<ul> <li>flake8</li> <li>isort</li> <li>Black : The uncompromising code formatter</li> <li>mypy</li> <li>rope (refactoring)</li> <li>Static Code Analysis for Python</li> <li>Complexity Waterfall</li> <li>Jones complexity</li> <li>wemake-python-styleguide Best practices</li> </ul>"},{"location":"#test-unitaires","title":"Test unitaires","text":"<ul> <li>Pytest</li> <li>Coverage</li> <li>The tests talk</li> <li>code coverage</li> <li>The Wide World of Software Testing</li> </ul>"},{"location":"#integration-developpement-continu","title":"Int\u00e9gration, d\u00e9veloppement continu","text":"<ul> <li>Gestion des d\u00e9pendances</li> <li>pre-commit</li> <li>Automate Python workflow using pre-commits: black and flake8</li> <li>Continuous Delivery for Machine Learning</li> <li>Continuous Delivery 101</li> <li>GoCD User Documentation</li> <li>GitHub Actions for perfect Python Continuous Integration</li> <li>Build Bot</li> <li>How to Add Domains</li> <li>How to Create, Edit, and Delete DNS Records</li> </ul>"},{"location":"#docker","title":"Docker","text":"<ul> <li>Dockerize your Development Environment</li> <li>Developing inside a Container</li> <li>devcontainer.json reference</li> <li>Advanced Container Configuration</li> <li>Creating an API with FastAPI and Docker</li> <li>Faster Docker builds with pipenv, poetry, or pip-tools</li> <li>Publishing Docker images</li> <li>Awesome Docker Compose samples</li> </ul>"},{"location":"#code-quality","title":"Code quality","text":"<ul> <li>Radon and code metrics</li> <li>M\u00e9triques d'Halstead</li> <li>Think Twice Before Using the \u201cMaintainability Index\u201d</li> <li>Using Metrics to Evaluate Software System Maintainabilitv</li> </ul>"},{"location":"#code-security","title":"Code security","text":"<ul> <li>Bandit</li> <li>Bandit Doc</li> <li>Typosquatting programming language package managers</li> <li>Safety</li> </ul>"},{"location":"#a-trier","title":"A trier","text":"<ul> <li>How to set up a perfect Python project</li> <li>The Magical Number Seven, Plus or Minus Two</li> <li>Key Kubernetes Concepts</li> <li>GoCD User Documentation</li> <li>Starting New Python Project in VSCode</li> <li>Configuring Python Workspace: Poetry</li> <li>Vulture - Find dead code</li> </ul>"},{"location":"#packaging","title":"packaging","text":"<ul> <li>The Hitchhiker\u2019s Guide to Packaging</li> <li>Poetry</li> </ul>"},{"location":"#fastapi","title":"FastAPI","text":""},{"location":"#guidelines-machine-learning-mlops-ressources","title":"Guidelines machine learning, MLOps : ressources","text":""},{"location":"#deep-learning-tips-and-tricks","title":"Deep Learning tips and tricks","text":"<ul> <li>Encoding Cyclical Features for Deep Learning</li> <li>2D Convolution as a Doubly Block Circulant Matrix Operating on a Vector</li> <li>Tuning the \\(\\varepsilon\\) parameter</li> <li>Super-convergence in Tensorflow 2 with the 1Cycle Policy</li> <li>The Mathematical Engineering of Deep Learning</li> </ul>"},{"location":"#mlops","title":"MLOps","text":"<ul> <li>Awesome MLOps</li> <li>Full Stack Deep Learning</li> <li>A Guide to Terraform for Data Scientists</li> <li>From Training to Serving: Machine Learning Models with Terraform</li> <li>Manage Azure Machine Learning workspaces using Terraform</li> </ul>"},{"location":"#data-versioning","title":"Data Versioning","text":"<ul> <li>DVC</li> <li>Data Version Control With Python and DVC</li> <li>First Impressions of Data Science Version Control (DVC)</li> </ul>"},{"location":"#monitoring","title":"Monitoring","text":"<ul> <li>Monitoring is a means, not an end</li> <li>MLOps: Model Monitoring 101</li> <li>Data Monitoring avec great_expectations</li> <li>Great expectations \u2014 An Introduction.</li> <li>TensorFlow Data Validation</li> </ul>"},{"location":"#pipeline-workflow","title":"Pipeline, workflow","text":"<ul> <li>Airflow</li> <li>Prefect</li> </ul>"},{"location":"#ressources-azureml","title":"Ressources AzureML","text":"<ul> <li>Azure Machine Learning Deployment using Terraform</li> <li>Introduction to the Azure ML-Ops Project Accelerator</li> <li>Manages a Azure Machine Learning Workspace</li> <li>Azure Terraform QuickStart Templates</li> <li>MLOps on Azure</li> <li>MLOps with Azure ML</li> </ul>"},{"location":"algebra/algebra/","title":"Alg\u00e8bre tensorielle","text":"<p>Sources :</p> <ul> <li>Einstein summation for multi-dimensional arrays</li> <li>Einsum</li> <li>Einsum is All you Need - Einstein Summation in Deep Learning</li> <li>Understanding einsum for Deep learning: implement a transformer with multi-head self-attention from scratch</li> <li>einops</li> <li>Generalized Low Rank Models</li> <li>Tensor Decompositions and Applications</li> <li>Multilinear operators for higher-order decompositions</li> </ul>"},{"location":"algebra/algebra/#introduction","title":"Introduction","text":"<p>Un tenseur est un \"tableau multidimensionnel\". L'ordre d'un tenseur est son nombre de dimensions :</p> <ul> <li>un tenseur d'ordre 1 est un vecteur,</li> <li>un tenseur d'ordre 2 est une matrice,</li> <li>de fa\u00e7on g\u00e9n\u00e9rale, un tenseur d'ordre \\(N\\) est un \u00e9l\u00e9ment du produit tensoriel de \\(N\\) espaces vectoriels.</li> </ul> <p>Pour un tenseur \\(\\mathfrak{X}\\) d'ordre \\(N\\), ie \\(\\mathfrak{X} \\in \\mathbb{R}^{I_{1} \\times \\cdots \\times I_{N}}\\), l'axe \\(k\\) du tenseur correspond</p>"},{"location":"az104/az-ad/","title":"AZ-104: Microsoft Azure Administrator - Managing Azure Active Directory","text":"<p>On se concentre ici sur le concept d'identit\u00e9. Comment Azure Active Directory s\u00e9curise votre identit\u00e9. Comment les utilisateurs et les groupes sont impl\u00e9ment\u00e9s dans Azure AD.</p>"},{"location":"az104/az-ad/#introduction-a-azure-ad","title":"Introduction \u00e0 Azure AD","text":"<p>Azure AD est le service de gestion des identit\u00e9s et des r\u00e9pertoires bas\u00e9 sur le cloud qui permet l'acc\u00e8s aux services Azure et \u00e0 d'autres solutions SaaS comme Microsoft 365, DropBox, Concur, Salesforce, etc.</p> <p>Il offre \u00e9galement des options de self-service, notamment la r\u00e9initialisation des mots de passe, l'authentification, la gestion des appareils, les identit\u00e9s hybrides et l'authentification unique (SSO).</p>"},{"location":"az104/az-ad/#concepts","title":"Concepts","text":""},{"location":"az104/az-ad/#identite","title":"Identit\u00e9","text":"<p>Un objet qui peut \u00eatre authentifi\u00e9 est consid\u00e9r\u00e9 comme une identit\u00e9. Ca peut \u00eatre :</p> <ul> <li>un utilisateur,</li> <li>un groupe,</li> <li>une identit\u00e9 manag\u00e9e (corresponf \u00e0 une identit\u00e9 d'une ressource Azure),</li> <li>un service principal.</li> </ul>"},{"location":"az104/az-ad/#compte","title":"Compte","text":"<p>Lorsque nous associons des attributs \u00e0 une identit\u00e9, cela devient un compte.</p>  <p>Exemple</p> <ul> <li> <p>Un utilisateur aura plusieurs attributs comme sa localisation, son d\u00e9partement, son manager, son num\u00e9ro de t\u00e9l\u00e9phones, etc. On obtient un compte d'utilisateur.</p> </li> <li> <p>Un groupe aura comme attributs ses membres, une description, une adresse email. On obtient un compte de groupe.</p> </li> </ul>"},{"location":"az104/az-ad/#compte-azure-ad","title":"Compte Azure AD","text":"<p>Tout compte qui est cr\u00e9\u00e9 dans Azure AD ou par un autre service cloud de Microsoft est connu comme un compte Azure AD.</p>"},{"location":"az104/az-ad/#azure-ad-tenant-ou-directory","title":"Azure AD Tenant ou directory","text":"<p>C'est un instance d\u00e9di\u00e9e de Azure AD qui est cr\u00e9\u00e9e durant la cr\u00e9ation de n'importe quelle souscription \u00e0 un service Microsoft cloud. Tenant et Directory siginifient la m\u00eame chose et sont interchangeables.</p> <p>C'est via ce Tenant que l'on peut g\u00e9rer les utilisateurs, groupes attenants \u00e0 la souscription.</p> <p>Une organisation peut avoir un voire plusieurs Azure AD tenants suivant ses besoins.</p>"},{"location":"az104/az-ad/#azure-ad-vs-active-directory-domain-services-adds","title":"Azure AD vs Active Directory Domain Services (ADDS)","text":"<p>Attention</p> <p>Ce sont deux services compl\u00e8tement diff\u00e9rents</p>     Azure AD ADDS     Requ\u00eatage via HTTP/HTTPS Requ\u00eatage via LDAP   Les prot\u00f4coles utilis\u00e9s pour l'authentification incluent SAML, WS-Federation, OpenID connect. OAuth est utilis\u00e9 pour l'authorisation. Kerberos est utilis\u00e9 pour l'authentification dans ADDS   La f\u00e9d\u00e9ration peut \u00eatre mise en place avec des fournisseurs tiers comme Facebook. La f\u00e9d\u00e9ration est possible uniquement avec d'autres domaines. Les services tiers en sont pas support\u00e9s.   Azure AD est une offre manag\u00e9e. ADDS tourne sur une VM ou un serverus physique."},{"location":"az104/az-ad/#les-differentes-editions-dazure-ad","title":"Les diff\u00e9rentes \u00e9ditions d'Azure AD","text":"<p>On a 4 versions diff\u00e9rentes d'azure AD :</p> <ul> <li>Premium P2,</li> <li>Premium P1,</li> <li>M365 Apps,</li> <li>Free.</li> </ul> <p>Premium P2 poss\u00e8de le plus grand nombre de features, tandis que l'\u00e9dition free en a le moins.</p>     No object directory limit SSO &amp; Core IAM B2B collaboration O365 Identity and access Hybrid identity Conditional access Identity protection Identity governance     Premium P2           Premium P1           M365 Apps           Free Limit\u00e9 \u00e0 50 000 objets"},{"location":"az104/az-ad/#les-comptes-dutilisateurs","title":"Les comptes d'utilisateurs","text":"<p>Les comptes utilisateurs sont utilis\u00e9s pour l'authentification et l'authorisation. Tous les utilisateurs doivent avoir un compte.</p> <p>Chaque compte peux avoir plusieurs propri\u00e9t\u00e9s facultatives, comme l'adresse, le d\u00e9partement (dans l'entreprise, l'organisation).</p> <p>Il est possble d'obtenir la liste compl\u00e8te des utilisateurs via le chemin Azure Active Directory &gt; Users &gt; All Users (pourvu que votre compte ait le droit d'y acc\u00e9der).</p> <p>Il est possible d'appliquer des op\u00e9rations en masse sur les utilisateurs.</p> <p>Il existe 3 types de comptes diff\u00e9rents :</p> <ol> <li>Les identit\u00e9s cloud. Ce sont des utilisateurs qui existent uniquement dans l'AD Azure, peuent aussi provenir d'un AD Azure externe.</li> <li>Les comptes invit\u00e9s. Ce sont des utilisateurs qui existent en dehors de l'Ad Azure et qui ont \u00e9t\u00e9 invit\u00e9s \u00e0 le rejoindre, par exemple des comptes Microsoft, Lives, etc.</li> <li>Des comptes synchronis\u00e9s. Ce sont des comptes qui proviennent d'un AD Windows onprem, ils sont impossible \u00e0 cr\u00e9er et ne peuvent \u00eatre que synchronis\u00e9s.</li> </ol> <p>Si un utilisateur est supprim\u00e9 de l'AD Azure, il sera gard\u00e9 en m\u00e9moire pendant 30 jours.</p>"},{"location":"az104/az-ad/#operations-on-masse-bulk-operations","title":"Op\u00e9rations on masse (bulk operations)","text":"<p>Plut\u00f4t que d'inviter, supprimer ou cr\u00e9er des utilisateurs un par un, il est possible de faire ces m\u00eames op\u00e9rations en masse via la commande Bulk operations, qui permet de t\u00e9l\u00e9charger un csv pour :</p> <ol> <li>cr\u00e9er des utilisateurs en masse,</li> <li>inviter des utilisateurs en masse,</li> <li>supprimer des des utilisateurs en masse.</li> </ol>"},{"location":"az104/az-ad/#les-comptes-de-groupes","title":"Les comptes de groupes","text":"<p>Tout comme pour les utilisateurs, il est possible de cr\u00e9er des groupes. Il en existe deux types :</p> <ol> <li>Les groupes de s\u00e9curit\u00e9, qui permettent des g\u00e9rer les autorisations d'acc\u00e8s.</li> <li>Les groupes M365, qui permettent la collaboration et l'acc\u00e8s aux ressources de Microsoft 365.</li> </ol> <p>Types d'assignations \u00e0 un groupe :</p> <ol> <li>Les utilisateurs peuvent \u00eatres assign\u00e9s (<code>Assigned</code>) par un administrateur \u00e0 un groupe et ils ne peuvent pas quitter per eux m\u00eames, seul l'administrateur peut les r\u00e9voquer.</li> <li>On peut avoir des assignations dynamiques (<code>Dynamic user</code>) o\u00f9 l'assignation se fait via attributs, l'AD assignera ou supprimera un utilisateur de fa\u00e7on dynamique d'un groupe suivant ses attributs.</li> <li>On peut avoir des assignations dynamiques pour les devices (<code>Dynamic device</code>), uniquement pour les groupes de s\u00e9curit\u00e9, les devices seront assign\u00e9s ou supprim\u00e9s de fa\u00e7on dynamique en fonction de leur OS, de leur version, etc.</li> </ol>"},{"location":"az104/az-ad/#azure-ad-join","title":"Azure AD Join","text":"<p>Azure AD join permet de g\u00e9rer les diff\u00e9rents devices pour \u00eatre sur qu'ils suivent les r\u00e8gles standards de s\u00e9curit\u00e9 \u00e9dit\u00e9es dans l'entreprise.</p> <p>Azure AD join permet aussi d'avoir acc\u00e8s \u00e0 ces diff\u00e9rents services.</p> <ol> <li>Single Sign-On : autorise le SSO pour vos applications, services, SaaS solutions.</li> <li>Acc\u00e8s \u00e0 Microsoft Store for Business : permet de publier vos applications internes sur le store  pour un usage interne.</li> <li>Roaming d'entreprise : pour synchroniser vos param\u00e8tres et configurations entre vos diff\u00e9rents devices.</li> <li>Windows Hello : support pour windows Hello, pour de la reconnaissance biom\u00e9trique.</li> <li>Gestion des devices : pour g\u00e9rer et si n\u00e9cessaire restreindre l'acc\u00e8s aux applications.</li> <li>Acc\u00e8s onprem :acc\u00e8s aux ressources et applications onprem.</li> </ol>"},{"location":"az104/az-ad/#sspr-self-service-password-reset","title":"SSPR : Self Service Password Reset","text":"<p>Permet \u00e0 l'utilisateur de modifier son mot de passe sans passer par l'IT, plusieurs m\u00e9thodes peuvent \u00eatres propos\u00e9es (via mail, sms, mobile app code  etc.). Il est possible d'en configurer 2 maximums.</p> <p>C'est une feature qui n'est accessible qu'avec une licence Premium P2. Le SSPR peut \u00eatre mis en place au niveau des groupes directement. il est activ\u00e9 par d\u00e9faut pour les comptes admins.</p>"},{"location":"az104/az-ad/#environnements-multi-tenant","title":"Environnements multi tenant","text":""},{"location":"az104/sub_and_gov/","title":"Souscription et Gouvernance","text":""},{"location":"az104/sub_and_gov/#gerer-les-souscriptions","title":"G\u00e9rer les souscriptions","text":"<p>Pour pouvoir utiliser Azure une souscription est n\u00e9cessaire. Les ressources d\u00e9ploy\u00e9es sur votre compte Azure sont donc li\u00e9es \u00e0 cette souscription.</p> <p>Les souscriptions permettent aussi de configurer les environnements diff\u00e9remmment. Ainsi, on peut par exemple imaginer une souscription de d\u00e9veloppment, de production, de tests, chacune ayant des ressources et des param\u00e8tres diff\u00e9rents.</p> <p>Chaque souscription poss\u00e8de une id unique, r\u00e9f\u00e9renc\u00e9e sous le nom <code>id</code> lorsque l'on utilise la commande <code>az account show</code> de l'az-cli.</p> <p><pre><code>\u276f az account show\n{\n  \"environmentName\": \"AzureCloud\",\n  \"homeTenantId\": \"xxx-6490-xxxx-a448-xxxx\",\n  \"id\": \"xxxxxx-0cde-xxxxx-9162-xxxxxx\",\n  \"isDefault\": true,\n  \"managedByTenants\": [],\n  \"name\": \"Abonnement Azure 1\",\n  \"state\": \"Enabled\",\n  \"tenantId\": \"xxxxx-6490-xxx-a448-xxx\",\n  \"user\": {\n    \"name\": \"john.doe@me\",\n    \"type\": \"user\"\n  }\n}\n</code></pre> Un compte peut avoir plusieurs souscription, et toute identit\u00e9 provenant de l'Azure AD ou d'un service cloud de Microsoft peut cr\u00e9er une souscription.</p> <p>Les souscriptions peuvent aussi air comme p\u00e9rim\u00e8tre pour la gestion des acc\u00e8s.</p> <p>il existe 4 types de souscriptions diff\u00e9rentes.</p> <ol> <li>Enterprise Agreements. Recommand\u00e9 pour les organisations de plus de 500 utilisateurs ou devices, offre les services cloud et les licences softwares \u00e0 un prix r\u00e9duit.</li> <li>Pay-as-you-go. Id\u00e9al pour les petites organisations ou les individus, seuls les services utilis\u00e9s sont pay\u00e9s au fur et \u00e0 mesure.</li> <li>Cloud Solution Provider. Obtenu via Microsoft Partners, id\u00e9al pour les organisations petites \u00e0 medium. La facturation est g\u00e9r\u00e9e par le partenaire.</li> <li>Free Trial. 200$ de cr\u00e9dits sur 30 jours et acc\u00e8s limit\u00e9 gratuit pendant 12 mois.</li> <li>Azure for students. Les \u00e9tudiants sont \u00e9ligibles \u00e0 un cr\u00e9dit de 100$ sur 12 mois apr\u00e8s v\u00e9rification.</li> <li>Visual Studio. Souscription g\u00e9r\u00e9e par cr\u00e9dit \u00e0 toute pesonne ayant souscrit \u00e0 Visual Studio Professionnal ou Enterprise.</li> </ol>"},{"location":"az104/sub_and_gov/#comprendre-la-hierarchie","title":"Comprendre la hi\u00e9rarchie","text":"<ul> <li>Les <code>management groups</code> offrent un niveau de vision au dessus des <code>resource groups</code>, ce qui permet de grouper ces derniers ensembles.</li> <li>Un <code>management group</code> racine est cr\u00e9\u00e9 par d\u00e9faut, et l'on peut avoir jusque 6 niveaux de <code>management group</code> en dehors du groupe racine.</li> <li>Chaque souscription peut contenir un ou plusieurs <code>resource groups</code> permettant de grouper logiquement les ressources telles que les machines virtuelles, les base de donn\u00e9es, etc.</li> <li>Cette hi\u00e9rarchie permet d'impl\u00e9menter les politiques d'acc\u00e8s, de g\u00e9rer les co\u00fbts, etc.</li> </ul> \\[ \\text{Resource} \\subset \\text {Resource Group} \\subset \\text {Subscription} \\subset \\text {Management Group} \\]"},{"location":"az104/sub_and_gov/#travailler-avec-le-mode-rbac","title":"Travailler avec le mode RBAC","text":"<p>RBAC : Role Based Access Control</p> <p>Permet aux administrateurs des donner l'acc\u00e8s aux ressources Azure et de s\u00e9parer les responsabilit\u00e9s dans l'\u00e9quipe.</p> <ul> <li>Qui : N'importe quelle identit\u00e9 demandant un acc\u00e8s. Ca peut \u00eatre un utilisateur, un groupe, un service principal ou une identit\u00e9 manag\u00e9e.</li> <li>Quoi : D\u00e9finition du r\u00f4le, ensemble d'op\u00e9rations que l'identit\u00e9 pourra effectuer. Ecrit au format JSON.</li> <li>O\u00f9 : D\u00e9finir les limites de l'acc\u00e8s.</li> </ul> <p>Ces 3 points d\u00e9finissent un r\u00f4le, qui peut alors \u00eatre assign\u00e9e \u00e0 l'identit\u00e9.</p> <p>On peut avoir au maximum jusque 2000 r\u00f4les par souscription.</p> <p>Principe du moindre privil\u00e8ge.</p> <p>Il existe deux types de r\u00f4les :</p> <ul> <li>les r\u00f4les \"BuiltIn\", d\u00e9finis par Azure lui m\u00eame,</li> <li>les r\u00f4les \"Custom\", d\u00e9finis par l'administrateur.</li> </ul>  <p>Les r\u00f4les et l'az cli</p> <p>Pour avoir acc\u00e8s \u00e0 l'ensemble des r\u00f4les d\u00e9finis par Azure, on peut utiliser la commande suivante via l'az cli.</p> <pre><code>az role definition list --query \"[].{name:name, roleType:roleType, roleName:roleName}\" --output tsv\n</code></pre> <p>Pour avoir acc\u00e8s \u00e0 tous les r\u00f4les attenants \u00e0 AzureML, on peut taper la commande suivante.</p> <pre><code>$ az role definition list --query \"[].{name:name, roleType:roleType, roleName:roleName}\" --output tsv | grep AzureML\n\n635dd51f-9968-44d3-b7fb-6d9a6bd613ae    BuiltInRole AzureML Metrics Writer (preview)\nf6c7c914-8db3-469d-8ca1-694a8f32e121    BuiltInRole AzureML Data Scientist\ne503ece1-11d0-4e8e-8e2c-7a6c3bf38815    BuiltInRole AzureML Compute Operator\n1823dd4f-9b8c-4ab6-ab4e-7397a3684615    BuiltInRole AzureML Registry User\n</code></pre>   <p>Contributor role au format JSON</p> <pre><code>{\n  \"assignableScopes\": [\n  \"/\"\n  ],\n  \"description\": \"Grants full access to manage all resources, but does not allow you to assign roles in Azure RBAC, manage assignments in Azure Blueprints, or share image galleries.\",\n  \"id\": \"/subscriptions/{subscriptionId}/providers/Microsoft.Authorization/roleDefinitions/b24988ac-6180-42a0-ab88-20f7382dd24c\",\n  \"name\": \"b24988ac-6180-42a0-ab88-20f7382dd24c\",\n  \"permissions\": [\n    {\n      \"actions\": [\n        \"*\"\n      ],\n      \"notActions\": [\n        \"Microsoft.Authorization/*/Delete\",\n        \"Microsoft.Authorization/*/Write\",\n        \"Microsoft.Authorization/elevateAccess/Action\",\n        \"Microsoft.Blueprint/blueprintAssignments/write\",\n        \"Microsoft.Blueprint/blueprintAssignments/delete\",\n        \"Microsoft.Compute/galleries/share/action\"\n      ],\n      \"dataActions\": [],\n      \"notDataActions\": []\n    }\n  ],\n  \"roleName\": \"Contributor\",\n  \"roleType\": \"BuiltInRole\",\n  \"type\": \"Microsoft.Authorization/roleDefinitions\"\n}\n</code></pre>"},{"location":"az104/sub_and_gov/#scope","title":"Scope","text":"<p>Il est important de savoir \u00e0 quel niveau assigner le r\u00f4le, pour rappel on a 4 niveau d'assignations possibles.</p> \\[ \\text{Resource} \\subset \\text {Resource Group} \\subset \\text {Subscription} \\subset \\text {Management Group} \\] <p>Si un r\u00f4le est affect\u00e9 \u00e0 un certain niveaux, les objets sous ce niveau lui appartenant h\u00e9riteront de ce r\u00f4le.</p>  <p>Exemple</p> <p>Si le r\u00f4le <code>AzureML Data Scientist</code> est assign\u00e9e au niveau de la souscription \\(A\\), tous les <code>resource groups</code> \\(RG_{A}\\) de la souscription \\(A\\) h\u00e9riteront de ce r\u00f4le et de ses droits.</p>"},{"location":"az104/sub_and_gov/#azure-rbac-vs-azure-ad-roles","title":"Azure RBAC vs Azure AD roles","text":"Azure RBAC Azure AD role     Use Utilis\u00e9 pour g\u00e9rer les acc\u00e8s aux ressources Azure Utiliser pour g\u00e9rer les options de l'AD Azure   Scope Management Group, Subscription, Resource Group, Resource Azure AD tenant   Assignment method Portail Azure, Azure PowerShell, Azure CLI, ARM Templates, REST API Portail Azure, M365 Admin Portail, Microsoft Graph API, Azure AD, Graph PS Module   Example Owner, Contributor, Reader, User Access Admin, AzureML Data Scientist, etc. Global Administrator, Billing Administrator, Global Reader, etc."},{"location":"az104/sub_and_gov/#built-in-roles-et-custom-roles","title":"Built-in roles et Custom Roles","text":""},{"location":"az104/sub_and_gov/#built-in","title":"Built-in","text":"<p>Les r\u00f4les \"built-in\", d\u00e9finis par Azure, peuvent \u00eatre assign\u00e9s \u00e0 un utilisateur, un groupe, un service principal, ou une identit\u00e9 manag\u00e9e. Pour voir la liste compl\u00e8te, on peut utiliser comme citer pr\u00e9c\u00e9demment la commande suivante <code>az role definition list</code>.</p> <p>Les 4 r\u00f4les principaux, et les plus utilis\u00e9s, sont les suivants :</p> <ul> <li>Owner : Acc\u00e8s complet \u00e0 toutes les ressources et peut d\u00e9l\u00e9geur l'acc\u00e8s de ces derni\u00e8res \u00e0 d'autres utilisateurs.</li> <li>Contributor : Peut cr\u00e9er et g\u00e9rer tous les types de ressources, mais ne peut pas en donner l'acc\u00e8s \u00e0 d'autres personnes.</li> <li>Reader : Acc\u00e8s en lecture \u00e0 toutes les ressources, mais ne peut pas en modifier le contenu.</li> <li>User Access Administrator : Peut g\u00e9rer l'acc\u00e8s aux ressources des autres utilisateurs.</li> </ul> <p>Attribuer des r\u00f4les Azure \u00e0 l\u2019aide d\u2019Azure CLI</p>"},{"location":"az104/sub_and_gov/#custom","title":"Custom","text":"<p>Si les r\u00f4les de base ne sont pas suffisants, Azure permet de cr\u00e9er ses propres r\u00f4les. Ils peuvent \u00eatres cr\u00e9\u00e9s via le Portail Azure, Azure PowerShell, Azure CLI, REST API.</p> <p>Chaque instance d'Azure AD peut avoir jusque 5000 r\u00f4les custom.</p> <p>Ils peuvent \u00eatre assign\u00e9s aux utilisateurs, groups, service principal et identit\u00e9s manag\u00e9es \u00e0 n'importe quel niveau, comme les r\u00f4les par d\u00e9faut.</p> <p>Tutorial: Create an Azure custom role using Azure CLI</p>"},{"location":"az104/sub_and_gov/#les-tags-azure","title":"Les tags Azure","text":"<p>Il est possible d'associer \u00e0 chaque ressource des tags. Un tag est une paire cl\u00e9-valeu. Avec un tag il est possible :</p> <ul> <li>d'ajouter des metadonn\u00e9es \u00e0 nos souscriptions, resource group, ressources,</li> <li>de grouper ou filtrer logiquement les ressources pour des besoins de gestion,</li> <li>de les utiliser pour g\u00e9rer l'utilisation d'Azure et la facturation : les tags ajout\u00e9s aux ressources sont propag\u00e9s au syst\u00e8me de facturation Azure.</li> </ul> <p>La cl\u00e9 du tag est limit\u00e9e \u00e0 512 caract\u00e8res, sa valeur est limit\u00e9e \u00e0 256 caract\u00e8res, chaque ressource peut se voir assigner maximum 50 tags.</p> <p>az tag</p>"},{"location":"az104/sub_and_gov/#verrouillage-des-ressources","title":"Verrouillage des ressources","text":"<p>Il est possible de verrouiller les ressources, pour les prot\u00e9ger d'un changement accidentel ou d'une suppression.</p> <p>Les verrous peuvent \u00eatre appliqu\u00e9s au niveau de la souscription, du resource groupe, ou de la ressource. Pour un objet verrouill\u00e9, les d\u00e9pendances de plus bas niveau de ce dernier en h\u00e9ritetont.</p> <p>Il existe de verrous :</p> <ul> <li>verrou lecture-seule, qui emp\u00eache la modification de la resource.</li> <li>verrour de suppression, qui emp\u00eache la suppression de la ressource.</li> </ul> <p>az lock</p>"},{"location":"az104/sub_and_gov/#analyse-des-couts","title":"Analyse des co\u00fbts","text":""},{"location":"az104/sub_and_gov/#politiques-azure","title":"Politiques Azure","text":""},{"location":"azure_ml/annex1/","title":"Annex : Traefik and Azure VM","text":"<p>https://kumar-allamraju.medium.com/using-traefik-as-a-layer-7-ingress-controller-in-azure-kubernetes-service-2997eb29228b</p> <p>Using Traefik as a Layer 7 Ingress Controller in Azure Kubernetes Service</p> <p>Traefik is the leading open source reverse proxy and load balancer for HTTP/HTTPS and TCP-based applications that makes deploying micro services very easy.</p> <p>Traefik integrates with your existing infrastructure components (Docker, Kubernetes, AKS, EKS, GKE etc..) and configures itself automatically and dynamically. Pointing Traefik at your orchestrator (e.g. AKS) should be the only configuration step we need to do.</p> <p>In this article I plan to talk about how to integrate traefik with AKS.</p> <p>Consider a scenario where you have deployed a bunch of micro services in your Azure Kubernetes cluster. Now you want users to access these micro services, from public internet. Traditional reverse-proxies like NGINX ingress controller requires you to configure each route that will connect paths and subdomains to each micro service. In an environment where you add, remove, kill, upgrade, or scale your services many times a day, the task of keeping the routes up to date becomes tedious.</p> <p>Traefik comes to the rescue and simplifies the networking complexity while designing, deploying and running micro services. Run Traefik and let it do the work for you! (But if you\u2019d rather configure some of your routes manually, Traefik supports that too!)</p>"},{"location":"azure_ml/annex1/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>An Azure subscription. If you don\u2019t have one, sign up here for free</li> <li>Install az cli</li> <li>Run <code>az login</code> and authenticate to your Azure subscription</li> <li>Install <code>kubectl</code></li> </ul>"},{"location":"azure_ml/annex1/#steps-to-configure-traefik-in-aks-cluster","title":"Steps to configure Traefik in AKS Cluster","text":"<ol> <li>Create a resource group</li> </ol> <p><code>az group create -l eastus -n aksRG</code></p> <ol> <li>Create an AKS cluster</li> </ol> <p><code>az aks create --resource-group aksRG --name myAKS -l eastus --node-count 2</code></p> <ol> <li>Get the AKS credentials</li> </ol> <p><code>az aks get-credentials -n myAKS -g aksRG</code></p> <ol> <li>Get the AKS nodes</li> </ol> <p><code>kubectl get nodes</code></p> <ol> <li>By default AKS cluster is enabled with Role Based Access Control (RBAC) to allow fine-grained control of Kubernetes resources and API. So we need to authorize Traefik to use the Kubernetes API. There are two ways to set up the proper permission: via namespace-specific RoleBindings or a single, global ClusterRoleBinding. Refer to this article to understand RoleBindings or ClusterRoleBinding. For the sake of simplicity I\u2019m using ClusterRoleBinding</li> </ol> <pre><code>---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: traefik-ingress-controller\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - services\n      - endpoints\n      - secrets\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - extensions\n    resources:\n      - ingresses\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n    - extensions\n    resources:\n    - ingresses/status\n    verbs:\n    - update\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: traefik-ingress-controller\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: traefik-ingress-controller\nsubjects:\n- kind: ServiceAccount\n  name: traefik-ingress-controller\n  namespace: kube-system\n</code></pre> <ol> <li>Apply the same to your AKS cluster</li> </ol> <p><code>kubectl apply -f traefik-rbac.yaml</code></p> <ol> <li> <p>We can deploy Traefik via Helm charts or via Deployment/DaemonSet. I have used the latter approach to setup Traefik in my AKS cluster. It is possible to use Traefik with a Deployment or a DaemonSet object, whereas both options have their own pros and cons: In this article, I will be using DaemonSet and it looks no different from Deployment .In Kubernetes, we will use a</p> </li> <li> <p>Deployment/DaemonSet to deploy a Pod,</p> </li> <li>Service \u2014 to expose the service,</li> <li>Ingress \u2014 to allow the access from external world</li> </ol> <p><pre><code>---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: traefik-ingress-controller\n  namespace: kube-system\n---\nkind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: traefik-ingress-controller\n  namespace: kube-system\n  labels:\n    k8s-app: traefik-ingress-lb\nspec:\n  selector:\n    matchLabels:\n      k8s-app: traefik-ingress-lb\n      name: traefik-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: traefik-ingress-lb\n        name: traefik-ingress-lb\n    spec:\n      serviceAccountName: traefik-ingress-controller\n      terminationGracePeriodSeconds: 60\n      containers:\n      - image: traefik:v1.7\n        name: traefik-ingress-lb\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 80\n        - name: admin\n          containerPort: 8080\n          hostPort: 8080\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        args:\n        - --api\n        - --kubernetes\n        - --logLevel=INFO\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: traefik-ingress-service\n  namespace: kube-system\nspec: type: LoadBalancer\n selector:\n    k8s-app: traefik-ingress-lb\n  ports:\n    - protocol: TCP\n      port: 80\n      name: web\n    - protocol: TCP\n      port: 8080\n      name: admin\n</code></pre> 8. Deploy the Traefik DaemonSet and Service to your AKS cluster</p> <p><code>kubectl apply -f traefik-ds.yaml</code></p> <ol> <li> <p>For simplicity sake, we can use Minikube instance but I have used Azure App Service domains feature to quickly setup a custom domain and added an \u201cA\u201d record that mapped the load balancer public IP to this custom domain</p> </li> <li> <p>Go to Azure Portal</p> </li> <li>Enter \u201cApp service domain\u201d in the search box</li> <li> <p>Click on + Add</p> </li> <li> <p>It takes 5 minutes to provision your custom domain. After your custom domain is created, Click on + Manage DNS Records</p> </li> </ol> <p>Note: Creating a custom domain is not a free service</p> <ul> <li>Click on + Record Set</li> <li> <p>Name: *, Type: A, IP address: public IP of your Load Balancer that was created above.</p> </li> <li> <p>The following code will allow us to access Traefik dashboard via your custom domain name.</p> </li> </ul> <p>Note: kubernetes.io/ingress.class: traefik \u2014 this allows us to use traefik as an Ingress controller.</p> <p><pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\nname: traefik-web-ui\nnamespace: kube-system\nannotations:\nkubernetes.io/ingress.class: traefik\nspec:\n  rules:\n  - host: www.custom-domain.com\n  http:\n    paths:\n    - path: /\n    backend:\n      serviceName: traefik-web-ui\n      servicePort: web\n</code></pre> 11. Let\u2019s check the pods and services</p> <p><pre><code>kubectl get all -n kube-system | grep traefik\npod/traefik-ingress-controller-pngvm       1/1     Running   0          15h\npod/traefik-ingress-controller-q6ctg       1/1     Running   0          15h\nservice/traefik-ingress-service          LoadBalancer   10.0.39.252    x.x.x.x   8\n0:30879/TCP,8080:31163/TCP   15h\nservice/traefik-web-ui                   ClusterIP      10.0.138.134   &lt;none&gt;          8\n0/TCP                        16h\ndaemonset.apps/traefik-ingress-controller   2         2         2       2            2\n         &lt;none&gt;                                                 15h\n</code></pre> 12. Point your browser to http://{custom-domain}/dashboard/ to access Traefik\u2019s dashboard.</p> <p>Note: You should enable https to securely access your dashboard</p>"},{"location":"azure_ml/annex1/#frontend-types-in-traefik","title":"Frontend Types in Traefik","text":"<p>Traefik supports name based routing and path based routing Name Base Routing</p> <p>To demonstrate this feature, I have taken the example from containous website and this works flawlessly in AKS cluster</p> <p><pre><code>- Create a Deployment\nkubectl apply -f https://raw.githubusercontent.com/containous/traefik/v1.7/examples/k8s/cheese-deployments.yaml\n\n- Create a Service\nkubectl apply -f https://raw.githubusercontent.com/containous/traefik/v1.7/examples/k8s/cheese-services.yaml\n\n- Create an Ingress service\nkubectl apply -f https://raw.githubusercontent.com/containous/traefik/v1.7/examples/k8s/cheese-ingress.yaml\n</code></pre> Make sure to replace the host name with your custom domain or host name.</p> <p>Now visit the traefik dashboard and you should see a frontend for each host. Along with a backend listing for each service with a server set up for each pod.</p> <p>You should now be able to visit the websites as http://stilton.{custom-domain.com}/, http://cheddar.{custom-domain.com}/ or http://{custom-domain.com}/wensleydale/</p>"},{"location":"azure_ml/annex1/#path-bath-routing","title":"Path Bath Routing","text":"<p>This routing rule is helpful if you want to host all your services under one domain. All we have to do is specify the path instead of the domain name. You will also notice in the yaml file we are configuring Traefik to strip the prefix from the url path with traefik.frontend.rule.type annotation</p> <p><code>kubectl apply -f https://raw.githubusercontent.com/containous/traefik/v1.7/examples/k8s/cheeses-ingress.yaml</code></p> <p>We should now visit the website with a single domain name</p> <p>i.e. http://{custom-domain.com}/stilton, http://{custom-domain.com}/cheddar, http://{custom-domain.com}/wensleydale</p>"},{"location":"azure_ml/annex1/#conclusion","title":"Conclusion","text":"<p>Traefik is an open-source Edge Router that makes publishing your services a fun and easy experience. It receives requests on behalf of your system and finds out which components are responsible for handling them.</p> <p>What sets Traefik apart, besides its many features, is that it automatically discovers the right configuration for your services. The magic happens when Traefik inspects your infrastructure, where it finds relevant information and discovers which service serves which request.</p> <p>Traefik is natively compliant with every major cluster technology, such as Kubernetes, Docker, AKS, AWS, Mesos, Marathon, and the list goes on; and can handle many at the same time. (It even works for legacy software running on bare metal.) With Traefik, you spend time developing and deploying new features to your system, not on configuring and maintaining its working state. References</p> <pre><code>https://docs.traefik.io/\nhttps://containo.us/traefik/\nhttps://docs.traefik.io/getting-started/install-traefik/\nhttps://azure.microsoft.com/en-us/services/kubernetes-service/\n</code></pre>"},{"location":"azure_ml/annex2/","title":"Annex : Automatic HTTPS with Azure Container Instances (ACI)","text":"<p>https://itnext.io/automatic-https-with-azure-container-instances-aci-4c4c8b03e8c9</p> <p>Automatic HTTPS with Azure Container Instances (ACI)</p> <p>Let\u2019s assume you want to deploy a simple containerized application or service to the Azure cloud. Additionally, your service needs to be reachable publicly via HTTPS. This technical article shows you how to achieve this goal. Azure Container Instances</p> <p>According to the architecture guide Choosing an Azure compute service you\u2019ve got several options to deploy your containerized service, one of them is Azure Container Instances (ACI):</p> <pre><code>Container Instances: The fastest and simplest way to run a container in Azure, without having to provision any virtual machines and without having to adopt a higher-level service.\n</code></pre> <p>Simple also means that you don\u2019t get all the options and features of a full-blown orchestration solution, such as Azure Kubernetes Service (AKS). ACI provides features like sidecars and persistent volumes. With ACI, however, you have to live with a downtime when upgrading your deployment.</p> <p>And you have to set up TLS manually. There is a guide, Enable TLS with a sidecar container, which tells you how to set up HTTPS with Nginx and a self-signed certificate. Ugh. The guide also mentions Caddy as an alternate TLS provider but doesn\u2019t provide more details. Caddy</p> <pre><code>Caddy 2 is a powerful, enterprise-ready, open source web server with automatic HTTPS written in Go.\n</code></pre> <p>Ok, sounds nice! Automatic HTTPS sounds really intriguing. What does it mean? \u201cCaddy obtains and renews TLS certificates for your sites automatically. It even staples OCSP responses.\u201d Wow! But how is this done?</p> <p>\u201cCaddy serves public DNS names over HTTPS using certificates from a public ACME CA such as Let\u2019s Encrypt\u201d. This means, you just need a public DNS record and Caddy needs to be reachable via ports 80 and 443. Nice! Setup Instructions</p> <p>So let\u2019s combine ACI and Caddy to achieve our goal. I\u2019ll use Terraform to set up the infrastructure in Azure. We\u2019ll start with a new Terraform file and configure it with the Azure Provider (azurerm) and a local value for the Azure region:</p> <pre><code>terraform {\n  required_version = \"&gt;= 0.14, &lt; 0.15\"\n\n  required_providers {\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \"~&gt; 2.0\"\n    }\n  }\n}\n\nprovider \"azurerm\" {\n  features {}\n}\n\nlocals {\n  location = \"West Europe\"\n}\n</code></pre> <p>Next, we are going to define three resources so that we can provide persistent storage for Caddy:</p> <pre><code>resource \"azurerm_resource_group\" \"aci_caddy\" {\n  name     = \"aci_caddy\"\n  location = local.location\n}\n\nresource \"azurerm_storage_account\" \"aci_caddy\" {\n  name                      = \"acicaddy\"\n  resource_group_name       = azurerm_resource_group.aci_caddy.name\n  location                  = azurerm_resource_group.aci_caddy.location\n  account_tier              = \"Standard\"\n  account_replication_type  = \"LRS\"\n  enable_https_traffic_only = true\n}\n\nresource \"azurerm_storage_share\" \"aci_caddy\" {\n  name                 = \"aci-caddy-data\"\n  storage_account_name = azurerm_storage_account.aci_caddy.name\n}\n</code></pre> <p>This is needed so that the certificate from Let\u2019s Encrypt is not lost between deployments. If you deploy frequently and Caddy can\u2019t remember the previous certificate, you will probably run into a rate limit of Let\u2019s Encrypt which means you won\u2019t be able to get any new certificate for your domain for some time.</p> <p>Now we\u2019re ready to define our main resource, the container instance (called container group in Terraform):</p> <pre><code>resource \"azurerm_container_group\" \"aci_caddy\" {\n  resource_group_name = \"aci_caddy\"\n  location            = local.location\n  name                = \"aci_caddy\"\n  os_type             = \"Linux\"\n  dns_name_label      = \"aci-caddy\"\n  ip_address_type     = \"public\"\n\n  container {\n    name   = \"app\"\n    image  = \"nginxinc/nginx-unprivileged\"\n    cpu    = \"0.5\"\n    memory = \"0.5\"\n  }\n\n  container {\n    name   = \"caddy\"\n    image  = \"caddy\"\n    cpu    = \"0.5\"\n    memory = \"0.5\"\n\n    ports {\n      port     = 443\n      protocol = \"TCP\"\n    }\n\n    ports {\n      port     = 80\n      protocol = \"TCP\"\n    }\n\n    volume {\n      name                 = \"aci-caddy-data\"\n      mount_path           = \"/data\"\n      storage_account_name = azurerm_storage_account.aci_caddy.name\n      storage_account_key  = azurerm_storage_account.aci_caddy.primary_access_key\n      share_name           = azurerm_storage_share.aci_caddy.name\n    }\n\n    commands = [\"caddy\", \"reverse-proxy\", \"--from\", \"aci-caddy.westeurope.azurecontainer.io\", \"--to\", \"localhost:8080\"]\n  }\n}\n\noutput \"url\" {\n  value = \"https://${azurerm_container_group.aci_caddy.fqdn}\"\n  description = \"URL\"\n}\n</code></pre> <p>Note that we define two containers. On line 9, we use an Nginx unprivileged image which serves as a surrogate for our real service and listens on port 8080.</p> <p>On line 16, we define another container (sidecar) which contains our Caddy server. As mentioned previously, Caddy needs ports 80 and 443, so we assign those ports. Also, note that we are using a public IP (line 7) and we define a DNS subdomain (line 6).</p> <p>Lines 32\u201338 contain the configuration for the shared volume which reference the storage resources we defined before. Caddy stores its data in the /data directory.</p> <p>Line 40 contains all the magic to start Caddy. We tell it to act as a reverse proxy for our main service, the address to listen to (from parameter), and the forwarding address for our main service (to parameter), which is localhost:8080. That\u2019s it. Caddy can be started with a one-liner and requires almost no configuration! (This is a concept I call zero config which I will treat in a future article.)</p> <p>Finally, we print the address of our new service which should be accessible via HTTPS with a valid certificate from Let\u2019s Encrypt.</p> <p>Let\u2019s log in with the Azure CLI (az), and let\u2019s initialize and apply our new Terraform config:</p> <pre><code>\u276f az login -o none\nThe default web browser has been opened at https://login.microsoftonline.com/common/oauth2/authorize. Please continue the login in the web browser. If no web browser is available or if the web browser fails to open, use device code flow with `az login --use-device-code`.\nYou have logged in. Now let us find all the subscriptions to which you have access...\n\n\u276f terraform init\n\nInitializing the backend...\n\nInitializing provider plugins...\n- Finding hashicorp/azurerm versions matching \"~&gt; 2.0\"...\n- Installing hashicorp/azurerm v2.45.1...\n- Installed hashicorp/azurerm v2.45.1 (signed by HashiCorp)\n\nTerraform has created a lock file .terraform.lock.hcl to record the provider\nselections it made above. Include this file in your version control repository\nso that Terraform can guarantee to make the same selections by default when\nyou run \"terraform init\" in the future.\n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running \"terraform plan\" to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.\n\n\u276f terraform apply\n\nAn execution plan has been generated and is shown below.\nResource actions are indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # azurerm_container_group.aci_caddy will be created\n  (...)\n\n  # azurerm_resource_group.aci_caddy will be created\n  (...)\n\n  # azurerm_storage_account.aci_caddy will be created\n  (...)\n\n  # azurerm_storage_share.aci_caddy will be created\n  (...)\n\nPlan: 4 to add, 0 to change, 0 to destroy.\n\nChanges to Outputs:\n  + url = (known after apply)\n\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n\n  Enter a value: yes\n\n(...)\n\nApply complete! Resources: 4 added, 0 changed, 0 destroyed.\n\nOutputs:\n\nurl = \"https://aci-caddy.westeurope.azurecontainer.io\"\n</code></pre> <p>Nice work! Let\u2019s test our service in a browser by invoking the URL provided in the output. If the page displays \u201cWelcome to nginx!\u201d and the browser doesn\u2019t complain about invalid certificates then we achieved our goal.</p> <p>There are some restrictions you need to be aware of: First, for each service you have to spin up a separate Caddy service. This consumes extra resources. Second, you also have to make sure that your service doesn\u2019t listen on ports 80 and 443 as those are reserved for Caddy. Third, Caddy requires a public IP. Conclusion</p> <p>In this technical guide, I demonstrated how you can overcome one of the shortcomings of ACI when it comes to managing TLS certificates for an HTTPS connection.</p>"},{"location":"azure_ml/intro/","title":"Introduction &amp; r\u00e9f\u00e9rences","text":""},{"location":"azure_ml/intro/#data-validation","title":"Data validation","text":"<ul> <li>How to configure a Validation Result Store in Azure Blob Storage</li> </ul>"},{"location":"azure_ml/intro/#iac","title":"IaC","text":"<ul> <li>Manages a Azure Machine Learning Workspace with Terraform</li> </ul>"},{"location":"azure_ml/intro/#azureml","title":"AzureML","text":"<ul> <li> <p>Azure Machine Learning Cheat Sheet</p> </li> <li> <p>Create &amp; use software environments in Azure Machine Learning</p> </li> <li>Train models with Azure Machine Learning datasets</li> <li>Set up AutoML training with the Azure ML Python SDK v2 (preview)</li> <li>Build AI solutions with Azure Machine Learning</li> <li>Quantized Discrete Parameter Sampling in AzureML HyperDrive</li> <li>Hyperparameter Tuning using HyperDrive</li> <li>Machine Learning in the Cloud using Azure ML Studio</li> <li>Automated Machine Learning</li> <li>What is automated machine learning (AutoML)?</li> <li>Track ML models with MLflow and Azure Machine Learning</li> <li> <p>How to Publish a Pipeline and Invoke the REST endpoint</p> </li> <li> <p>Use automated ML in an Azure Machine Learning pipeline in Python runconfig here !</p> </li> <li>MLOps: Model management, deployment, lineage, and monitoring with Azure Machine Learning</li> <li>Deploying a Machine Learning Model with Azure ML Pipelines</li> </ul>"},{"location":"azure_ml/lesson1/","title":"Introduction au SDK Azure","text":"<p>Attention</p> <p>Un jour, peut \u00eatre, tout sera traduit en fran\u00e7ais. Pour l'instant, il y aura les deux : anglais et fran\u00e7ais, la plupart du texte en anglais provient du nanodegree Machine Learning Engineer with Microsoft Azure Nanodegree Program d'Udacity que j'ai suivi. L'id\u00e9e \u00e9tant ici de consolider et de formaliser ce qu'il y a dedans.</p>"},{"location":"azure_ml/lesson1/#la-base-le-workspace","title":"La base : le Workspace","text":"<p>Le SDK Azure est d\u00e9j\u00e0 install\u00e9 de base dans Azure \u00e9videmment, si vous souhaitez l'installer sur votre pc local, ou dans tout autre environnement autre que Azure, il suffit alors d'installer la librairie suivante.</p> <pre><code>python -m pip install azureml-core==la_version_de_votre_choix\n</code></pre> <p>Pour pouvoir l'utiliser correctement et lancer des commandes vers le cloud Azure depuis le SDK Azure install\u00e9 sur votre environnement local, il doit \u00eatre reli\u00e9 \u00e0 un abonnement Azure, ce qui peut se faire via l'installation de l'<code>Azure cli</code>, nous y reviendrons plus tard.</p>  <p>Remarque</p> <p>Pour l'instant, supposons que nous sommes dans un environnement Azure, par exemple un Notebook Azure qui tourne sur un compute cluster. Et faisons un tour rapide de ce qu'il est possible rapidement de faire avec Azure.</p>  <p>Lorsque que l'on souhaite travailler avec Azure, il est important de d\u00e9finir dans quel <code>Workspace</code> (espace de travail), vous allez travailler. Le <code>Workspace</code> est l'espace de travail contenant vos datasets, vos exp\u00e9rimentations AutoML, vos pipelines, ainsi que les diff\u00e9rentes instances de calculs que vous pouvez utiliser.</p> <p>Pour citer la documentation Azure :</p>  <p>Quote</p> <p>L\u2019espace de travail est la ressource de niveau sup\u00e9rieur pour Azure Machine Learning. Il fournit un emplacement centralis\u00e9 dans lequel exploiter tous les artefacts que vous cr\u00e9ez lorsque vous utilisez Azure Machine Learning. L\u2019espace de travail conserve un historique de toutes les ex\u00e9cutions d\u2019entra\u00eenement, y compris les journaux d\u2019activit\u00e9, les m\u00e9triques, les sorties et un instantan\u00e9 de vos scripts. Vous utilisez ces informations pour d\u00e9terminer quelle ex\u00e9cution d\u2019entra\u00eenement produit le meilleur mod\u00e8le.</p> <p>Une fois que vous disposez d\u2019un mod\u00e8le qui vous convient, inscrivez-le avec l\u2019espace de travail. Ainsi, gr\u00e2ce au mod\u00e8le inscrit et aux scripts de scoring, vous pouvez d\u00e9ployer sur Azure Container Instances, Azure Kubernetes Service ou sur un tableau FPGA (field programmable gate array) comme point de terminaison HTTP bas\u00e9 sur REST.</p>  <p>On le d\u00e9finit de la fa\u00e7on suivante.</p>  <p>Workspace</p> <pre><code>from azureml.core import Workspace\nws = Workspace.from_config()\n</code></pre>  <p>Le <code>from_config()</code> ici pr\u00e9sent d\u00e9termine de quelle source doit \u00eatre obtenue la configuration du <code>Workspace</code> dans lequel vous allez travailler. Si vous \u00eates sur Azure, vous \u00eates surement d\u00e9j\u00e0 en train de travailler dans un <code>Workspace</code> et donc la configuration prise sera celle ambiante, si vous \u00eates en local la configuration peut \u00eatre charg\u00e9e depuis un fichier <code>config.json</code> contenant la configuration de votre abonnement Azure sur lequel vous travaillez.</p> <p>Pour les autres m\u00e9thodes de la classe <code>Workspace</code>, on se r\u00e9f\u00e8re \u00e0 la documentation.</p>"},{"location":"azure_ml/lesson1/#importer-des-donnees","title":"Importer des donn\u00e9es","text":"<p>L'importation des donn\u00e9es dans AzureML peut se faire de deux fa\u00e7ons.</p> <ol> <li>Importer un dataset d\u00e9j\u00e0 pr\u00e9sent de base ou versionn\u00e9 dans Azure pour le r\u00e9utiliser.</li> <li>Importer un jeu de donn\u00e9es depuis l'ext\u00e9rieur, au format <code>.csv</code>, <code>.parquet</code>, compress\u00e9 ou autre.</li> </ol> <p>Concentrons-nous d'abord sur la seconde m\u00e9thode.</p>"},{"location":"azure_ml/lesson1/#importer-un-dataset-de-lexterieur","title":"Importer un dataset de l'ext\u00e9rieur","text":"<p>La classe de base pour les datasets dans Azure s'instancie comme suit.</p>  <p>Dataset</p> <pre><code>from azureml.core.dataset import Dataset\n</code></pre>  <p>il est par exemple possible de r\u00e9cup\u00e9rer des urls vers des datasets compress\u00e9s pour en sortir des dataframes.</p>  <p>Exemple</p> <pre><code>from azureml.core.dataset import Dataset\n\nurl_paths = [\n            'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n            'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n            'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n            'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\n            ]\n\ndataset = Dataset.File.from_files(path=url_paths)\ndf = dataset.to_pandas_dataframe()\n</code></pre>   <p>TODO</p> <p>Comment se fait la distinction entre les liens dans cette dataframe ? l'ajout d'une nouvelle colonne indiquant la provenance ?</p>   <p>TODO</p> <p>Ajouter les notions suppl\u00e9mentaires provenants des notebooks du module 1.</p>"},{"location":"azure_ml/lesson1/#versionner-un-dataset-et-lutiliser","title":"Versionner un dataset et l'utiliser","text":"<p>TODO</p>"},{"location":"azure_ml/lesson1/#definir-une-experimentation","title":"D\u00e9finir une exp\u00e9rimentation","text":"<ul> <li>ressource : Azure Machine Learning Pipelines: Getting Started</li> </ul> <p>L'int\u00e9r\u00eat d'AzureML est \u00e9videmment d'utiliser le cloud pour pouvoir entra\u00eener des mod\u00e8les et it\u00e9rer rapidement. Le principe de base dans AzureML pour faire cela est de d\u00e9finir des exp\u00e9rimentations. En voici un exemple.</p>  <p>Exp\u00e9rimentation</p> <pre><code>from azureml.core import Workspace, Dataset, Datastore\nfrom azureml.core.experiment import Experiment\nfrom azureml.pipeline.core import Pipeline, PipelineData\nfrom azureml.pipeline.steps import PythonScriptStep\n\nws = Workspace.from_config()\ncompute_target = ws.compute_targets[\"STANDARD_NC6\"]\nblob_store = Datastore(ws, \"workspaceblobstore\")\n\nexperiment = Experiment(ws, 'MyExperiment')\n\ninput_data = Dataset.File.from_files(\n    DataPath(datastore, '20newsgroups/20news.pkl'))\ninput_named = input_data.as_named_input('input')\n\noutput_data = PipelineData(\"output_data\", datastore=blob_store)\n\nsteps = [PythonScriptStep(\n    script_name=\"train.py\",\n    arguments=[\"--input\", input_named.as_download(),\n        \"--output\", output_data],\n    inputs=[input_data],\n    outputs=[output_data],\n    compute_target=compute_target,\n    source_directory=\"myfolder\"\n)]\n\npipeline = Pipeline(workspace=ws, steps=steps)\n\npipeline_run = experiment.submit(pipeline)\npipeline_run.wait_for_completion()\n</code></pre>  <p>D\u00e9taillons cet exemple.</p> <ol> <li>Configuration g\u00e9n\u00e9rale de l'exp\u00e9rimentation.</li> </ol> <pre><code>ws = Workspace.from_config()\ncompute_target = ws.compute_targets[\"STANDARD_NC6\"]\nblob_store = Datastore(ws, \"workspaceblobstore\")\n</code></pre> <p>D\u00e9finit le Workspace dans lequel va se d\u00e9rouler l'exp\u00e9rimentation, et \u00e0 ce Workspace est attach\u00e9 une instance de calcul <code>ws.compute_targets[\"STANDARD_NC6\"]</code>, <code>\"STANDARD_NC6\"</code> \u00e9tant l'une des d\u00e9nominations utilis\u00e9es par Azure pour d\u00e9finir ses configurations d'instances. <code>blob_store = Datastore(ws, \"workspaceblobstore\")</code> d\u00e9finit dans quel datastore seront stock\u00e9s les artefacts de l'exp\u00e9rimentation, ici celui par d\u00e9faut associ\u00e9 au Workspace.</p> <ol> <li>Nommage de l'exp\u00e9rimentation.</li> </ol> <p><code>experiment = Experiment(ws, 'MyExperiment')</code></p> <ol> <li> <p>D\u00e9finition des inputs, outputs</p> </li> <li> <p>ressource : documentation sur la classe PipelineData .</p> </li> </ol> <pre><code>input_data = Dataset.File.from_files(\n    DataPath(datastore, '20newsgroups/20news.pkl'))\ninput_named = input_data.as_named_input('input')\n\noutput_data = PipelineData(\"output_data\", datastore=blob_store)\n</code></pre> <ol> <li>D\u00e9finition des \u00e9tapes de l'exp\u00e9rimentation</li> </ol> <p>ressource : documentation sur la classe PythonScriptStep.</p> <pre><code>steps = [PythonScriptStep(\n    script_name=\"train.py\",\n    arguments=[\"--input\", input_named.as_download(),\n        \"--output\", output_data],\n    inputs=[input_data],\n    outputs=[output_data],\n    compute_target=compute_target,\n    source_directory=\"myfolder\"\n)]\n</code></pre> <p>Comme on peut le voir ici, l'objet <code>PythonScriptStep</code> est d\u00e9fini comme \u00e9l\u00e9ment d'une liste, il est donc possible de d\u00e9finir une suite d'\u00e9tapes comme une liste d'objets <code>PythonScriptStep</code> et de les passer apr\u00e8s dans l'argument <code>steps</code> de l'objet <code>Pipeline</code> d\u00e9fini dans le point suivant.</p> <ol> <li>Instanciation du pipeline</li> </ol> <pre><code>pipeline = Pipeline(workspace=ws, steps=steps)\npipeline_run = experiment.submit(pipeline)\npipeline_run.wait_for_completion()\n</code></pre>"},{"location":"azure_ml/lesson1/#listing-past-experiments","title":"Listing Past Experiments","text":"<p>To do this, we need to pass in a workspace object, and then list the collection of trials. This will give us data very similar to the list we've accessed previously using the Designer. Here's the example code we looked at:</p> <pre><code>from azureml.core.experiment import Experiment\n\n# First, we pass in the workspace object `ws` to the `list` method\n# and it returns a Python list of `Experiment` objects.\nlist_experiments = Experiment.list(ws)\n\n# If we print the contents of the variable,\n# we can see the list of all the experiments\n# that have been run in that workspace.\nprint(list_experiments)\n[Experiment(Name: dataset_profile,\n Workspace: Azure-ML-Workspace), Experiment(Name: binary-classification,\n Workspace: Azure-ML-Workspace), Experiment(Name: regression,\n Workspace: Azure-ML-Workspace),]\n</code></pre>"},{"location":"azure_ml/lesson1/#submitting-new-experiments","title":"Submitting New Experiments","text":"<p>To submit a new experiment (such as if we wanted to try a different model type or a different algorithm), we would again pass in a workspace object and then submit the experiment. Here's the example code:</p> <pre><code>from azureml.core.experiment import Experiment\n\nexperiment = Experiment(ws, \"automl_test_experiment\")\nrun = experiment.submit(config=automl_config, show_output=True)\n</code></pre>"},{"location":"azure_ml/lesson1/#optimisation-des-hyperparametres-avec-hyperdrive","title":"Optimisation des hyperparam\u00e8tres avec HyperDrive","text":"<p>Main Steps for Tuning with HyperDrive</p> <ul> <li>Define the parameter search space. This could be a discrete/categorical variable (e.g., apple, banana, pair) or it can be a continuous value (e.g., a time series value).</li> <li>Define the sampling method over the search space. This is a question of the method you want to use to find the values. For example, you can use a random, grid, or Bayesian search strategy.</li> <li>Specify the primary metric to optimize. For example, the Area Under the Curve (AUC) is a common optimization metric.</li> <li>Define an early termination policy. An early termination policy specifies that if you have a certain number of failures, HyperDrive will stop looking for the answer.</li> </ul>  <p>Attention</p> <p>Note that to use HyperDrive, you must have a custom-coded machine learning model. Otherwise, HyperDrive won't know what model to optimize the parameters for!</p>   <p>TODO</p> <p>A comparer avec KerasTuner.</p>"},{"location":"azure_ml/lesson1/#controlling-hyperdrive-with-the-sdk","title":"Controlling HyperDrive with the SDK","text":"<p>You can control HyperDrive with the SDK. Here is an example code.</p> <p><pre><code>from azureml.train.hyperdrive import BayesianParameterSampling\nfrom azureml.train.hyperdrive import uniform, choice\nparam_sampling = BayesianParameterSampling( {\n        \"learning_rate\": uniform(0.05, 0.1),\n        \"batch_size\": choice(16, 32, 64, 128)\n    }\n)\n</code></pre> We also saw that we can specify whether we are tuning a discrete or continuous variable.</p> <p>Discrete example:</p> <p><pre><code>{\n    \"batch_size\": choice(16, 32, 64, 128)\n    \"number_of_hidden_layers\": choice(range(1,5))\n}\n</code></pre> Continuous example:</p> <pre><code>{\n    \"learning_rate\": normal(10, 3),\n    \"keep_probability\": uniform(0.05, 0.1)\n}\n</code></pre> <p>And finally, we will need to find the best model parameters. Here's an example:</p> <pre><code>best_run = hyperdrive_run.get_best_run_by_primary_metric()\nbest_run_metrics = best_run.get_metrics()\nparameter_values = best_run.get_details()['runDefinition']['Arguments']\n\nprint('Best Run Id: ', best_run.id)\nprint('\\n Accuracy:', best_run_metrics['accuracy'])\nprint('\\n learning rate:',parameter_values[3])\nprint('\\n keep probability:',parameter_values[5])\nprint('\\n batch size:',parameter_values[7])\n</code></pre> <p>The ultimate result is that we are able to choose the best tuning and use it in our final machine learning model.</p>"},{"location":"azure_ml/lesson1/#automl-vs-traditionnal-ml","title":"AutoML vs traditionnal ML","text":"<p>TODO</p> <p>A comparer avec AutoKeras.</p>"},{"location":"azure_ml/lesson1/#traditional-ml","title":"Traditional ML","text":"<p>To understand why Automated ML is a useful tool, it helps to first understand some of the challenges we face with traditional ML. These include:</p> <ul> <li>Focus on technical details vs the business problem. The code and technical details can consume large amounts of the available resources, distracting our focus from the business problem we want to use the ML to solve.</li> <li>Lack of automation. With traditional ML, we have to do many things manually, even though they could easily be automated with tools like Azure ML Studio.</li> <li>Too much HiPPO influence. The Highest Paid Person's Opinion (HiPPO) can have an unduly large influence on decisions about the output of the model, even though this decision might be better made automatically.</li> <li>Feature engineering. What are the features that I need to get the best accuracy? What are the columns I should select? This can be a huge task that requires a lot of human effort.</li> <li>Hyperparameter selection. For example, with a clustering model, what number of clusters will give the best results? There can be a lot of trial and error and many false starts.</li> <li>Training and Tuning. What are the different parameters you're using when training your model? What machines and resources should you use? How should you best tune the parameters? In traditional ML, these questions require a human to supervise the process.</li> </ul>"},{"location":"azure_ml/lesson1/#automated-ml","title":"Automated ML","text":"<p>Automated ML can help with all of the above problems. Essentially, AutoML involves the application of DevOps principles to machine learning, in order to automate all aspects of the process. For example, we can automate feature engineering, hyperparameter selection, model training, and tuning. With AutoML, we can:</p> <ul> <li>Create hundreds of models a day</li> <li>Get better model accuracy</li> <li>Deploy models faster</li> </ul> <p>This creates a quicker feedback loop and allows us to bring ideas to market much sooner. Overall, it reduces the time that we have to spend on technical details, allowing for more effort to be put into solving the underlying business problems.</p>"},{"location":"azure_ml/lesson1/#configuring-automl-from-the-sdk","title":"Configuring AutoML from the SDK","text":"<p>We can easily leverage AutoML from the SDK to automate many aspects of our pipeline, including:</p> <ul> <li>Task type</li> <li>Algorithm iterations</li> <li>Accuracy metric to optimize</li> <li>Algorithms to blacklist/whitelist</li> <li>Number of cross-validations</li> <li>Compute targets</li> <li>Training data</li> </ul> <p>To do this, we first use the <code>AutoMLConfig</code> class. In the code example below, you can see that we are creating an automl_config object and setting many of the parameters listed above:</p> <pre><code>from azureml.train.automl import AutoMLConfig\n\nautoml_config = AutoMLConfig(task=\"classification\",\n                             X=your_training_features,\n                             y=your_training_labels,\n                             iterations=30,\n                             iteration_timeout_minutes=5,\n                             primary_metric=\"AUC_weighted\",\n                             n_cross_validations=5\n                            )\n</code></pre>"},{"location":"azure_ml/lesson1/#running-automl-from-the-sdk","title":"Running AutoML from the SDK","text":"<p>Once we have completed our configuration, we can then run it using the SDK. Here's a typical example of what that would look like:</p> <pre><code>from azureml.core.experiment import Experiment\n\nexperiment = Experiment(ws, \"automl_test_experiment\")\nrun = experiment.submit(config=automl_config, show_output=True)\n</code></pre>"},{"location":"azure_ml/lesson1/#automl-example","title":"AutoML Example","text":"<pre><code>from azureml.core import Workspace, Dataset\nfrom azureml.core.experiment import Experiment\nfrom azureml.core.workspace import Workspace\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\nfrom azureml.train.automl import AutoMLConfig\nimport pandas as pd\n\nsubscription_id = '6971f5ac-8af1-446e-8034-05acea24681f'\nresource_group = 'aml-quickstarts-190413'\nworkspace_name = 'quick-starts-ws-190413'\n\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\nws = Workspace.from_config()\n\noutput = {}\noutput['Subscription ID'] = ws.subscription_id\noutput['Workspace'] = ws.name\noutput['Resource Group'] = ws.resource_group\noutput['Location'] = ws.location\noutput['Experiment Name'] = experiment.name\npd.set_option('display.max_colwidth', -1)\noutputDf = pd.DataFrame(data = output, index = [''])\noutputDf.T\n\n# choose a name for experiment\nexperiment_name = 'automl-nba-position'\nexperiment=Experiment(ws, experiment_name)\n\n# Choose a name for your CPU cluster\ncpu_cluster_name = \"auto-ml\"\n\n# Verify that cluster does not exist already\ntry:\n    compute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n    print('Found existing cluster, use it.')\nexcept ComputeTargetException:\n    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n                                                           max_nodes=6)\n    compute_target = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n\ncompute_target.wait_for_completion(show_output=True)\n\ndataset = Dataset.get_by_name(workspace, name='Nba-Dataset')\ndataset.to_pandas_dataframe()\n\nautoml_settings = {\n    \"experiment_timeout_hours\" : 0.3,\n    \"enable_early_stopping\" : True,\n    \"iteration_timeout_minutes\": 5,\n    \"max_concurrent_iterations\": 4,\n    \"max_cores_per_iteration\": -1,\n    \"n_cross_validations\": 2,\n    \"primary_metric\": 'AUC_weighted',\n    \"featurization\": 'auto',\n    \"verbosity\": logging.INFO,\n}\n\nautoml_config = AutoMLConfig(task = 'classification',\n                             debug_log = 'automl_errors.log',\n                             compute_target=compute_target,\n                             experiment_exit_score = 0.9984,\n                             blocked_models = ['KNN','LinearSVM'],\n                             enable_onnx_compatible_models=True,\n                             training_data = dataset,\n                             label_column_name ='POSITION',\n                             **automl_settings\n                            )\n\nremote_run = experiment.submit(automl_config, show_output = False)\n</code></pre>"},{"location":"azure_ml/lesson1_project/","title":"Comparaison entre HyperDrive et AutoML","text":""},{"location":"azure_ml/lesson1_project/#set-up-workspace-parameters","title":"Set up workspace parameters","text":"<pre><code>from azureml.core import Workspace, Experiment\n\nws = Workspace.from_config()\nexp = Experiment(workspace=ws, name=\"udacity-project\")\n\nprint('Workspace name: ' + ws.name,\n      'Azure region: ' + ws.location,\n      'Subscription id: ' + ws.subscription_id,\n      'Resource group: ' + ws.resource_group, sep = '\\n')\n\nrun = exp.start_logging()\n</code></pre>"},{"location":"azure_ml/lesson1_project/#set-up-compute-cluster","title":"Set up Compute cluster","text":"<pre><code>from azureml.core.compute import ComputeTarget, AmlCompute\n\n# TODO: Create compute cluster\n# Use vm_size = \"Standard_D2_V2\" in your provisioning configuration.\n# max_nodes should be no greater than 4.\ncluster_name = \"udacity-project\"\n\ncompute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n                                                            vm_priority='lowpriority',\n                                                            max_nodes=4)\ncpu_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n</code></pre>"},{"location":"azure_ml/lesson1_project/#set-up-hyperdrive-configuration-and-runs","title":"Set up HyperDrive configuration and runs","text":"<pre><code>from azureml.widgets import RunDetails\nfrom azureml.train.sklearn import SKLearn\nfrom azureml.train.hyperdrive.run import PrimaryMetricGoal\nfrom azureml.train.hyperdrive.policy import BanditPolicy\nfrom azureml.train.hyperdrive.sampling import RandomParameterSampling\nfrom azureml.train.hyperdrive.runconfig import HyperDriveConfig\nfrom azureml.train.hyperdrive.parameter_expressions import choice, uniform\nfrom azureml.core import Environment, ScriptRunConfig\nimport os\n\n# Specify parameter sampler\nps = RandomParameterSampling( {\n        \"--C\": uniform(0.1, 1.0),\n        \"--max_iter\": choice(25,50,75,100,125,150,175,200)\n    }\n)\n\n# Specify a Policy\npolicy = BanditPolicy(evaluation_interval=1, slack_factor=0.2, slack_amount=None, delay_evaluation=0)\n\nif \"training\" not in os.listdir():\n    os.mkdir(\"./training\")\n\n# Setup environment for your training run\n# Note that conda isn't the only spec available, you can set up compute env with pip or a dockerfile.\nsklearn_env = Environment.from_conda_specification(name='sklearn-env', file_path='conda_dependencies.yml')\n\n# Create a ScriptRunConfig Object to specify the configuration details of your training job\nsrc = ScriptRunConfig(source_directory='.',\n                            script='train.py',\n                            compute_target=cpu_cluster,\n                            environment=sklearn_env)\n\n# Create a HyperDriveConfig using the src object, hyperparameter sampler, and policy.\nhyperdrive_config = HyperDriveConfig(run_config=src,\n                                hyperparameter_sampling=ps,\n                                policy=policy,\n                                primary_metric_name='Accuracy',\n                                primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n                                max_total_runs=10,\n                                max_concurrent_runs=4)\n</code></pre>"},{"location":"azure_ml/lesson1_project/#run-hyperdrive-experiments","title":"Run HyperDrive experiments","text":"<pre><code># Submit your hyperdrive run to the experiment and show run details with the widget.\n\n### YOUR CODE HERE ###\nhyperdrive_run = exp.submit(hyperdrive_config)\nRunDetails(hyperdrive_run).show()\nhyperdrive_run.wait_for_completion(show_output=True)\n</code></pre>"},{"location":"azure_ml/lesson1_project/#get-results-and-save-the-best-one","title":"Get results and save the best one","text":"<pre><code>import joblib\n# Get your best run and save the model from that run.\n\n### YOUR CODE HERE ###\n# Get your best run and save the model from that run.\nbest_run = hyperdrive_run.get_best_run_by_primary_metric()\nbest_run_metrics = best_run.get_metrics()\nparameter_values = best_run.get_details()['runDefinition']['arguments']\n\nprint(f'Best Run Id: {best_run.id}')\nprint(f'Best run parameters : {parameter_values}')\nprint(f'Accuracy: {best_run_metrics[\"Accuracy\"]:.3f}')\n</code></pre> <pre><code>Best Run Id: HD_bf0c2c3d-713b-44ce-ac46-80f90e50feb0_9\nBest run parameters : ['--C', '0.20624530875187946', '--max_iter', '125']\nAccuracy: 0.914\n</code></pre> <pre><code>best_run.get_file_names()\n</code></pre> <pre><code>['logs/azureml/dataprep/backgroundProcess.log',\n 'logs/azureml/dataprep/backgroundProcess_Telemetry.log',\n 'logs/azureml/dataprep/rslex.log',\n 'logs/azureml/dataprep/rslex.log.2022-04-05-14',\n 'outputs/.amlignore',\n 'outputs/model.joblib',\n 'outputs/model_0.20624530875187946_125.joblib',\n 'system_logs/cs_capability/cs-capability.log',\n 'system_logs/hosttools_capability/hosttools-capability.log',\n 'system_logs/lifecycler/execution-wrapper.log',\n 'system_logs/lifecycler/lifecycler.log',\n 'system_logs/lifecycler/vm-bootstrapper.log',\n 'user_logs/std_log.txt']\n</code></pre> <pre><code>joblib.dump(value=\"model.joblib\", filename=\"outputs/model.joblib\")\n</code></pre>"},{"location":"azure_ml/lesson1_project/#set-up-automl-run","title":"Set up AutoML run","text":""},{"location":"azure_ml/lesson1_project/#refactor-datatset","title":"Refactor datatset","text":"<pre><code>from azureml.data.dataset_factory import TabularDatasetFactory\n\n# Create TabularDataset using TabularDatasetFactory\n# Data is available at:\n# \"https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv\"\ncsv_url= \"https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv\"\n\nds = TabularDatasetFactory.from_delimited_files(path=csv_url)\n</code></pre> <pre><code>from train import clean_data\n\n# Use the clean_data function to clean your data.\nx, y = clean_data(ds)\n</code></pre> <pre><code># WARNING:root:The AutoMLConfig parameters, X and y, will soon be deprecated. Please refer to our documentation for the latest interface: https://aka.ms/AutoMLConfig\n# will have to concat x and y in the future and \"use training_data\" and \"label_column_name\" parameters\n\n# Reference: \"Create a dataset from pandas dataframe\" section\n# at https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets\n\nimport pandas as pd\nfrom azureml.core import Dataset, Datastore\n\ntraining_data = pd.concat([x, y], axis = 1)\n\ntraining_data.head()\n</code></pre> <pre><code>#dirname = \"./training_data\"\nos.makedirs('training_data', exist_ok=True)\n\nlocal_path = './training_data/training_data.csv'\ntraining_data.to_csv(local_path, index=False)\n</code></pre> <pre><code>Dataset.File.upload_directory(src_dir= \"training_data\", target=(datastore, \"training_data\"), overwrite=True)\nds = TabularDatasetFactory.from_delimited_files(path=[(datastore, ('training_data/training_data.csv'))])\n</code></pre> <pre><code>Validating arguments.\nArguments validated.\nUploading file to training_data\nUploading an estimated of 3 files\nUploading training_data/.amlignore\nUploaded training_data/.amlignore, 1 files out of an estimated total of 3\nUploading training_data/.amlignore.amltmp\nUploaded training_data/.amlignore.amltmp, 2 files out of an estimated total of 3\nUploading training_data/training_data.csv\nUploaded training_data/training_data.csv, 3 files out of an estimated total of 3\nUploaded 3 files\nCreating new dataset\n</code></pre> <pre><code>ds = ds.to_pandas_dataframe()\n</code></pre>"},{"location":"azure_ml/lesson1_project/#register-dataset","title":"Register dataset","text":"<pre><code># get the datastore to upload prepared data\n# https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets#create-a-dataset-from-pandas-dataframe\ndatastore = ws.get_default_datastore()\ndatastore\n</code></pre> <pre><code>{\n  \"name\": \"workspaceblobstore\",\n  \"container_name\": \"azureml-blobstore-2b053715-edc8-4d3b-a8c1-d7ee4050384e\",\n  \"account_name\": \"workspaceperso5448820782\",\n  \"protocol\": \"https\",\n  \"endpoint\": \"core.windows.net\"\n}\n</code></pre> <pre><code>dataset = Dataset.Tabular.register_pandas_dataframe(training_data, datastore, \"udacity_project_dataset\", show_progress=True)\n</code></pre>"},{"location":"azure_ml/lesson1_project/#set-up-automl-experiment","title":"Set up AutoML experiment","text":"<pre><code>from azureml.train.automl import AutoMLConfig\n\n# Set parameters for AutoMLConfig\n# NOTE: DO NOT CHANGE THE experiment_timeout_minutes PARAMETER OR YOUR INSTANCE WILL TIME OUT.\n# If you wish to run the experiment longer, you will need to run this notebook in your own\n# Azure tenant, which will incur personal costs.\nautoml_config = AutoMLConfig(\n                             experiment_timeout_minutes=30,\n                             task=\"classification\",\n                             training_data = ds,\n                             label_column_name = \"y\",\n                             compute_target=cpu_cluster,\n                             iterations=30,\n                             iteration_timeout_minutes=5,\n                             primary_metric=\"accuracy\",\n                             n_cross_validations=5\n                            )\n</code></pre>"},{"location":"azure_ml/lesson1_project/#run-automl","title":"Run AutoML","text":"<pre><code># Submit your automl run\n\nautoml_run = exp.submit(automl_config)\nRunDetails(automl_run).show()\nautoml_run.wait_for_completion(show_output=True)\n</code></pre>"},{"location":"azure_ml/lesson1_project/#get-results-and-save-the-best-one_1","title":"Get results and save the best one","text":"<pre><code>best_run, fitted_model = automl_run.get_output()\nprint(best_run)\nprint(fitted_model)\n</code></pre> <pre><code>Run(Experiment: udacity-project,\nId: AutoML_b07a763b-8e10-44da-925e-b794e0fe356c_27,\nType: azureml.scriptrun,\nStatus: Completed)\nPipeline(memory=None,\n         steps=[('datatransformer',\n                 DataTransformer(enable_dnn=False, enable_feature_sweeping=True, feature_sweeping_config={}, feature_sweeping_timeout=86400, featurization_config=None, force_text_dnn=False, is_cross_validation=True, is_onnx_compatible=False, observer=None, task='classification', working_dir='/mnt/batch/tasks/shared/LS_root/mount...\n), random_state=None, reg_alpha=0.9473684210526315, reg_lambda=0.42105263157894735, subsample=0.49526315789473685))], verbose=False)), ('15', Pipeline(memory=None, steps=[('sparsenormalizer', Normalizer(copy=True, norm='l2')), ('randomforestclassifier', RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced', criterion='gini', max_depth=None, max_features='sqrt', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=0.01, min_samples_split=0.01, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1, oob_score=True, random_state=None, verbose=0, warm_start=False))], verbose=False))], flatten_transform=None, weights=[0.07692307692307693, 0.15384615384615385, 0.07692307692307693, 0.15384615384615385, 0.38461538461538464, 0.07692307692307693, 0.07692307692307693]))],\n         verbose=False)\n</code></pre> <pre><code>run_properties = best_run.properties\nrun_properties[\"model_output_path\"]\n</code></pre> <p><code>'outputs/model.pkl'</code></p> <pre><code>run_properties[\"score\"]\n</code></pre> <p><code>'0.9180880121396056'</code></p> <pre><code># Retrieve and save your best automl model.\n\n### YOUR CODE HERE ###\nbest_run.register_model(model_name = 'automl_best_model.pkl', model_path = 'outputs/')\n</code></pre>"},{"location":"azure_ml/lesson1_project/#compute-cluster-cleaning","title":"Compute cluster cleaning","text":"<pre><code>cpu_cluster.delete()\n</code></pre>"},{"location":"azure_ml/lesson2/","title":"Lesson 2 : Deploy a model","text":""},{"location":"azure_ml/lesson2/#enable-security-and-authentication","title":"Enable Security and Authentication","text":"<p>Summary</p> <p>Authentication is crucial for the continuous flow of operations. Continuous Integration and Delivery system (CI/CD) rely on uninterrupted flows. When authentication is not set properly, it requires human interaction and thus, the flow is interrupted. An ideal scenario is that the system doesn't stop waiting for a user to input a password. So whenever possible, it's good to use authentication with automation.</p>      Key-based Authentication Token-based Authentication Interactive Authentication     Azure Kubernetes Service (AKS) Azure Kubernetes service enabled by default Azure Kubernetes service disabled by default    Azure Container Instance (ACI) Not support Azure Container Instances Disabled by default Used by local deployment and experimentation (e.g. using Jupyter notebook)"},{"location":"azure_ml/lesson2/#what-is-azure-service-principal","title":"What is Azure Service Principal?","text":"<ul> <li>Authenticate your Azure deployment pipeline by using service principals</li> </ul> <p>A \u201cService Principal\u201d is a user role with controlled permissions to access specific resources. Using a service principal is a great way to allow authentication while reducing the scope of permissions, which enhances security.</p> <p>If you want to deploy resources to Azure, you need create a sp and give it owner role, of course you could give custom role that only could create public IP and gateway.</p> <p>When you login Azure with cli 2.0. appid is user name. It also called client id. password called client secret.</p> <p>When you have an application that needs to access or modify resources, you must set up an Azure Active Directory (AD) application and assign the required permissions to it. This approach is preferable to running the app under your own credentials because:</p> <ul> <li>You can assign permissions to the app identity that are different than your own permissions. Typically, these permissions are restricted to exactly what the app needs to do.</li> <li>You do not have to change the app's credentials if your responsibilities change.</li> <li> <p>You can use a certificate to automate authentication when executing an unattended script.</p> </li> <li> <p>Demystifying Service Principals \u2013 Managed Identities</p> </li> </ul>"},{"location":"azure_ml/lesson2/#azure-ad-identity","title":"Azure AD Identity","text":"<p>Azure AD is the trusted Identity Object store, in which you can create different Identity Object types. The most common ones are Users and Groups, but you can also have Applications in there, also known as Enterprise Apps.</p> <p>An example for each could be:</p> <ul> <li>Users: you create a user object in Azure AD, and from there allow the user to authenticate to the Azure Portal, to start using Office 365,\u2026</li> <li>Groups: you define a security group in Azure AD, which can be used to specify permissions to SharePoint sites for example</li> <li>Enterprise Apps: using OpenIDConnect and OAuth, you allow a cloud-based application to trust your Azure AD for user authentication; the trusting app is known as an enterprise app object in Azure AD.</li> </ul>"},{"location":"azure_ml/lesson2/#service-principal","title":"Service Principal","text":"<p>Most relevant to Service Principal, is the Enterprise apps; according to the formal definition, a service principal is \u201c\u2026An application whose tokens can be used to authenticate and grant access to specific Azure resources from a user-app, service or automation tool, when an organization is using Azure Active Directory\u2026\u201d</p> <p>In essence, by using a Service Principal, you avoid creating \"fake users\" (we would call them service account in on-premises Active Directory\u2026) in Azure AD to manage authentication when you need to access Azure Resources.</p> <p>Typical use cases where you would rely on a Service Principal is for example when running Terraform IAC (Infrastructure as Code) deployments, or when using Azure DevOps for example, where you define a Service Connection from DevOps Pipelines to Azure; or basically any other 3rd party application requiring an authentication token to connect to Azure resources.</p> <p>An Azure Service Principal can be created using \"any\" traditional way like the Azure Portal, Azure PowerShell, Rest API or Azure CLI.</p>"},{"location":"azure_ml/lesson2/#azure-cli-installation","title":"Azure cli installation","text":"<p>see here</p> <ul> <li>Add azureml extension for cli.</li> </ul> <pre><code>az extension add -n azure-cli-ml\n</code></pre> <p>2. create the service principal.</p> <p>R\u00e9f\u00e9rence</p> <p>command that interact with the Active Directory.</p> <pre><code>az ad sp create-for-rbac --sdk-auth --name ml-auth\n</code></pre> <p><code>ml-auth</code> can be changed to whatever name you want, it's just the name used in the Azure doc.</p>  <p>D\u00e9finition</p> <p>RBAC : Role-Based Access Control</p>  <ul> <li>After running <code>az ad sp create-for-rbac --sdk-auth --name ml-auth</code>, Azure responds with output similar to this:</li> </ul> <pre><code>Changing \"ml-auth\" to a valid URI of \"http://ml-auth\", which is the required format used for service principal names\nCreating a role assignment under the scope of \"/subscriptions/xxxxxxxx-2cb7-4cc5-90b4-xxxxxxxx24c6\"\n  Retrying role assignment creation: 1/36\n  Retrying role assignment creation: 2/36\n{\n  \"clientId\": \"xxxxxxxx-3af0-4065-8e14-xxxxxxxxxxxx\",\n  \"clientSecret\": \"xxxxxxxxxxxxx.IPgqLjBH2.Uj6VCo1hk3\",\n  \"subscriptionId\": \"39b85eca-2cb7-4cc5-90b4-eb1d0c6c24c6\",\n  \"tenantId\": \"xxxxxxxx-cbdb-4c04-89fc-xxxxxxxxxxxx\",\n  \"activeDirectoryEndpointUrl\": \"https://login.microsoftonline.com\",\n  \"resourceManagerEndpointUrl\": \"https://management.azure.com/\",\n  \"activeDirectoryGraphResourceId\": \"https://graph.windows.net/\",\n  \"sqlManagementEndpointUrl\": \"https://management.core.windows.net:8443/\",\n  \"galleryEndpointUrl\": \"https://gallery.azure.com/\",\n  \"managementEndpointUrl\": \"https://management.core.windows.net/\"\n}\n</code></pre> <ul> <li>Capture the <code>objectId</code> using the <code>clientId</code>:</li> </ul> <pre><code>az ad sp show --id xxxxxxxx-3af0-4065-8e14-xxxxxxxxxxxx\n</code></pre> <p>Where <code>xxxxxxxx-3af0-4065-8e14-xxxxxxxxxxxx</code> corresponds to the <code>clientId</code> value. This step will output some information and you will find the <code>objectId</code> to assign the role.</p> <ul> <li>Finally, allow the Service Principal access to the workspace. You will need to change the code to match your workspace, subscription, and the <code>objectId</code> value retrieved from the previous step.</li> </ul> <pre><code>az ml worskspace share -w Demo -g demo --user xxxxxxxx-cbdb-4cfd-089f-xxxxxxxxxxxx --role owner\n</code></pre> <p>Where <code>xxxxxxxx-cbdb-4cfd-089f-xxxxxxxxxxxx</code> corresponds to the <code>objectId</code> value.</p> <p>Note: This command should complete without any output</p> <pre><code>\u276f az ad sp show --id 4f9ebbd3-c840-4900-97d6-e4b0e3329b61 | jq \".oauth2Permissions[0]\"\n\n{\n  \"adminConsentDescription\": \"Allow the application to access ml-auth on behalf of the signed-in user.\",\n  \"adminConsentDisplayName\": \"Access ml-auth\",\n  \"id\": \"xxxxxxxx-27da-xxxxxxx-989c-xxxxxxxxx\",\n  \"isEnabled\": true,\n  \"type\": \"User\",\n  \"userConsentDescription\": \"Allow the application to access ml-auth on your behalf.\",\n  \"userConsentDisplayName\": \"Access ml-auth\",\n  \"value\": \"user_impersonation\"\n}\n</code></pre>"},{"location":"azure_ml/lesson2/#configure-deployment-settings","title":"Configure Deployment Settings","text":"<p>Deployment is about delivering a trained model into production so that it can be consumed by others. Configuring deployment settings means making choices on cluster settings and other types of interaction with a deployment. Having a good grasp on configuring production environments in Azure ML Studio and the Python SDK is the key to get robust deployments. ACI and AKS</p> <p>Both ACI and AKS are available in the Azure ML platform as deployment options for models.</p> <p>ACI is a container offering from Azure, which uses container technology to quickly deploy compute instances. The flexibility of ACI is reduced as to what AKS offers, but it is far simpler to use.</p> <p>AKS, on the other hand, is a Kubernetes offering. The Kubernetes service is a cluster that can expand and contract given on demand, and it does take more effort than the container instance to configure and setup.</p>  <p>D\u00e9finition</p> <ul> <li>ACI: Azure Container Instance</li> <li>AKS: Azure Kubernetes Service</li> <li>Deployment: A way to deliver work into production</li> <li>Concurrent Operations: Also referred to as \"concurrency\", it is the number of operations to run at the same time</li> </ul>"},{"location":"azure_ml/lesson2/#deploy-an-azure-machine-learning-model","title":"Deploy an Azure Machine Learning model","text":"<p>Summary</p> <p>The primary task as a Machine Learning engineer is to ship models into production. Constant evaluation allows identifying potential issues and creating a baseline so that adapting or updating is possible.</p> <p>Some key steps to deploy a model are:</p> <ul> <li>A previously trained model</li> <li>Complete the deployment form</li> <li>Enable authentication</li> <li>Select or create a new compute cluster</li> </ul>"},{"location":"azure_ml/lesson2/#enable-application-insights","title":"Enable Application Insights","text":"<p>In this section, we discussed Application Insights that is a very useful tool to detect anomalies, visualize performance. It can be enabled before or after a deployment. To enable Application Insights after a model is deployed, you can use the below command with the python SDK. In the next section, you will learn how to do it.</p> <pre><code># enable application insight\nservice.update(enable_app_insights=True)\n</code></pre>  <p>D\u00e9finition</p> <ul> <li>Logging: Informational output produced by the software, usually in the form of text</li> <li>Application Insights: A special Azure service which provides key facts about an application</li> <li>Webservice: One of the most used Python classes from Azure's Python SDK</li> </ul>"},{"location":"azure_ml/lesson2/#troubleshoot-deployment-issues","title":"Troubleshoot Deployment Issues","text":"<p>Summary</p> <p>In this section, we covered different techniques and diagnosis that you can use to identify potential issues like unhandled exceptions from a deployed service. Using local deployment is a special technique, which makes it easier to identify some of these potential issues. Common HTTP errors:</p> <ul> <li>502: the application crashes because of an unhandled exception.</li> <li>503: there are large spikes in requests and the system is not able to cope with all of them.</li> <li>504: request timed out.</li> </ul>"},{"location":"azure_ml/lesson2/#deploy-locally","title":"Deploy Locally","text":"<p>To deploy locally using the Python SDK you will need to use the LocalWebService class and configure it for a local deployment</p> <pre><code>from azureml.core.webservice import LocalWebservice\ndeployment_config = LocalWebservice.deploy_configuration(port=9001)\n# Deploy the service\nservice = Model.deploy(ws, \"local-service\", [model], inference_config, deployment_config)\n\nservice.reload()\nprint(service.run(input_data=json_data))\n</code></pre> <p>Deploying locally has some benefits. First, it is easier and faster to verify unhandled exceptions from the scoring script since you don't have to wait for deployment in Azure. Also, many people or teams can debug at the same time.</p>  <p>D\u00e9finition</p> <p>HTTP Status code: A number that represents a status when an HTTP server responds. Error conditions in the server side start at 500</p>  <p>There are multiple things you can expect to go wrong. When you submit HTTP requests to a deployed model, there are three HTTP codes that you may encounter:</p> <ul> <li>HTTP STATUS 502: After a deployment, the application crashes because of an unhandled exception.</li> <li>HTTP STATUS 503: When there are large spikes in requests, the system may not be able to cope with all of them and some clients may see this code.</li> <li>HTTP STATUS 504: The request timed out. In Azure, the requests time out after 1 minute. If the score.py script is taking longer than a minute, this error code will be produced.</li> </ul> <p>When an error code shows up, one thing you can do is retrieving the logs output. Logs output is always useful to debug problems in deployed containers. Showing below is an extract of what you should see in a successful response to a scoring request.</p> <pre><code>Validation Request Content-Type\nReceived input: {'data': [{'instant': 1, 'date': '2011-01-01 00:00:00,000000', 'season': 1, 'yr': 0, 'mnth': 1, 'weekday': 6, 'weathersit': 2, 'temp': 0.344167, 'atemp': 0.363625, 'hum': 0.805833, 'windspeed': 0.160446, 'casual': 331, 'registered': 654 }]}\nHeaders passed in (total 12):\n    Host: localhost:5001\n    X-Real-Ip: 127.0.0.1\n    X-Forwarded-For: 127.0.0.1\n    X-Forwarded-Proto: http\n    Connection: close\n    Content-Length: 812\n    User-Agent: ApacheBench/2.3\n    Accept: */*\n    Authorization: Bearer q8szMDbCoNlxDZCpiGI8tnqaxtC1yDiy\n    Content-Type: application/json\n    X-Ms-Request-Id: 7cb6f8b9-e511-43b7-982f-e413d6e3239d\n    Accept-Encoding: gzip\nScoring Timer is set to 60.0 seconds\n200\n</code></pre>  <p>D\u00e9finition</p> <ul> <li>ACI: Azure Container Instance</li> <li>AKS: Azure Kubernetes Service</li> <li>Application Insights: A special Azure service which provides key facts about an application</li> <li>CI/CD: Continuous Integration and Continuous Delivery platform. Jenkins, CircleCI, and Github Actions, are a few examples</li> <li>Cloud-based workstation: Sometimes, compute instances are referred to as a cloud-based workstation, because it is ready to start developing</li> <li>Compute Instance: A distinct type of a compute offering from Azure</li> <li>DevOps: A set of best practices that helps provide continuous delivery of software at the highest quality with a constant feedback loop</li> <li>Deployment: A way to deliver work into production</li> <li>Endpoint: A part of an HTTP API. Either a full URL or a partial URL identifying a part</li> <li>HTTP API: A URL that exposes logic to interact with software, in this case, a trained model</li> <li>HTTP Status code: A number that represents a status when an HTTP server responds. Error conditions in the server side start at 500</li> <li>Logging: Informational output produced by software, usually in the form of text</li> <li>Shipping into production: The most important aspect of a Machine Learning specialist</li> <li>Webservice: One of the most used Python classes from Azure's Python SDK</li> </ul>"},{"location":"azure_ml/lesson3/","title":"Lesson 3 : Consume Endpoints","text":"<p>Summary</p> <p>This is the lesson about Consuming Endpoints. These endpoints allow other services to interact with deployed models. And in this lesson, you will learn all the key facts about interacting with them.</p>  <p>There are some interesting details you need to be aware of when trying to use HTTP and you will go through each of these:</p> <ul> <li>Swagger</li> <li>Consuming deployed services</li> <li>Benchmarking</li> </ul>  <p>D\u00e9finition</p> <ul> <li>Swagger: A tool that eases the documentation efforts of HTTP APIs</li> <li>Benchmarking: being able to create a baseline of acceptable performance so that it can be compared to day-to-day behavior</li> </ul>"},{"location":"azure_ml/lesson3/#swagger-documentation","title":"Swagger Documentation","text":"<p>Attention</p> <p>In the video, the instructor used localhoston port 80 to display the Swagger page. It may not work for everyone. If localhost doesn't work for you, check if you can use a different port other than 80, for example, port 9000. Ensure that the updated port is used when trying to reach the swagger instance by localhost, for example localhost:9000.</p> <p>If you see code 400, message bad request version in the Python script, it means that you are using https instead of http in the Swagger page. Remember to use http only when accessing these URLs.</p>  <p>Summary</p> <p>In the <code>swagger.sh</code> file, it has a command line:</p> <p><code>docker run -p 80:8080 swaggerapi/swagger-ui</code></p> <p>This command runs the swagger UI container and makes it available on port 80. This will need to be updated in the lab because port 80 is being used already. Set the port to 9000 would be a good choice here. So the updated command will look like this:</p> <p><code>docker run -p 9000:8080 swaggerapi/swagger-ui</code></p> <p>After the Swagger UI container is running, you can access the website on <code>http://localhost:80</code> (or <code>http://localhost:9000</code> for example if you modified the port).</p> <p>Running <code>serve.py</code> is crucial so that the contents of <code>swagger.json</code> can be consumed locally by Swagger. If <code>swagger.json</code> is not present, or if the local server is not running, then Swagger will not be able to produce the docs.</p>  <p>serve.py</p> <pre><code>from http.server import HTTPServer, SimpleHTTPRequestHandler, test\nimport sys\n\n\nclass CORSRequestHandler(SimpleHTTPRequestHandler):\n    def end_headers(self):\n        self.send_header(\"Access-Control-Allow-Origin\", \"*\")\n        SimpleHTTPRequestHandler.end_headers(self)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) &gt; 1:\n        # Allows the port to be passed in as an argument\n        port = sys.argv[-1]\n    else:\n        port = 8000\n\n    test(CORSRequestHandler, HTTPServer, port=port)\n</code></pre>  <p>To give you more information: running serve.py is needed because Azure protects against CORS (Cross Origin Resource Sharing) and the server that hosts swagger.json needs to be allowed to happen. This is done in the script with the following method:</p> <pre><code>def end_headers(self):\n    self.send_header(\"Access-Control-Allow-Origin\", \"*\")\n    SimpleHTTPRequestHandler.end_headers(self)\n</code></pre> <p>By default, the serve.py script will run and serve contents on <code>localhost:8000</code> - this is an important detail because it is required as input in the Swagger UI page. The value that is required in the Swagger UI is <code>http://localhost:8000/swagger.json</code>. Please notice that you should use http instead of https.</p> <p>Note that since you change the port to 8080 here, the code to run <code>serve.py</code> is <code>python serve.py 8080</code></p> <p>Run first <code>serve.py</code> then <code>swagger.sh</code> to get Docker running locally serving Swagger so that you can interact with the deployed model Documentation. Use the <code>http://localhost:80/</code> and <code>http://localhost:8080/swagger.json</code>, to look at your swagger document and specifics of your model.</p>  <p>Atention</p> <p>If you change the Swagger UI port to something other than 80, <code>http://localhost/</code> will not load the Swagger page. You need to include the new port to load the Swagger page. For example, use <code>http://localhost:9000</code> if the new port is set to 9000.</p>"},{"location":"azure_ml/lesson3/#consume-deployed-service","title":"Consume Deployed Service","text":"<p>Summary</p> <p>You can consume a deployed service via an HTTP API. An HTTP API is a URL that is exposed over the network so that interaction with a trained model can happen via HTTP requests.</p> <p>Users can initiate an input request, usually via an HTTP POST request. HTTP POST is a request method that is used to submit data. The HTTP GET is another commonly used request method. HTTP GET is used to retrieve information from a URL. The allowed requests methods and the different URLs exposed by Azure create a bi-directional flow of information.</p> <p>The APIs exposed by Azure ML will use JSON (JavaScript Object Notation) to accept data and submit responses. It served as a bridge language among different environments.</p>   <p>D\u00e9finition</p> <ul> <li>JSON: JavaScript Object Notation, also referred to as a \"bridge language\" used to make communication possible between two groups who do not share a native dialect</li> <li>GET request method: GET is a request method supported by HTTP. This method should only be used to retrieve data from a web server</li> <li>POST request method: POST is a request method supported by HTTP. This method requests that a web server accepts the data enclosed in the body of the request message</li> </ul>   <p>Azure Consume Deployed Service</p> <p>In Azure ML Studio, head over to the \"Endpoints\" section and find a previously deployed model. The compute type should be ACI (Azure Container Instance).</p> <p>In the \"Consume\" tab, of the endpoint, a \"Basic consumption info\" will show the endpoint URL and the authentication types. Take note of the URL and the \"Primary Key\" authentication type.</p> <p>Using the provided <code>endpoint.py</code> replace the <code>scoring_uri</code> and <code>key</code> to match the REST endpoint and primary key respectively. The script issues a POST request to the deployed model and gets a JSON response that gets printed to the terminal.</p>   <p>Endpoint.py</p> <pre><code>import requests\nimport json\n\n# URL for the web service, should be similar to:\n# 'http://8530a665-66f3-49c8-a953-b82a2d312917.eastus.azurecontainer.io/score'\nscoring_uri = (\n    \"your_endpoint_uri\"\n)\n\n# If the service is authenticated, set the key or token\nkey = \"your_key\"\n\n# Two sets of data to score, so we get two results back\ndata = {\n    \"Inputs\": {\n        \"data\": [\n            {\n                \"date\": \"2013-01-01 00:00:00,000000\",\n                \"season\": 1,\n                \"yr\": 0,\n                \"mnth\": 1,\n                \"weekday\": 6,\n                \"weathersit\": 2,\n                \"temp\": 0.344167,\n                \"atemp\": 0.363625,\n                \"hum\": 0.805833,\n                \"windspeed\": 0.160446,\n                \"casual\": 331,\n                \"registered\": 654,\n            },\n        ]\n    },\n    \"GlobalParameters\": 0.0,\n}\n# Convert to JSON string\ninput_data = json.dumps(data)\nwith open(\"data.json\", \"w\") as _f:\n    _f.write(input_data)\n\n# Set the content type\nheaders = {\"Content-Type\": \"application/json\"}\n# If authentication is enabled, set the authorization header\nheaders[\"Authorization\"] = f\"Bearer {key}\"\n\n# Make the request and display the response\nresp = requests.post(scoring_uri, input_data, headers=headers)\nprint(resp.json())\n</code></pre>  <p>Running it should produce similar results to this:</p> <pre><code>$ python endpoint.py\n{\"result\": [2553]}\n</code></pre> <p>A <code>data.json</code> file will appear after you run <code>endpoint.py</code>.</p>"},{"location":"azure_ml/lesson3/#benchmark-the-endpoint","title":"Benchmark the Endpoint","text":"<p>Summary</p> <p>A benchmark is used to create a baseline or acceptable performance measure. Benchmarking HTTP APIs is used to find the average response time for a deployed model.</p> <p>One of the most significant metrics is the response time since Azure will timeout if the response times are longer than sixty seconds.</p>  <p>Apache Benchmark is an easy and popular tool for benchmarking HTTP services. You will learn about it on the next page.</p>  <p>D\u00e9finition</p> <ul> <li>Response Time: The time in seconds (or milliseconds) that service takes to produce a response</li> <li>Timeout: When a request is sent, this is an error when the server cannot produce a response in a given amount of time</li> </ul>  <p>Benchmarking services is an interesting topic. Everyone likes to have a highly performant endpoint, but the answer to what it takes to have a performant endpoint varies. It is useful to create a baseline for benchmarks so that comparing subsequent results is meaningful.</p> <p>The <code>benchmark.sh</code> script doesn't have much code at all in it. It does include the ab command that runs against the selected endpoint using the <code>data.json</code> file created by the same <code>endpoint.py</code> file you used in the previous exercise. The <code>ab</code> command looks like this:</p> <pre><code>ab -n 10 -v 4 -p data.json -T 'application/json' -H 'Authorization: Bearer SECRET' http://URL.azurecontainer.io/score\n</code></pre> <p>After running the <code>benchmark.sh</code> or simply after running the above command, you will see the output of requests sent to and responses from the endpoint, and at the end, a summary with key information to determine response performance.</p> <p>Use the Apache Benchmark command-line tool (ab) to generate lots of HTTP POST requests to get performance metrics out of the Azure Container Instance.</p> <p>Make sure you have the Apache Benchmark command-line tool installed and available in your path:</p> <p><pre><code>which ab\n/usr/bin/ab\n\nab --help\nUsage: ab [options] [http[s]://]hostname[:port]/path\nOptions are:\n...\n</code></pre> You can use the provided endpoint.py and benchmark.sh script in the Exercise_starter_files directory to generate the benchmark. Make sure you modify it to match the URI and Keys.</p> <p>R\u00e9f\u00e9rence : How to install and use Apache Benchmark.</p>  <p>TODO</p> <p>Parler de Molotov et Locust.</p>"},{"location":"azure_ml/lesson3/#curating-data-input","title":"Curating Data Input","text":"<p>Summary</p> <p>There are some key items to ensure when sending data to a deployed endpoint. You need to make sure that the keys and values are following the constraints. For example, if one field is repeated, this could potentially cause an error response, or if the format needs a date and time as a string, rather than an integer.</p>  <p>Remember, using values that the service doesn't expect would produce an error response.</p>  <p>D\u00e9finition</p> <ul> <li>Benchmarking: being able to create a baseline of acceptable performance so that it can be compared to day-to-day behavior</li> <li>GET request method: GET is a request method supported by HTTP. This method should only be used to retrieve data from a web server</li> <li>JSON: JavaScript Object Notation, also referred to as a \"bridge language\" used to make communication possible between two groups who do not share a native dialect</li> <li>POST request method: POST is a request method supported by HTTP. This method requests that a web server accepts the data enclosed in the body of the request message</li> <li>RESTful: A style for building HTTP endpoints that emphasizes separation of concerns</li> <li>Response Time: The time in seconds (or milliseconds) that service takes to produce a response</li> <li>Swagger: A tool that eases the documentation efforts of HTTP APIs</li> <li>Timeout: When a request is sent, this is an error when the server cannot produce a response in a given amount of time</li> </ul>"},{"location":"azure_ml/lesson4/","title":"Lesson 4 : Pipeline automation","text":"<p>https://docs.microsoft.com/fr-fr/azure/machine-learning/how-to-use-automlstep-in-pipelines#configure-and-create-the-automated-ml-pipeline-step</p> <p>https://docs.microsoft.com/fr-fr/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py</p> <p>https://docs.microsoft.com/fr-fr/python/api/azureml-pipeline-core/azureml.pipeline.core.portdatareference?view=azure-ml-py#azureml-pipeline-core-portdatareference-path-on-datastore</p> <p>https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-container-instance</p> <p>https://docs.microsoft.com/en-us/azure/machine-learning/how-to-secure-web-service</p>"},{"location":"azure_ml/lesson4/#create-a-pipeline","title":"Create a Pipeline","text":"<p>Summary</p> <p>The most common SDK class is the Pipeline class. You will use this when creating a Pipeline. Pipelines can take configuration and different steps, like AutoML for example.</p> <p>Different steps can have different arguments and parameters. Parameters are just like variables in a Python script.</p>  <p>There are areas you can play with when creating a pipeline and we covered two:</p> <ul> <li>Use pipeline parameters</li> <li>Recurring Scheduled Pipelines</li> <li>Batch Inference Pipelines</li> </ul>"},{"location":"azure_ml/lesson4/#pipeline-class","title":"Pipeline Class","text":"<p>This is the most common Python SDK class you will see when dealing with Pipelines. Aside from accepting a workspace and allowing multiple steps to be passed in, it uses a description that is useful to identify it later.</p> <pre><code>from azureml.pipeline.core import Pipeline\n\npipeline = Pipeline(\n    description=\"pipeline_with_automlstep\",\n    workspace=ws,\n    steps=[automl_step])\n</code></pre>"},{"location":"azure_ml/lesson4/#using-pipeline-parameters","title":"Using Pipeline Parameters","text":"<p>Pipeline parameters are also available as a class. You configure this class with the various different parameters needed so that they can later be used.</p> <p>In this example, the <code>avg_rate_param</code> is used in the arguments attribute of the PythonScriptStep.</p> <pre><code>from azureml.pipeline.steps import PythonScriptStep\nfrom azureml.pipeline.core import PipelineParameter\n\navg_rate_param = PipelineParameter(name=\"avg_rate\", default_value=0.5)\ntrain_step = PythonScriptStep(script_name=\"train.py\",\n                              arguments=[\"--input\", avg_rate_param],\n                              target=compute_target,\n                              source_directory=project_folder)\n</code></pre>"},{"location":"azure_ml/lesson4/#scheduling-a-recurring-pipeline","title":"Scheduling a recurring Pipeline","text":"<p>To schedule a Pipeline, you must use the ScheduleRecurrence class which has the information necessary to set the interval.</p> <p>Once that has been created, it has to be passed into the <code>create()</code> method of the <code>Schedule</code> class as a recurrence value.</p> <pre><code>from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\n\nhourly = ScheduleRecurrence(frequency=\"Hourly\", interval=4)\npipeline_schedule = Schedule.create(ws, name=\"RecurringSchedule\",\n                            description=\"Trains model every few hours\",\n                            pipeline_id=pipeline_id,\n                            experiment_name=\"Recurring_Pipeline_name\",\n                            recurrence=hourly)\n</code></pre>"},{"location":"azure_ml/lesson4/#batch-inference-pipeline","title":"Batch Inference Pipeline","text":"<p>One of the core responsibilities of a batch inference pipeline is to run in parallel. For this to happen, you must use the ParallelRunConfig class which helps define the configuration needed to run in parallel.</p> <p>Some important aspects of this are the script that will do the work (<code>entry_script</code> parameter), how many failures it should tolerate (<code>error_threshold</code> parameter), and the number of nodes/batches needed to run (<code>mini_batch_size</code> parameter, 5 in this example).</p> <pre><code>from azureml.pipeline.steps import ParallelRunConfig\n\nparallel_run_config = ParallelRunConfig(\n    source_directory='scripts',\n    entry_script=\"scoring.py\",\n    mini_batch_size=\"5\",\n    error_threshold=4,\n    output_action=\"append_row\",\n    environment=batch_env,\n    compute_target=aml_target,\n    node_count=5)\n\nparallelrun_step = ParallelRunStep(\n    name=\"batch-score\",\n    parallel_run_config=parallel_run_config,\n    inputs=[batch_data_set.as_named_input('batch_data')],\n    output=output_dir,\n    arguments=[],\n    allow_reuse=True\n)\n\n# create the pipeline\npipeline = Pipeline(workspace=ws, steps=[parallerun_step])\n</code></pre>  <p>D\u00e9finition</p> <ul> <li>Batch inference: The process of doing predictions using parallelism. In a pipeline, it will usually be on a recurring schedule</li> <li>Recurring schedule: A way to schedule pipelines to run at a given interval</li> <li>Pipeline parameters: Like variables in a Python script, these can be passed into a script argument</li> </ul>"},{"location":"azure_ml/lesson4/#exercise","title":"Exercise","text":""},{"location":"azure_ml/lesson4/#step-1-create-a-pipeline","title":"Step 1 : Create a Pipeline","text":"<p>Summary</p> <p>Pipelines are very useful and are a foundation of automation and operations in general. Being able to create a Pipeline allows for easier interaction with model deployments.</p>  <p>This demo shows you how to use the Python SDK to create a pipeline with AutoML steps.</p> <p>For this exercise, you will create a pipeline using the python SDK.</p> <p>First, create a pipeline using the Python SDK. (This is the part that up until the Examine Results section in the provided notebook)</p> <p>It is optional, you can copy and run cells in Examine Results section to test the pipeline and retrieve the best model. This step involves running an Automated ML experiment so it will take about 30 min to complete. Please keep track of the remaining time before you run these cells.</p>  <p>Attention</p> <p>Make sure you update cells to match your dataset and other variables. These are noted in comments like this:</p> <pre><code># Choose a name for the run history container in the workspace.\n# NOTE: update these to match your existing experiment name\nexperiment_name = 'ml-experiment-1'\nproject_folder = './pipeline-project'\n</code></pre>  <p>Free free to modify the code to explore the different pipeline features and parameters. To speed up and shorten the total amount to train the model, you can change the <code>experiment_timeout_minutes</code> value from 20 to 10. These settings are in the Python Notebook, which are currently set to 20 minutes:</p> <pre><code>automl_settings = {\n    \"experiment_timeout_minutes\": 20,\n    \"max_concurrent_iterations\": 4,\n    \"primary_metric\" : 'normalized_root_mean_squared_error',\n    \"n_cross_validations\": 5\n}\n</code></pre> <p>Create and run the pipelines using the Python SDK.</p>"},{"location":"azure_ml/lesson4/#step-2-publish-a-pipeline","title":"Step 2 : Publish a pipeline","text":"<p>In this part, you need to publish a pipeline using the both Azure ML studio and the Python SDK. Please re-use the Pipeline created in the previous part.</p> <p>You are recommended to write your own code to publish the pipeline. If you get stuck, review the first a few cells in the Publish and run from REST endpoint section in the provided notebook.</p>"},{"location":"azure_ml/lesson4/#azure-machine-learning-pipeline-with-automlstep","title":"Azure Machine Learning Pipeline with AutoMLStep","text":"<p>We demonstrate the use of AutoMLStep in Azure Machine Learning Pipeline.</p>"},{"location":"azure_ml/lesson4/#introduction","title":"Introduction","text":"<p>In this example we showcase how you can use AzureML Dataset to load data for AutoML via AML Pipeline.</p> <p>If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you have executed the configuration before running this notebook.</p> <p>Here, you will learn how to:</p> <ol> <li>Create an <code>Experiment</code> in an existing <code>Workspace</code>.</li> <li>Create or Attach existing AmlCompute to a workspace.</li> <li>Define data loading in a <code>TabularDataset</code>.</li> <li>Configure AutoML using <code>AutoMLConfig</code>.</li> <li>Use <code>AutoMLStep</code>.</li> <li>Train the model using AmlCompute.</li> <li>Explore the results.</li> <li>Test the best fitted model.</li> </ol>"},{"location":"azure_ml/lesson4/#initialize-workspace","title":"Initialize Workspace","text":"<p>Initialize a workspace object from persisted configuration. Make sure the config file is present at <code>.\\config.json</code>.</p>"},{"location":"azure_ml/lesson4/#create-an-azure-ml-experiment","title":"Create an Azure ML experiment","text":"<p>Let's create an experiment named \"automlstep-classification\" and a folder to hold the training scripts. The script runs will be recorded under the experiment in Azure.</p> <p>The best practice is to use separate folders for scripts and its dependent files for each step and specify that folder as the <code>source_directory</code> for the step.</p> <p>This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the <code>source_directory</code> would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the <code>source_directory</code> of the step.</p> <pre><code># Choose a name for the run history container in the workspace.\n# NOTE: update these to match your existing experiment name\nexperiment_name = \"automlstep-regression\"\nproject_folder = './pipeline-project'\n\nexperiment = Experiment(ws, experiment_name)\nexperiment\n</code></pre>"},{"location":"azure_ml/lesson4/#create-or-attach-an-amlcompute-cluster","title":"Create or Attach an AmlCompute cluster","text":"<p>You will need to create a compute target for your AutoML run. In this tutorial, you get the default <code>AmlCompute</code> as your training compute resource.</p> <pre><code>from azureml.core.compute import AmlCompute\nfrom azureml.core.compute import ComputeTarget\nfrom azureml.core.compute_target import ComputeTargetException\n\n# NOTE: update the cluster name to match the existing cluster\n# Choose a name for your CPU cluster\namlcompute_cluster_name = \"cpu-cluster\"\n\n# Verify that cluster does not exist already\ntry:\n    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n    print('Found existing cluster, use it.')\nexcept ComputeTargetException:\n    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',# for GPU, use \"STANDARD_NC6\"\n                                                           #vm_priority = 'lowpriority', # optional\n                                                           max_nodes=4)\n    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n    compute_target.wait_for_completion(show_output=True, min_node_count = 1, timeout_in_minutes = 10)\n# For a more detailed view of current AmlCompute status, use get_status().\n</code></pre>"},{"location":"azure_ml/lesson4/#data","title":"Data","text":"<pre><code># Try to load the dataset from the Workspace. Otherwise, create it from the file\n# NOTE: update the key to match the dataset name\nfound = False\nkey = \"bike-no-aci-deployment\"\ndescription_text = \"dataset used for testing ACI deployment in udacity nanodegree\"\n\nif key in ws.datasets.keys():\n        found = True\n        dataset = ws.datasets[key]\n\nif not found:\n        # Create AML Dataset and register it into Workspace\n        example_data = \"your_csv_url.csv\"\n        dataset = Dataset.Tabular.from_delimited_files(example_data)\n        #Register Dataset in Workspace\n        dataset = dataset.register(workspace=ws,\n                                   name=key,\n                                   description=description_text)\n\n\ndf = dataset.to_pandas_dataframe()\ndf.head()\n</code></pre>"},{"location":"azure_ml/lesson4/#train","title":"Train","text":"<p>This creates a general AutoML settings object.</p> <pre><code>automl_settings = {\n    \"experiment_timeout_minutes\": 20,\n    \"max_concurrent_iterations\": 4,\n    \"primary_metric\" : 'normalized_root_mean_squared_error',\n    \"n_cross_validations\": 5\n}\n\nautoml_config = AutoMLConfig(compute_target=compute_target,\n                             task = \"regression\",\n                             training_data=dataset,\n                             label_column_name=\"cnt\",\n                             path = project_folder,\n                             enable_early_stopping= True,\n                             featurization= 'auto',\n                             debug_log = \"automl_errors.log\",\n                             **automl_settings\n                            )\n</code></pre>"},{"location":"azure_ml/lesson4/#create-pipeline-and-automlstep","title":"Create Pipeline and AutoMLStep","text":""},{"location":"azure_ml/lesson4/#define-outputs","title":"Define outputs","text":"<p>You can define outputs for the AutoMLStep using <code>TrainingOutput</code>.</p> <pre><code>from azureml.pipeline.core import PipelineData, TrainingOutput\n\nds = ws.get_default_datastore()\nmetrics_output_name = 'metrics_output'\nbest_model_output_name = 'best_model_output'\n\nmetrics_data = PipelineData(name='metrics_data',\n                           datastore=ds,\n                           pipeline_output_name=metrics_output_name,\n                           training_output=TrainingOutput(type='Metrics'))\n\nmodel_data = PipelineData(name='model_data',\n                           datastore=ds,\n                           pipeline_output_name=best_model_output_name,\n                           training_output=TrainingOutput(type='Model'))\n</code></pre> <p><code>PipelineData</code> Class.</p>  <p><code>name</code> (str, Required)</p> <p>The name of the <code>PipelineData</code> object, which can contain only letters, digits, and underscores.</p> <p><code>PipelineData</code> names are used to identify the outputs of a step. After a pipeline run has completed, you can use the step name with an output name to access a particular output. Names should be unique within a single step in a pipeline.</p>   <p><code>pipeline_output_name</code> (Required)</p> <p>If provided this output will be available by using <code>PipelineRun.get_pipeline_output()</code>. Pipeline output names must be unique in the pipeline.</p>  <p>More on <code>TrainingOutput</code>.</p>  <p>D\u00e9finition</p> <p>Defines a specialized output of certain <code>PipelineSteps</code> for use in a pipeline.</p> <p><code>TrainingOutput</code> enables an automated machine learning metric or model to be made available as a step output to be consumed by another step in an Azure Machine Learning Pipeline. Can be used with <code>AutoMLStep</code> or <code>HyperDriveStep</code>.</p> <p><code>TrainingOutput</code> is used with <code>PipelineData</code> when constructing a Pipeline to enable other steps to consume the metrics or models generated by an <code>AutoMLStep</code> or <code>HyperDriveStep</code>.</p>"},{"location":"azure_ml/lesson4/#create-an-automlstep","title":"Create an AutoMLStep","text":"<pre><code>automl_step = AutoMLStep(\n    name='automl_module',\n    automl_config=automl_config,\n    outputs=[metrics_data, model_data],\n    allow_reuse=True)\n</code></pre>"},{"location":"azure_ml/lesson4/#define-the-pipeline","title":"Define the pipeline","text":"<pre><code>from azureml.pipeline.core import Pipeline\npipeline = Pipeline(\n    description=\"pipeline_with_automlstep\",\n    workspace=ws,\n    steps=[automl_step])\n</code></pre>"},{"location":"azure_ml/lesson4/#run-the-pipeline","title":"Run the pipeline","text":"<pre><code>from azureml.widgets import RunDetails\npipeline_run = experiment.submit(pipeline)\nRunDetails(pipeline_run).show()\npipeline_run.wait_for_completion()\n</code></pre>"},{"location":"azure_ml/lesson4/#examine-results","title":"Examine Results","text":""},{"location":"azure_ml/lesson4/#retrieve-the-metrics-of-all-child-runs","title":"Retrieve the metrics of all child runs","text":"<p>Outputs of above run can be used as inputs of other steps in pipeline. In this tutorial, we will examine the outputs by retrieve output data and running some tests.</p> <pre><code>metrics_output = pipeline_run.get_pipeline_output(metrics_output_name)\nnum_file_downloaded = metrics_output.download('.', show_progress=True)\n</code></pre> <pre><code>import json\nwith open(metrics_output._path_on_datastore) as f:\n    metrics_output_result = f.read()\n\ndeserialized_metrics_output = json.loads(metrics_output_result)\ndf = pd.DataFrame(deserialized_metrics_output)\ndf\n</code></pre>"},{"location":"azure_ml/lesson4/#retrieve-the-best-model","title":"Retrieve the Best Model","text":"<pre><code># Retrieve best model from Pipeline Run\nbest_model_output = pipeline_run.get_pipeline_output(best_model_output_name)\nnum_file_downloaded = best_model_output.download('.', show_progress=True)\n</code></pre> <pre><code>import pickle\n\nmodel_filename = best_model_output._path_on_datastore\n# model_filename = path to downloaded file\nmodel_filename\n</code></pre> <pre><code>with open(model_filename, \"rb\" ) as f:\n    best_model = pickle.load(f)\n\nbest_model.steps\n</code></pre>"},{"location":"azure_ml/lesson4/#test-the-model","title":"Test the Model","text":""},{"location":"azure_ml/lesson4/#load-test-data","title":"Load Test Data","text":"<p>For the test data, it should have the same preparation step as the train data. Otherwise it might get failed at the preprocessing step.</p> <pre><code>dataset_test = Dataset.Tabular.from_delimited_files(path='https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_test.csv')\ndf_test = dataset_test.to_pandas_dataframe()\ndf_test = df_test[pd.notnull(df_test['y'])]\n\ny_test = df_test['y']\nX_test = df_test.drop(['y'], axis=1)\n</code></pre>"},{"location":"azure_ml/lesson4/#testing-our-best-fitted-model","title":"Testing Our Best Fitted Model","text":"<p>We will use confusion matrix to see how our model works.</p> <pre><code>from sklearn.metrics import confusion_matrix\nypred = best_model.predict(X_test)\ncm = confusion_matrix(y_test, ypred)\n# Visualize the confusion matrix\npd.DataFrame(cm).style.background_gradient(cmap='Blues', low=0, high=0.9)\n</code></pre>"},{"location":"azure_ml/lesson4/#publish-and-run-from-rest-endpoint","title":"Publish and run from REST endpoint","text":"<p>Run the following code to publish the pipeline to your workspace. In your workspace in the portal, you can see metadata for the pipeline including run history and durations. You can also run the pipeline manually from the portal.</p> <p>Additionally, publishing the pipeline enables a REST endpoint to rerun the pipeline from any HTTP library on any platform.</p> <pre><code>published_pipeline = pipeline_run.publish_pipeline(\n    name=\"Bike sharing Train\", description=\"Training bike sharing pipeline\", version=\"1.0\")\n\npublished_pipeline\n</code></pre> <p>Authenticate once again, to retrieve the <code>auth_header</code> so that the endpoint can be used.</p> <pre><code>from azureml.core.authentication import InteractiveLoginAuthentication\n\ninteractive_auth = InteractiveLoginAuthentication()\nauth_header = interactive_auth.get_authentication_header()\n</code></pre> <p>Get the REST url from the endpoint property of the published pipeline object. You can also find the REST url in your workspace in the portal. Build an HTTP POST request to the endpoint, specifying your authentication header. Additionally, add a JSON payload object with the experiment name and the batch size parameter. As a reminder, the process_count_per_node is passed through to ParallelRunStep because you defined it is defined as a PipelineParameter object in the step configuration.</p> <p>Make the request to trigger the run. Access the Id key from the response dict to get the value of the run id.</p> <pre><code>import requests\n\nrest_endpoint = published_pipeline.endpoint\nresponse = requests.post(rest_endpoint,\n                         headers=auth_header,\n                         json={\"ExperimentName\": \"pipeline-rest-endpoint\"}\n                        )\n</code></pre> <pre><code>try:\n    response.raise_for_status()\nexcept Exception:\n    raise Exception(\"Received bad response from the endpoint: {}\\n\"\n                    \"Response Code: {}\\n\"\n                    \"Headers: {}\\n\"\n                    \"Content: {}\".format(rest_endpoint, response.status_code, response.headers, response.content))\n\nrun_id = response.json().get('Id')\nprint('Submitted pipeline run: ', run_id)\n</code></pre> <p>Use the run id to monitor the status of the new run. This will take another 10-15 min to run and will look similar to the previous pipeline run, so if you don't need to see another pipeline run, you can skip watching the full output.</p> <pre><code>from azureml.pipeline.core.run import PipelineRun\nfrom azureml.widgets import RunDetails\n\npublished_pipeline_run = PipelineRun(ws.experiments[\"pipeline-rest-endpoint\"], run_id)\nRunDetails(published_pipeline_run).show()\n</code></pre>"},{"location":"azure_ml/lesson4/#consume-pipeline-endpoint-api","title":"Consume Pipeline Endpoint (API)","text":"<p>Summary</p> <p>Pipeline endpoints can be consumed via HTTP, but it is also possible to do so via the Python SDK. Since there are different ways to interact with published Pipelines, this makes the whole pipeline environment very flexible.</p> <p>It is key to find and use the correct HTTP endpoint to interact with a published pipeline. Sending a request over HTTP to a pipeline endpoint will require authentication in the request headers. We will talk more about it later.</p> <p>Pipelines can perform several other tasks aside from training a model. Some of these tasks, or steps are:</p> <ul> <li>Data Preparation</li> <li>Validation</li> <li>Deployment</li> <li>Combined tasks</li> </ul>   <p>D\u00e9finition</p> <ul> <li>Pipeline endpoint: The URL of the published Pipeline</li> <li>HTTP Headers: Part of the HTTP specification, where a request can attach extra information, like authentication</li> <li>Automation: A core pillar of DevOps which is applicable to Machine Learning</li> <li>Batch inference: The process of doing predictions using parallelism. In a pipeline, it will usually be on a recurring schedule</li> <li>HTTP trigger: With configuration, a service can create an HTTP request based on certain conditions</li> <li>Pipeline parameters: Like variables in a Python script, these can be passed into a script argument</li> <li>Publishing a Pipeline: Allowing external access to a Pipeline over an HTTP endpoint</li> <li>Recurring schedule: A way to schedule pipelines to run at a given interval</li> </ul>"},{"location":"azure_ml/lesson4/#documentation","title":"Documentation","text":"<ul> <li>PipelineEndpoint Class.</li> <li>Create and run machine learning pipelines with Azure Machine Learning SDK</li> <li> <p>What are Azure Machine Learning pipelines?</p> </li> <li> <p>Tutorial: Create Training and Inferencing Pipelines with Azure ML Designer</p> </li> <li>Build Repeatable ML Workflows with Azure Machine Learning Pipelines</li> <li>Tutorial: Build an End-to-End Azure ML Pipeline with the Python SDK</li> <li>Tutorial: Train Machine Learning Models with Automated ML Feature of Azure ML</li> </ul>"},{"location":"azure_ml/lesson4/#best-practices-for-azure-machine-learning-pipelines","title":"Best Practices for Azure Machine Learning Pipelines","text":"<p>Attention</p> <p>Rephrasing of the StackOverflow answer, go check it for complete anwsers.</p>  <p>Most of the time, a pipeline has at least 4 steps.</p> <ul> <li>Input data</li> <li>Data transformation step</li> <li>Model Training step</li> <li>Model scoring step</li> </ul> <p>There are a bunch of things that are completely unclear from the documentation and the examples and I'm struggling to fully grasp the concept.</p> <ol> <li>When I look at <code>batch scoring</code> examples, it is implemented as a Pipeline Step. This raises the question:</li> </ol>  <p>Question</p> <p>Does this mean that the <code>predicting part</code> is part of the same pipeline as the <code>training part</code>, or should there be separate 2 separate pipelines for this ?</p>  <p>Making 1 pipeline that combines both steps seems odd to me, because you don't want to run your predicting part every time you change something to the training part (and vice versa).</p> <p>A pipeline architecture depends on if:</p> <ul> <li>you need to predict live (else batch prediction is sufficient), and</li> <li>your data is already transformed and ready for scoring.</li> </ul> <p>If you need live scoring, you should deploy your model. If batch scoring, is fine. You could either have:</p> <ul> <li>a training pipeline at the end of which you register a model that is then used in a scoring pipeline, or</li> <li>have one pipeline that can be configured to do either using script arguments.</li> </ul>  <p>Attention</p> <p>From March 2021. PipelineData is no longer a preferred way: \"PipelineData use DataReference underlying which is no longer the recommended approach for data access and delivery, please use OutputFileDatasetConfig instead\".</p>  <p>In the batch scoring examples, the assumption is that there is already a trained model, which could be coming from another pipeline, or in the case of the notebook, it's a pre-trained model not built in a pipeline at all.</p> <p>However, running both training and prediction in the same pipeline is a valid use-case. Use the <code>allow_reuse</code> param and set to <code>True</code>, which will cache the step output in the pipeline to prevent unnecessary reruns.</p> <p>Take a model training step for example, and consider the following input to that step:</p> <ul> <li>training script</li> <li>input data</li> <li>additional step params</li> </ul> <p>If you set <code>allow_reuse=True</code>, and your training script, input data, and other step params are the same as the last time the pipeline ran, it will not rerun that step, it will use the cached output from the last time the pipeline ran. But let's say your data input changed, then the step would rerun.</p> <p>In general, pipelines are pretty modular and you can build them how you see fit. You could maintain separate pipelines for training and scoring, or bundle everything in one pipeline but leverage the automatic caching.</p> <ol> <li>What parts should be implemented as a Pipeline Step and what parts shouldn't? Should the creation of the Datastore and Dataset be implemented as a step? Should registering a model be implemented as a step?</li> </ol> <p>All transformations you do to your data (munging, featurization, training, scoring) should take place inside of <code>PipelineStep</code>'s. The inputs and outputs of which should be <code>PipelineData</code>'s.</p> <p>Azure ML artifacts should be:</p> <ul> <li>created in the pipeline control plane using <code>PipelineData</code>, and</li> <li>registered either ad-hoc, as opposed to with every run, or</li> <li>when you need to pass artifacts between pipelines.</li> </ul> <p>In this way <code>PipelineData</code> is the glue that connects pipeline steps directly rather than being indirectly connected with <code>.register()</code> and <code>.download()</code></p> <p><code>PipelineData</code>'s are ultimately just ephemeral directories that can also be used as placeholders before steps are run to create and register artifacts.</p> <p><code>Dataset</code>'s are abstractions of <code>PipelineData</code>s in that they make things easier to pass to <code>AutoMLStep</code> and <code>HyperDriveStep</code>, and <code>DataDrift</code>.</p> <ol> <li>What isn't shown anywhere is how to deal with model registry. I create the model in the training step and then write it to the output folder as a pickle file. Then what? How do I get the model in the next step? Should I pass it on as a <code>PipelineData</code> object? Should <code>train.py</code> itself be responsible for registering the trained model?</li> </ol> <p>During development, I recommend that you don't register your model and that the scoring step receives your model via a PipelineData as a pickled file.</p> <p>In production, the scoring step should use a previously registered model. The registration of the new model should be done via comparison of the metrics and then trigger the registration if needed.</p> <pre><code>http -A bearer -a xxxxxxxxCPAhdOZ9Ixxxxxxxxx xxxxxx-bb55-xxxx-9ca8-xxxxxxxx.westeurope.azurecontainer.io/score &lt; data.json\n\nhttp -A bearer -a xxxxxxxxCPAhdOZ9Ixxxxxxxxx xxxxxx-bb55-xxxx-9ca8-xxxxxxxx.westeurope.azurecontainer.io/score &lt; data.json | jq .Results\n</code></pre>"},{"location":"deep_learning/module1/Module1/","title":"Module 1 : Introduction au deep learning, prise en main de Tensorflow et Keras","text":""},{"location":"deep_learning/module1/Module1/#preliminaires-notations-et-conventions","title":"Pr\u00e9liminaires, notations et conventions","text":"<p>Dans la suite de ces modules, on se place de le cadre d'un apprentissage dit supervis\u00e9, on consid\u00e9rera donc la probl\u00e9matique suivante :</p>  <p>Probl\u00e9matique</p> <p>On note \\(\\mathbf{R}^{j}\\) l'espace vectoriel r\u00e9el de dimension \\(j\\). Etant donn\u00e9 le dataset (fini) suivant.</p> \\[     \\mathcal{X} = \\lbrace (\\mathbf{x}_{i}, \\mathbf{y}_{i})\\rbrace_{i \\in I} \\quad (\\mathbf{x}_{i}, \\mathbf{y}_{i}) \\in \\mathbf{R}^{m} \\times \\mathbf{R}^{k} \\] <p>On suppose que \\(\\mathbf{x}_{i}\\) et \\(\\mathbf{y}_{i}\\) sont reli\u00e9s entre eux par une fonction inconnue \\(f : \\mathbf{R}^{m} \\rightarrow \\mathbf{R}^{k}\\) v\u00e9rifiant la relation suivante.</p> \\[     f(\\mathbf{x}_{i}) = \\mathbf{y}_{i} + \\varepsilon \\] <p>D\u00e9terminer un algorithme estimant \\(f\\), c'est \u00e0 dire produisant une fonction</p> \\[     \\hat{f} : \\mathbf{R}^{m} \\rightarrow \\mathbf{R}^{k} \\] <p>telle que \\(\\hat{f}(\\mathbf{x}_{i}) = \\hat{\\mathbf{y}}_{i}\\) avec \\(\\hat{\\mathbf{y}}_{i}  \\simeq \\mathbf{y}_{i}\\).</p>  <p>Pour d\u00e9terminer \\(\\hat{f}\\), on se place alors dans le cadre des r\u00e9seaux de neurones. On utilisera les conventions suivantes.</p> <ul> <li>\\(I = \\lbrace 1, \\dots, n \\rbrace\\) est un ensemble discret fini, son cardinal \\(|I| = n\\) correspond au nombre d'observations dans le dataset.</li> <li>Le couple \\((\\mathbf{x}_{i}, \\mathbf{y}_{i})\\) est alors appel\u00e9 la \\(i\\)-i\u00e8me observation du dataset.</li> <li>\\(\\mathbf{x}_{i} = (x_{i,1}, \\dots, x_{i,m}) \\in \\mathbf{R}^{m}\\) est l'ensemble des features (caract\u00e9ristiques) de la \\(i\\)-i\u00e8me observation du dataset et \\(\\mathbf{y}_{i} = (y_{i,1}, \\dots, y_{i,k}) \\in \\mathbf{R}^{k}\\) est la cible de la \\(i\\)-i\u00e8me observation du dataset.</li> <li>La fonction \\(\\hat{f}\\) est un mod\u00e8le de \\(f\\), et \\(\\hat{f}(\\mathbf{x}_{i}) = \\hat{\\mathbf{y}}_{i}\\) est une pr\u00e9diction.</li> </ul>"},{"location":"deep_learning/module1/Module1/#le-commencement-du-debut-les-neurosciences-le-fonctionnement-dun-neurone-biologique","title":"Le commencement du d\u00e9but : les neurosciences &amp; le fonctionnement d'un neurone biologique","text":"<p>Avant de parler des neurones artificiels, jetons un coup d'\u0153il rapide sur un neurone biologique.</p> <p>Il s'agit d'une cellule d'apparence inhabituelle que l'on trouve surtout dans les cerveaux d'animaux. Elle est compos\u00e9e d'un corps cellulaire contenant le noyau et la plupart des \u00e9l\u00e9ments du complexe cellulaire, de nombreuses extensions de ramification appel\u00e9es dendrites, plus une tr\u00e8s longue extension appel\u00e9e l'axone. La longueur de l'axone peut \u00eatre juste quelques fois plus longue que la cellule, ou jusqu'\u00e0 des dizaines de milliers de fois plus.</p> <p>Pr\u00e8s de son extr\u00e9mit\u00e9, l'axone se d\u00e9tache en de nombreuses branches appel\u00e9es t\u00e9lodendries, et \u00e0 l'extr\u00e9mit\u00e9 de ces branches se trouvent de minuscules structures appel\u00e9es bornes synaptiques (ou simplement synapses), qui sont connect\u00e9es \u00e0 la dendrite ou aux corps d'autres neurones. Les neurones biologiques produisent de courtes impulsions \u00e9lectrique appel\u00e9es potentiels d'action (PA, ou simplement des signaux) qui se d\u00e9placent le long des axones et font en sorte que les synapses \u00e9mettent des signaux chimiques appel\u00e9s neurotransmetteurs. Quand un neurone re\u00e7oit une quantit\u00e9 suffisante de ces neurotransmetteurs en quelques millisecondes, il envoie ses propres impulsions \u00e9lectriques (en fait, cela d\u00e9pend des neurotransmetteurs, car certains d'entre eux emp\u00eachent le neurone de s'activer).</p>  <p>Anatomie d'un neurone</p>  <p>source</p>  <p>Ainsi, les neurones biologiques individuels semblent se comporter de mani\u00e8re assez simple, mais ils sont organis\u00e9s en un vaste r\u00e9seau de plusieurs milliards, chaque neurone \u00e9tant g\u00e9n\u00e9ralement connect\u00e9 \u00e0 des milliers d'autres neurones. Des calculs tr\u00e8s complexes peuvent \u00eatre effectu\u00e9s par un r\u00e9seau de neurones assez simples, de la m\u00eame fa\u00e7on que d'une fourmili\u00e8re peut \u00e9merger les efforts combin\u00e9s de simples fourmis.</p> <p>L'architecture des r\u00e9seaux de neurones biologiques (BNN) fait toujours l'objet de recherches actives, mais certaines parties du cerveau ont \u00e9t\u00e9 cartographi\u00e9es et il semble que les neurones sont souvent organis\u00e9s en couches cons\u00e9cutives, sp\u00e9cialement dans le cortex c\u00e9r\u00e9bral (la couche externe de votre cerveau).</p>"},{"location":"deep_learning/module1/Module1/#le-neurone-de-mcculloch-pitts-et-le-perceptron","title":"Le neurone de McCulloch-Pitts et le perceptron","text":"<p>L'id\u00e9e premi\u00e8re de laquelle d\u00e9coule l'invention des neurones artificiels est la volont\u00e9e d'avoir un algorithme de classification binaire.</p>"},{"location":"deep_learning/module1/Module1/#le-neurone-de-mcculloch-pitts","title":"Le neurone de McCulloch-Pitts","text":"<p>Le premier article scientifique mod\u00e9lisant de fa\u00e7on math\u00e9matique un neurone biologique a \u00e9t\u00e9 r\u00e9dig\u00e9 en 1943 par le neurobiologiste Warren McCulloch et le math\u00e9maticien Walter Pitts.</p>  <p>McCulloch (droite) et Pitts (gauche) en 1949</p>  <p>source</p> <p>En 1943, le neurophysiologiste et cybern\u00e9ticien am\u00e9ricain Warren McCulloch, de l'universit\u00e9 de l'Illinois \u00e0 Chicago, et l'autodidacte Walter Pitts, logicien et psychologue cognitif, ont publi\u00e9 \"A Logical Calculus of the ideas Imminent in Nervous Activity\", qui d\u00e9crit le \"neurone McCulloch-Pitts\", premier mod\u00e8le math\u00e9matique d'un r\u00e9seau de neurones.</p> <p>S'appuyant sur les id\u00e9es contenues dans l'ouvrage d'Alan Turing \"On Computable Numbers\", l'article de McCulloch et Pitts a permis de d\u00e9crire les fonctions c\u00e9r\u00e9brales en termes abstraits, et a montr\u00e9 que de simples \u00e9l\u00e9ments connect\u00e9s dans un r\u00e9seau neuronal peuvent avoir une immense puissance de calcul. Le document a re\u00e7u peu d'attention jusqu'\u00e0 ce que ses id\u00e9es soient appliqu\u00e9es par John von Neumann, Norbert Wiener et d'autres.</p>  <p>Le neurone de McCulloch-Pitts est simple : Le neurone correspond \u00e0 une fonction ayant une ou plusieurs entr\u00e9e binaires et une sortie binaire (0 ou 1). Le neurone ne s'active (produit une sortie) que si le nombre d'entr\u00e9e active d\u00e9passe un certain seuil.</p> <p>On rappelle que la fonction de Heaviside \\(H\\) est d\u00e9finie par :</p> \\[     \\begin{array}{ccccc} H &amp; : &amp; \\mathbf{R} &amp; \\to &amp; [0,1] \\\\ &amp; &amp; x &amp; \\mapsto &amp; H(x) \\\\ \\end{array} \\] <p>avec</p> \\[     H(x) = \\begin{cases} 1 &amp; x \\geq 0, \\\\                          0 &amp; x &lt; 0.\\end{cases} \\]   <p>D\u00e9finition</p> <p>Un neurone de McCulloch-Pitts est donn\u00e9 par :</p> <ol> <li>des entr\u00e9es binaires \\((x_{1}, \\dots, x_{m})\\),</li> <li>un r\u00e9el \\(\\vartheta \\in \\mathbf{R}\\),</li> <li>une sortie \\(\\hat{y}\\), d\u00e9finie par l'\u00e9quation suivante.</li> </ol> \\[     \\hat{y} = H(\\sum_{i=1}^{m} x_{i} - \\vartheta), \\quad \\forall i \\in I, x_{i} \\in \\lbrace 0,1 \\rbrace \\] <p>o\u00f9 \\(H\\) est la fonction de Heaviside, et \\(\\vartheta\\) est le seuil.</p> <p>On dit que le neurone s'active si \\(\\sum_{i=1}^{m} x_{i} - \\vartheta \\geq 0\\).</p>  <p>Le neurone tel que d\u00e9fini par McCulloch et Pitts est consid\u00e9r\u00e9 comme une simple porte logique : il n'y a pas d'algorithme associ\u00e9 afin de l'entra\u00eener. Ils montr\u00e8rent cependant qu'un r\u00e9seau constitu\u00e9 des neurones formels de leur invention a la m\u00eame puissance de calcul qu'une machine de Turing, ie ce r\u00e9seau est capable de calculer toutes les propositions logiques.</p>  <p>Mod\u00e9lisons les portes logiques ET et OU via les neurones de McCulloch-Pitts.</p> <p>Quelle valeur de \\(\\vartheta\\) prendre ?</p>    <p>D\u00e9finition</p> <p>La fonction brisant la lin\u00e9arit\u00e9 \u00e0 la sortie du neurone, dans notre cas pour l'instant la fonction de Heaviside \\(H\\), sera appel\u00e9e fonction d'activation du neurone.</p>   <p>Attention</p> <p>Le neurone de McCulloch Pitts poss\u00e8de les limites suivantes :</p> <ol> <li>Impossibilit\u00e9 de fournir des entr\u00e9es non bool\u00e9ennes.</li> <li>Le seuil doit toujours \u00eatre d\u00e9fini manuellement.</li> <li>Toutes les entr\u00e9es sont \u00e9galement importante, on ne peut pas assigner une importance plus grande \u00e0 certaines entr\u00e9es.</li> </ol>"},{"location":"deep_learning/module1/Module1/#le-perceptron","title":"Le Perceptron","text":"<p>En 1958, puis 1962, Frank Rosenblatt g\u00e9n\u00e9ralise les travaux de McCulloch et Pitts en d\u00e9veloppant le Perceptron. Le Perceptron de Rosenblatt est essentiellement un neurone de McCulloch-Pitts, o\u00f9 les entr\u00e9es \\((x_{1}, \\dots, x_{m})\\) peuvent cette fois ci prendre des valeurs r\u00e9elles. De plus, chaque entr\u00e9e est maintenant pond\u00e9r\u00e9e, le poids \\(w_{i}\\) \u00e9tant lui aussi \u00e0 valeur r\u00e9elle. Un poids positif (\\(w_{i} &gt; 0\\)) refl\u00e9tant une synapse excitatrice, tandis qu'un poids n\u00e9gatif (\\(w_{i} &lt; 0\\)) repr\u00e9sente lui une synapse inhibitrice.</p>  <p>Frank Rosenblatt</p>  <p>En novembre 1958, Frank Rosenblatt a invent\u00e9 le Perceptron, ou Mark I, \u00e0 l'universit\u00e9 de Cornell. Achev\u00e9 en 1960, c'\u00e9tait le premier ordinateur capable d'apprendre de nouvelles comp\u00e9tences par essais et erreurs, en utilisant une sorte de r\u00e9seau neuronal qui simulait les processus de la pens\u00e9e humaine.</p> <p>source</p>   <p>D\u00e9finition</p> <p>Un Perceptron est donn\u00e9 par :</p> <ul> <li>des entr\u00e9es \\((x_{1}, \\dots, x_{m}) \\in \\mathbf{R}^{m}\\),</li> <li>des poids \\((w_{1}, \\dots, w_{m}) \\in \\mathbf{R}^{m}\\),</li> <li>un r\u00e9el \\(\\vartheta \\in \\mathbf{R}\\),</li> <li>une sortie \\(\\hat{y}\\), d\u00e9finie par l'\u00e9quation suivante.</li> </ul> \\[     \\hat{y} = H(\\sum_{i=1}^{m} w_{i}x_{i} - \\vartheta), \\forall i \\in I, (w_{i}, x_{i}) \\in \\mathbf{R}^{2} \\] <p>o\u00f9 \\(H\\) est la fonction de Heavyside, et \\(\\vartheta\\) est le seuil.</p> <p>On dit que le neurone s'active si \\(\\sum_{i=1}^{m} w_{i}x_{i} - \\vartheta &gt; 0\\).</p>   <p>Exemple</p> <p>Mod\u00e9lisons la porte logique A et (non B) via le Perceptron.</p> <p>Quelle valeur de \\(\\vartheta\\) prendre ?</p>   <p>Le Perceptron, contrairement au neurone de McCulloch-Pitts, est lui muni d'un algorithme d'entra\u00eenement afin de trouver les poids optimaux pour la pr\u00e9diction.</p> <p>La r\u00e8gle d'apprentissage du Perceptron prend en compte l'erreur faite durant la pr\u00e9diction, et modifie les poids du neurone afin de r\u00e9duire l'erreur. Plus pr\u00e9cis\u00e9ment, le Perceptron re\u00e7oit une observation \u00e0 la fois (ie la batchsize = 1) et sort une pr\u00e9diction \\(\\hat{y}\\). Pour chaque mauvaise pr\u00e9diction, les poids sont chang\u00e9s en renfor\u00e7ant ceux qui auraient contribu\u00e9 le plus \u00e0 une pr\u00e9diction correcte.</p> <p>Ainsi, pour passer de l'\u00e9tape \\(k\\) \u00e0 l'\u00e9tape \\(k+1\\), on mets \u00e0 jour les poids via la formule suivante.</p> \\[     w_{i}^{k+1} = w_{i}^{k} + \\eta (y - \\hat{y})x_{i} \\] <p>o\u00f9 :</p> <ul> <li>\\(w_{i}\\) est le poids de la connexion \\(i\\),</li> <li>\\(x_{i}\\) est la valeur d'entr\u00e9e de la connexion \\(i\\),</li> <li>\\(\\hat{y}\\) est la pr\u00e9diction obtenue par \\(H(\\sum_{i=1}^{m} w_{i}x_{i} - \\vartheta)\\),</li> <li>\\(y\\) est la cible de la pr\u00e9diction,</li> <li>\\(\\eta\\) est le taux d'appentissage.</li> </ul>  <p>Remarque</p> <ol> <li>Pour le neurone de McCulloch-Pitts, comme pour le Perceptron, la sortie \\(\\hat{y}\\) est binaire.</li> <li>Un neurone de McCulloch-Pitts est un Perceptron o\u00f9 tous les poids sont \u00e9gaux \u00e0 1.</li> </ol> \\[     w_{1} = \\cdots = w_{n} = 1 \\]  <p>Sch\u00e9ma g\u00e9n\u00e9ral d'un neurone de McCulloch-Pitts (haut) et d'un Perceptron (bas)</p>  <p>Un des premiers r\u00e9sultats li\u00e9 au Perceptron est qu'il est capable de mod\u00e9liser et de r\u00e9soudre des probl\u00e8mes o\u00f9 les donn\u00e9es sont lin\u00e9airement s\u00e9parables.</p>  <p>D\u00e9finition</p> <ol> <li>Une fonction binaire</li> </ol> \\[ \\hat{y} \\, : \\, \\mathbf{R}^{n} \\longrightarrow \\lbrace 0,1 \\rbrace \\] <p>est dite Perceptron calculable s'il existe un seuil \\(\\vartheta\\) et des poids \\((w_{1}, \\dots, w_{n}) \\in \\mathbf{R}^{n}\\) tels que l'hyperplan d'\u00e9quation</p> \\[ \\sum_{i=1}^{n} w_{i}x_{i} = \\vartheta \\] <p>divise l'espace \\(\\mathbf{R}^{n}\\) en deux regions</p> \\[ \\mathbf{R}^{n} = R_{0} \\bigcup R_{1} = \\lbrace \\hat{y} =0 \\rbrace \\bigcup \\lbrace \\hat{y}=1 \\rbrace \\] <ol> <li>Un ensemble de points \\((x_{1}, \\dots, x_{n}) \\in \\mathbf{R}^{n}\\) pouvant \u00eatre s\u00e9par\u00e9s par une fonction Perceptron calculabe est dit lin\u00e9airement s\u00e9parable.</li> </ol>   <p> Ensemble de points lin\u00e9airement s\u00e9parables</p>    <p>Ensemble de points non lin\u00e9airement s\u00e9parables</p>    <p>Remarque</p> <ol> <li> <p>Lin\u00e9airement ind\u00e9pendant \\(\\implies\\) Lin\u00e9airement s\u00e9parable.</p> </li> <li> <p>La r\u00e9ciproque est fausse : les points \\(\\lbrace (0,0), (1,0), (0,1) \\rbrace\\) sont lin\u00e9airement s\u00e9parables dans \\(\\mathbf{R}^{2}\\), mais ne sont pas lin\u00e9airement ind\u00e9pendants.</p> </li> </ol>  <p>Cette propri\u00e9t\u00e9 de s\u00e9parabilit\u00e9 lin\u00e9aire permet au Perceptron de r\u00e9soudre certains probl\u00e8mes de classification binaire.</p>  <p>Th\u00e9or\u00e8me de convergence du Perceptron</p> <p>Etant donn\u00e9 un probl\u00e8me de classification binaire avec des classes lin\u00e9airement s\u00e9parables, si une solution \\((\\vartheta^{\\ast}, w_{1}^{\\ast}, \\dots, w_{n}^{\\ast}) \\in \\mathbf{R}^{n+1}\\) existe, alors l'algorithme du Perceptron trouvera cette solution en un nombre fini \\(h_{\\mathrm{max}}\\) d'it\u00e9rations.</p>  <p>En d'autres termes, si on a un ensemble de points que l'on sait lin\u00e9airement s\u00e9parable, et qu'en plus on sait qu'une solution existe, alors le Perceptron la trouvera.</p> <p>Conceptuellement c'est un r\u00e9sultat important. Cependant, ce r\u00e9sultat a deux difficult\u00e9es :</p> <ol> <li>Il est n\u00e9c\u00e9ssaire de savoir qu'une  solution \\((\\vartheta^{\\ast}, w_{1}^{\\ast}, \\dots, w_{n}^{\\ast}) \\in \\mathbf{R}^{n+1}\\) existe. En effet, il existe des probl\u00e8mes pour lesquels aucune solution par le Perceptron n'existe.</li> <li>La seconde diffcult\u00e9e est que, m\u00eame si l'on sait que le Perceptron trouvera une solution en un nombre fini d'it\u00e9rations, il nous est impossible de calculer \\(h_{\\mathrm{max}}\\) car il d\u00e9pend du vecteur de solution \\((\\vartheta^{\\ast}, w_{1}^{\\ast}, \\dots, w_{n}^{\\ast}) \\in \\mathbf{R}^{n+1}\\), qui nous est inconnu.</li> </ol>  <p>Attention</p> <p>La fonction XOR n'est pas Perceptron calculabe, sa table de v\u00e9rit\u00e9 \u00e9tant la suivante.</p>     \\(x_{1}\\) \\(x_{2}\\) \\(x_{1} \\oplus x_{2}\\) couleur     \\(0\\) \\(0\\) \\(0\\) rouge   \\(0\\) \\(1\\) \\(1\\) vert   \\(1\\) \\(0\\) \\(1\\) vert   \\(1\\) \\(1\\) \\(0\\) rouge     <p>Le probl\u00e8me de la fonction XOR a rapidement montr\u00e9 les limitations du Perceptron. Pour avoir plus de fl\u00e9xibilit\u00e9, l'id\u00e9e est alors d'empiler de fa\u00e7on hi\u00e9rarchique et en plusieurs couches des Perceptrons.</p>  <p>Et les neurosciences dans tout \u00e7a ?</p> <p>De nouvelles recherches en neuroscience ont d\u00e9montr\u00e9es que les dendrites des neurones pyramidaux du n\u00e9ocortex (la couche de substance grise particuli\u00e8rement d\u00e9velopp\u00e9e chez les mammif\u00e8res et qui forme la paroi des h\u00e9misph\u00e8res c\u00e9r\u00e9braux) sont en fait capables de classifier des entr\u00e9es non lin\u00e9airement s\u00e9parabes. En d'autres termes, les dendrites sont capables de calculer la fonction XOR, et le cerveau est (encore une fois) bien plus complexe que nous le pensions.</p> <p>Concernant la fonction XOR, il faut un r\u00e9seau de neurones denses \u00e0 2 couches pour pouvoir la calculer.</p> <p>Dendritic action potentials and computation in human layer 2/3 cortical neurons, Albert Gidon, Timothy Adam Zolnik, Pawel Fidzinski, Felix Bolduan, Athanasia Papoutsi, Panayiota Poirazi, Martin Holtkamp, Imre Vida, Matthew Evan Larkum</p>   <p>R\u00e9captulatif</p> <p>R\u00e9capitulatif et suite</p>"},{"location":"deep_learning/module1/Module1/#generalisation-les-reseaux-de-neurones-denses","title":"Gen\u00e9ralisation : Les r\u00e9seaux de neurones denses","text":"<p>On peut donc r\u00e9sumer la partie pr\u00e9c\u00e9dente dans le diagramme suivant.</p>  <p>avec \\(b = -\\vartheta\\) et la fonction de Heaviside \\(H\\) \u00e9tant ici la fonction d'activation \\(f\\).</p>  <p>Attention</p> <p>Le Perceptron ne peut faire que de la classification binaire.</p>  <p>Pour r\u00e9soudre le probl\u00e8me de la fonction XOR, l'id\u00e9e est d'empiler de fa\u00e7on hi\u00e9rarchique en plusieurs couches des Perceptrons succ\u00e9ssifs. On parle alors de Perceptron Multicouche (MLP), premier exemple de r\u00e9seau de neurones artificiels (ANN).</p> <p>Pour avoir une notion plus int\u00e9r\u00e9ssante, il est n\u00e9c\u00e9ssaire de modifier la d\u00e9finition du Perceptron.</p> <p>Avant toute modification, posons une d\u00e9finition g\u00e9n\u00e9rale, qui nous sera utile dans toute la suite de la formation. c'est celle de graphe acyclique orient\u00e9.</p> <p>Structurellement, un MLP, et donc un ANN, est un graphe orient\u00e9 acyclique (**D**irected **A**cyclic **G**raph : DAG).</p>  <p>D\u00e9finition</p> <ol> <li>Un graphe, est une collection \\(G = (S,A)\\) o\u00f9 \\(S\\) correspond \u00e0 la collection des sommets et \\(A\\) correspond \u00e0 la collection des ar\u00eates.</li> <li>Un graphe est dit orient\u00e9 lorsque chacune des ar\u00eates poss\u00e8de une orientation.</li> <li>Un graphe orient\u00e9 est dit acyclique s'il n'y a aucune boucles.</li> </ol>    <p>D\u00e9finition</p> <p>Un Perceptron Multicouche est un DAG o\u00f9 chaque sommets est un Perceptron.</p>  <p>Les neurones correspondent aux sommets et les dendrites et axones correspondent aux ar\u00eates du graphe.</p> <p>Les neurones, ou sommets, sont organis\u00e9s en couches successives reli\u00e9es entres elles par les ar\u00eates, les MLP poss\u00e8dent un point de d\u00e9part, la couche d'entr\u00e9e, et un point d'arriv\u00e9e, la couche de sortie, les couches interm\u00e9diaires sont elles appel\u00e9es les couches cach\u00e9es.</p> <p>G\u00e9n\u00e9raliser la m\u00e9thode d'aprentissage du Perceptron \u00e0 un MLP \u00e0 plusieurs couches cach\u00e9es est compliqu\u00e9e, en partie d\u00fb au nombres importants de param\u00e8tres pr\u00e9sents dans les r\u00e9seaux de neurones.</p> <p>Le travail r\u00e9volutionnaire permettant d'entrainer des ANN avec un nombre quelconque de couches cach\u00e9es en un temps fini, date de 1986. Dans l'article Learning internal representations by error propagations, David Rumelhart, Geoffrey Hinton et Ronald Williams introduise l'algorithme de r\u00e9tropropagation pour l'entra\u00eenement.</p> <p>Cependant, pour que cette algorithme fonctionne, il est n\u00e9c\u00e9ssaire de faire des changements dans la d\u00e9finition du MLP. Voyons les changements \u00e0 \u00e9ffectuer points par points.</p>"},{"location":"deep_learning/module1/Module1/#fonction-dactivation-et-etape-feedforward","title":"Fonction d'activation et \u00e9tape feedforward","text":"<p>Les fonctions d'activations utilis\u00e9es dans le Perceptron sont des fonctions de Heaviside.</p> <p>Cette fonction n'est pas adapt\u00e9e \u00e0 l'algorithme de r\u00e9tropropagation, le point cl\u00e9 de cette algorithme, qui sera d\u00e9taill\u00e9 plus tard, est l'utilisation de la descente du gradient. La fonction de Heaviside \u00e9tant constante par morceaux (et poss\u00e8de donc une d\u00e9riv\u00e9e nulle en tout point), une telle technique ne marche pas dessus.</p> <p>Il est donc n\u00e9c\u00e9ssaire de remplacer ces fonctions de Heaviside par une autre fonction, la fonction d'activation choisie par Rumelhart, Hinton, et Williams pour la remplacer est la fonction sigmo\u00efde (logistique).</p>  <p>D\u00e9finition : fonction logistique</p> \\[     \\begin{array}{ccccc} \\sigma &amp; : &amp; \\mathbf{R} &amp; \\to &amp; [0,1] \\\\ &amp; &amp; x &amp; \\mapsto &amp; \\sigma(x) \\\\ \\end{array} \\] \\[     \\sigma(x) := \\frac{1}{1 + \\exp(-x)} \\]   <p>La fonction logistique poss\u00e8de, au contraire de la fonction de Heaviside, une d\u00e9riv\u00e9e bien d\u00e9finie et non nulle en tout point.</p> <p>L'algorithme de r\u00e9tropropagation fonctionne avec de nombreuses autres fonctions d'activations. Avant de d\u00e9finir les autres fonctions, il est utile de distinguer deux types de fonctions d'activations :</p> <ol> <li>Les fonctions d'activations uniquement pr\u00e9sentes en sortie de couches cach\u00e9es,</li> <li>Les fonctions d'activations pouvant \u00eatre pr\u00e9sentes aussi en en sortie du DAG.</li> </ol> <p>Le deuxi\u00e8me type de fonction d'activation est tr\u00e8s restreint et d\u00e9pend de la probl\u00e9matique que l'on souhaite r\u00e9soudre.</p> <ol> <li>Dans le cas d'un probl\u00e8me de r\u00e9gression lin\u00e9aire, aucune fonction d'activation en sortie n'est demand\u00e9e.</li> <li>Dans le cas d'un probl\u00e8me de classification binaire, la fonction d'activation en sortie sera la fonction logistique</li> </ol>  <p>Et dans le cas d'une classification multinomiale ?</p> <p>Pour une classification multinomiale, la fonction d'activation privil\u00e9gi\u00e9e en sortie du r\u00e9seau est la fonction softmax</p> \\[     \\begin{array}{ccccc}     \\mathrm{softmax} &amp; : &amp; \\mathbf{R}^{k} &amp; \\to     &amp; [0,1]^{k} \\\\                     &amp;   &amp; x              &amp; \\mapsto &amp; \\mathrm{softmax}(x) \\\\     \\end{array} \\] \\[\\mathrm{softmax}(x) := (\\frac{\\exp(x_{1})}{\\sum_{i=1}^{k}\\exp(x_{i})}, \\dots, \\frac{\\exp(x_{k})}{\\sum_{i=1}^{k}\\exp(x_{i})})\\]   <p>Remarque</p> <p>Dans le cas d'un probl\u00e8me de classification binaire, il est aussi possible d'utiliser en sortie la fonction softmax \u00e0 condition d'avoir modifier la cible \\(\\mathbf{y}_{i}\\) par un One-hot Encoding, ce cas l\u00e0 sera tra\u00eet\u00e9 en TP.</p>  <p>Dans le cas des  fonctions d'activations uniquement pr\u00e9sentes en sortie de couches cach\u00e9es, on a un plus grand choix possibles. Dans la pratique cependant, 2 fonctions d'activations sont plus utilis\u00e9es que les autres.</p> <p>La fonction d'activation devenue un standard est la fonction \\(\\mathrm{ReLU}\\) : **Re**ctified **L**inear **U**nit, d\u00e9finie de la fa\u00e7on suivnate.</p>  <p>D\u00e9finition : fonction ReLU</p> \\[     \\begin{array}{ccccc} \\mathrm{ReLU} &amp; : &amp; \\mathbf{R} &amp; \\to     &amp; \\mathbf{R} \\\\                                     &amp;   &amp; x              &amp; \\mapsto &amp; \\mathrm{ReLU}(x) \\\\     \\end{array} \\] \\[\\mathrm{ReLU}(x) := \\max(0,x)\\]    <p>Remarque</p> <ol> <li> <p>La fonction \\(\\mathrm{ReLU}\\) n'est utilis\u00e9e qu'en sortie des couches cach\u00e9es, et non pas en sortie du  r\u00e9seau.</p> </li> <li> <p>La fonction \\(\\mathrm{ReLU}\\) n'est pas diff\u00e9rentiable en 0, et sa d\u00e9riv\u00e9e est nulle pour \\(x&lt;0\\). Dans la pratique cependant, elle fonctionne tr\u00e8s bien et surtout, sa d\u00e9riv\u00e9e est tr\u00e8s rapide \u00e0 calculer.</p> </li> </ol>  <p>Un deuxi\u00e8me choix, aussi tr\u00e8s courant, est la fonction tangente hyperbolique, \\(\\tanh\\).</p>  <p>D\u00e9finition : fonction \\(\\tanh\\)</p> \\[     \\begin{array}{ccccc} \\tanh         &amp; : &amp; \\mathbf{R} &amp; \\to     &amp; [-1,1] \\\\                                     &amp;   &amp; x          &amp; \\mapsto &amp; \\frac{\\mathrm{e}^{x}-\\mathrm{e}^{-x}}{\\mathrm{e}^{x}+\\mathrm{e}^{-x}} \\\\     \\end{array} \\]    <p>Remarque</p> <p>Les fonctions d'activations doivent poss\u00e9der les caract\u00e9ristiques suivantes.</p> <ul> <li>La fonction doit \u00eatre continue et d\u00e9finie partout,</li> <li>La fonction doit \u00eatre monotone,</li> <li>La fonction ne doit pas \u00eatre lin\u00e9aire,</li> <li>La fonction, et ses d\u00e9riv\u00e9es, doit \u00eatre facilement calculable.</li> </ul>  <p>Le parcours complet d'une observation \\(\\mathbf{x}_{i}\\) \u00e0 travers le DAG se nomme dans le jargon l'\u00e9tape de feedforward. Pour que cela soit plus clair, voyons cela sur un exemple.</p>   <p>Exemple d'\u00e9tape feedforward</p>  <p>On a deux entr\u00e9es \\(x_{1}, x_{2}\\), et une sortie \\(\\hat{y}\\). On peut donc supposer que les observations du dataset sont de la forme \\((x_{1}, x_{2}, y)\\).</p> <ol> <li> <p>Etape 1 : Les features de l'observation \\((x_{1}, x_{2})\\) sont pass\u00e9es en entr\u00e9e, chacune de ces features est envoy\u00e9e \u00e0 chaucn des neurones \\(h_{1}, h_{2}\\) de l'unique couche cach\u00e9e. Les connexions \u00e9tant pond\u00e9r\u00e9es, la r\u00e8gle des noeuds s'applique et au niveau de la couche cach\u00e9e on se retrouve avec les valeurs \\(z_{1}, z_{2}\\) d\u00e9fini\u00e9es par l'\u00e9quation \\((1)\\), ou de fa\u00e7on \u00e9quivalente par l'\u00e9quation \\((2)\\) sous forme matricielle. La matrice \\(2 \\times 2\\) de l'\u00e9quation \\((2)\\) est la matrice de poids de la couche cach\u00e9e.</p> </li> <li> <p>Etape 2 : En sortie de la couche cach\u00e9e, la fonction d'activation \\(\\sigma^{1}\\) s'applique, on est alors \u00e0 l'\u00e9quation \\((3)\\) avec les valeurs \\((y_{1}, y_{2})\\).</p> </li> <li> <p>Etape 3 : En arrivant au neurone de sortie, une nouvelle loi des noeuds s'applique \u00e0 l'int\u00e9rieur du neurone rouge, puis la fonction d'activation \\(\\sigma^{2}\\). D'o\u00f9 l'\u00e9quation \\((4)\\).</p> </li> </ol>  <p>Le vecteur \\(\\hat{y}\\) que l'on a en sortie de l'\u00e9tape de feedforward pour l'observation \\(x_{1}, x_{2}\\) est alors la cible pr\u00e9dite (ou simplement pr\u00e9diction) par l'ANN. Comment alors mesurer l'erreur faite en pr\u00e9disant \\(\\hat{y}\\) par rapport \u00e0 la cible \\(y\\) ?</p>  <p>Exemple</p>  <p>Un r\u00e9seau de neurones dense avec 3 couches cach\u00e9es, 5 entr\u00e9es et 3 sorties, on peut supposer que l'on est dans le cas d'un probl\u00e8me de classification avec 3 classes distinctes.</p>"},{"location":"deep_learning/module1/Module1/#fonction-de-perte","title":"Fonction de perte","text":"<p>La fonction de perte est l\u00e0 pour calculer l'erreur obtenue entre la pr\u00e9diction et la cible. Elle est traditionnelement not\u00e9e \\(\\mathcal{L}_{\\theta}\\).</p> <p>Suivant le but du r\u00e9seau de neurone on a plusieurs fonctions de pertes standards.</p>    But Fonction de pertes Activation     R\u00e9gression Erreur Moyenne Absolue (MAE) aucune   R\u00e9gression Erreur Moyenne Quadratique (MSE) aucune   Classification Binomiale Entropie Crois\u00e9e Binomiale  (BCE) sigmo\u00efde   Classification Binomiale (One Hot Encoding) Entropie Crois\u00e9e Cat\u00e9gorielle (CCE) softmax   Classification multinomiale Entropie Crois\u00e9e Cat\u00e9gorielle Eparse (SCCE) softmax   Classification multinomiale (One Hot Encoding) Entropie Crois\u00e9e Cat\u00e9gorielle (CCE) softmax    <p>Dans le cadre des probl\u00e8mes de classification, les noms des fonctions de pertes peut \u00eatre diff\u00e9rents mais la formule est fondamentalement la m\u00eame. les modifications apport\u00e9es ne sont l\u00e0 que pour prendre en compte la forme des pr\u00e9dictions et cibles : est ce que la classe est repr\u00e9sent\u00e9e par un vecteur ou simplement par un nombre ?</p> \\[     MAE := \\frac{1}{N}\\sum_{i=1}^{N} ||y_{i} - \\hat{y}_{i}||_{1} \\] \\[     MSE := \\frac{1}{N}\\sum_{i=1}^{N} ||y_{i} - \\hat{y}_{i}||^{2}_{2} \\] \\[     CE := -\\frac{1}{N}\\sum_{i=1}^{N} \\langle y_{i}, \\log(\\hat{y}_{i}) \\rangle \\] <p>Le nombre \\(N\\) pr\u00e9sent dans les formules ci dessus est la taille du minibatch d'observations, pour l'instant on peut supposer que \\(N=32\\). Sa d\u00e9finition sera claire par la suite.</p>  <p>Remarque</p> <p>Dans le jargon, elle est appel\u00e9e loss function et est traditionnelement not\u00e9e \\(\\mathcal{L}_{\\theta}\\), o\u00f9 \\(\\theta\\) repr\u00e9sente les poids et biais du r\u00e9seau.</p>  <p>On a fait une \u00e9tape de feedforward, on a obtenu une pr\u00e9diction \\(\\hat{y}\\) dont on a calcul\u00e9e l'erreur \\(\\mathcal{L}_{\\theta}(\\hat{y})\\) gr\u00e2ce \u00e0 a fonction de perte \\(\\mathcal{L}_{\\theta}\\). La question qui se pose maintenant est la suivante.</p> <p>Comment minimiser cette erreur ?</p>"},{"location":"deep_learning/module1/Module1/#descente-du-gradient-stochastique","title":"Descente du gradient stochastique","text":"<p>De fa\u00e7on succinte, on utilise la methode de la technique du gradient coupl\u00e9e \u00e0 une m\u00e9thode efficace pour calculer automatiquement le gradient.</p>  <p>Descente du gradient</p> <p>La m\u00e9thode de la descente du gradient est un algorithme d'optimisation permettant de trouver le minimum d'une fonction \\(f\\). Pour simplifier l'explication, supposons que l'on consid\u00e8re la fonction d'une seule variable.</p> \\[     \\begin{array}{ccccc}     C &amp; : &amp; \\mathbf{R} &amp; \\to     &amp; \\mathbf{R} \\\\       &amp;   &amp; w          &amp; \\mapsto &amp; C(w) \\\\     \\end{array} \\] <p>La m\u00e9thode pour trouver un minimum de \\(C\\) est alors d'appliquer l'algorithme suivant :</p> <p>Initialisation</p> <ol> <li>Choisir un point de d\u00e9part \\(w \\in \\mathrm{dom}(C)\\).</li> <li>Choisir un pas \\(\\eta\\) \"tr\u00e8s petit\", et r\u00e9p\u00e9ter :<ol> <li>Calculer \\(C'(w)\\)</li> <li>Mettre \u00e0 jour \\(w := w - \\eta C'(w)\\)</li> </ol> </li> </ol> <p>Si \\(w\\) est un minimum de \\(C\\), alors \\(C'(w) = 0\\) et l'\u00e9tape 2 de la phase de r\u00e9p\u00e9tition reste bloqu\u00e9e sur \\(w\\).</p>  <p>Exemple de descente du gradient (mlfromscratch)</p>  <p>La descente du gradient se g\u00e9n\u00e9ralise de la m\u00eame fa\u00e7on \u00e0 une fonction de plusieurs variables. Pour une fonction</p> \\[f \\, : \\, \\mathbf{R}^{k} \\longrightarrow \\mathbf{R}\\] <p>Les \u00e9tapes 2.a et 2.b ci dessus sont alors remplac\u00e9es par les \u00e9tapes suivantes.</p> <ol> <li>Calculer \\(\\nabla f := ( \\frac{\\partial f}{\\partial w_{1}}, \\dots, \\frac{\\partial f}{\\partial w_{k}})\\)</li> <li>Mettre \u00e0 jour \\(w := w - \\eta \\nabla f\\)</li> </ol> <p>Le but de la descente du gradient \u00e9tant de d\u00e9terminer le minimum d'une fonction, la question que l'on peut se poser de fa\u00e7on l\u00e9gitime est alors la suivante.</p>  <p>Question</p> <p>Si l'on utilise la descente du gradient ici, quelle fonction souhaite-t-on minimiser ?</p>  <p>Dans le cas du Deep Learning, la fonction que l'on cherche \u00e0 minimiser est la fonction de perte moyenne totale.</p> \\[     \\mathcal{L}_{\\theta}^{\\mathrm{tot}} := \\frac{1}{n} \\sum_{i=1}^{n} \\mathcal{L}_{\\theta}(\\hat{y}_{i}) \\] <p>On rappelle que \\(n\\) est le cardinal de \\(\\mathcal{X}\\), ie le nombre total d'observations dans le dataset. On notera \\(\\theta\\) l'ensemble des param\u00e8tres du r\u00e9seau, on pose \\(p\\) son cardinal, (le nombre total de param\u00e8tres, poids et biais combin\u00e9s) il peut aller d'une dizaine \u00e0 plusieurs milliards pour les mod\u00e8les les plus r\u00e9cents.</p> <p>On a donc l'ensemble suivant,</p> \\[     \\theta := \\lbrace w_{1}, \\dots, w_{\\alpha}, b_{1}, \\dots, b_{\\beta} \\rbrace \\quad \\alpha + \\beta = p \\] <p>Reste alors \u00e0 savoir par rapport \u00e0 quelles variables l'on souhaite calculer le gradient.</p> <p>Rapellons nous que dans la fonction de perte, pour une observation donn\u00e9e \\((\\mathbf{x}_{i}, \\mathbf{y}_{i})\\), la pr\u00e9diction \\(\\hat{y}_{i}\\) est une combinaison des \u00e9l\u00e9ments de \\(\\mathbf{x}_{i}= (x_{i,1}, \\dots, x_{i,m})\\), des param\u00e8tres du r\u00e9seau, et des fonctions d'activations. La valeur de \\(\\mathbf{x}_{i}=(x_{i,1}, \\dots, x_{i,m})\\) \u00e9tant fixe, la seule chose qui peut varier dans la fonction de perte est la valeur des param\u00e8tres \\(\\theta\\).</p>  <p>Remarque</p> <p>Le but de l'algorithme de r\u00e9tropropagation du gradient est de trouver les param\u00e8tres \\(\\theta\\) optimaux pour minimiser la fonction de perte. Le gradient \u00e0 calculer est donc le suivant.</p> \\[     \\nabla_{\\theta} \\mathcal{L}_{\\theta}^{\\mathrm{tot}} := \\begin{pmatrix}     \\frac{\\partial \\mathcal{L}_{\\theta}^{\\mathrm{tot}}}{\\partial w_{1}} \\\\     \\frac{\\partial \\mathcal{L}_{\\theta}^{\\mathrm{tot}}}{\\partial w_{2}} \\\\     \\vdots  \\\\     \\frac{\\partial \\mathcal{L}_{\\theta}^{\\mathrm{tot}}}{\\partial w_{\\alpha}} \\\\     \\frac{\\partial \\mathcal{L}_{\\theta}^{\\mathrm{tot}}}{\\partial b_{1}} \\\\     \\vdots \\\\     \\frac{\\partial \\mathcal{L}_{\\theta}^{\\mathrm{tot}}}{\\partial b_{\\beta}}     \\end{pmatrix} \\]  <p>Calculer ce gradient se fait alors via l'algorithme dit d'auto-diff\u00e9rentiation inverse, c'est le choix fait par Tensorflow.</p>  <p>Exemple : Graphe de calcul et auto-diff\u00e9rentiation inverse</p> <p>On prend l'exemple de la fonction suivante.</p> \\[     f(x) := \\log(x) + \\sqrt{\\log(x)} \\] <p>On souhaite calculer sa d\u00e9riv\u00e9, son graphe de calcul est alors le suivant.</p>  <p>L'algorithme d'auto-diff\u00e9rentiation prend alors la forme suivante.</p> \\[     \\begin{align}     (1) \\quad \\frac{\\partial f}{\\partial f} &amp; = 1 \\\\     (2) \\quad \\frac{\\partial f}{\\partial z} &amp; = \\frac{\\partial f}{\\partial f} \\cdot \\frac{\\partial f}{\\partial z} =\\frac{\\partial f}{\\partial f}1 \\\\     (3) \\quad \\frac{\\partial f}{\\partial y} &amp; = \\frac{\\partial f}{\\partial z} \\cdot \\frac{\\partial z}{\\partial y} + \\frac{\\partial f}{\\partial f}\\cdot \\frac{\\partial f}{\\partial y} = \\frac{\\partial f}{\\partial z}\\cdot \\frac{1}{2\\sqrt{y}} + \\frac{\\partial f}{\\partial f}1 \\\\     (4) \\quad \\frac{\\partial f}{\\partial x} &amp; = \\frac{\\partial f}{\\partial y}\\cdot \\frac{\\partial y}{\\partial x} = \\frac{\\partial f}{\\partial y} \\cdot\\frac{1}{x}     \\end{align} \\]  <p>Le but de la descente du gradient dans l'algorithme de r\u00e9tropropagation est alors de minimiser cette fonction de perte, et donc de minimiser l'erreur moyenne faite durant la pr\u00e9diction. Cependant pour un dataset comprenant plusieurs millions d'observations calculer le gradient complet \\(\\nabla_{\\theta}\\mathcal{L}_{\\theta}\\) est prohibitf. L'id\u00e9e est alors d'\u00e9changer ce gradient complet pour un gradient approximatif mais plus simple \u00e0 calculer. C'est le principe du minibatch.</p> <p>De fa\u00e7on plus d\u00e9taill\u00e9e, voici comment fonctionne l'algorithme :</p>  <p>Algorithme de r\u00e9tropropagation</p> <ol> <li> <p>L'algorithme consid\u00e8re un minibatch de taile \\(N\\) \u00e0 la fois (par exemple, avec \\(N=32\\) observations \u00e0 chaque fois), lorsque l'on parle de mini-batch de taille \\(N\\) il faut comprendre s\u00e9lection de \\(N\\) observations par un tirage sans remise. Chaque passage du dataset complet s'appelle une \u00e9poque.</p> </li> <li> <p>Durant l'\u00e9tape de feedforward, \u00e0 chaque passage du mini-batch dans une couche du r\u00e9seau le r\u00e9sultat obtenu est conserv\u00e9 en m\u00e9moire.</p> </li> <li> <p>Une fois le mini-batch pass\u00e9 compl\u00e8tement dans l'ANN, la pr\u00e9diction est alors \u00e9valu\u00e9e avec la fonction de perte pour en d\u00e9duire l'erreur de pr\u00e9diction faite par rapport par rapport \u00e0 la cible.</p> </li> <li> <p>L'algorithme calcule alors la contribution de chacun des poids et biais dans le calcul de l'erreur obtenue par la fonction de perte.</p> </li> <li> <p>Une descente du gradient sur la fonction de perte est alors appliqu\u00e9e pour modifier les poids et les biais, et \u00e0 terme minimiser la fonction de perte.</p> </li> </ol>  <p>La mise \u00e0 jour des param\u00e8tres de l'ANN se faisant suite au passage du mini-batch, la technique de descente du gradient utilis\u00e9e ici est dite descente du gradient stochastique, stochastique faisant r\u00e9f\u00e9rence ici \u00e0 la mani\u00e8re al\u00e9atoire par laquelle sont s\u00e9lectionn\u00e9es les observations composants le mini-batch.</p>  <p>Exemple</p> <p>La m\u00e9thode du gradient est utilis\u00e9 pour optimiser les param\u00e8tres du r\u00e9seau de neurones. Les \u00e9tapes sont les suivantes.</p> <ol> <li>Comme on travaille en mini-batch de taille \\(N\\) (eg \\(N=32\\)) on a une valeur d'erreur pour chaque pr\u00e9dictions faite sur ce mini-batch.</li> </ol> \\[     \\mathcal{L}_{\\theta}(\\hat{y}_{1}), \\mathcal{L}_{\\theta}(\\hat{y}_{2}), \\dots, \\mathcal{L}_{\\theta}(\\hat{y}_{N}) \\] <ol> <li>La fonction que l'on va donc utiliser pour appliquer la m\u00e9thode du gradient est la fonction de perte moyenne sur ce mini-batch.</li> </ol> \\[     \\mathcal{L}_{\\theta} := \\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{L}_{\\theta}(\\hat{y}_{i}) \\] <ol> <li>Le gradient de cette fonction est alors d\u00e9fini par :</li> </ol> \\[     \\nabla_{\\theta} \\mathcal{L}_{\\theta} := \\begin{pmatrix}     \\frac{\\partial \\mathcal{L}_{\\theta}}{\\partial w_{1}} \\\\     \\frac{\\partial \\mathcal{L}_{\\theta}}{\\partial w_{2}} \\\\     \\vdots  \\\\     \\frac{\\partial \\mathcal{L}_{\\theta}}{\\partial w_{\\alpha}} \\\\     \\frac{\\partial \\mathcal{L}_{\\theta}}{\\partial b_{1}} \\\\     \\vdots \\\\     \\frac{\\partial \\mathcal{L}_{\\theta}}{\\partial b_{\\beta}}     \\end{pmatrix} \\] <ol> <li>La mise \u00e0 jour des param\u00e8tres se fait alors via la formule suivante :</li> </ol> \\[    w_{i} \\leftarrow w_{i} - \\eta \\frac{\\partial \\mathcal{L}_{\\theta}}{\\partial w_{i}}(\\theta) \\] \\[     b_{i} \\leftarrow b_{i} - \\eta \\frac{\\partial \\mathcal{L}_{\\theta}}{\\partial b_{i}}(\\theta) \\] <p>\\(\\eta\\) est ici un nombre r\u00e9el que l'on appelle le taux d'apprentissage, il n'est pas appris par l'algorithme et doit \u00eatre fix\u00e9 \u00e0 la main (c'est un hyperparam\u00e8tre).</p>   <p>Pourquoi choisir une m\u00e9thode stochastique ?</p> <p>Etant donn\u00e9e une fonction diff\u00e9rentiable, il est th\u00e9oriquement possible de trouver son minimum de fa\u00e7on purement analytique : une fonction \\(f : \\mathbf{R} \\rightarrow \\mathbf{R}\\) poss\u00e8de un extremum en un point \\(x\\) si sa d\u00e9riv\u00e9e \\(f'(x)\\) est nulle. Une fois trouv\u00e9 tous ces points on prend celui pour lequel \\(f(x)\\) est la plus petite valeur.</p> <p>Dans le cadre des r\u00e9seaux de neurones, cela revient \u00e0 devoir r\u00e9soudre \\(\\nabla \\mathcal{L}_{\\theta} = 0\\). C'est une \u00e9quation polynomiale en \\(p\\) variables, o\u00f9 \\(p\\) est le cardinal de \\(\\theta\\) ensemble des param\u00e8tres du r\u00e9seau.</p> <p>Premi\u00e8rement, si cela est faisable pour \\(p=2\\) ou \\(p=3\\), c'est difficilement r\u00e9alisable dans la pratique d'une r\u00e9seau de neurones o\u00f9 \\(p\\) d\u00e9passe facilement la centaine de milliers.</p> <p>Deuxi\u00e8mement, effectuer une descente du gradient classique supposerait d'avoir calcul\u00e9 les pr\u00e9dictions sur l'ensemble du dataset, et de garder en m\u00e9moire ces valeurs afin de calculer \\(\\nabla \\mathcal{L}_{\\theta}\\).</p> <p>Du point de vue de la complexit\u00e9 algorithmique, le co\u00fbt de calcul de \\(\\nabla \\mathcal{L}_{\\theta}=0\\) cro\u00eet alors de fa\u00e7on lin\u00e9aire avec la taille du dataset.</p> <p>Dans une m\u00e9thode stochastique, la taille du mini-batch \u00e9tant fix\u00e9e (eg \\(N=32\\)) et relativement petite par rapport \u00e0 la taille du dataset, la compl\u00e9xit\u00e9 est moindre et constante tout au long de l'algorithme. Les mises \u00e0 jour des param\u00e8tres \u00e9tant plus fr\u00e9quentes, l'algorithme converge plus rapidement vers un optimum.</p>   <p>Remarque</p> <p>La descente du gradient, stochastique ou non, est \u00e0 la base une m\u00e9thode d'optimisation convexe, hors les fonctions de pertes utilis\u00e9es dans la pratique ne sont pas convexes. On se retrouve donc g\u00e9n\u00e9ralement uniquement avec des minima locaux pour la fonction de perte. Dans la pratique ce n'est pas un soucis, et de nombreuses techniques d'optimisation ont \u00e9t\u00e9 introduites pour pouvoir converger vers un minimum global au lieu de \"rester coinc\u00e9 dans un minimum local\". M\u00e9thodes que l'on verra dans les modules suivants.</p>   <p>Sym\u00e9trie des poids</p>    <p>La descente du gradient stochastique, n'est qu'une des m\u00e9thodes d'optimisation afin de minimiser la fonction de perte. Il existe aujourd'hui de nombreux optimiseurs pour la descente du gradient.</p>  <p>Exemple</p> <p>D\u00e9finissons un exemple de l'\u00e9tape de r\u00e9tropropagation du gradient.</p>  \\[ \\begin{align}     \\frac{\\partial \\mathcal{L}_{\\vartheta}}{ \\partial w_{1,1}^{1}} =  &amp; \\frac{\\partial \\mathcal{L}_{\\vartheta}}{\\partial s} \\cdot \\frac{\\partial s}{\\partial h_{1}^{2}} \\cdot \\frac{\\partial h_{1}^{2}}{\\partial h_{1}^{1}} \\cdot \\frac{\\partial h_{1}^{1}}{\\partial w_{1,1}^{1}} \\\\                                                                     + &amp; \\frac{\\partial \\mathcal{L}_{\\vartheta}}{\\partial s} \\cdot \\frac{\\partial s}{\\partial h_{2}^{2}}     \\cdot \\frac{\\partial h_{2}^{2}}{\\partial h_{1}^{1}} \\cdot \\frac{\\partial h_{1}^{1}}{\\partial w_{1,1}^{1}} \\end{align} \\] \\[     w_{1,1}^{1} \\leftarrow w_{1,1}^{1} - \\eta \\frac{\\partial \\mathcal{L}_{\\vartheta}}{ \\partial w_{1,1}^{1}}. \\]  <p>Les neurones denses sont une g\u00e9n\u00e9ralisation tr\u00e8s puissante des MLP, car ce sont des approximateurs universels.</p>  <p>Th\u00e9or\u00e8me d'approximation universelle de Kolmogorov</p> <p>Toute fonction continue d\u00e9finie sur un compact \\(K \\subset \\mathbf{R}^{r}\\) peut \u00eatre uniform\u00e9ment approxim\u00e9e par un r\u00e9seau de neurones denses avec une couche cach\u00e9e.</p>"},{"location":"deep_learning/module1/Module1/#resume","title":"R\u00e9sum\u00e9","text":"<p>Math\u00e9matiquement, un MLP peut se repr\u00e9senter par la fonction suivante.</p> \\[     \\begin{array}{ccccc} f_{NN} &amp; : &amp; \\mathbf{R}^{m} &amp; \\to &amp; \\mathbf{R}^{k} \\\\ &amp; &amp; \\mathbf{x} &amp; \\mapsto &amp; f_{NN}(\\mathbf{x}) \\\\ \\end{array} \\] \\[     \\hat{\\mathbf{y}} := f_{NN}(\\mathbf{x}) = \\sigma^{r} \\circ \\cdots \\circ \\sigma^{1} (\\mathbf{x}) \\] <p>L'entier \\(m\\) d\u00e9pend du nombre de features dans le dataset, l'entier \\(k\\) d\u00e9pend lui du probl\u00e8me consid\u00e9r\u00e9.</p> <p>Le nombre de fonctions \\(\\sigma^{i}\\) d\u00e9pend de l'architecture du r\u00e9seau et correspond au nombre de couches cach\u00e9es.</p> \\[     \\mathbf{y}^{\\ell} := \\sigma^{\\ell}(\\mathbf{X}\\mathbf{W}_{\\ell} + \\mathbf{b}_{\\ell}) \\] <p>O\u00f9 \\(\\mathbf{W}_{\\ell}\\) correspond \u00e0 la matrice de poids de la couche \\(\\ell\\) et \\(\\mathbf{b}_{\\ell}\\) au vecteur de biais correspondant. On a</p> <ol> <li>Une ligne par features dans \\(\\mathbf{X}\\),</li> <li>Une colonne par neurones dans la couche cach\u00e9e,</li> <li>Autant de biais que de neurones dans la couche cach\u00e9e.</li> </ol> \\[     \\begin{equation*}     \\mathbf{W}_{\\ell} = \\begin{pmatrix}     w_{1,1}^{\\ell} &amp; w_{1,2}^{\\ell} &amp; \\cdots &amp; w_{1,r}^{\\ell} \\\\     w_{2,1}^{\\ell} &amp; w_{2,2}^{\\ell} &amp; \\cdots &amp; w_{2,r}^{\\ell} \\\\     \\vdots  &amp; \\vdots  &amp; \\ddots &amp; \\vdots  \\\\     w_{l,1}^{\\ell} &amp; w_{l,2}^{\\ell} &amp; \\cdots &amp; w_{l,r}^{\\ell}     \\end{pmatrix}\\end{equation*}     \\in \\mathcal{M}_{l,r}(\\mathbf{R}) \\quad \\mathbf{b}_{\\ell} = \\begin{pmatrix}b_{1}^{\\ell} &amp; b_{2}^{i} &amp; \\cdots &amp; b_{r}^{\\ell} \\end{pmatrix} \\] <p>La topologie du r\u00e9seau : Le nombres de neurones, de couches, de neurones par couches, et d'arr\u00eates d\u00e9pendent du probl\u00e8me consid\u00e9r\u00e9. Il existe toutefois des architectures connues et sp\u00e9cialis\u00e9es dans certains probl\u00e8mes. Certaines couches de neurones sont ainsi sp\u00e9cialis\u00e9es dans le traitement d'images, d'autres encore dans le traitement des s\u00e9ries temporelles.</p>  <p>Exemple</p>  <p>Un r\u00e9seau de neurones dense avec 3 couches cach\u00e9es, 5 entr\u00e9es et 3 sorties, on peut supposer que l'on est dans le cas d'un probl\u00e8me de classification avec 3 classes distinctes.</p> <ol> <li>La premi\u00e8re couche cach\u00e9e a pour matrice de poids une matrice de taille \\(5\\times 10\\),  \\(\\mathbf{W}_{1} \\in \\mathcal{M}_{5,10}(\\mathbf{R})\\),</li> <li>La deuxi\u00e8me couche cach\u00e9e a pour matrice de poids une matrice de taille \\(10\\times 10\\),  \\(\\mathbf{W}_{2} \\in \\mathcal{M}_{10,10}(\\mathbf{R})\\),</li> <li>La trois\u00e8me couche cach\u00e9e a pour matrice de poids une matrice de taille \\(10\\times 10\\),  \\(\\mathbf{W}_{3} \\in \\mathcal{M}_{10,10}(\\mathbf{R})\\),</li> <li>La couche de sortie a pour matrice de poids une matrice de taille \\(10\\times 3\\),  \\(\\mathbf{W}_{4} \\in \\mathcal{M}_{10,3}(\\mathbf{R})\\).</li> </ol> <p>Concernant les biais on a des vecteurs :</p> <ol> <li>\\(\\mathbf{b}_{1} \\in \\mathbf{R}^{10}\\) pour la premi\u00e8re couche cach\u00e9e,</li> <li>\\(\\mathbf{b}_{2} \\in \\mathbf{R}^{10}\\) pour la deuxi\u00e8me couche cach\u00e9e,</li> <li>\\(\\mathbf{b}_{3} \\in \\mathbf{R}^{10}\\) pour la troisi\u00e8me couche cach\u00e9e,</li> <li>\\(\\mathbf{b}_{4} \\in \\mathbf{R}^{3}\\) pour la couche de sortie.</li> </ol> <p>Ce qui nous fait un nombre total de param\u00e8tres \u00e9gal \u00e0</p> <p>\\(\\dim( \\mathcal{M}_{5,10}(\\mathbf{R})) + \\dim(\\mathbf{R}^{10}) + \\dim( \\mathcal{M}_{10,10}(\\mathbf{R})) + \\dim(\\mathbf{R}^{10}) +\\dim( \\mathcal{M}_{10,10}(\\mathbf{R})) + \\\\ \\dim(\\mathbf{R}^{10}) + \\dim( \\mathcal{M}_{10,3}(\\mathbf{R})) + \\dim(\\mathbf{R}^{3}) = 313\\)</p> <p>313 param\u00e8tres, ce qui est loin d'\u00eatre \u00e9norme.</p> <p>En python, avec tensorflow, un tel r\u00e9seau ce code de la mani\u00e8re suivante.</p> <pre><code>model = models.Sequential([\nInput(shape=(5,), name='Input'),\nDense(10),\nActivation('relu'),\nDense(10),\nActivation('relu'),\nDense(10),\nActivation('relu'),\nDense(3),\nActivation('softmax')\n], name='SeqAPI')\n</code></pre>"},{"location":"deep_learning/module1/tp1/","title":"TP Module 1 : Introduction au deep learning, prise en main de TensorFlow et Keras","text":"<p>Le but de ce tp est ici de se familiariser avec TensorFlow et son API haut niveau Keras. Le but \u00e9tant de vous familiariser avec TensorFlow, Keras, et aussi l'outil de travail Colab, n'h\u00e9sitez pas \u00e0 modifier le code que vous \u00e9crirez sur Colab pour voir les changements.</p> <pre><code>import Tensorflow as tf\nfrom tensorflow import keras\n\nprint(tf.__version__)\nprint(keras.__version__)\n\nimport numpy as np\nimport random\nimport os\n\n# freeze de l'al\u00e9atoire, pour avoir des exp\u00e9riences reproductibles.\nRANDOM_SEED = 42\n\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\ntf.random.set_seed(RANDOM_SEED)\n</code></pre>  <p>Bash</p> <p>Google Colab est aussi capable de comprendre la plupart des commandes unix et shell que vous lui donnerez. Par exemple, comme Colab tourne avec des GPUs Nvidia, pour voir lequel vous est attribu\u00e9, vous n'avez qu'\u00e0 lanc\u00e9 la commande.</p>  <pre><code>!nvidia-smi\n</code></pre> <p>Avant de voir la cr\u00e9ation de mod\u00e8les en tant que tel, discutons un peu de TensorFlow.</p>"},{"location":"deep_learning/module1/tp1/#tenseurs-et-operations","title":"Tenseurs et op\u00e9rations","text":"<p>L'objet de base dans TensorFlow est le tenseur. Si vous avez les connaissances de bases en alg\u00e8bre lin\u00e9aire, \u00e7a ne devrait pas \u00eatre compliqu\u00e9.</p> <p>Un tenseur est un tableau multidimensionnel, tout comme l'\u00e9quivalent Numpy avec ndarray.</p> <ul> <li>Un tenseur en dimension 0 correspond \u00e0 un scalaire (un nombre),</li> <li>Un tenseur de dimension 1 correspond \u00e0 un vecteur,</li> <li>Un tenseur de dimension 2 correspond \u00e0 une matrice,</li> <li>Une fois que l'on commence \u00e0 empiler des matrices ensemble dans un nouveau tableau, on obtient un tenseur en 3 dimensions, que l'on peut interpr\u00e9ter comme un cube de nombres.</li> </ul> <p>Les op\u00e9rations math\u00e9matiques possibles avec Numpy se font exactement de la m\u00eame fa\u00e7on avec les tenseurs, et il se d\u00e9finissent exactement de la m\u00eame mani\u00e8re. Il suffit principalement de remplacer <code>np.array</code> par <code>tf.constant</code>.</p>"},{"location":"deep_learning/module1/tp1/#exemple","title":"Exemple","text":"<pre><code>a = np.array([1, 2, 3])\n\nprint(f'{a}, {a.dtype}')\n</code></pre> <p>On peut directement transformer un numpy array en tenseur TensorFlow.</p> <pre><code>tf_a = tf.constant(a)\ntf_a\n</code></pre> <pre><code>b = tf.constant([1,2,3])\nb\n</code></pre>  <p>Attention</p> <p>Comme vous le verrez en tapant ces lignes de code, le type n'est pas le m\u00eame, pour <code>tf_a</code> le type est h\u00e9rit\u00e9 de Numpy, les entiers de <code>tf.constant</code> sont au type <code>int32</code>. Comme pour Numpy on peut changer le type des tenseurs.</p>  <pre><code>b = tf.constant([1,2,3], dtype = 'int64')\ntf_a == b\n</code></pre>"},{"location":"deep_learning/module1/tp1/#exercice","title":"Exercice","text":"<p>Ecrire comme un tenseur la matrice \\(2 \\times 3\\) suivante :</p> \\[     \\mathbf{W} = \\begin{pmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{pmatrix} \\] <p>des 2 fa\u00e7ons possibles :</p> <ul> <li>Ecrire sous la forme d'un ndarray Numpy puis le convertir,</li> <li>Directement via tf.constant</li> </ul> <p>Via les commandes shape et dtype, afficher la forme et le data type de de chacun, puis faire sorte que les 2 fa\u00e7ons co\u00efncident.</p> <ul> <li>Via Numpy.</li> </ul>  TensorFlow <pre><code>a = np.array([[1,2,3],[4,5,6]])\n\nprint(f'shape = {a.shape}, data type = {a.dtype}')\n</code></pre>  <ul> <li>Conversion vers <code>tf.constant</code>.</li> </ul>  TensorFlow <pre><code>tf_a = tf.constant(a)\n\nprint(f'shape = {tf_a.shape}, data type = {tf_a.dtype}')\n</code></pre>  <ul> <li>Directement via TensorFlow.</li> </ul>  TensorFlow <pre><code>b = tf.constant([[1,2,3],[4,5,6]], dtype=tf.int64)\n\nprint(f'shape = {b.shape}, data type = {b.dtype}')\n</code></pre>  <ul> <li>V\u00e9rification.</li> </ul>  TensorFlow <pre><code>tf_a == b\n</code></pre>"},{"location":"deep_learning/module1/tp1/#exercice_1","title":"Exercice","text":"<p>A l'aide de la commande tf.reshape, modifier</p> <pre><code>b = tf.constant([[1,2,3],[4,5,6]], dtype=tf.int64)\n</code></pre> <p>en un tenseur de forme \\((3,2)\\).</p>  TensorFlow <pre><code>b = tf.reshape(b,(3,2))\nb\n</code></pre>   <p>TensorFlow</p> <p>D'autres op\u00e9ration classique dans Numpy existe aussi dans TensorFlow, comme la transpos\u00e9e.</p>  <pre><code>tf.transpose(b)\n</code></pre> <p>Le produit matriciel avec la commande <code>@</code>.</p> <pre><code>tf.transpose(b)@b\n</code></pre>  <p>TensorFlow</p> <p>Une des options de la commande tf.reshape qui sera utile par la suite, et le fait de ne pouvoir que partiellement sp\u00e9cifier les dimensions. Mettre un \\(-1\\) dans un des axes permet de le sp\u00e9cifier par rapport aux autres dimensions.</p>  <pre><code>b = tf.reshape(b,(1,-1))\nb\n</code></pre> <pre><code>c = np.random.random_sample((100,))\nc = tf.reshape(c,(-1,25,2))\nc\n</code></pre>"},{"location":"deep_learning/module1/tp1/#anatomie-dun-reseau-de-neurones","title":"Anatomie d'un r\u00e9seau de neurones","text":"<p>Keras est l'API de haut niveau de Tensorflow, elle facilite la contruction des r\u00e9seaux de neurones en automatisant beaucoup de pratiques, comme la r\u00e9tropropagation du gradient.</p> <p>Entra\u00eener un r\u00e9seau de neurones d\u00e9pend des param\u00e8tres suivants :</p> <ol> <li>Les couches de neurones, qui se combinent en un r\u00e9seau (ou mod\u00e8le),</li> <li>Les donn\u00e9es : les observations et les cibles correspondantes,</li> <li>La fonction de perte, qui d\u00e9finira le gradient \u00e0 calculer,</li> <li>la m\u00e9thode d'optimisation, pour la descente du gradient.</li> </ol> <p>L'ensemble des couches de neurones n\u00e9cessaires \u00e0 la constitution des r\u00e9seaux se trouve dans la partie <code>tensorflow.keras.layers</code> de la librairie Tensorflow. Par exemple, appelons une couches de neurones denses avec 10 neurones dedans.</p> <pre><code>from tensorflow.keras.layers import Dense\n\nlayer = Dense(10)\n</code></pre> <pre><code>from tensorflow import keras\n\nlayer = keras.layers.Dense(10)\n</code></pre> <pre><code>layer = tf.keras.layers.Dense(10)\n</code></pre> <p>Toutes ces m\u00e9thodes, pour appeler la couche dense sont identiques :</p> <ul> <li>dans le premier cas on a import\u00e9 les couches Dense depuis <code>from tensorflow.keras.layers import Dense</code>,</li> <li>dans le deuxi\u00e8me cas, on a import\u00e9 keras via from <code>tensorflow import keras</code>,</li> <li>dans le dernier cas, on a import\u00e9 Tensorflow via <code>import tensorflow as tf</code>.</li> </ul> <p>Les couches que l'on utiliserons dans ce TP seront les couches :</p> <ul> <li><code>Input</code></li> <li><code>Dense</code></li> <li><code>Activation</code></li> <li><code>Flatten</code></li> </ul> <p>On importe ces couches via les commandes suivantes.</p> <pre><code>from tensorflow.keras import models\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Activation\n</code></pre> <p>Il existe 3 fa\u00e7ons de construire des mod\u00e8les avec Tensorflow via Keras :</p> <ul> <li>L'API S\u00e9quentielle,</li> <li>L'API Fonctionelle,</li> <li>L'API Subclassing.</li> </ul>"},{"location":"deep_learning/module1/tp1/#api-sequentielle","title":"API s\u00e9quentielle","text":"<p>Pour construire un mod\u00e8le \u00e0 partir de l'API s\u00e9quentielle, on proc\u00e8de la fa\u00e7on suivante :</p> <pre><code>model = models.Sequential([\nInputLayer(input_shape=(dim)),\nCouche_1(params),\nCouche_2(params),\nCouche_3(params),\n...\nCouche_n(params)\n], name='nom_du_mod\u00e8le')\n</code></pre> <p>D\u00e9crivons plus en avant les parties de ce code.</p> <ol> <li> <p><code>model = models.Sequential([....], name='nom_du_mod\u00e8le')</code> instancie le mod\u00e8le s\u00e9quentiel, c'est \u00e0 l'int\u00e9rieur des crochets que l'on d\u00e9finit la topologie du mod\u00e8le.</p> </li> <li> <p><code>InputLayer(input_shape=(dim))</code> est toujours la premi\u00e8re couche d'entr\u00e9e \u00e0 mettre dans le cas d'un mod\u00e8le s\u00e9quentiel, c'est la couche qui va prendre en entr\u00e9e les donn\u00e9es pour ensuite les faire passer dans le mod\u00e8le. Il est n\u00e9cessaire de pr\u00e9ciser les dimensions des donn\u00e9es d'entr\u00e9es via le param\u00e8tres input_shape.</p> </li> <li> <p><code>Couche_n(params)</code> correspond aux diff\u00e9rentes couches que l'on peut mettre dans l'archtecture du neurones. Pour l'instant nous travaillerons avec les deux couches suivantes :</p> <ul> <li>Les couches de neurones denses,</li> <li>Les couches de fonction d'activations.</li> </ul> </li> </ol> <p>N'h\u00e9sitez pas \u00e0 vous reporter \u00e0 la documentation de l'API S\u00e9quentielle de Tensorlfow, notamment sur les deux couches qui nous int\u00e9ressent :</p> <ul> <li>Couche dense</li> <li>Fonction d'activation comme couche</li> <li>Fonction d'activation</li> </ul>"},{"location":"deep_learning/module1/tp1/#exemple-perceptron","title":"Exemple : Perceptron","text":"<pre><code>model = models.Sequential([\n    Input(shape=(10,), name='Input'),\n    Dense(1),\n    Activation('sigmoid')\n    ], name='SeqAPI')\n\nmodel.summary()\n</code></pre> <p>la commande <code>model.summary()</code> permet d'avoir un r\u00e9capitulatif de l'architecture du r\u00e9seau, en particuliers le nombre de param\u00e8tres du r\u00e9seau, ici on en a 11. Voici ce que l'on obtient en appelant cette commande.</p> <pre><code>Model: \"SeqAPI\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\ndense_4 (Dense)              (None, 1)                 11\n_________________________________________________________________\nactivation_4 (Activation)    (None, 1)                 0\n=================================================================\nTotal params: 11\nTrainable params: 11\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre> <p>Avec le code pr\u00e9cedent, on a instanci\u00e9 un r\u00e9seau contenant :</p> <ul> <li>10 entr\u00e9es,</li> <li>1 neurone,</li> <li>1 fonction d'activation sigmo\u00efde \u00e0 la fin.</li> </ul> <p>On a donc ici un mod\u00e8le de Perceptron.</p>"},{"location":"deep_learning/module1/tp1/#api-fonctionnelle","title":"API fonctionnelle","text":"<p>Pour construire un mod\u00e8le \u00e0 partir de l'API fonctionnelle, on proc\u00e8de de la fa\u00e7on suivante :</p> <pre><code>from tensorflow.keras import Model\n\ninputs = Input(shape=(dim))\nx = Couche_1(params)(inputs)\nx = Couche_2(params)(x)\nx = Couche_3(params)(x)\n...\noutputs = Couche_n(params)(x)\nmodel = Model(inputs=inputs, outputs=output)\n</code></pre> <p>Ici comme le suppose le nom de l'API, chaque couche est alors consid\u00e9r\u00e9e comme une fonction. On empile alors les couches comme l'on compose les fonctions.</p> \\[ \\text{sortie = Couche(params)(entr\u00e9e)} \\iff y =f(x) \\] <p>Les mod\u00e8les cr\u00e9\u00e9s via l'API s\u00e9quentielle pouvant \u00eatre bien plus complexes qu'un simple mod\u00e8le s\u00e9quentiel, avec par exemple plusieurs entr\u00e9es et sorties. Pour instancier le mod\u00e8le il est donc n\u00e9cessaire de lui pr\u00e9ciser quelles sont les entr\u00e9es et sorties via la commande</p> <p><code>model = Model(inputs=inputs, outputs=output)</code></p> <p>Une fois que l'architecture du mod\u00e8le est d\u00e9finie, qu'elle soit faite \u00e0 partir de l'API s\u00e9quentielle ou de l'API fonctionnelle ne change rien. Les m\u00e9thodes pour compiler le mod\u00e8le et l'entra\u00eener sont les m\u00eames.</p>"},{"location":"deep_learning/module1/tp1/#exercice-perceptron","title":"Exercice : Perceptron","text":"<p>R\u00e9\u00e9crivez le mod\u00e8le du Perceptron via l'API fonctionnelle.</p>  TensorFlow <p><pre><code>inputs = Input(shape=(10,))\nhid = Dense(1)(inputs)\nout = Activation('sigmoid')(hid)\n\nmodel = tf.keras.Model(inputs = inputs, outputs = out, name='FuncAPI')\nmodel.summary()\n</code></pre> <pre><code>Model: \"FuncAPI\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\ninput_8 (InputLayer)         [(None, 10)]              0\n_________________________________________________________________\ndense (Dense)                (None, 1)                 11\n_________________________________________________________________\nactivation_7 (Activation)    (None, 1)                 0\n=================================================================\nTotal params: 11\nTrainable params: 11\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre></p>"},{"location":"deep_learning/module1/tp1/#exercice_2","title":"Exercice","text":"<ol> <li>Construire un mod\u00e8le via l'API S\u00e9quentielle avec les couches suivantes :</li> </ol> <pre><code>Input avec dim = (784,)\nDense(256)\nActivation RELU\nDense(256)\nActivation RELU\nDense(128)\nActivation RELU\nDense(10)\nActivation softmax\n</code></pre>  TensorFlow <pre><code>model = models.Sequential([\n    Input(shape=(784,)),\n    Dense(256, name='dense1'),\n    Activation('relu', name='relu1'),\n    Dense(256, name='dense2'),\n    Activation('relu', name='relu2'),\n    Dense(128, name='dense3'),\n    Activation('relu'),\n    Dense(10, name='logits'),\n    Activation('softmax', name='clf')\n], name='SeqAPI')\n</code></pre>  <ol> <li>Combien de param\u00e8tres ce mod\u00e8les poss\u00e8de-t-il ?</li> </ol>  TensorFlow <pre><code>model.summary()\n</code></pre> <pre><code>Model: \"SeqAPI\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\ndense1 (Dense)               (None, 256)               200960\n_________________________________________________________________\nrelu1 (Activation)           (None, 256)               0\n_________________________________________________________________\ndense2 (Dense)               (None, 256)               65792\n_________________________________________________________________\nrelu2 (Activation)           (None, 256)               0\n_________________________________________________________________\ndense3 (Dense)               (None, 128)               32896\n_________________________________________________________________\nactivation_6 (Activation)    (None, 128)               0\n_________________________________________________________________\nlogits (Dense)               (None, 10)                1290\n_________________________________________________________________\nclf (Activation)             (None, 10)                0\n=================================================================\nTotal params: 300,938\nTrainable params: 300,938\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre>  <ol> <li>Refaire le m\u00eame mod\u00e8le avec l'API fonctionnelle.</li> </ol>  TensorFlow <pre><code>inputs = Input(shape=(784,), name='Input')\ndns1 = Dense(256, name='dense1')(inputs)\nact1 = Activation('relu', name='relu1')(dns1)\ndns2 = Dense(256, name='dense2')(act1)\nact2 = Activation('relu', name='relu2')(dns2)\ndns3 = Dense(128, name='dense3')(act2)\nact3 = Activation('relu', name='relu3')(dns3)\ndns4 = Dense(10, name='logits')(act3)\nclf = Activation('softmax', name='softmax')(dns4)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=clf, name='FuncAPI')\n</code></pre>"},{"location":"deep_learning/module1/tp1/#weights-biases","title":"Weights &amp; Biases","text":"<p>Il est possible de nommer les couches des r\u00e9seaux, ainsi on peut les analyser de fa\u00e7on unique avec la commande <code>get_layer()</code>. Par exemple, pour observer la couche nomm\u00e9e 'dense1' dans le mod\u00e8le, on peut faire comme suit :</p> <pre><code>layer = model.get_layer('dense1')\nweights, biases = layer.get_weights()\nprint(f\"Poids : {weights.shape},\\n\"\n      f\"Nombre de neurones dans la couche : {weights.shape[1]},\\n\"\n      f\"Biais : {biases.shape[0]}\")\n</code></pre> <p>On obtient un r\u00e9sultat comme suit.</p> <pre><code>Poids : (784, 256),\nNombre de neurones dans la couche : 256,\nBiais : 256\n</code></pre> <p>Ou alors, on peut directement les appeler via leur numero de couche et l'attribut <code>model.layer</code>, la couche 0 \u00e9tant la premi\u00e8re en haut.</p> <pre><code>layer = model.layers[3]\nweights, biases = layer.get_weights()\nprint(f\"Poids : {weights.shape},\\n\"\n      f\"Nombre de neurones dans la couche : {weights.shape[1]},\\n\"\n      f\"Biais : {biases.shape[0]}\")\n</code></pre>"},{"location":"deep_learning/module1/tp1/#compilation-du-modele-lancement-de-lapprentissage-mnist-dataset","title":"Compilation du mod\u00e8le, lancement de l'apprentissage : MNIST Dataset","text":"<p>Le dataset MNIST des chiffres manuscrits, comporte un ensemble d'entra\u00eenement de \\(60 000\\) exemples, et un ensemble de test de \\(10 000\\) exemples. Les chiffres ont \u00e9t\u00e9 normalis\u00e9s en taille et centr\u00e9s dans une image de taille fixe.</p> <p>C'est une bonne base de donn\u00e9es pour les personnes qui veulent essayer des techniques d'apprentissage sur des donn\u00e9es du monde r\u00e9el tout en d\u00e9pensant un minimum d'efforts en pr\u00e9traitement et en formatage.</p> <p>C'est aussi et surtout l'un des datasets les plus utilis\u00e9s au monde dans la recherche acad\u00e9mique pour faire du benchmark de mod\u00e8le.</p> <p>Importons cette base de donn\u00e9es et regardons \u00e0 quoi ressemble un exemple.</p> <pre><code># Importons cette librairie pour obtenir un jeu de validation\nfrom sklearn.model_selection import train_test_split\n</code></pre> <pre><code>(X_train,y_train), (X_test,y_test) = tf.keras.datasets.mnist.load_data()\n\n# reshape des donn\u00e9es pour les faire correspondre au format de tensorflow\nX_train = X_train.reshape(-1, 28, 28, 1).astype('float32')\nX_test = X_test.reshape(-1, 28, 28, 1).astype('float32')\n</code></pre> <pre><code>X_train.shape\n</code></pre> <pre><code># par d\u00e9faut, train_test_split prend 25% des donn\u00e9es pour la validation, soit ici 15000 observations.\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, random_state=42)\n\nX_test = (X_test - 127.5) / 127.5 # Normalize the images to [-1, 1]\nX_train = (X_train - 127.5) / 127.5 # Normalize the images to [-1, 1]\nX_valid = (X_valid - 127.5) / 127.5 # Normalize the images to [-1, 1]\n</code></pre> <pre><code>import matplotlib.pyplot as plt\nclass_names = [\"zero\", \"un\", \"deux\", \"trois\", \"quatre\",\n               \"cinq\", \"six\", \"sept\", \"huit\", \"neuf\"]\n\nn_rows = 4\nn_cols = 10\nplt.figure(figsize=(n_cols * 2, n_rows * 2))\nfor row in range(n_rows):\n    for col in range(n_cols):\n        index = n_cols * row + col\n        plt.subplot(n_rows, n_cols, index + 1)\n        plt.imshow(X_train[index, : , :,0], cmap=\"binary\", interpolation=\"nearest\")\n        plt.axis('off')\n        plt.title(class_names[y_train[index]], fontsize=12)\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\n</code></pre>  <p><pre><code># Vous pouvez mettre un chiffre quelconque entre 0 et 45000 dans la premi\u00e8re variable de X_train[valeur_variable, :, :, 0] et y_train pour afficher l'observation que vous voulez\nplt.imshow(X_train[2000, :, :, 0])\nprint(y_train[2000])\n# Ici un z\u00e9ro\n</code></pre> </p> <p>Le dataset comprend donc des images des chiffres de \\(0\\) \u00e0 \\(9\\), (des tenseurs 3D) de dimension \\(28 \\times 28\\) en niveau de gris (le dernier \\(1\\) dans la valeur de <code>X_train.shape</code>).</p> <p>Le but est ici de construire un classifieur, de telle sorte qu'il soit capable de nous pr\u00e9dire une fois entra\u00een\u00e9 la bonne valeur du chiffre qui lui sera pr\u00e9sent\u00e9.</p> <p>Ici, on triche un peu, les r\u00e9seaux dense ne sont pas sp\u00e9cialis\u00e9s dans le tra\u00eetement des images, c'est pour cela qu'une fois l'input pass\u00e9e, on ajoute la couche <code>Flatten()</code> qui va se charger d'applatir l'image en un vecteur de dimension \\(28\\times28\\times1 = 784\\).</p> <p>C'est une couche tr\u00e8s importante que nous continuerons de rencontrer tr\u00e8s fr\u00e9quemment dans la suite des modules.</p> <pre><code>model = models.Sequential([\n    Input(shape=(28,28,1)),\n    Flatten(),\n    Dense(256, name='dense1'),\n    Activation('relu', name='relu1'),\n    Dense(256, name='dense2'),\n    Activation('relu', name='relu2'),\n    Dense(128, name='dense3'),\n    Activation('relu'),\n    Dense(10, name='logits'),\n    Activation('softmax', name='clf')\n], name='SeqAPI')\n</code></pre> <p>Pour finaliser le mod\u00e8le, on a 3 hyperparam\u00e8tres \u00e0 lui faire passer via la commande <code>model.compile()</code>:</p> <ul> <li>La fonction de perte utilis\u00e9e, qui sera utilis\u00e9e pour optimiser les poids et les biais du r\u00e9seau lors de la r\u00e9tropropagation,</li> <li>La m\u00e9thode d'optimisation utilis\u00e9e pour la descente du gradient stochastique,</li> <li>Les m\u00e9triques de pr\u00e9cision qui seront utilis\u00e9s pour \u00e9valuer le mod\u00e8le.</li> </ul> <p>Math\u00e9matiquement, il n'y a pas de relations entre entre la fonction de perte et les m\u00e9triques de pr\u00e9cision.</p> <p>La perte peut \u00eatre consid\u00e9r\u00e9e comme une distance entre les vraies valeurs du probl\u00e8me et les valeurs pr\u00e9dites par le mod\u00e8le. Plus la perte est importante, plus les erreurs que vous avez commises sur les donn\u00e9es sont \u00e9normes.</p> <p>La pr\u00e9cision peut \u00eatre consid\u00e9r\u00e9e comme le pourcentage d'erreurs que vous avez faites sur les donn\u00e9es.</p> <p>Cela signifie que :</p> <ul> <li>Une faible pr\u00e9cision et une perte \u00e9norme signifient que vous avez fait d'\u00e9normes erreurs sur un grand nombre de donn\u00e9es</li> <li>Une faible pr\u00e9cision mais une faible perte signifie que vous avez fait peu d'erreurs sur un grand nombre de donn\u00e9es</li> <li>Une grande pr\u00e9cision avec peu de pertes signifie que vous avez fait peu d'erreurs sur quelques donn\u00e9es (dans le meilleur des cas)</li> <li>Une grande pr\u00e9cision mais une perte \u00e9norme, signifie que vous avez fait d'\u00e9normes erreurs sur quelques donn\u00e9es.</li> </ul> <pre><code>model.compile(loss = 'sparse_categorical_crossentropy',\n             optimizer=tf.keras.optimizers.SGD(lr=0.001),\n             metrics=['accuracy'])\n</code></pre> <p>Lancer l'entra\u00eenement du mod\u00e8le se fait via la commande <code>model.fit()</code>. On a alors plusieurs param\u00e8tres \u00e0 rentrer :</p> <ul> <li><code>X_train</code>, <code>y_train</code>, qui sont les donn\u00e9es sur lesquelles va s'entra\u00eener le mod\u00e8le,</li> <li><code>epoch</code> correspond au nombre d'\u00e9poques pour l'entra\u00eenement du mod\u00e8le. On rappelle qu'une \u00e9poque correspond \u00e0 un passage complet du dataset d'entra\u00eenement dans le mod\u00e8le.</li> <li><code>batch_size</code> qui correspond au nombre d'observations utilis\u00e9es pour effectuer la descente du gradient, ie la taillle du minibatch</li> <li><code>validation_data</code> donn\u00e9es sur lesquelles \u00e9valuer la perte et les m\u00e9triques de pr\u00e9cision du mod\u00e8le \u00e0 la fin de chaque \u00e9poque. Le mod\u00e8le ne sera pas entra\u00een\u00e9 sur ces donn\u00e9es.</li> </ul> <pre><code>history = model.fit(X_train, y_train,\n                   epochs = 20,\n                   batch_size=64,\n                   validation_data=(X_valid, y_valid))\n</code></pre> <p>Une fois l'entra\u00eenement termin\u00e9, voyons comment s'en est sorti ce mod\u00e8le.</p> <pre><code>import pandas as pd\npd.DataFrame(history.history).plot(figsize=(12,8))\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()\n</code></pre> <p>Vous devriez obtenir un graphe similaire.</p>  <p>Sur ce graphe, on observe l'\u00e9volution de la fonction de perte et la pr\u00e9cision \u00e0 la fois sur le dataset d'entra\u00eenement et sur le dataset de validation, au fil des \u00e9poques. C'est un graphe utile pour juger si oui ou non notre mod\u00e8le est en sur-apprentissage.</p> <p>Evaluons maintenant notre mod\u00e8le sur le jeu de test.</p> <pre><code>model.evaluate(X_test,\n               y_test,\n               verbose=2)\n</code></pre>"},{"location":"deep_learning/module1/tp1/#autre-methode-one-hot-encoding","title":"Autre m\u00e9thode : One hot Encoding","text":"<p>Ici <code>y_train</code> va de \\(0\\) \u00e0 \\(9\\), or ces donn\u00e9es ne sont pas des donn\u00e9es ordinales, il n'y a pas de relations d'ordre dedans cela repr\u00e9sente juste le nombre attendu. En les laissant comme \u00e7a, l'algorithme pourrait apprendre une repr\u00e9sentation hi\u00e9rarchique des donn\u00e9es l\u00e0 o\u00f9 il n'y en pas.</p> <p>La bonne pratique est alors de transformer ces labels, en leur appliquant une transformation dite de \"One hot encoding\" (encore une fois, le terme fran\u00e7ais est manquant). De \\(0\\) \u00e0 \\(9\\), on a \\(10\\) chiffres, chaque nombre va donc \u00eatre remplac\u00e9 par sa coordonn\u00e9e correspondante dans \\(\\mathbf{R}^{10}\\). Ainsi, on aura la transformation suivante.</p> \\[     1 \\rightarrow (0,1,0,0,0,0,0,0,0,0)     \\dots     5 \\rightarrow (0,0,0,0,0,1,0,0,0,0) \\] <pre><code>y_train_oh = tf.keras.utils.to_categorical(y_train, num_classes=10)\ny_test_oh = tf.keras.utils.to_categorical(y_test, num_classes=10)\ny_valid_oh = tf.keras.utils.to_categorical(y_valid, num_classes=10)\n\nprint(y_train_oh)\n</code></pre> <p>En faisant \u00e7a, il faut alors changer la fonction de perte pour la faire correspondre au format de \\(y\\).</p> <pre><code>model = models.Sequential([\n    Input(shape=(28,28,1)),\n    Flatten(),\n    Dense(256, name='dense1'),\n    Activation('relu', name='relu1'),\n    Dense(256, name='dense2'),\n    Activation('relu', name='relu2'),\n    Dense(128, name='dense3'),\n    Activation('relu'),\n    Dense(10, name='logits'),\n    Activation('softmax', name='clf')\n], name='SeqAPI')\n\nmodel.compile(loss = 'categorical_crossentropy',\n              optimizer=tf.keras.optimizers.SGD(lr=0.001),\n              metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train_oh,\n                   epochs = 20,\n                   batch_size=64,\n                   validation_data=(X_valid, y_valid_oh))\n</code></pre> <pre><code>pd.DataFrame(history.history).plot(figsize=(12,8))\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()\n</code></pre> <p>Vous devriez obtenir un graphe similaire.</p>  <pre><code>model.evaluate(X_test,\n               y_test_oh,\n               verbose=2)\n</code></pre> <p>On arrive donc \u00e0 avoir des r\u00e9sultats relativement bon, m\u00eame avec des neurones qui ne sont pas sp\u00e9cialis\u00e9s dans le traitement de l'image.</p> <p>Mais nous sommes loin de l'\u00e9tat de l'art qui est aux alentours de \\(99,9\\%\\) sur MNIST.</p>"},{"location":"deep_learning/module1/tp1/#exercice_3","title":"Exercice","text":"<p>Prenez en main cet exemple, changez le nombre de couches, le nombres de neurones, essayez avec ou sans one hot encoding. Cr\u00e9er votre classifieur de nombre.</p>"},{"location":"deep_learning/module1/tp1/#cifar-10-dataset","title":"CIFAR-10 Dataset","text":"<p>Le dataset CIFAR-10 comprend 60000 images couleur \\(32\\times32\\) r\u00e9parties en \\(10\\) classes, avec \\(6000\\) images par classe. Il y a \\(50000\\) images d'entra\u00eenement et \\(10000\\) images de test.</p> <p>Le dataset est divis\u00e9 en un dataset d'entra\u00eenement et un dataset de test, contenant \\(50000\\) images pour le dataset d'entra\u00eenement, le dataset de test contient exactement \\(1000\\) images s\u00e9lectionn\u00e9es au hasard dans chaque classe.</p> <pre><code>(X_train,y_train), (X_test,y_test)  = tf.keras.datasets.cifar10.load_data()\n\nX_train.shape\n</code></pre> <p>Les classes pr\u00e9sentes dans le dataset sont les suivantes, chacunes repr\u00e9sent\u00e9es par un chiffre de 0 \u00e0 9. Dans l'ordre nous avons :</p> <ul> <li>airplane</li> <li>automobile</li> <li>bird</li> <li>cat</li> <li>deer</li> <li>dog</li> <li>frog</li> <li>horse</li> <li>ship</li> <li>truck</li> </ul> <p>C'est aussi un dataset tr\u00e8s connu pour faire du benchmark de mod\u00e8le dans le milieu acad\u00e9mique. Visualisons un peu ce que cela donne.</p> <p><pre><code>class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n\nn_rows = 4\nn_cols = 10\nplt.figure(figsize=(n_cols * 2, n_rows * 2))\nfor row in range(n_rows):\n    for col in range(n_cols):\n        index = n_cols * row + col\n        plt.subplot(n_rows, n_cols, index + 1)\n        plt.imshow(X_train[index, : , :, :], cmap=\"binary\", interpolation=\"nearest\")\n        plt.axis('off')\n        plt.title(class_names[y_train[:,0][index]], fontsize=12)\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\n</code></pre> </p> <pre><code>plt.imshow(X_train[25000, :, :, :])\nprint(y_train[25000])\n</code></pre>  <pre><code>X_train = X_train.reshape(-1, 32, 32, 3).astype('float32')\nX_test = X_test.reshape(-1, 32, 32, 3).astype('float32')\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, random_state=RANDOM_SEED)\n\nX_test = (X_test - 127.5) / 127.5 # Normalize the images to [-1, 1]\nX_train = (X_train - 127.5) / 127.5 # Normalize the images to [-1, 1]\nX_valid = (X_valid - 127.5) / 127.5 # Normalize the images to [-1, 1]\n</code></pre>"},{"location":"deep_learning/module1/tp1/#exercice_4","title":"Exercice","text":"<p>Sous quelle forme sont cod\u00e9es les labels de <code>y_train</code>, <code>y_test</code>, <code>y_valid</code> ?</p>"},{"location":"deep_learning/module1/tp1/#exercice_5","title":"Exercice","text":"<p>Le dataset a \u00e9t\u00e9 charg\u00e9, partag\u00e9 en train, validation, test. A vous de jouer, construisez une r\u00e9seau de neurones pour classifer ces images.</p> <ul> <li>Le nombres de couches de neurones,</li> <li>Le nombres de neurones dans chaque couche,</li> <li>Si oui ou non <code>y_train</code>, <code>y_test</code>, <code>y_valid</code> sont au format one_hot,</li> <li>Le nombre d'\u00e9poque,</li> <li>La taille du batch.</li> </ul> <p>Tout cela d\u00e9pend de vous, le squelette est l\u00e0, \u00e0 vous de le remplir.</p> <pre><code>model = models.Sequential([\n    Input(shape=(32,32,3)),\n    Flatten(),\n    ...\n\n\n    Dense(10, name='logits'),\n    Activation('softmax', name='clf')\n], name='SeqAPI')\n\nmodel.compile(loss = ...,\n              optimizer=tf.keras.optimizers.SGD(lr=0.001),\n              metrics=['accuracy'])\n\nhistory = model.fit(X_train, ...,\n                   epochs = ...,\n                   batch_size = ...,\n                   validation_data = (X_valid, ...))\n</code></pre>  TensorFlow <pre><code>model = models.Sequential([\n    Input(shape=(32,32,3)),\n    Flatten(),\n    Dense(256, name='dense1'),\n    Activation('relu', name='relu1'),\n    Dense(256, name='dense2'),\n    Activation('relu', name='relu2'),\n    Dense(128, name='dense3'),\n    Activation('relu'),\n    Dense(10, name='logits'),\n    Activation('softmax', name='clf')\n], name='SeqAPI')\n\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n            optimizer=tf.keras.optimizers.SGD(lr=0.001),\n            metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train,\n                epochs = 20,\n                batch_size = 64,\n                validation_data = (X_valid, y_valid))\n</code></pre> <pre><code>pd.DataFrame(history.history).plot(figsize=(12,8))\nplt.grid(True)\nplt.gca().set_ylim(0,2)\nplt.show()\n</code></pre>  <p>Ayant \\(10\\) classes diff\u00e9rentes, un choix au hasard donnerait une pr\u00e9cision de l'ordre de \\(10\\%\\), le r\u00e9seau fait donc mieux que choisir une classe au hasard.</p> <p>Cependant on est clairement en sur-apprentissage, la fonction de perte sur le dataset d'entra\u00eenement ne fait que baisser, ce qui est normal puisque l'on optimise les poids pour \u00e0 chaque \u00e9tape mais :</p> <ol> <li>La fonction de perte sur le dataset de validation grimpe en fl\u00eache,</li> <li>La pr\u00e9cision sur le dataset de validation reste bloqu\u00e9 \u00e0 \\(50\\%\\).</li> </ol> <p>Pour r\u00e9sumer,</p> <p>Le mod\u00e8le \u00e0 appris par coeur les photos qu'il a pour s'entra\u00eener, mais si on lui donne une photo au hasard du dataset de validation, il a une chance sur deux de se tromper.</p> <pre><code>model.evaluate(X_test,\n           y_test,\n           verbose=2)\n</code></pre>"},{"location":"deep_learning/module1/tp1/#exemple-overfit-sur-une-regression-lineaire","title":"Exemple : Overfit sur une r\u00e9gression lin\u00e9aire","text":"<pre><code>from sklearn.datasets import fetch_california_housing\nfrom sklearn.preprocessing import StandardScaler\nhousing = fetch_california_housing()\n</code></pre> <pre><code>X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, random_state=RANDOM_SEED)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, random_state=RANDOM_SEED)\n</code></pre> <pre><code>X_train.shape[1:]\n\n(8,)\n</code></pre> <pre><code>scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_valid = scaler.fit_transform(X_valid)\nX_test = scaler.fit_transform(X_test)\n</code></pre> <pre><code>model = models.Sequential([\n    Input(shape=(8,)),\n    Dense(10),\n    Activation('relu'),\n    Dense(1)\n    ], name='SeqAPI')\n\nmodel.compile(loss = \"mean_squared_error\",\n              optimizer=tf.keras.optimizers.SGD(lr=0.001))\n\nhistory = model.fit(X_train, y_train,\n                   epochs = 50,\n                   validation_data=(X_valid, y_valid))\n</code></pre> <pre><code>pd.DataFrame(history.history).plot(figsize=(12,8))\nplt.grid(True)\nplt.gca().set_ylim(0,10)\nplt.show()\n</code></pre>"},{"location":"deep_learning/module1/tp1/#conclusions","title":"Conclusions","text":"<p>Pour r\u00e9sumer, la contruction d'un mod\u00e8le se divise en ces \u00e9tapes :</p> <ul> <li>(Collecte et mise en forme de la donn\u00e9e),</li> <li>Mise en place de l'architecture : API s\u00e9quentielle/fonctionelle,</li> <li>D\u00e9finition du triplet [Perte, Optimiseur, M\u00e9trique],</li> <li>D\u00e9finition de la commande <code>model.fit()</code></li> </ul> <p>Il semble apparent, et cela sera r\u00e9currant par la suite, que le nombre de neurones dans les couches cach\u00e9es ne peut s'exprimer qu'en multiple de \\(32\\) : \\(32\\), \\(64\\), \\(128\\), \\(256\\), \\(512 \\dots\\) Aucun th\u00e9or\u00e8me math\u00e9matique ne vient soutenir cette hypoth\u00e8se.</p> <p>Le fait de choisir des multiples de \\(32\\) (des puissances de \\(2\\)) pour le nombres de neurones est plus une tradition et une convention informatique qu'autre chose. Si dans une moindre mesure, il est vrai que pour des neurones convolutifs (que nous verrons dans le module suivant) cela peut aider au niveau de la vitesse d'entra\u00eenement car les algorithmes de transform\u00e9e de Fourier rapide fonctionnent mieux avec un \u00e9chantillonage en 2^n, c'est pas le cas du reste.</p>"},{"location":"deep_learning/module2/Module2/","title":"Module 2 : Les r\u00e9seaux de neurones convolutifs","text":""},{"location":"deep_learning/module2/Module2/#les-neurosciences-toujours-la-vision-chez-letre-humain","title":"Les neurosciences toujours : La vision chez l'\u00eatre humain","text":"<p>Les CNN (r\u00e9seau de neurones convolutifs) ont \u00e9t\u00e9 inspir\u00e9s par les d\u00e9couvertes lors de l'\u00e9tude de la vision biologique.</p> <p>Au milieu du 20e si\u00e8cle, David Hubel et Torsten Wiesel d\u00e9crouvrent 2 types majeurs de cellules dans le cortex visuel primaire des chats :</p> <ol> <li>Les cellules dites simples,</li> <li>Les cellules dites complexes.</li> </ol>  <p>David Hubel (gauche), Torsten Wiesel (droite)</p>  <p>Prix Nobel de m\u00e9decine pour \"leurs d\u00e9couvertes concernant le traitement de l'information dans le syst\u00e8me visuel\".</p>  <p>Les cellules simples r\u00e9agissent \u00e0 des barres lumineuses ou sombres plac\u00e9es \u00e0 des endroits sp\u00e9cifiques, de plus chaque cellules \u00e0 une orientation pr\u00e9f\u00e9r\u00e9e pour laquelle la r\u00e9action est maximale.</p> <p>Les cellules complexes ont un profil de r\u00e9ponse moins strict, elles ont toujours une orientation pr\u00e9f\u00e9r\u00e9e mais elles r\u00e9agissent de la m\u00eame fa\u00e7on \u00e0 des barres situ\u00e9es \u00e0 des endroits proches.</p> <p>Hubel et Wiesel concluent alors que ces cellules re\u00e7oivent des donn\u00e9es de plusieurs cellules simples, toutes de la m\u00eame orientation, mais \u00e0 des positions proches diff\u00e9rentes.</p>  <p>En 1980, Fukushima transforme les d\u00e9couvertes de Hubel et Wiesel en un mod\u00e8le du syst\u00e8me visuel : Le NeoCognitron. Pr\u00e9curseur des CNNs il contient deux types de cellules :</p>  <p>Architecture du NeoCognitron</p>  <ul> <li> <p>Les celulles S : Une grille 2D de poids est appliqu\u00e9e \u00e0 chaque image d'entr\u00e9e, cr\u00e9ant la r\u00e9ponse des cellules S. Un plan de cellules S a une structure similaire \u00e0 celle de l'oeil humain, avec les cellules partageant toutes les m\u00eames caract\u00e9ristiques visuelles pr\u00e9f\u00e9r\u00e9es. Plusieurs plans peuvent alors s'organiser en couches succ\u00e9ssives.</p> </li> <li> <p>Les cellules C : La r\u00e9ponse de ces cellules est donn\u00e9e par une fonction non lin\u00e9aire de plusieurs cellules S provenant du m\u00eame plan mais \u00e0 des positions diff\u00e9rentes.</p> </li> </ul>  <p>L'architecture de ce mod\u00e8le est alors une simple r\u00e9p\u00e9tition de ces cellules, les r\u00e9p\u00e9titions permettant de simuler le chemin visuel ventral en entier.</p> <p>Dans les ann\u00e9es 1990, le n\u00e9ocognitron est am\u00e9lior\u00e9 et devient le HMAX. On utilise alors la fonction maximum pour d\u00e9finir la r\u00e9ponse des cellules C.</p> <p>Les CNN modernes d\u00e9coulent de ces d\u00e9couvertes, les cellules S correspondents au couches convolutives, tandis que les cellules C correspondent aux couches de pooling. L'inter\u00eat des neuroscientifiques pour les CNNs est important car ils sont capables de r\u00e9capituler la repr\u00e9sentation de l'information visuelle le long du courant ventral (responsable de la reconnaissance d'objets et de la repr\u00e9sentation des formes.)</p>  <p>Pour r\u00e9sumer, si notre but est de d\u00e9tecter des objets dans des images, il est clair que la position de l'objet dans l'image ne doit pas \u00eatre un facteur impactant pour le mod\u00e8le, en d'autres termes, le mod\u00e8le doit \u00eatre invariant par translation le plus possible.</p> <p>Se contenter de r\u00e9seaux denses, comme dans le module pr\u00e9c\u00e9dent, nous fait ignorer une des propri\u00e9t\u00e9s cl\u00e9s des images : les pixels proches sont plus fortement corr\u00e9l\u00e9s entre eux que des pixels distants. Pour prendre en compte cette propri\u00e9t\u00e9, l'id\u00e9e est alors d'extraire des caract\u00e9ristiques locales ne d\u00e9pendant que de petites sous-r\u00e9gions de l'image. Ces caract\u00e9ristiques locales pourront alors \u00eatre plus tard regroup\u00e9es pour d\u00e9tecter des caract\u00e9ristiques de plus haut niveau.</p> <p>Ces notions sont alors incorpor\u00e9es dans les CNN via les 3 m\u00e9canismes suivants :</p> <ol> <li>Les champs r\u00e9ceptifs locaux,</li> <li>Le partage des poids,</li> <li>Le sous \u00e9chantillonage.</li> </ol>"},{"location":"deep_learning/module2/Module2/#les-reseaux-convolutifs-cnn","title":"Les r\u00e9seaux convolutifs (CNN)","text":"<p>R\u00e9sum\u00e9</p> <p>Les CNN sont sp\u00e9cialis\u00e9s en traitement des donn\u00e9es dont la topologie ressemble \u00e0 celle d'une grille connue.</p> <p>Les r\u00e9seaux de neurones convolutifs sont simplement des ANN qui utilisent le produit de convolution \u00e0 la place du produit matriciel classique.</p>   <p>Exemple</p> <ol> <li>Donn\u00e9es de s\u00e9ries temporelles : grille 1D \u00e9chantillon\u00e9e \u00e0 intervalles r\u00e9guliers.</li> <li>Donn\u00e9es d'images : grille 2D de pixels.</li> </ol>"},{"location":"deep_learning/module2/Module2/#loperation-de-convolution","title":"L'op\u00e9ration de convolution","text":"<p>Mettons tout d'abord d'accord sur ce que l'on appelle le produit de convolution.</p>  <p>D\u00e9finition</p> <p>Pour \\(f\\) et \\(g\\) deux fonction de \\(L^{1}(\\mathbb{R})\\) on d\u00e9finit le produit de convolution de \\(f\\) et \\(g\\) via la formule suivante.</p> \\[     (f \\ast g )(t) := \\int_{\\mathbb{R}} f(s)g(s-t) \\mathrm{d}s \\] <p>Pour \\((f_{n})_{n \\in \\mathbb{N}}\\) et \\((g_{n})_{n \\in \\mathbb{N}}\\) deux suites de  \\(\\ell^{1}(\\mathbb{R})\\) le produit de convolution est d\u00e9fini par la formule suivante.</p> \\[     (f \\ast g )_{n} := \\sum_{k} f_{k}g_{n-k} \\]  <p>Par convention, en Deep Learning, le premier argument \\(f\\) du produit de convolution est d\u00e9nomm\u00e9 l'entr\u00e9e. Le second argument est appel\u00e9 le noyau.</p> <p>Le r\u00e9sultat est appel\u00e9 la carte de caract\u00e9ristique (feature map).</p> <p>Typiquement, l'entr\u00e9e est un tableau multidimensionnel de donn\u00e9es et le noyau est un tableau multidimensionnel de param\u00e8tres estim\u00e9s par l'algorithme. On se place donc dans le cadre d'une convolution discr\u00e8te (la deuxi\u00e8me formule).</p>  <p>Attention</p> <p>On utilise souvent des convolutions sur plus d'un axe \u00e0 la fois.</p>"},{"location":"deep_learning/module2/Module2/#motivation-de-la-convolution","title":"Motivation de la convolution","text":"<p>La convolution est une op\u00e9ration sp\u00e9cialis\u00e9e dans le traitement des images. Cherchons d'abord \u00e0 savoir comment est repr\u00e9sent\u00e9e une image pour Tensorflow.</p> <pre><code>from sklearn.datasets import load_sample_image\nimport matplotlib.pyplot as plt\n\nchina = load_sample_image('china.jpg')/255\nplt.figure(figsize = (20,18))\nplt.imshow(china)\nplt.show\n</code></pre>  <p><pre><code>china.shape\n\n(427,640,3)\n</code></pre> Pour reprendre les termes du premier module, une image est un tenseur 3D d\u00e9finie suivant 3 axes.</p> <ul> <li>La hauteur,</li> <li>la largeur,</li> <li>le nombre de canaux.</li> </ul> <p>Un mini-batch d'images utilis\u00e9 pour l'entra\u00eenement sera alors un tenseur 4D de la forme (taille du mini-batch, hauteur, largeur, canaux). D\u00e9taillons, la hauteur et la largeur de l'image sont exprim\u00e9es en nombre de pixels, le nombre de canaux nous informe si oui ou non une image est en couleur.</p> <ul> <li>Si l'image est en couleur, on a alors 3 canaux qui correspondent qui correspondent aux 3 canaux de couleurs RGB.</li> <li>Si l'image n'est qu'en niveau de gris, on a un seul canal.</li> </ul> <p>Une image en couleur sera donc de la forme \\((H,W,3)\\), tandis d'une image en niveau de gris sera de la forme \\((H,W,1)\\).</p>"},{"location":"deep_learning/module2/Module2/#la-convolution-dans-un-cnn","title":"La convolution dans un CNN","text":"<p>Dans le contexte des CNN, l'op\u00e9ration de convolution diff\u00e8re de celle de la litt\u00e9rature math\u00e9matique classique : elle implique souvent plusieurs noyaux faisant des convolutions en parall\u00e8le sur plusieurs axes pour obtenir autant de cartes de caract\u00e9ristique que d\u00e9sir\u00e9e.</p> <p>La collection de noyaux d\u00e9finissant une convolution discr\u00e8te est d\u00e9finie par la famille suivante.</p> \\[     (n, m, k_{1}, \\dots, k_{N}) \\] <p>o\u00f9 :</p> <ol> <li>\\(n :=\\) nombre de cartes de caract\u00e9ristiques en sortie.</li> <li>\\(m :=\\) nombre de cartes de caract\u00e9ristiques en entr\u00e9e.</li> <li>\\(k_{j} :=\\) dimension du noyau le long de l'axe \\(j\\).</li> </ol> <p>De la m\u00eame fa\u00e7on, la dimension de la sortie du CNN le long de l'axe \\(j\\), not\u00e9e \\(o_{j}\\), est d\u00e9finie par la famille suivante.</p> \\[     (i_{j}, k_{j}, s_{j}, p_{j}) \\] <p>o\u00f9 :</p> <ol> <li>\\(i_{j} :=\\) dimension de l'entr\u00e9e le long de l'axe \\(j\\).</li> <li>\\(k_{j} :=\\) dimension du noyau le long de l'axe \\(j\\).</li> <li>\\(s_{j} :=\\) foul\u00e9e (stride) le long de l'axe \\(j\\) : distance entre deux positions cons\u00e9cutives du noyau.</li> <li>\\(p_{j} :=\\) valeur de la marge \u00e0 z\u00e9ro (zero padding) le long de l'axe \\(j\\) : le nombre de position que le noyau peut visiter hors de l'image.</li> </ol>  <p>Remarqe</p> <p>Dans la pratique, la dimension des noyaux d'une m\u00eame couche ne varie pas le long de chaque axe : on travaille avec des noyaux de forme \"cubique\". On choisit une dimension qui est fixe dans la couche convolutive.</p>  <p>G\u00e9n\u00e9ralement, nous consid\u00e9rons chaque pixel de l'image source comme un pixel d'ancrage, mais nous ne sommes pas contraints de le faire. En fait, il n'est pas rare d'inclure une foul\u00e9e (stride), o\u00f9 les pixels d'ancrage/source sont s\u00e9par\u00e9s par un nombre sp\u00e9cifique de pixels.</p> <p>D'accord, alors quel est le pixel source ? C'est le point d'ancrage autour duquel le noyau est centr\u00e9 et o\u00f9 nous encodons tous les pixels voisins, y compris le pixel d'ancrage. Comme le noyau est de forme sym\u00e9trique (mais pas n\u00e9cessairement sym\u00e9trique en termes de valeurs), il y a un nombre \u00e9gal \\(n\\) de pixels de tous les c\u00f4t\u00e9s du pixel d'ancrage. Par cons\u00e9quent, quel que soit ce nombre de pixels, la longueur de chaque c\u00f4t\u00e9 de notre noyau de forme sym\u00e9trique est de \\(2n+1\\) (chaque c\u00f4t\u00e9 de l'ancre + le pixel d'ancrage), et les noyaux sont donc toujours de taille impaire. Les tailles de noyaux les plus courantes \u00e9tant \\(\\lbrace 1,3,5,7 \\rbrace\\).</p>  <p>Dans la d\u00e9finition classique de la convolution, le noyau de convolution n'est alors autoris\u00e9 \u00e0 visiter que les positions o\u00f9 le noyau est enti\u00e8rement dans l'image.</p> \\[     \\begin{align}     Z_{i,j,k} &amp; = (V_{\\bullet} \\ast K_{\\bullet})_{i,j,k} \\\\               &amp; = \\sum_{l,m,n} V_{l, j-1+m, k-1+n} \\cdot K_{i,l,m,n}     \\end{align} \\] <p>La dimension des couches r\u00e9duit donc \u00e0 chacune de ces convolutions. En effet, dans une convolution classique, la dimension de la sortie \\(o_{j}\\), est d\u00e9finie par la formule suivante.</p> \\[     o_{j} := i_{j} - k_{j} + 1 \\]  <p>Attention</p> <p>D\u00e8s que \\(k_{j} &gt; 1\\) la dimension de la sortie suvant cette axe diminue. On ne peut donc pas empiler autant de couches convolutives que l'on veut.</p>   <p>Convolution simple : pas de foul\u00e9e, pas de marge \u00e0 z\u00e9ro, noyau de taille \\((3,3)\\). La dimension de la sortie est alors plus petite que la dimension de la couche de sortie.</p>  <pre><code>https://github.com/vdumoulin/conv_arithmetic\n</code></pre>  <pre><code>Conv2D(64, (3, 3), padding=\"valid\")\n</code></pre> <p>La solution est alors d'ajouter ce que l'on appelle une marge de z\u00e9ros, padding, pour garder la m\u00eame dimension en sortie.</p>  <p>Convolution simple : pas de foul\u00e9e, marge \u00e0 z\u00e9ro \u00e9gale, noyau de taille \\((3,3)\\). La dimension de sortie est la m\u00eame que celle d'entr\u00e9e.</p>  <pre><code>https://github.com/vdumoulin/conv_arithmetic\n</code></pre>  <p>La nouvelle dimension de l'output est alors donn\u00e9e par la formule suivante.</p> \\[     o := i - k + 1 +2p \\] <p>Ce qui donne, si \\(o=i\\), un padding \u00e9gal \u00e0</p> \\[    p = \\frac{k-1}{2}. \\] <p>D'o\u00f9 un int\u00e9ret suppl\u00e9mentaire aux noyaux de tailles impaires.</p>  <p>Convolution simple : pas de foul\u00e9e, marge \u00e0 z\u00e9ro \u00e9gale, noyau de taille \\((3,3)\\).</p>  <pre><code>https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/\n</code></pre>  <pre><code>Conv2D(64, (3, 3), padding=\"same\")\n</code></pre> <p>On peut alors empiler autant de couches que l'on souhaite. Cependant, les pixels du bord influencent moins les pixels de sorties que ceux \u00e0 l'int\u00e9rieur, ce qui peut les rendre sous repr\u00e9sent\u00e9s.</p> <p>En espa\u00e7ant les champs r\u00e9ceptifs par une foul\u00e9e sup\u00e9rieure \u00e0 1, il est possible de r\u00e9duire la compl\u00e9xit\u00e9 du r\u00e9seau de neurones.</p>  <p>Convolution simple : foul\u00e9e de 1, pas de marge \u00e0 z\u00e9ro, noyau de taille \\((3,3)\\).</p>  <pre><code>https://github.com/vdumoulin/conv_arithmetic\n</code></pre>"},{"location":"deep_learning/module2/Module2/#les-champs-receptifs-locaux","title":"Les champs r\u00e9ceptifs locaux...","text":"<p>Lors de l'op\u00e9ration de convolution, chaque unit\u00e9 de la feature map ne prend en entr\u00e9e qu'une petite sous r\u00e9gion de l'image, d\u00e9termin\u00e9e par la taille du noyau.</p>  <p>Le champ r\u00e9ceptif local de chaque unit\u00e9 de la feature map est repr\u00e9sent\u00e9 par la partie de l'image s'intersectant avec le noyau de convolution.</p>  <pre><code>https://github.com/vdumoulin/conv_arithmetic\n</code></pre>  <p>Si l'on pense aux unit\u00e9s des features maps comme des d\u00e9tecteurs, alors toutes les unit\u00e9s d'une m\u00eame feature map d\u00e9tectent le m\u00eame sch\u00e9ma mais \u00e0 des endroits diff\u00e9rents.</p>"},{"location":"deep_learning/module2/Module2/#permettent-le-partage-des-parametres","title":"...permettent le partage des param\u00e8tres","text":"<p>Le partage de param\u00e8tres fait r\u00e9f\u00e9rence \u00e0 l'utilisation d'un m\u00eame param\u00e8tre par plusieurs fonctions dans un m\u00eame mod\u00e8le.</p> <p>Dans le cas d'un ANN classique, chaque \u00e9l\u00e9ment de la matrice de poids est utilis\u00e9 exactement une seule fois lors du calcul de la sortie. Dans un CNN, chaque \u00e9l\u00e9ment du noyau est utilis\u00e9 par chaque position de l'entr\u00e9e. Au lieu d'apprendre un ensemble de param\u00e8tres distincts pour chaque emplacement, on apprend donc un unique ensemble.</p>"},{"location":"deep_learning/module2/Module2/#interactions-parcimonieuses","title":"Interactions parcimonieuses","text":"<p>Comme on l'a vu pour le cas des r\u00e9seaux de neurones denses, pour les ANN utilisant le produit matriciel, la matrice contient des param\u00e8tres d\u00e9crivant l'interaction entre chaque unit\u00e9 d'entr\u00e9e et chaque unit\u00e9 de sortie.</p>  <p>Remarque</p> <p>Chaque unit\u00e9 de sortie interagit avec chaque unit\u00e9 de d'entr\u00e9e.</p>  <p>Les CNN, eux ont des interactions parcimonieuses, du \u00e0 la taille r\u00e9duite du noyau. Cela a plusieurs avantages, un gain de m\u00e9moire et une am\u00e9lioration de l'\u00e9fficacit\u00e9 statistique, ainsi qu'un gain en complexit\u00e9 algorothmique.</p> <p>Pour \\(m\\) entr\u00e9es et \\(n\\) sorties, le produit matriciel avec une matrice de param\u00e8tres de dimension \\(m \\times n\\) induit une compl\u00e9xit\u00e9 algorithmique en \\(\\mathcal{O}(m \\times n)\\). Si l'on limite \u00e0 \\(k\\) le nombre de connexion des sorties possibles, la compl\u00e9xit\u00e9 chute \u00e0 \\(\\mathcal{O}(k \\times n)\\).</p>  <p>Equivariance par translation</p> <p>La convolution rend le r\u00e9sultat \u00e9quivariant par translation. Ce qui fait qu'une caract\u00e9ristique g\u00e9om\u00e9trique dans une photo ne d\u00e9pend pas de sa position (dans une photo pas exemple) pour \u00eatre apprise par un CNN.</p>   <p>Attention</p> <p>La convolution n'est pas naturellement \u00e9quivariante par d'autres transformations, par exemple le changement d'\u00e9chelle ou la rotation.</p>"},{"location":"deep_learning/module2/Module2/#le-passage-dune-couche-convolutive-a-lautre","title":"Le passage d'une couche convolutive \u00e0 l'autre","text":"<p>Lorsque l'entr\u00e9e d'une couche convolutive sont les features maps d'une couche convolutive pr\u00e9c\u00e9dente, chaque filtre se \"d\u00e9multiplie\" pour avoir autant de copie de ce filtre que de features maps en entr\u00e9e. A une copie est assign\u00e9e une feature map d'entr\u00e9e, le r\u00e9sultat de cette convolution donne une \"feature map partielle\".</p> <p>Pour un filtre, on aura autant de feature map partielle que de feature maps en entr\u00e9e, la somme de ces feature maps partielle donnera le r\u00e9sultat de la convolution des feature maps d'entr\u00e9e avec le filtre choisi.</p>  <p>8 feature maps en entr\u00e9e, 3 filtres \"d\u00e9multipli\u00e9s\" chacun en 8 copie, et 3 feature maps en sortie.</p>"},{"location":"deep_learning/module2/Module2/#pooling","title":"Pooling","text":"<p>Le pooling correspond \u00e0 la propri\u00e9t\u00e9 de sous \u00e9chantillonnage. Dans un CNN, les couches de pooling fournissent une invariance aux petites translations qui peuvent \u00eatre pr\u00e9sentes dans les images.</p> <p>Le but du pooling est de r\u00e9duire la taille de l'image afin d'en r\u00e9duire le co\u00fbt en calcul, le co\u00fbt en m\u00e9moire, ainsi que le nombre de param\u00e8tres, ce qui permet de limiter le sur-apprentissage.</p> <p>De la m\u00eame fa\u00e7on qu'une couche convolutive, les sorties d'une couche de pooling poss\u00e8dent un champs r\u00e9ceptif local, mais au contraire d'une couche convolutive il n'y a aucun poids associ\u00e9s \u00e0 cette couche. L'op\u00e9ration \u00e9ffectu\u00e9e dans une couche de pooling est une op\u00e9ration d'agr\u00e9gration, telle que le maximum (MaxPooling) ou la moyenne (AvgPooling), cette op\u00e9ration n'\u00e9tant effectu\u00e9e que sur les \u00e9l\u00e9ment pr\u00e9sent dans le champs r\u00e9ceptif.</p> <p>De fa\u00e7on usuelle, une couche de pooling se trouve apr\u00e8s une couche convolutive et prend en entr\u00e9e les feature maps obtenues en sortie de cette couche.</p>  <p>Pooling</p>    <p>Remarque</p> <p>Dans la pratique, le MaxPooling est plus populaire que l'AvgPooling : il est plus rapide de calculer le maximum que la moyenne, de plus il offre de meilleurs r\u00e9sultats.</p> <p>La taille du champs  r\u00e9ceptifs est souvent de 2 ou 3.</p>"},{"location":"deep_learning/module2/Module2/#architecture-classique","title":"Architecture classique","text":"<p>VGG16</p>"},{"location":"deep_learning/module2/Module2/#transfert-dapprentissage","title":"Transfert d'apprentissage","text":"<p>L'id\u00e9e est que des poids qui ont \u00e9t\u00e9 entra\u00een\u00e9 pour un t\u00e2che sp\u00e9cifique, devrait \u00eatre performant pour une t\u00e2che similaire, m\u00eame si les deux datasets sont totalements disjoints.</p> <p>Des couches entra\u00een\u00e9es \u00e0 classifier des v\u00eatements devraient \u00eatre performants pour en classifier d'autres.</p>"},{"location":"deep_learning/module2/module2_annexe/","title":"Relations entre convolution, convolution transpos\u00e9e et r\u00e9tropropagation du gradient","text":""},{"location":"deep_learning/module2/module2_annexe/#representation-du-produit-de-convolution-comme-un-produit-matriciel","title":"Repr\u00e9sentation du produit de convolution comme un produit matriciel","text":"<p>Soit \\(F\\) une feature map de taille \\(3 \\times 3\\), \\(k\\) un noyau de convolution de taille \\(2 \\times 2\\) et \\(G\\) la feature map obtenue comme le r\u00e9sultat de cette convolution, ie on a l'\u00e9quation suivante.</p> \\[     G := F \\odot k \\] <p>avec les matrices suivantes.</p> \\[     G := \\begin{pmatrix}         g_{0,0} &amp; g_{0,1} \\\\         g_{1,0} &amp; g_{1,1}     \\end{pmatrix}     \\quad     F :=     \\begin{pmatrix}         f_{0,0} &amp; f_{0,1} &amp; f_{0,2} \\\\         f_{1,0} &amp; f_{1,1} &amp; f_{1,2} \\\\         f_{2,0} &amp; f_{2,1} &amp; f_{2,2}         \\\\     \\end{pmatrix}     \\quad     k := \\begin{pmatrix}         k_{0,0} &amp; k_{0,1} \\\\         k_{1,0} &amp; k_{1,1}     \\end{pmatrix} \\] <p>On ne consid\u00e8re pour l'instant que le cas de la convolution dite \"valide\". Par d\u00e9finition du produit de convolution, on a les \u00e9quation suivantes.</p> \\[ \\begin{equation}     \\begin{split}         g_{0,0}  &amp;:= f_{0,0}k_{0,0} + f_{0,1}k_{0,1} + f_{1,0}k_{1,0} + f_{1,1}k_{1,1} \\\\         g_{0,1}  &amp;:= f_{0,1}k_{0,0} + f_{0,2}k_{0,1} + f_{1,1}k_{1,0} + f_{1,2}k_{1,1} \\\\         g_{1,0}  &amp;:= f_{1,0}k_{0,0} + f_{1,1}k_{0,1} + f_{2,0}k_{1,0} + f_{2,1}k_{1,1} \\\\         g_{1,1}  &amp;:= f_{1,1}k_{0,0} + f_{1,2}k_{0,1} + f_{2,1}k_{1,0} + f_{2,2}k_{1,1}     \\end{split} \\end{equation} \\] <p>R\u00e9\u00e9crivons cela sous la forme d'un produit matriciel, en d\u00e9finissant sous la forme de vecteurs les matrices donn\u00e9es plus haut.</p> \\[     F_{\\mathrm{Vec}} := \\begin{pmatrix}         f_{0,0} &amp; f_{0,1} &amp; f_{0,2} &amp; f_{1,0} &amp; f_{1,1} &amp; f_{1,2} &amp; f_{2,0} &amp; f_{2,1} &amp; f_{2,2}     \\end{pmatrix} \\] \\[     G_{\\mathrm{Vec}} := \\begin{pmatrix}         g_{0,0} &amp; g_{0,1} &amp; g_{1,0} &amp; g_{1,1}     \\end{pmatrix} \\] <p>On obtient la matrice suivant pour d\u00e9finir le noyau de convolution.</p> \\[     k_{\\mathrm{Mat}} := \\begin{pmatrix}         k_{0,0} &amp; k_{0,1} &amp; 0       &amp; k_{1,0} &amp; k_{1,1} &amp; 0       &amp; 0       &amp; 0       &amp; 0       \\\\         0       &amp; k_{0,0} &amp; k_{0,1} &amp; 0       &amp; k_{1,0} &amp; k_{1,1} &amp; 0       &amp; 0       &amp; 0       \\\\         0       &amp; 0       &amp; 0       &amp; k_{0,0} &amp; k_{0,1} &amp; 0       &amp; k_{1,0} &amp; k_{1,1} &amp; 0       \\\\         0       &amp; 0       &amp; 0       &amp; 0       &amp; k_{0,0} &amp; k_{0,1} &amp; 0       &amp; k_{1,0} &amp; k_{1,1}     \\end{pmatrix} \\] <p>Le produit de convolution se r\u00e9\u00e9crit alors comme le produit matriciel suivant.</p> \\[     k_{\\mathrm{Mat}} \\cdot (F_{\\mathrm{Vec}})^{T} = (G_{\\mathrm{Vec}})^{T} \\] <p>La matrice \\(k\\) d\u00e9finissant le produit de convolution est une matrice circulante double de Toeplitz</p>"},{"location":"deep_learning/module2/module2_annexe/#retropropagation-du-gradient-dans-les-cnn","title":"R\u00e9tropropagation du gradient dans les CNN","text":"<p>On souhaite savoir comment se propage le gradient dans une couche convolutive.</p> <p>Pour savoir cela on doit pouvoir calculer les deux d\u00e9riv\u00e9es partielles suivantes :\\(\\frac{\\partial \\mathcal{L}}{\\partial k}\\) et \\(\\frac{\\partial \\mathcal{L}}{\\partial F}\\).</p> <ol> <li>La premi\u00e8re \u00e9tant n\u00e9cessaire pour mettre \u00e0 jour les poids du noyau de convolution,</li> <li>La seconde pour continuer la r\u00e9tro-propagation.</li> </ol> <p>Pour les calculer, on utilise les deux propri\u00e9t\u00e9s suivantes :</p> <ol> <li>La propri\u00e9t\u00e9 des d\u00e9rivations en cha\u00eenes :</li> </ol> \\[     \\frac{\\partial \\mathcal{L}}{\\partial k}  := \\sum_{\\ell} \\frac{\\partial \\mathcal{L}}{\\partial G_{\\ell}} \\cdot \\frac{\\partial G_{\\ell}}{\\partial k} \\] \\[     \\frac{\\partial \\mathcal{L}}{\\partial F}  := \\sum_{\\ell} \\frac{\\partial \\mathcal{L}}{\\partial G_{\\ell}} \\cdot \\frac{\\partial G_{\\ell}}{\\partial F} \\] <ol> <li>La lin\u00e9arit\u00e9 de l'op\u00e9rateur de d\u00e9rivation :</li> </ol> \\[     \\forall \\,\\, i,j \\quad \\left(\\frac{\\partial \\mathcal{L}}{\\partial k} \\right)_{i,j}  := \\sum_{\\ell} \\frac{\\partial \\mathcal{L}}{\\partial G_{\\ell}} \\cdot \\frac{\\partial G_{\\ell}}{\\partial k_{i,j}} \\] \\[     \\forall \\,\\, i,j \\quad \\left(\\frac{\\partial \\mathcal{L}}{\\partial F}\\right)_{i,j}  := \\sum_{\\ell} \\frac{\\partial \\mathcal{L}}{\\partial G_{\\ell}} \\cdot \\frac{\\partial G_{\\ell}}{\\partial F_{i,j}} \\] <p>On suppose connu \\(\\frac{\\partial \\mathcal{L}}{\\partial G_{\\ell}}\\). Ce que l'on doit calculer ce sont les deux gradients locaux \\(\\frac{\\partial G_{\\ell}}{\\partial k_{i,j}}\\) et \\(\\frac{\\partial G_{\\ell}}{\\partial F_{i,j}}\\) pour toute les valeurs de \\((i,j,\\ell)\\).</p>"},{"location":"deep_learning/module2/module2_annexe/#premiere-derivee","title":"Premi\u00e8re d\u00e9riv\u00e9e","text":"<p>On doit donc calculer les d\u00e9riv\u00e9es partielles suivantes.</p> \\[ \\begin{equation}     \\begin{split}         \\left(\\frac{\\partial \\mathcal{L}}{\\partial k} \\right)_{0,0}  &amp; := \\sum_{\\ell} \\frac{\\partial \\mathcal{L}}{\\partial G_{\\ell}} \\cdot \\frac{\\partial G_{\\ell}}{\\partial k_{0,0}} \\\\         \\left(\\frac{\\partial \\mathcal{L}}{\\partial k} \\right)_{0,1}  &amp; := \\sum_{\\ell} \\frac{\\partial \\mathcal{L}}{\\partial G_{\\ell}} \\cdot \\frac{\\partial G_{\\ell}}{\\partial k_{0,1}} \\\\         \\left(\\frac{\\partial \\mathcal{L}}{\\partial k} \\right)_{1,0}  &amp; := \\sum_{\\ell} \\frac{\\partial \\mathcal{L}}{\\partial G_{\\ell}} \\cdot \\frac{\\partial G_{\\ell}}{\\partial k_{1,0}} \\\\         \\left(\\frac{\\partial \\mathcal{L}}{\\partial k} \\right)_{1,1}  &amp; := \\sum_{\\ell} \\frac{\\partial \\mathcal{L}}{\\partial G_{\\ell}} \\cdot \\frac{\\partial G_{\\ell}}{\\partial k_{1,1}}     \\end{split} \\end{equation} \\] <p>Pour \\(G\\), \\(G_{\\ell}\\) correspond aux 4 fonctions suivants : \\(g_{0,0}, g_{0,1}, g_{1,0}, g_{1,1}\\). On a alors la formule suivante.</p> \\[ \\begin{equation*}     \\begin{split}         \\left(\\frac{\\partial \\mathcal{L}}{\\partial k} \\right)_{i,j} &amp; := \\sum_{r,s} \\frac{\\partial \\mathcal{L}}{\\partial g_{r,s}} \\cdot \\frac{\\partial g_{r,s}}{\\partial k_{i,j}} \\\\         &amp; =\\frac{\\partial \\mathcal{L}}{\\partial g_{0,0}} \\cdot \\frac{\\partial g_{0,0}}{\\partial k_{i,j}} + \\frac{\\partial \\mathcal{L}}{\\partial g_{0,1}} \\cdot \\frac{\\partial g_{0,1}}{\\partial k_{i,j}} + \\frac{\\partial \\mathcal{L}}{\\partial g_{1,0}} \\cdot \\frac{\\partial g_{1,0}}{\\partial k_{i,j}} + \\frac{\\partial \\mathcal{L}}{\\partial g_{1,1}} \\cdot \\frac{\\partial g_{1,1}}{\\partial k_{i,j}}     \\end{split} \\end{equation*} \\] <p>\\(\\frac{\\partial \\mathcal{L}}{\\partial g_{r,s}}\\) \u00e9tant suppos\u00e9 connu, il nous reste \u00e0 calculer les gradient locaux \\(\\frac{\\partial g_{r,s}}{\\partial k_{i,j}}\\) , ce que l'on sait faire facilement gr\u00e2ce aux formules de l'\u00e9quation pr\u00e9c\u00e9dente. En appliquant la formule pr\u00e9c\u00e9dente pour tous les couples \\((i,j)\\), on obtient les r\u00e9sultats suivants.</p> \\[ \\begin{equation}     \\left(\\frac{\\partial \\mathcal{L}}{\\partial k} \\right)_{0,0} = \\frac{\\partial \\mathcal{L}}{\\partial g_{0,0}} \\cdot f_{0,0} + \\frac{\\partial \\mathcal{L}}{\\partial g_{0,1}} \\cdot f_{0,1} + \\frac{\\partial \\mathcal{L}}{\\partial g_{1,0}} \\cdot f_{1,0} + \\frac{\\partial \\mathcal{L}}{\\partial g_{1,1}} \\cdot f_{1,1} \\end{equation} \\] \\[ \\begin{equation}     \\left(\\frac{\\partial \\mathcal{L}}{\\partial k} \\right)_{0,1}  = \\frac{\\partial \\mathcal{L}}{\\partial g_{0,0}} \\cdot f_{0,1} + \\frac{\\partial \\mathcal{L}}{\\partial g_{0,1}} \\cdot f_{0,2} + \\frac{\\partial \\mathcal{L}}{\\partial g_{1,0}} \\cdot f_{1,1} + \\frac{\\partial \\mathcal{L}}{\\partial g_{1,1}} \\cdot f_{1,2} \\end{equation} \\] \\[ \\begin{equation}     \\left(\\frac{\\partial \\mathcal{L}}{\\partial k} \\right)_{1,0} = \\frac{\\partial \\mathcal{L}}{\\partial g_{0,0}} \\cdot f_{1,0} + \\frac{\\partial \\mathcal{L}}{\\partial g_{0,1}} \\cdot f_{1,1} + \\frac{\\partial \\mathcal{L}}{\\partial g_{1,0}} \\cdot f_{2,0} + \\frac{\\partial \\mathcal{L}}{\\partial g_{1,1}} \\cdot f_{2,1} \\end{equation} \\] \\[ \\begin{equation}     \\left(\\frac{\\partial \\mathcal{L}}{\\partial k} \\right)_{1,1} = \\frac{\\partial \\mathcal{L}}{\\partial g_{0,0}} \\cdot f_{1,1} + \\frac{\\partial \\mathcal{L}}{\\partial g_{0,1}} \\cdot f_{1,2} + \\frac{\\partial \\mathcal{L}}{\\partial g_{1,0}} \\cdot f_{2,1} + \\frac{\\partial \\mathcal{L}}{\\partial g_{1,1}} \\cdot f_{2,2} \\end{equation} \\] <p>Si on remet tout cela sous forme matricielle, on obtient alors la matrice suivante.</p> \\[     \\frac{\\partial \\mathcal{L}}{\\partial k}  =     \\begin{pmatrix}         \\left(\\frac{\\partial \\mathcal{L}}{\\partial k} \\right)_{0,0} &amp; \\left(\\frac{\\partial \\mathcal{L}}{\\partial k} \\right)_{0,1} \\\\         \\left(\\frac{\\partial \\mathcal{L}}{\\partial k} \\right)_{1,0} &amp; \\left(\\frac{\\partial \\mathcal{L}}{\\partial k} \\right)_{1,1}     \\end{pmatrix} \\] <p>La matrice \\(\\frac{\\partial \\mathcal{L}}{\\partial k}\\) peut s'\u00e9crire comme le produit de convolution suivant.</p> \\[     \\frac{\\partial \\mathcal{L}}{\\partial k}     =     \\begin{pmatrix}         f_{0,0} &amp; f_{0,1} &amp; f_{0,2} \\\\         f_{1,0} &amp; f_{1,1} &amp; f_{1,2} \\\\         f_{2,0} &amp; f_{2,1} &amp; f_{2,2}     \\end{pmatrix} \\odot \\begin{pmatrix}         \\frac{\\partial \\mathcal{L}}{\\partial g_{0,0}} &amp; \\frac{\\partial \\mathcal{L}}{\\partial g_{0,1}} \\\\         \\frac{\\partial \\mathcal{L}}{\\partial g_{1,0}} &amp; \\frac{\\partial \\mathcal{L}}{\\partial g_{1,1}}     \\end{pmatrix} \\]"},{"location":"deep_learning/module2/module2_annexe/#deuxieme-derivee","title":"Deuxi\u00e8me d\u00e9riv\u00e9e","text":"<p>On doit donc calculer les d\u00e9riv\u00e9es partielles suivantes.</p> \\[ \\begin{equation}     \\left(\\frac{\\partial \\mathcal{L}}{\\partial F} \\right)_{i,j}  := \\sum_{\\ell} \\frac{\\partial \\mathcal{L}}{\\partial G_{\\ell}} \\cdot \\frac{\\partial G_{\\ell}}{\\partial F_{i,j}} \\end{equation} \\] <p>Pour \\(G\\), \\(G_{\\ell}\\) correspond aux 4 fonctions suivants : \\(g_{0,0}, g_{0,1}, g_{1,0}, g_{1,1}\\), et pour \\(F\\), \\(F_{i,j}\\) correspond \u00e0 \\(f_{i,j}\\). De la m\u00eame fa\u00e7on que pour le calcul pr\u00e9c\u00e9dent, on a la formule suivante.</p> \\[ \\begin{equation*}     \\begin{split}         \\left(\\frac{\\partial \\mathcal{L}}{\\partial F} \\right)_{i,j} &amp; := \\sum_{r,s} \\frac{\\partial \\mathcal{L}}{\\partial g_{r,s}} \\cdot \\frac{\\partial g_{r,s}}{\\partial f_{i,j}} \\\\         &amp; =\\frac{\\partial \\mathcal{L}}{\\partial g_{0,0}} \\cdot \\frac{\\partial g_{0,0}}{\\partial f_{i,j}} + \\frac{\\partial \\mathcal{L}}{\\partial g_{0,1}} \\cdot \\frac{\\partial g_{0,1}}{\\partial f_{i,j}} + \\frac{\\partial \\mathcal{L}}{\\partial g_{1,0}} \\cdot \\frac{\\partial g_{1,0}}{\\partial f_{i,j}} + \\frac{\\partial \\mathcal{L}}{\\partial g_{1,1}} \\cdot \\frac{\\partial g_{1,1}}{\\partial f_{i,j}}     \\end{split} \\end{equation*} \\] <p>En appliquant cette formule, on peut calculer les gradient locaux \\(\\frac{\\partial g_{r,s}}{\\partial f_{i,j}}\\) gr\u00e2ce aux formules de l'\u00e9quation</p> \\[ \\begin{align*}     \\left(\\frac{\\partial \\mathcal{L}}{\\partial F} \\right)_{0,0} &amp; =     \\frac{\\partial \\mathcal{L}}{\\partial g_{0,0}} \\cdot k_{0,0}     \\\\     \\left(\\frac{\\partial \\mathcal{L}}{\\partial F} \\right)_{0,1} &amp; =     \\frac{\\partial \\mathcal{L}}{\\partial g_{0,0}} \\cdot k_{0,1} +     \\frac{\\partial \\mathcal{L}}{\\partial g_{0,1}} \\cdot k_{0,0}     \\\\     \\left(\\frac{\\partial \\mathcal{L}}{\\partial F} \\right)_{0,2} &amp; =     \\frac{\\partial \\mathcal{L}}{\\partial g_{0,1}} \\cdot k_{0,1}     \\\\     \\left(\\frac{\\partial \\mathcal{L}}{\\partial F} \\right)_{1,0} &amp; =     \\frac{\\partial \\mathcal{L}}{\\partial g_{0,0}} \\cdot k_{1,0} +     \\frac{\\partial \\mathcal{L}}{\\partial g_{1,0}} \\cdot k_{0,0}     \\\\     \\left(\\frac{\\partial \\mathcal{L}}{\\partial F} \\right)_{1,1} &amp; =     \\frac{\\partial \\mathcal{L}}{\\partial g_{0,0}} \\cdot k_{1,1} +     \\frac{\\partial \\mathcal{L}}{\\partial g_{0,1}} \\cdot k_{1,0} +     \\frac{\\partial \\mathcal{L}}{\\partial g_{1,0}} \\cdot k_{0,1} +     \\frac{\\partial \\mathcal{L}}{\\partial g_{1,1}} \\cdot k_{0,0}     \\\\     \\left(\\frac{\\partial \\mathcal{L}}{\\partial F} \\right)_{1,2} &amp; =     \\frac{\\partial \\mathcal{L}}{\\partial g_{0,1}} \\cdot k_{1,1} +     \\frac{\\partial \\mathcal{L}}{\\partial g_{1,1}} \\cdot k_{0,1}     \\\\     \\left(\\frac{\\partial \\mathcal{L}}{\\partial F} \\right)_{2,0} &amp; =     \\frac{\\partial \\mathcal{L}}{\\partial g_{1,0}} \\cdot k_{1,0}     \\\\     \\left(\\frac{\\partial \\mathcal{L}}{\\partial F} \\right)_{2,1} &amp; =     \\frac{\\partial \\mathcal{L}}{\\partial g_{1,0}} \\cdot k_{1,1} +     \\frac{\\partial \\mathcal{L}}{\\partial g_{1,1}} \\cdot k_{1,0}     \\\\     \\left(\\frac{\\partial \\mathcal{L}}{\\partial F} \\right)_{2,2} &amp; =     \\frac{\\partial \\mathcal{L}}{\\partial g_{1,1}} \\cdot k_{1,1} \\end{align*} \\] <p>Si on remet tout cela sous forme matricielle, on obtient alors la matrice suivante.</p> \\[     \\frac{\\partial \\mathcal{L}}{\\partial F}  =     \\begin{pmatrix}     \\frac{\\partial \\mathcal{L}}{\\partial g_{0,0}} \\cdot k_{0,0} &amp;     \\frac{\\partial \\mathcal{L}}{\\partial g_{0,0}} \\cdot k_{0,1} +     \\frac{\\partial \\mathcal{L}}{\\partial g_{0,1}} \\cdot k_{0,0} &amp;     \\frac{\\partial \\mathcal{L}}{\\partial g_{0,1}} \\cdot k_{0,1}   \\\\     \\\\     \\frac{\\partial \\mathcal{L}}{\\partial g_{0,0}} \\cdot k_{1,0} +     \\frac{\\partial \\mathcal{L}}{\\partial g_{1,0}} \\cdot k_{0,0} &amp;     \\frac{\\partial \\mathcal{L}}{\\partial g_{0,0}} \\cdot k_{1,1} +     \\frac{\\partial \\mathcal{L}}{\\partial g_{0,1}} \\cdot k_{1,0} +     \\frac{\\partial \\mathcal{L}}{\\partial g_{1,0}} \\cdot k_{0,1} +     \\frac{\\partial \\mathcal{L}}{\\partial g_{1,1}} \\cdot k_{0,0} &amp;     \\frac{\\partial \\mathcal{L}}{\\partial g_{0,1}} \\cdot k_{1,1} +     \\frac{\\partial \\mathcal{L}}{\\partial g_{1,1}} \\cdot k_{0,1}     \\\\     \\\\     \\frac{\\partial \\mathcal{L}}{\\partial g_{1,0}} \\cdot k_{1,0} &amp;     \\frac{\\partial \\mathcal{L}}{\\partial g_{1,0}} \\cdot k_{1,1} +     \\frac{\\partial \\mathcal{L}}{\\partial g_{1,1}} \\cdot k_{1,0} &amp;     \\frac{\\partial \\mathcal{L}}{\\partial g_{1,1}} \\cdot k_{1,1}     \\end{pmatrix} \\] <p>Cette matrice provient en fait du produit de convolution complet suivant.</p> \\[     \\frac{\\partial \\mathcal{L}}{\\partial F}  =     \\begin{pmatrix}         0 &amp; 0                                             &amp; 0                                             &amp; 0 \\\\         0 &amp; \\frac{\\partial \\mathcal{L}}{\\partial g_{0,0}} &amp; \\frac{\\partial \\mathcal{L}}{\\partial g_{0,1}} &amp; 0 \\\\         0 &amp; \\frac{\\partial \\mathcal{L}}{\\partial g_{1,0}} &amp; \\frac{\\partial \\mathcal{L}}{\\partial g_{1,1}} &amp; 0 \\\\         0 &amp; 0                                             &amp; 0                                             &amp; 0 \\\\     \\end{pmatrix}     \\odot     \\begin{pmatrix}         k_{1,1} &amp; k_{1,0} \\\\         k_{0,1} &amp; k_{0,0}     \\end{pmatrix} \\]"},{"location":"deep_learning/module2/module2_annexe/#lien-avec-la-convolution-initiale","title":"Lien avec la convolution initiale","text":"<p>Si on met le produit de convolution pr\u00e9c\u00e9dent sous la forme d'un produit matriciel, on obtient le r\u00e9sultat suivant.</p> \\[ \\begin{equation}     \\begin{split}         \\frac{\\partial \\mathcal{L}}{\\partial F}  =         \\begin{pmatrix}             k_{0,0} &amp; 0       &amp; 0       &amp; 0       \\\\             k_{0,1} &amp; k_{0,0} &amp; 0       &amp; 0       \\\\             0       &amp; k_{0,1} &amp; 0       &amp; 0       \\\\             k_{1,0} &amp; 0       &amp; k_{0,0} &amp; 0       \\\\             k_{1,1} &amp; k_{1,0} &amp; k_{0,1} &amp; k_{0,0} \\\\             0       &amp; k_{1,1} &amp; 0       &amp; k_{0,1} \\\\             0       &amp; 0       &amp; k_{1,0} &amp; 0       \\\\             0       &amp; 0       &amp; k_{1,1} &amp; k_{1,0} \\\\             0       &amp; 0       &amp; 0       &amp; k_{1,1} \\\\         \\end{pmatrix}         \\cdot         \\begin{pmatrix}             \\frac{\\partial \\mathcal{L}}{\\partial g_{0,0}} \\\\             \\frac{\\partial \\mathcal{L}}{\\partial g_{0,1}} \\\\             \\frac{\\partial \\mathcal{L}}{\\partial g_{1,0}} \\\\             \\frac{\\partial \\mathcal{L}}{\\partial g_{1,1}} \\\\         \\end{pmatrix}     \\end{split} \\end{equation} \\] <p>Remarquez que le fait d'avoir fait une convolution compl\u00e8te n'appara\u00eet pas dans le produit matriciel (ie on a pas rajout\u00e9 plus de z\u00e9ros), cette matrice l\u00e0 est la transpos\u00e9e de celle de l'\u00e9quation.</p> \\[     \\begin{pmatrix}         k_{0,0} &amp; 0       &amp; 0       &amp; 0       \\\\         k_{0,1} &amp; k_{0,0} &amp; 0       &amp; 0       \\\\         0       &amp; k_{0,1} &amp; 0       &amp; 0       \\\\         k_{1,0} &amp; 0       &amp; k_{0,0} &amp; 0       \\\\         k_{1,1} &amp; k_{1,0} &amp; k_{0,1} &amp; k_{0,0} \\\\         0       &amp; k_{1,1} &amp; 0       &amp; k_{0,1} \\\\         0       &amp; 0       &amp; k_{1,0} &amp; 0       \\\\         0       &amp; 0       &amp; k_{1,1} &amp; k_{1,0} \\\\         0       &amp; 0       &amp; 0       &amp; k_{1,1} \\\\     \\end{pmatrix}     =     \\begin{pmatrix}         k_{0,0} &amp; k_{0,1} &amp; 0       &amp; k_{1,0} &amp; k_{1,1} &amp; 0       &amp; 0       &amp; 0       &amp; 0       \\\\         0       &amp; k_{0,0} &amp; k_{0,1} &amp; 0       &amp; k_{1,0} &amp; k_{1,1} &amp; 0       &amp; 0       &amp; 0       \\\\         0       &amp; 0       &amp; 0       &amp; k_{0,0} &amp; k_{0,1} &amp; 0       &amp; k_{1,0} &amp; k_{1,1} &amp; 0       \\\\         0       &amp; 0       &amp; 0       &amp; 0       &amp; k_{0,0} &amp; k_{0,1} &amp; 0       &amp; k_{1,0} &amp; k_{1,1}     \\end{pmatrix}^{T} = k_{\\mathrm{Mat}}^{T} \\]  <p>Th\u00e9or\u00e8me</p> <p>La transpos\u00e9e de la matrice d\u00e9finissant le noyau de convolution d\u00e9termine comment se propage le gradient dans les couches en amont.</p> <p>En d'autres termes, l'erreur \\(\\frac{\\partial \\mathcal{L}}{\\partial G}\\) est r\u00e9tro-propag\u00e9e en la multipliant par \\(k_{\\mathrm{Mat}}^{T}\\).</p>"},{"location":"deep_learning/module2/tp2/","title":"TP Module 2 : Les r\u00e9seaux de neurones convolutifs","text":"<p>Concentrons nous maintenant sur les r\u00e9seaux sp\u00e9cialis\u00e9s dans le tra\u00eetement d'images.</p> <pre><code>import tensorflow as tf\nfrom tensorflow import keras\n\nprint(tf.__version__)\nprint(keras.__version__)\n\n# Splitting\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nimport os\nimport numpy as np\n\n# freeze de l'al\u00e9atoire, pour avoir des exp\u00e9riences reproductibles.\nRANDOM_SEED = 42\n\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\ntf.random.set_seed(RANDOM_SEED)\n</code></pre> <pre><code>!nvidia-smi\n</code></pre> <pre><code>comp = pd.DataFrame()\ncomp['run'] = []\ncomp['Perte'] = []\ncomp['Pr\u00e9cision'] = []\n</code></pre>"},{"location":"deep_learning/module2/tp2/#introduction-convolution-et-pooling","title":"Introduction : Convolution et Pooling","text":"<p>La vision assist\u00e9e par ordinateur (Computer Vision) n'est pas n\u00e9e avec le Deep Learning, c'est un domaine bien plus vieux que \u00e7a.</p> <p>La diff\u00e9rence est que pr\u00e9c\u00e9demment, les filtres et leur poids \u00e9taient d\u00e9finis \u00e0 la main.</p> <p>Pour voir un peu ce que fait une convolution classique, observons ce que cela fait sur l'image suivante.</p> <pre><code>from sklearn.datasets import load_sample_image\n\nflower = load_sample_image('flower.jpg')/255\n\n# L'image \u00e9tant sur 3 canaux RGB, transformons la en noir et blanc.\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrayscale = rgb2gray(flower).astype(np.float32)\n</code></pre> <pre><code>plt.figure(figsize = (20,18))\nplt.imshow(grayscale, cmap=\"gray\")\nplt.show\ngrayscale.shape\n\n\n(427, 640)\n</code></pre>  <p>Pour les besoins de l'exemple, l'image a \u00e9t\u00e9 convertie en niveau de gris, pour savoir pour les coefficients \\([0.2989, 0.5870, 0.1140]\\) sont pr\u00e9sents dans la fonction <code>rgb2gray</code>, vous pouvez regarder la source suivante.</p> <p>V\u00e9rifions qu'elle a bien \u00e9t\u00e9 transform\u00e9e en niveau de gris, ie il n'y plus qu'un seul canal (et donc il n'est pas affich\u00e9 par <code>shape</code>.)</p> <pre><code>print(flower.shape, grayscale.shape)\n\n(427, 640, 3) (427, 640)\n</code></pre> <p>Voici quelques noyaux de convolutions utilis\u00e9s avant les techniques de deep learning modernes, source.</p> <pre><code>identity = np.array([[0, 0, 0],\n                     [0, 1, 0],\n                     [0, 0, 0]], dtype=np.float32)\n\ncontour1 = np.array([[1, 0, -1],\n                      [0, 0, 0],\n                      [-1, 0, 1]], dtype=np.float32)\n\ncontour2 = np.array([[0, 1, 0],\n                     [1, -4, 1],\n                     [0, 1, 0]], dtype=np.float32)\n\ncontour3 = np.array([[-1, -1, -1],\n                     [-1, 8, -1],\n                     [-1, -1, -1]], dtype=np.float32)\n\nupscaling = np.array([[0, -1, 0],\n                     [-1, 5, -1],\n                     [0, -1, 0]], dtype=np.float32)\n\nbox_blur = np.array([[1/9, 1/9, 1/9],\n                     [1/9, 1/9, 1/9],\n                     [1/9, 1/9, 1/9]], dtype=np.float32)\n\ngauss_blur = np.array([[1/16, 1/8, 1/16],\n                     [1/8, 1/4, 1/8],\n                     [1/16, 1/8, 1/6]], dtype=np.float32)\n</code></pre> <pre><code>def conv(img,kernel):\n\n    ker_height, ker_width = kernel.shape\n    height, width = img.shape\n\n    tf_ker = tf.reshape(kernel, (ker_height, ker_width, 1, 1))\n\n    tf_img = tf.reshape(img, (-1, height, width, 1))\n\n    #print(tf_ker.dtype, tf_gray.dtype)\n\n    outputs= tf.nn.conv2d(tf_img, tf_ker, strides=1, padding=\"SAME\")\n\n    plt.figure(figsize = (20,18))\n    plt.imshow(outputs[0,:,:,0], cmap='gray')\n    plt.show\n</code></pre>  TensorFlow <pre><code>conv(grayscale, gauss_blur)\n</code></pre>  <p><pre><code>conv(grayscale,contour1)\n</code></pre> </p>"},{"location":"deep_learning/module2/tp2/#anatomie-dune-couche-convolutive-cnn","title":"Anatomie d'une couche convolutive, CNN","text":"<p>Premi\u00e8rement, importons les librairies dont nous aurons besoin.</p> <pre><code>from tensorflow.keras import models\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras import optimizers\n</code></pre> <p>Avec l'API s\u00e9quentielle, \u00e9crivons un CNN avec uniquement 2 couches convolutives. Evidemment il n'est pas fait pour de l'entra\u00eenement, il n'y a m\u00eame pas de couche classifiante, mais utilisons le pour comprendre la structure.</p> <pre><code>model = models.Sequential([\n    Input(shape=(4,4,3)),\n    Conv2D(8, (3,3), padding='same', name='conv1'),\n    Conv2D(4, (3,3), padding='same', name='conv2')\n])\n</code></pre> <pre><code>model.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\nconv1 (Conv2D)               (None, 4, 4, 8)           224\n_________________________________________________________________\nconv2 (Conv2D)               (None, 4, 4, 4)           292\n=================================================================\nTotal params: 516\nTrainable params: 516\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre> <p>Regardons comment se comporte la premi\u00e8re couche convolutive nomm\u00e9e <code>conv1</code>.</p> <pre><code>layer = model.get_layer('conv1')\nweights, biases = layer.get_weights()\nprint(f\"Nombre de canaux en entr\u00e9e : {weights.shape[2]},\\n\"\n      f\"Nombre de neurones dans la couche : {weights.shape},\\n\"\n      f\"Biais : {biases.shape}\")\n\nNombre de canaux en entr\u00e9e : 3,\nNombre de neurones dans la couche : (3, 3, 3, 8),\nBiais : (8,)\n</code></pre>"},{"location":"deep_learning/module2/tp2/#exercice","title":"Exercice","text":"<p>Pouvez vous d\u00e9terminer \u00e0 quoi correspond chacun des nombres dans <code>weights.shape</code> ?</p>  TensorFlow <p>Dans une couche convolutive, les poids obtenues par <code>weights.shape</code> correspondent \u00e0</p> \\[ (\\text{kernel_height}, \\text{kernel_width}, \\text{in_channels}, \\text{out_channels}). \\]"},{"location":"deep_learning/module2/tp2/#exercice_1","title":"Exercice","text":"<ul> <li>Pourquoi le nombre d'entr\u00e9es de <code>conv1</code> est il \u00e9gal \u00e0 \\(3\\) ?</li> <li>Quel est le nombre total de poids de <code>conv1</code> ?</li> </ul>  TensorFlow \\[     3 \\times 3 \\times 3 \\times 8 = 216 \\]  <ul> <li>Pourquoi n'y a t'il que \\(8\\) biais ?</li> </ul>"},{"location":"deep_learning/module2/tp2/#exercice_2","title":"Exercice","text":"<pre><code>layer = model.get_layer('conv2')\nweights2, biases2 = layer.get_weights()\nprint(f\"Nombre d'entr\u00e9es : {weights2.shape[2]},\\n\"\n      f\"Nombre de neurones dans la couche : {weights2.shape},\\n\"\n      f\"Biais : {biases2.shape}\")\n</code></pre> <ul> <li>Pourquoi le nombre d'entr\u00e9es de <code>conv2</code> est il \u00e9gal \u00e0 \\(8\\) ?</li> <li>Quel est le nombre total de poids de <code>conv2</code> ?</li> </ul>  TensorFlow \\[     3 \\times 3 \\times 8 \\times 4 = 198 \\]  <ul> <li>Pourquoi n'y a t'il que \\(4\\) biais ?</li> </ul>"},{"location":"deep_learning/module2/tp2/#loperation-de-pooling","title":"L'op\u00e9ration de pooling","text":"<p>Comme expliqu\u00e9e, l'op\u00e9ration de pooling est l\u00e0 pour diminuer l'information pr\u00e9sente dans les pixels proches, en appliquant une op\u00e9ration d'aggr\u00e9gation.</p> <p>Avec <code>tf.keras</code> les couches de pooling sont appell\u00e9es via <code>tf.keras.layers</code>.</p>"},{"location":"deep_learning/module2/tp2/#exercice_3","title":"Exercice","text":"<p>Dans la suite est d\u00e9finie une fonction permettant de voir l'effet du pooling sur notre fleur.</p> <p>Malheureusement j'ai oubli\u00e9 comment on appelle les couches</p> <ul> <li>average pooling 2D,</li> <li>maximum pooling 2D.</li> </ul> <p>Regardez la doc de l'API tf.keras.layers et chercher la d\u00e9nomination de ces deux couches (indice, le nom est dans le menu d\u00e9filent de droite)</p> <p>et placez le nom nom de la couche \u00e0 la place de <code>tf.keras.layers.FONCTION</code></p> <ul> <li>L'image que l'on placera dans la variable img sera celle d\u00e9finie avant grayscale.</li> <li><code>pool_size</code> correspond \u00e0 la taille du filtre de pooling. Commencez avec 2, puis n'h\u00e9sitez pas \u00e0 changer voir voir ce que cela fait.</li> </ul> <pre><code>def pool(img,pool_size):\n\n    height, width = img.shape\n    tf_img = tf.reshape(img, (-1, height, width, 1))\n\n    out=tf.keras.layers.FONCTION(pool_size, padding='valid')(tf_img)\n    pool = out[0,:,:,0]\n\n    plt.figure(figsize = (16,16))\n    plt.grid(True)\n    plt.imshow(pool)\n    plt.show\n\n    print(f'Les dimensions avant pooling sont {img.shape} \\n'\n          f'Les dimensions apr\u00e8s pooling sont {pool.shape}')\n</code></pre>  TensorFlow : average pooling 2D <pre><code>def pool(img,pool_size):\n\nheight, width = img.shape\ntf_img = tf.reshape(img, (-1, height, width, 1))\n\nout=tf.keras.layers.AvgPool2D(pool_size, padding='valid')(tf_img)\npool = out[0,:,:,0]\n\nplt.figure(figsize = (16,16))\nplt.grid(True)\nplt.imshow(pool)\nplt.show\n\nprint(f'Les dimensions avant pooling sont {img.shape} \\n'\n      f'Les dimensions apr\u00e8s pooling sont {pool.shape}')\n</code></pre>"},{"location":"deep_learning/module2/tp2/#exercice_4","title":"Exercice","text":"<ul> <li>Essayez avec <code>pool(grayscale,4)</code>.</li> </ul>  TensorFlow <pre><code>pool(grayscale,4)\n\nLes dimensions avant pooling sont (427, 640)\nLes dimensions apr\u00e8s pooling sont (106, 160)\n</code></pre>   <ul> <li>Dans <code>pool</code>, changez la fonction d'<code>average pooling 2D</code> par un <code>max pool 2D</code>, puis voyez le r\u00e9sultat avec <code>pool(grayscale,2)</code>.</li> </ul>  TensorFlow <pre><code>def pool(img,pool_size):\n\nheight, width = img.shape\ntf_img = tf.reshape(img, (-1, height, width, 1))\n\nout=tf.keras.layers.MaxPool2D(pool_size, padding='valid')(tf_img)\npool = out[0,:,:,0]\n\nplt.figure(figsize = (16,16))\nplt.grid(True)\nplt.imshow(pool)\nplt.show\n\nprint(f'Les dimensions avant pooling sont {img.shape} \\n'\n      f'Les dimensions apr\u00e8s pooling sont {pool.shape}')\n</code></pre> <pre><code>pool(grayscale,2)\n\nLes dimensions avant pooling sont (427, 640)\nLes dimensions apr\u00e8s pooling sont (213, 320)\n</code></pre>   <p>Voici un exemple \"\u00e0 la main\".</p> <pre><code>test_img2 = np.array([[0,0,0,0,0,0,0,0],\n                       [0,0,1,1,1,0,0,0],\n                       [0,0,1,0,1,0,0,0],\n                       [0,0,1,0,1,0,0,0],\n                       [0,0,1,1,1,0,0,0],\n                       [0,0,1,0,1,0,0,0],\n                       [0,0,1,0,1,0,0,0],\n                       [0,0,1,1,1,0,0,0]], dtype=np.float32)\n\nplt.figure(figsize = (16,16))\nplt.grid(True)\nplt.imshow(test_img2)\nplt.show\n</code></pre>  <pre><code>tf_img = tf.reshape(test_img2, (1,8, 8, 1))\n\nout=tf.keras.layers.MaxPool2D(2)(tf_img)\npool = out[0,:,:,0]\n\nplt.figure(figsize = (8,8))\nplt.grid(True)\nplt.imshow(pool)\nplt.show\n\nprint(pool.shape)\n</code></pre>  <p>Effectivement on perd de l'information. Mais qu'est ce qui \u00e9tait le plus le plus important ? le fait que \u00e7a ressemble \u00e0 un 8, ou le fait que que la figure \u00e9tait plus haute que large ?</p>"},{"location":"deep_learning/module2/tp2/#construction-et-entrainement-dun-cnn","title":"Construction et entra\u00eenement d'un CNN","text":"<p>Pour voir la diff\u00e9rence entre les r\u00e9seaux de neurones denses vu \u00e0 la s\u00e9ance pr\u00e9c\u00e9dente et nos CNN, nous utiliserons de nouveau le dataset CIFAR-10.</p> <p>Rappelons que le dataset CIFAR-10 comprend \\(60000\\) images couleur \\(32\\times32\\) r\u00e9parties en \\(10\\) classes, avec \\(6000\\) images par classe. Il y a \\(50000\\) images d'entra\u00eenement et \\(10000\\) images de test.</p> <pre><code>(X_train,y_train), (X_test,y_test)  = tf.keras.datasets.cifar10.load_data()\n\nprint(X_train.shape, y_train.shape)\n\nDownloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170500096/170498071 [==============================] - 6s 0us/step\n(50000, 32, 32, 3) (50000, 1)\n</code></pre> <p>Les classes pr\u00e9sentes dans le dataset sont les suivantes, chacunes repr\u00e9sent\u00e9es par un chiffre de \\(0\\) \u00e0 \\(9\\). Dans l'ordre nous avons :</p> <ul> <li>airplane</li> <li>automobile</li> <li>bird</li> <li>cat</li> <li>deer</li> <li>dog</li> <li>frog</li> <li>horse</li> <li>ship</li> <li>truck</li> </ul> <p>C'est aussi un dataset tr\u00e8s connu pour faire du benchmark de mod\u00e8le dans le milieu acad\u00e9mique. Revoyons, comme dans le tp pr\u00e9c\u00e9dent, \u00e0 quoi ressemble ce dataset.</p> <pre><code>class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n\nn_rows = 4\nn_cols = 10\nplt.figure(figsize=(n_cols * 2, n_rows * 2))\nfor row in range(n_rows):\n    for col in range(n_cols):\n        index = n_cols * row + col\n        plt.subplot(n_rows, n_cols, index + 1)\n        plt.imshow(X_train[index, : , :, :], cmap=\"binary\", interpolation=\"nearest\")\n        plt.axis('off')\n        plt.title(class_names[y_train[:,0][index]], fontsize=12)\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\n</code></pre>  <p>Mettons le en forme et appliquons directement un One-Hot-Encoding sur les cibles.</p> <pre><code>X_train = X_train.reshape(-1, 32, 32, 3).astype('float32')\nX_test = X_test.reshape(-1, 32, 32, 3).astype('float32')\n\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, random_state=RANDOM_SEED)\n\nX_test = (X_test - 127.5) / 127.5 # Normalize the images to [-1, 1]\nX_train = (X_train - 127.5) / 127.5 # Normalize the images to [-1, 1]\nX_valid = (X_valid - 127.5) / 127.5 # Normalize the images to [-1, 1]\n\ny_train_oh = tf.keras.utils.to_categorical(y_train, num_classes=10)\ny_test_oh = tf.keras.utils.to_categorical(y_test, num_classes=10)\ny_valid_oh = tf.keras.utils.to_categorical(y_valid, num_classes=10)\n\nprint(y_train_oh.shape)\n\n(37500, 10)\n</code></pre> <pre><code>model = models.Sequential([\n    Input(shape=(32, 32, 3)),\n    Conv2D(32, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Conv2D(128, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Conv2D(128, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Flatten(),\n    Dense(10, activation='softmax')\n])\n\nmodel.summary()\n</code></pre> <p>Vous avez une erreur du type</p> <p><code>InvalidArgumentError: Negative dimension size caused by subtracting 3 from 2 ...</code></p> <pre><code>---------------------------------------------------------------------------\nInvalidArgumentError                      Traceback (most recent call last)\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs, op_def)\n   1653   try:\n-&gt; 1654     c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\n   1655   except errors.InvalidArgumentError as e:\n\nInvalidArgumentError: Negative dimension size caused by subtracting 3 from 2 for '{{node conv2d_3/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](max_pooling2d_2/Identity, conv2d_3/Conv2D/ReadVariableOp)' with input shapes: [?,2,2,128], [3,3,128,128].\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-13-7fe5ac773bd4&gt; in &lt;module&gt;()\n     10     MaxPooling2D((2, 2)),\n     11     Flatten(),\n---&gt; 12     Dense(10, activation='softmax')\n     13 ])\n     14\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\n    454     self._self_setattr_tracking = False  # pylint: disable=protected-access\n    455     try:\n--&gt; 456       result = method(self, *args, **kwargs)\n    457     finally:\n    458       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py in __init__(self, layers, name)\n    127       tf_utils.assert_no_legacy_layers(layers)\n    128       for layer in layers:\n--&gt; 129         self.add(layer)\n    130\n    131   @property\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\n    454     self._self_setattr_tracking = False  # pylint: disable=protected-access\n    455     try:\n--&gt; 456       result = method(self, *args, **kwargs)\n    457     finally:\n    458       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py in add(self, layer)\n    211       # If the model is being built continuously on top of an input layer:\n    212       # refresh its output.\n--&gt; 213       output_tensor = layer(self.outputs[0])\n    214       if len(nest.flatten(output_tensor)) != 1:\n    215         raise ValueError(SINGLE_LAYER_OUTPUT_ERROR_MSG)\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\n    920                     not base_layer_utils.is_in_eager_or_tf_function()):\n    921                   with auto_control_deps.AutomaticControlDependencies() as acd:\n--&gt; 922                     outputs = call_fn(cast_inputs, *args, **kwargs)\n    923                     # Wrap Tensors in `outputs` in `tf.identity` to avoid\n    924                     # circular dependencies.\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/convolutional.py in call(self, inputs)\n    205       inputs = array_ops.pad(inputs, self._compute_causal_padding())\n    206\n--&gt; 207     outputs = self._convolution_op(inputs, self.kernel)\n    208\n    209     if self.use_bias:\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py in __call__(self, inp, filter)\n   1104           call_from_convolution=False)\n   1105     else:\n-&gt; 1106       return self.conv_op(inp, filter)\n   1107\n   1108\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py in __call__(self, inp, filter)\n    636\n    637   def __call__(self, inp, filter):  # pylint: disable=redefined-builtin\n--&gt; 638     return self.call(inp, filter)\n    639\n    640\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py in __call__(self, inp, filter)\n    235         padding=self.padding,\n    236         data_format=self.data_format,\n--&gt; 237         name=self.name)\n    238\n    239\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py in conv2d(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name, filters)\n   2012                            data_format=data_format,\n   2013                            dilations=dilations,\n-&gt; 2014                            name=name)\n   2015\n   2016\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_nn_ops.py in conv2d(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\n    967                   padding=padding, use_cudnn_on_gpu=use_cudnn_on_gpu,\n    968                   explicit_paddings=explicit_paddings,\n--&gt; 969                   data_format=data_format, dilations=dilations, name=name)\n    970   _result = _outputs[:]\n    971   if _execute.must_record_gradient():\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)\n    742       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    743                                  name=scope, input_types=input_types,\n--&gt; 744                                  attrs=attr_protos, op_def=op_def)\n    745\n    746     # `outputs` is returned as a separate return value so that the output\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in _create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\n    593     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    594         op_type, inputs, dtypes, input_types, name, attrs, op_def,\n--&gt; 595         compute_device)\n    596\n    597   def capture(self, tensor, name=None, shape=None):\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\n   3325           input_types=input_types,\n   3326           original_op=self._default_original_op,\n-&gt; 3327           op_def=op_def)\n   3328       self._create_op_helper(ret, compute_device=compute_device)\n   3329     return ret\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\n   1815         op_def = self._graph._get_op_def(node_def.op)\n   1816       self._c_op = _create_c_op(self._graph, node_def, inputs,\n-&gt; 1817                                 control_input_ops, op_def)\n   1818       name = compat.as_str(node_def.name)\n   1819     # pylint: enable=protected-access\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs, op_def)\n   1655   except errors.InvalidArgumentError as e:\n   1656     # Convert to ValueError for backwards compatibility.\n-&gt; 1657     raise ValueError(str(e))\n   1658\n   1659   return c_op\n\nValueError: Negative dimension size caused by subtracting 3 from 2 for '{{node conv2d_3/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](max_pooling2d_2/Identity, conv2d_3/Conv2D/ReadVariableOp)' with input shapes: [?,2,2,128], [3,3,128,128].\n</code></pre> <p>C'est normal, Rappelez vous que par d\u00e9faut, l'op\u00e9ration de convolution, fait baisser la dimension des features maps. Plus on empile de couches convolutives, plus la dimension baisse, jusqu'\u00e0 un point o\u00f9 la taille des features maps est tellement petite qu'il n'est plus possible d'appliquer une op\u00e9ration de convolution.</p> <p>Pour contrer cela, on doit rajouter le param\u00e8tre <code>padding='same'</code> dans les couches de convolution.</p> <pre><code>model = models.Sequential([\n    Input(shape=(32, 32, 3)),\n    Conv2D(32, (3, 3), padding='same'),\n    Activation('relu'),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), padding='same'),\n    Activation('relu'),\n    MaxPooling2D((2, 2)),\n    Conv2D(128, (3, 3), padding='same'),\n    Activation('relu'),\n    MaxPooling2D((2, 2)),\n    Conv2D(128, (3, 3), padding='same'),\n    Activation('relu'),\n    MaxPooling2D((2, 2)),\n    Flatten(),\n    Dense(10, activation='softmax')\n])\n\nmodel.summary()\n\nmodel.compile(loss = 'categorical_crossentropy',\n              optimizer=tf.keras.optimizers.SGD(lr=0.001),\n              metrics=['accuracy'])\n</code></pre> <p>Si l'on souhaite, il est \u00e9videmment possible de faire passer le mod\u00e8le dans une fonction.</p> <pre><code>def define_model(lr=0.001, num_classes=10):\n\n  model = models.Sequential([\n    Input(shape=(32, 32, 3)),\n    Conv2D(32, (3, 3), padding='same'),\n    Activation('relu'),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), padding='same'),\n    Activation('relu'),\n    MaxPooling2D((2, 2)),\n    Conv2D(128, (3, 3), padding='same'),\n    Activation('relu'),\n    MaxPooling2D((2, 2)),\n    Conv2D(128, (3, 3), padding='same'),\n    Activation('relu'),\n    MaxPooling2D((2, 2)),\n    Flatten(),\n    Dense(num_classes, activation='softmax')\n    ])\n\n  model.summary()\n\n  model.compile(loss = 'categorical_crossentropy',\n                optimizer=tf.keras.optimizers.SGD(lr=lr),\n                metrics=['accuracy'])\n\n  return model\n\nmodel = define_model()\n</code></pre> <pre><code>history = model.fit(X_train, y_train_oh,\n                    epochs = 20,\n                    batch_size=32,\n                    validation_data=(X_valid, y_valid_oh))\n</code></pre> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\npd.DataFrame(history.history).plot(figsize=(12,8))\nplt.grid(True)\nplt.gca().set_ylim(0,2)\nplt.show()\n</code></pre> <p>Vous devriez avoir un graphe similaire.</p>  <p>V\u00e9rifiez les m\u00e9triques obtenues avec les commandes suivantes.</p> <pre><code>loss, accuracy = model.evaluate(X_test,\n                                y_test_oh)\n\ncomp = comp.append({'run': 'basic_CNN', 'Perte' : loss, 'Pr\u00e9cision' : accuracy}, ignore_index=True)\nprint(comp)\n</code></pre> <p>Certes, la pr\u00e9cision n'est pas terrible, mais ce que l'on a \u00e9crit comme mod\u00e8le ici n'est pas terrible non plus.</p> <p>Cependant, on peut remarquer une chose.</p> <p>Avec un mod\u00e8le compos\u00e9 uniquement de neurones denses, on \u00e9tait en sur-apprentissage d\u00e8s l'\u00e9poque 3 ou 4. Sauf qu'ici :</p> <ul> <li>La fonction de perte, que ce soit sur le dataset d'entra\u00eenement, comme sur le dataset de validation, se suivent et ne d\u00e9crochent pas.</li> <li>Idem pour la pr\u00e9cision.</li> </ul> <p>Ce qui nous conforte dans notre id\u00e9e que les CNNs sont fait pour du traitement d'images, et on a m\u00eame pas encore rajout\u00e9 la couche classifiante !</p> <p>Pardon ? La couche classifiante n'est pas celle avec le softmax ?</p> <p>Il faut comprendre 2 choses :</p> <ul> <li>Les couches convolutives sont tr\u00e8s bonnes pour extraires des caract\u00e9ristiques geom\u00e9triques des images,</li> <li>Les couches denses sont l\u00e0 pour faire les combinaisons lin\u00e9aires des vecteurs qui semblent les plus appropri\u00e9es.</li> </ul> <p>Ainsi, un CNN peut se d\u00e9composer en deux parties :</p> <ul> <li>La partie d'extraction des caract\u00e9ristiques avec les couches convolutives et de pooling.</li> <li>La partie classifiante avec les neurones denses.</li> </ul> <p>Mais pour pouvoir passer de l'une \u00e0 l'autre, il faut \u00ea\u00eatre capable d'aplanir des matrices, c'est le fameux <code>Flatten()</code>.</p> <p>Dans la pratique on ne met pas non plus autant de d'op\u00e9ration de pooling entre les couches convolutives.</p>"},{"location":"deep_learning/module2/tp2/#exercice_5","title":"Exercice","text":"<p>En vous aidant du mod\u00e8le pr\u00e9cedent, construisez le mod\u00e8le s\u00e9quentiel avec l'architecture suivante.</p> <pre><code>Input(shape=(32, 32, 3)),\nConv2D(32, (3, 3)),\nConv2D(64, (3, 3)),\nMaxPooling2D((2, 2)),\nConv2D(128, (3, 3)),\nConv2D(128, (3, 3)),\nMaxPooling2D((2, 2)),\nConv2D(256, (3, 3)),\nConv2D(256, (3, 3)),\nMaxPooling2D((2, 2)),\nFlatten(),\nDense(256),\nDense(128),\nDense(10, activation='softmax')\n</code></pre> <p>La fonction d'activation pour passer d'une couche \u00e0 l'autre sera la fonction <code>relu</code>. Pour les couches convolutives on fixera <code>padding=same</code>.</p> <p>Entra\u00eenez ce mod\u00e8le, les param\u00e8tres de <code>model.compile</code> et <code>model.fit</code> seront les m\u00eames que pr\u00e9c\u00e9demment.</p>  TensorFlow <pre><code>model2 = models.Sequential([\n    Input(shape=(32, 32, 3)),\n    Conv2D(32, (3, 3), activation='relu', padding='same'),\n    Conv2D(64, (3, 3), activation='relu', padding='same'),\n    MaxPooling2D((2, 2)),\n    Conv2D(128, (3, 3), activation='relu', padding='same'),\n    Conv2D(128, (3, 3), activation='relu', padding='same'),\n    MaxPooling2D((2, 2)),\n    Conv2D(256, (3, 3), activation='relu', padding='same'),\n    Conv2D(256, (3, 3), activation='relu', padding='same'),\n    MaxPooling2D((2, 2)),\n    Flatten(),\n    Dense(256),\n    Activation('relu'),\n    Dense(128),\n    Activation('relu'),\n    Dense(10, activation='softmax')\n])\n\nmodel2.summary()\n</code></pre> <pre><code>Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\nconv2d_8 (Conv2D)            (None, 32, 32, 32)        896\n_________________________________________________________________\nconv2d_9 (Conv2D)            (None, 32, 32, 64)        18496\n_________________________________________________________________\nmax_pooling2d_8 (MaxPooling2 (None, 16, 16, 64)        0\n_________________________________________________________________\nconv2d_10 (Conv2D)           (None, 16, 16, 128)       73856\n_________________________________________________________________\nconv2d_11 (Conv2D)           (None, 16, 16, 128)       147584\n_________________________________________________________________\nmax_pooling2d_9 (MaxPooling2 (None, 8, 8, 128)         0\n_________________________________________________________________\nconv2d_12 (Conv2D)           (None, 8, 8, 256)         295168\n_________________________________________________________________\nconv2d_13 (Conv2D)           (None, 8, 8, 256)         590080\n_________________________________________________________________\nmax_pooling2d_10 (MaxPooling (None, 4, 4, 256)         0\n_________________________________________________________________\nflatten_2 (Flatten)          (None, 4096)              0\n_________________________________________________________________\ndense_2 (Dense)              (None, 256)               1048832\n_________________________________________________________________\nactivation_4 (Activation)    (None, 256)               0\n_________________________________________________________________\ndense_3 (Dense)              (None, 128)               32896\n_________________________________________________________________\nactivation_5 (Activation)    (None, 128)               0\n_________________________________________________________________\ndense_4 (Dense)              (None, 10)                1290\n=================================================================\nTotal params: 2,209,098\nTrainable params: 2,209,098\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre> <pre><code>model2.compile(loss = 'categorical_crossentropy',\n         optimizer=tf.keras.optimizers.SGD(lr=0.001),\n         metrics=['accuracy'])\n\nhistory = model2.fit(X_train, y_train_oh,\n                    epochs = 20,\n                    batch_size=32,\n                    validation_data=(X_valid, y_valid_oh))\n</code></pre>  <p>Apr\u00e8s l'entra\u00eenement, vous pouvez comme d'habitude voir comment il s'en sort avec les commandes suivantes.</p> <pre><code>pd.DataFrame(history.history).plot(figsize=(12,8))\nplt.grid(True)\nplt.gca().set_ylim(0,2)\nplt.show()\n</code></pre> <pre><code>loss, accuracy = model2.evaluate(X_test,\n                                y_test_oh)\n\ncomp = comp.append({'run': 'basic_CNN_2', 'Perte' : loss, 'Pr\u00e9cision' : accuracy}, ignore_index=True)\ncomp\n</code></pre> <p>Il est temps de nous attaquer \u00e0 un mod\u00e8le plus int\u00e9ressant : VGG16.</p>"},{"location":"deep_learning/module2/tp2/#vgg16","title":"VGG16","text":"<p>VGG16 est un r\u00e9seau de neurones publi\u00e9 en 2015 (du point de vue du deep learning, c'est vieux) par le groupe de recherche Visual Geometry Group de l'universit\u00e9 d'Oxford.</p> <p>A l'\u00e9poque, ce r\u00e9seau \u00e9tait au niveau de l'\u00e9tat de l'art, aujourd'hui des mod\u00e8les plus rapides \u00e0 entra\u00eener et plus pr\u00e9cis ont pris sa place, mais pour l'instant ils sont plus compliqu\u00e9 \u00e0 coder, ce qui fait qu'il peut toujours faire office de couteau suisse et qu'il est encore parfois utilis\u00e9 de nos jours.</p>  <p>Architecture</p>  <p>Dans l'article nous pouvons lire les choses suivantes :</p> <ul> <li> <p>\u201cAll hidden layers are equipped with the rectification (ReLU (Krizhevsky et al., 2012)) non-linearity.\u201d</p> </li> <li> <p>\u201cMax-pooling is performed over a 2 \u00d7 2 pixel window, with stride 2.\u201d</p> </li> </ul> <p>Donc la seule fonction d'activation, hormis celle pour classifier, est ReLU, de plus l'op\u00e9ration de pooling est le <code>MaxPooling2D((2,2))</code>, le stride est de 2.</p> <p>Remarque : Si le stride est le m\u00eame que la fen\u00eatre de pooling, on a pas \u00e0 le sp\u00e9cifier.</p> <p>Concernant l'architecture compl\u00e8te du mod\u00e8le, on la revoie ici, ie</p> <ul> <li> <p>Le r\u00e9seau consiste en 5 blocs convolutifs et 3 couches denses.</p> </li> <li> <p>Chaque bloc convolutif est constitu\u00e9 de 2 ou plus couches convolutives et d'une couche de max pooling.</p> </li> </ul>"},{"location":"deep_learning/module2/tp2/#exercice_6","title":"Exercice","text":"<p>Ecrivez chacun des blocs de VGG16, via l'API fonctionnelle. Pour toutes les couches convolutives, le padding est <code>padding='same'</code>.</p> <p>Attention \u00e0 ne pas oublier les fonctions d'activations !</p> <p>Regroupez tout cela pour construire le mod\u00e8le.</p> <p>N'oubliez pas que pour l'API fonctionnelle, on aura besoin des ces lignes de codes suivantes :</p> <pre><code>from tensorflow.keras import Model\n</code></pre> <pre><code>model = Model(inputs=input, outputs=output)\n</code></pre> <pre><code>input = Input(shape=(32, 32, 3))\n</code></pre> <p>Et n'oubliez pas le <code>Flatten()</code>.</p>"},{"location":"deep_learning/module2/tp2/#1er-bloc","title":"1er Bloc","text":"<ul> <li>\\(\\mathrm{Conv2D}(64, (3,3))\\)</li> <li>\\(\\mathrm{Conv2D}(64, (3,3))\\)</li> <li>\\(\\mathrm{MaxPool2D}\\)</li> </ul>  TensorFlow <pre><code>x = Conv2D(filters=64, kernel_size=3, padding='same', activation='relu')(input)\nx = Conv2D(filters=64, kernel_size=3, padding='same', activation='relu')(x)\nx = MaxPool2D(pool_size=2, strides=2, padding='same')(x)\n</code></pre>"},{"location":"deep_learning/module2/tp2/#2e-bloc","title":"2e Bloc","text":"<ul> <li>\\(\\mathrm{Conv2D}(128, (3,3))\\)</li> <li>\\(\\mathrm{Conv2D}(128, (3,3))\\)</li> <li>\\(\\mathrm{MaxPool2D}\\)</li> </ul>  TensorFlow <pre><code>x = Conv2D(filters=128, kernel_size=3, padding='same', activation='relu')(x)\nx = Conv2D(filters=128, kernel_size=3, padding='same', activation='relu')(x)\nx = MaxPool2D(pool_size=2, strides=2, padding='same')(x)\n</code></pre>"},{"location":"deep_learning/module2/tp2/#3e-bloc","title":"3e Bloc","text":"<ul> <li>\\(\\mathrm{Conv2D}(256, (3,3))\\)</li> <li>\\(\\mathrm{Conv2D}(256, (3,3))\\)</li> <li>\\(\\mathrm{Conv2D}(256, (3,3))\\)</li> <li>\\(\\mathrm{MaxPool2D}\\)</li> </ul>  TensorFlow <pre><code>x = Conv2D(filters=256, kernel_size=3, padding='same', activation='relu')(x)\nx = Conv2D(filters=256, kernel_size=3, padding='same', activation='relu')(x)\nx = Conv2D(filters=256, kernel_size=3, padding='same', activation='relu')(x)\nx = MaxPool2D(pool_size=2, strides=2, padding='same')(x)\n</code></pre>"},{"location":"deep_learning/module2/tp2/#4e-bloc","title":"4e Bloc","text":"<ul> <li>\\(\\mathrm{Conv2D}(512, (3,3))\\)</li> <li>\\(\\mathrm{Conv2D}(512, (3,3))\\)</li> <li>\\(\\mathrm{Conv2D}(512, (3,3))\\)</li> <li>\\(\\mathrm{MaxPool2D}\\)</li> </ul>  TensorFlow <pre><code>x = Conv2D(filters=512, kernel_size=3, padding='same', activation='relu')(x)\nx = Conv2D(filters=512, kernel_size=3, padding='same', activation='relu')(x)\nx = Conv2D(filters=512, kernel_size=3, padding='same', activation='relu')(x)\nx = MaxPool2D(pool_size=2, strides=2, padding='same')(x)\n</code></pre>"},{"location":"deep_learning/module2/tp2/#5e-bloc","title":"5e Bloc","text":"<ul> <li>\\(\\mathrm{Conv2D}(512, (3,3))\\)</li> <li>\\(\\mathrm{Conv2D}(512, (3,3))\\)</li> <li>\\(\\mathrm{Conv2D}(512, (3,3))\\)</li> <li>\\(\\mathrm{MaxPool2D}\\)</li> </ul>  TensorFlow <pre><code>x = Conv2D(filters=512, kernel_size=3, padding='same', activation='relu')(x)\nx = Conv2D(filters=512, kernel_size=3, padding='same', activation='relu')(x)\nx = Conv2D(filters=512, kernel_size=3, padding='same', activation='relu')(x)\nx = MaxPool2D(pool_size=2, strides=2, padding='same')(x)\n</code></pre>"},{"location":"deep_learning/module2/tp2/#couche-dense","title":"Couche dense","text":"<ul> <li>\\(\\mathrm{Dense}(256)\\)</li> <li>\\(\\mathrm{Dense}(256)\\)</li> <li>\\(\\mathrm{Dense}(10)\\)</li> <li>\\(\\mathrm{Softmax}\\)</li> </ul>  TensorFlow <pre><code>x = Flatten()(x)\nx = Dense(units=256, activation='relu')(x)\nx = Dense(units=128 activation='relu')(x)\noutput = Dense(units=10, activation='softmax')(x)\n</code></pre>   Mod\u00e8le complet <pre><code>input = Input(shape=(32, 32, 3))\n\nx = Conv2D(filters=64, kernel_size=3, padding='same', activation='relu')(input)\nx = Conv2D(filters=64, kernel_size=3, padding='same', activation='relu')(x)\nx = MaxPooling2D(pool_size=2, strides=2, padding='same')(x)\n\nx = Conv2D(filters=128, kernel_size=3, padding='same', activation='relu')(x)\nx = Conv2D(filters=128, kernel_size=3, padding='same', activation='relu')(x)\nx = MaxPooling2D(pool_size=2, strides=2, padding='same')(x)\n\nx = Conv2D(filters=256, kernel_size=3, padding='same', activation='relu')(x)\nx = Conv2D(filters=256, kernel_size=3, padding='same', activation='relu')(x)\nx = Conv2D(filters=256, kernel_size=3, padding='same', activation='relu')(x)\nx = MaxPooling2D(pool_size=2, strides=2, padding='same')(x)\n\nx = Conv2D(filters=512, kernel_size=3, padding='same', activation='relu')(x)\nx = Conv2D(filters=512, kernel_size=3, padding='same', activation='relu')(x)\nx = Conv2D(filters=512, kernel_size=3, padding='same', activation='relu')(x)\nx = MaxPooling2D(pool_size=2, strides=2, padding='same')(x)\n\nx = Conv2D(filters=512, kernel_size=3, padding='same', activation='relu')(x)\nx = Conv2D(filters=512, kernel_size=3, padding='same', activation='relu')(x)\nx = Conv2D(filters=512, kernel_size=3, padding='same', activation='relu')(x)\nx = MaxPooling2D(pool_size=2, strides=2, padding='same')(x)\n\nx = Flatten()(x)\nx = Dense(units=256, activation='relu')(x)\nx = Dense(units=128, activation='relu')(x)\noutput = Dense(units=10, activation='softmax')(x)\n\nvgg = tf.keras.Model(inputs=input, outputs=output)\n\nvgg.summary()\n</code></pre>   <p>Remarque</p> <p>Dans le cas o\u00f9 certains blocs se r\u00e9p\u00e8tent, ce qui est courant dans les architecture modernes, il est pratique des d\u00e9finir les blocs via des fonctions.</p>   <p>D\u00e9finition des fonctions de blocs</p> <pre><code>from tensorflow.keras.layers import ReLU\n\ndef conv2_relu_max(x, filters, kernel_size):\n    x = Conv2D(filters=filters,\n            kernel_size=kernel_size,\n            padding='same')(x)\n    x = ReLU()(x)\n    x = Conv2D(filters=filters,\n            kernel_size=kernel_size,\n            padding='same')(x)\n    x = ReLU()(x)\n    x = MaxPooling2D(pool_size=2, strides=2, padding='same')(x)\n    return x\n\ndef conv3_relu_max(x, filters, kernel_size):\n    x = Conv2D(filters=filters,\n            kernel_size=kernel_size,\n            padding='same')(x)\n    x = ReLU()(x)\n    x = Conv2D(filters=filters,\n            kernel_size=kernel_size,\n            padding='same')(x)\n    x = ReLU()(x)\n    x = Conv2D(filters=filters,\n            kernel_size=kernel_size,\n            padding='same')(x)\n    x = ReLU()(x)\n    x = MaxPooling2D(pool_size=2, strides=2, padding='same')(x)\n    return x\n</code></pre>  <p>VGG16 se r\u00e9\u00e9crit alors sous la forme plus concise suivante.</p> <pre><code>input = Input(shape=(32, 32, 3))\n\nx = conv2_relu_max(input, 64, 3)\nx = conv2_relu_max(x, 128, 3)\nx = conv3_relu_max(x, 256, 3)\nx = conv3_relu_max(x, 512, 3)\nx = conv3_relu_max(x, 512, 3)\nx = Flatten()(x)\nx = Dense(units=256, activation='relu')(x)\nx = Dense(units=128, activation='relu')(x)\noutput = Dense(units=10, activation='softmax')(x)\n\nvgg_short = tf.keras.Model(inputs=input, outputs=output)\n\nvgg_short.summary()\n</code></pre> <p><pre><code>top5 = tf.keras.metrics.TopKCategoricalAccuracy()\n\nvgg.compile(loss = 'categorical_crossentropy',\n            optimizer=tf.keras.optimizers.SGD(lr=0.001),\n            metrics=['accuracy',top5]\n            )\n\nhistory = vgg.fit(X_train, y_train_oh,\n                  epochs = 20,\n                  batch_size=32,\n                  validation_data=(X_valid, y_valid_oh))\n\npd.DataFrame(history.history).plot(figsize=(12,8))\nplt.grid(True)\nplt.gca().set_ylim(0,2)\nplt.show()\n</code></pre> </p> <p>L'architecture est certes plus complexe, mais on a une baisse constante de la perte, et la precision sur le dataset de validation suit de pr\u00e8s cette du dataset d'entra\u00eenement.</p> <p>Cependant, de par sa compl\u00e9xit\u00e9 nous sommes \u00e0 peine \u00e0 \\(20\\%\\) de pr\u00e9cision apr\u00e8s \\(20\\) \u00e9poques (en moyenne, je suis \u00e0 \\(54\\%\\) pour une \\(50\\)-aines d'\u00e9poques), ce qui veut dire que l'on aura besoin de l'entra\u00eener plus longtemps. Mais il n'est surtout pas en sur-apprentissage !</p> <pre><code>loss, accuracy, top5 = vgg.evaluate(X_test,\n                                y_test_oh)\n\nprint(f'Perte : {loss:.3f}, Pr\u00e9cision : {accuracy:.3f}')\n\ncomp = comp.append({'run': 'vgg_scratch', 'Perte' : loss, 'Pr\u00e9cision' : accuracy}, ignore_index=True)\ncomp.head()\n</code></pre> <p>On sait coder VGG16, c'est vraiment cool. Mais notre mod\u00e8le n'est pas optimis\u00e9, des gens avec beaucoup plus d'argent, des puissances de calculs et des plus gros dataset l'ont d\u00e9j\u00e0 optimis\u00e9.</p> <p>C'est le principe de transfer learning, pour des t\u00e2ches similaires, ici de la classification, autant r\u00e9utilis\u00e9 les poids d\u00e9j\u00e0 entra\u00een\u00e9s pour, heureusement, Tensorflow nous le permet via le transfert d'apprentissage.</p>"},{"location":"deep_learning/module2/tp2/#transfer-learning","title":"Transfer Learning","text":"<p>Voyons maintenant comment l'on peut booster la pr\u00e9cision en utilisant des mod\u00e8les d\u00e9j\u00e0 entra\u00een\u00e9s par d'autres.</p>"},{"location":"deep_learning/module2/tp2/#a-la-main","title":"A la main","text":"<p>Dans un premier temps, pour voir un peu comment cela marche nous allons le faire nous m\u00eame.</p> <p>Avec le dataset Fashion MNIST, nous allons entra\u00eener notre mod\u00e8le \u00e0 ne classifier que \\(8\\) classes sur les \\(10\\) pr\u00e9sentes, puis nous r\u00e9utiliserons les couches d\u00e9j\u00e0 entra\u00een\u00e9es afin de faire la base d'un nouveau mod\u00e8le pour classifier les deux derni\u00e8res. On esp\u00e8re que certaines caract\u00e9ristiques apprises sur les \\(8\\) premi\u00e8res classes pourront nous \u00eatre utiles pour le reste.</p> <pre><code># T\u00e9l\u00e9chargeons fashion_mnist et regardons \u00e0 quoi il ressemble.\n(X_train,y_train), (X_test,y_test)  = tf.keras.datasets.fashion_mnist.load_data()\n\nprint(X_train.shape, y_train.shape)\n\nclass_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n\nn_rows = 4\nn_cols = 10\nplt.figure(figsize=(n_cols * 2, n_rows * 2))\nfor row in range(n_rows):\n    for col in range(n_cols):\n        index = n_cols * row + col\n        plt.subplot(n_rows, n_cols, index + 1)\n        plt.imshow(X_train[index, : , :, ], cmap=\"binary\", interpolation=\"nearest\")\n        plt.axis('off')\n        plt.title(class_names[y_train[index]], fontsize=12)\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\n</code></pre>  <pre><code>X_train = X_train.reshape(-1, 28, 28, 1).astype('float32')\nX_test = X_test.reshape(-1, 28, 28, 1).astype('float32')\n\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, random_state=RANDOM_SEED)\n\nX_test = X_test/255\nX_train = X_train/255\nX_valid = X_valid/255\n</code></pre> <p>D\u00e9finissons la fonction suivante pour s\u00e9parer notre dataset en 2 t\u00e2ches distinctes.</p> <pre><code>def split_dataset(X, y):\n    y_8_or_9 = (y == 8) | (y == 9) # \"Bag\", \"Ankle boot\"\n    y_A = y[~y_8_or_9]\n\n    y_B = (y[y_8_or_9] == 9).astype(np.float32) # binary classification : est ce c'est une Ankle boot (classe 9)? (sinon on a le masque True, False)\n    return ((X[~y_8_or_9], y_A),\n            (X[y_8_or_9], y_B))\n\n(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n\n#Pour l'exemple, on prend beaucoup moins de donn\u00e9es pour la classification binaire.\nX_train_B = X_train_B[:200]\ny_train_B = y_train_B[:200]\n</code></pre> <pre><code>print(f'X_train_A.shape : {X_train_A.shape} \\n'\n      f'X_train_B.shape : {X_train_B.shape}')\n\nX_train_A.shape : (35996, 28, 28, 1)\nX_train_B.shape : (200, 28, 28, 1)\n</code></pre> <pre><code>set(y_train_A)\n\n{0, 1, 2, 3, 4, 5, 6, 7}\n</code></pre> <pre><code>set(y_train_B)\n\n{0.0, 1.0}\n</code></pre> <p>Reprenons le CNN classique du d\u00e9but, on l'utilise pour la classification du gros dataset.</p> <pre><code>modelA = models.Sequential([\n    Input(shape=(28, 28, 1)),\n    Conv2D(32, (3, 3), activation='relu', padding='same'),\n    Conv2D(64, (3, 3), activation='relu', padding='same'),\n    MaxPooling2D((2, 2)),\n    Conv2D(128, (3, 3), activation='relu', padding='same'),\n    Conv2D(128, (3, 3), activation='relu', padding='same'),\n    MaxPooling2D((2, 2)),\n    Flatten(),\n    Dense(256),\n    Activation('relu'),\n    Dense(128),\n    Activation('relu'),\n    Dense(8, activation='softmax')\n])\n</code></pre> <pre><code>modelA.compile(loss=\"sparse_categorical_crossentropy\",\n                optimizer=keras.optimizers.SGD(lr=0.001),\n                metrics=[\"accuracy\"])\n\nhistory = modelA.fit(X_train_A, y_train_A,\n                      epochs=20,\n                      validation_data=(X_valid_A, y_valid_A))\n</code></pre> <p>Une fois ce premier entra\u00eenement fait. On peut sauvegarder le mod\u00e8le (l'architecture et les poids) de la mani\u00e8re suivante.</p> <pre><code>modelA.save(\"modelA.h5\")\n</code></pre> <p>Reprenons le m\u00eame mod\u00e8le, mais utilsons le maintenant pour une t\u00e2che de classification binaire.</p> <pre><code>modelB = models.Sequential([\n    Input(shape=(28, 28, 1)),\n    Conv2D(32, (3, 3), activation='relu', padding='same'),\n    Conv2D(64, (3, 3), activation='relu', padding='same'),\n    MaxPooling2D((2, 2)),\n    Conv2D(128, (3, 3), activation='relu', padding='same'),\n    Conv2D(128, (3, 3), activation='relu', padding='same'),\n    MaxPooling2D((2, 2)),\n    Flatten(),\n    Dense(256),\n    Activation('relu'),\n    Dense(128),\n    Activation('relu'),\n    Dense(1, activation='sigmoid')\n])\n\nmodelB.compile(loss=\"binary_crossentropy\",\n                optimizer=keras.optimizers.SGD(lr=1e-3),\n                metrics=[\"accuracy\"])\n\nhistory = modelB.fit(X_train_B, y_train_B,\n                     epochs=20,\n                     validation_data=(X_valid_B, y_valid_B))\n</code></pre> <p>Passons \u00e0 l'\u00e9tape de transfer learning. On recharge d'abord le mod\u00e8le \u00e0 8 classes</p> <pre><code>modelA = keras.models.load_model(\"modelA.h5\")\n</code></pre> <p>On prend toute l'architecture et tous les poids, SAUF la derni\u00e8re couche de classification</p> <pre><code>model_B_on_A = keras.models.Sequential(modelA.layers[:-1])\n</code></pre> <p>On ajoute la couche de classification sp\u00e9cifique.</p> <pre><code>model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n</code></pre> <p>L'id\u00e9e est que des poids qui ont \u00e9t\u00e9 entra\u00een\u00e9 pour un t\u00e2che sp\u00e9cifique, devrait \u00eatre performant pour une t\u00e2che similaire, m\u00eame si les deux datasets sont totalements disjoints.</p> <p>Des couches entra\u00een\u00e9es \u00e0 classifier des v\u00eatements devraient \u00eatre performants pour en classifier d'autres.</p> <p>On \u00e0 g\u00e9n\u00e9ralement 2 \u00e9tapes.</p> <ul> <li> <p>On g\u00e8le les poids que l'on va utiliser pour le transfert d'apprentissage, seuls les poids de la couche classifiante form\u00e9es des neurones denses se mettra \u00e0 jour lors de l'\u00e9tape de r\u00e9tropropagation.</p> <pre><code>for layer in model_B_on_A.layers[:-1]:\n    layer.trainable = False\n</code></pre> </li> <li> <p>On d\u00e9g\u00e8le tout ou partie des poids, pour les sp\u00e9cialiser encore plus pour notre probl\u00e8me.</p> <pre><code>for layer in model_B_on_A.layers[:-1]:\nlayer.trainable = True\n</code></pre> </li> </ul> <p>Mettons \u00e7a en place.</p> <pre><code>for layer in model_B_on_A.layers[:-1]:\n  layer.trainable = False\n\nmodel_B_on_A.compile(loss=\"binary_crossentropy\",\n                     optimizer=keras.optimizers.SGD(lr=0.001),\n                     metrics=[\"accuracy\"])\n\n# On entra\u00eene avec les poids gel\u00e9s pour 4 \u00e9poques.\nhistory = model_B_on_A.fit(X_train_B,\n                           y_train_B,\n                           epochs=4,\n                           validation_data=(X_valid_B, y_valid_B))\n\n# on d\u00e9g\u00e8le tout pour 16 \u00e9poques.\nfor layer in model_B_on_A.layers[:-1]:\n  layer.trainable = True\n\nmodel_B_on_A.compile(loss=\"binary_crossentropy\",\n                     optimizer=keras.optimizers.SGD(lr=0.001),\n                     metrics=[\"accuracy\"])\n\nhistory = model_B_on_A.fit(X_train_B, y_train_B,\n                           epochs=16,\n                           validation_data=(X_valid_B, y_valid_B))\n</code></pre> <p>Voyons ce que cela donne au niveau du taux d'erreur.</p> <pre><code>_, accB = modelB.evaluate(X_test_B, y_test_B)\n_, accB_on_A = model_B_on_A.evaluate(X_test_B, y_test_B)\nnp.round((100 - (accB*100)) / (100 - (accB_on_A*100)),2)\n</code></pre> <p>Avec un tel transfert d'apprentissage, le taux d'erreur devrait diminuer d'un facteur important !</p>"},{"location":"deep_learning/module2/tp2/#importation-des-poids-de-imagenet","title":"Importation des poids de ImageNet","text":"<p>Imagenet et LE dataset de r\u00e9f\u00e9rence pour le benchmark de mod\u00e8le est l'entra\u00eenement des mod\u00e8les. Comme il contient plusieurs millions d'images, l'entra\u00eenement est long et co\u00fbteux, heureusement les g\u00e9ant de l'Internet comme Google l'ont d\u00e9j\u00e0 fait, ce qui veut dire que les poids de ce mod\u00e8le sont sp\u00e9cialis\u00e9s pour la classification.</p> <p>Importer des bouts de mod\u00e8les pr\u00e9-entra\u00een\u00e9s se fait via la commande</p> <pre><code>from tensorflow.keras.applications import VGG16\n\nconv_base = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n</code></pre> <p><code>include_top=False</code> correspond au fait que nous n'utiliserons que la partie convolutive du mod\u00e8le, c'est \u00e0 nous d'ajouter la partie classifiante avec les neurones denses et la couches de sortie.</p> <p>Dans une premi\u00e8re approche, on peut figer ces poids (la SGD ne les mettra pas \u00e0 jour).</p> <pre><code>from tensorflow.keras.applications import VGG16\n\nconv_base = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n\nconv_base.trainable = False\nfor layer in conv_base.layers:\n    print(f'{layer.name}, Trainable : {layer.trainable}')\n</code></pre> <p>Une fois que l'on a fait \u00e7a, on peut utiliser cette partie convolutive comme une couche \u00e0 part enti\u00e8re de notre mod\u00e8le. R\u00e9importons tout d'abord le dataset cifar10 complet.</p> <pre><code>(X_train,y_train), (X_test,y_test)  = tf.keras.datasets.cifar10.load_data()\n\nX_train = X_train.reshape(-1, 32, 32, 3).astype('float32')\nX_test = X_test.reshape(-1, 32, 32, 3).astype('float32')\n\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, random_state=RANDOM_SEED)\n\nX_test = (X_test - 127.5) / 127.5 # Normalize the images to [-1, 1]\nX_train = (X_train - 127.5) / 127.5 # Normalize the images to [-1, 1]\nX_valid = (X_valid - 127.5) / 127.5 # Normalize the images to [-1, 1]\n\n# Il existe une autre fonction de tf.keras pour transformer de l'ordinal en one_hot\n\ny_train_oh = tf.keras.utils.to_categorical(y_train, num_classes=10)\ny_test_oh = tf.keras.utils.to_categorical(y_test, num_classes=10)\ny_valid_oh = tf.keras.utils.to_categorical(y_valid, num_classes=10)\n</code></pre> <pre><code>vgg_tf = models.Sequential([\n    conv_base,\n    Flatten(),\n    Dense(256, activation='relu'),\n    Dense(128,activation='relu'),\n    Dense(10, activation='softmax')\n])\n\n# vgg_tf.summary()\n\nvgg_tf.compile(loss = 'categorical_crossentropy',\n              optimizer= tf.keras.optimizers.SGD(lr=0.001),\n              metrics=['accuracy'])\n\nhistory = vgg_tf.fit(X_train, y_train_oh,\n                   epochs = 20,\n                   batch_size=32,\n                   validation_data=(X_valid, y_valid_oh))\n</code></pre> <pre><code>pd.DataFrame(history.history).plot(figsize=(12,8))\nplt.grid(True)\nplt.gca().set_ylim(0,2)\nplt.show()\n</code></pre> <p>Ca avance doucement, mais on a encore du potentiel pour l'entra\u00eener.</p> <pre><code>loss, accuracy = vgg_tf.evaluate(X_test,\n                                y_test_oh)\n\ncomp.append({'run': 'vgg_imagenet', 'Perte' : loss, 'Pr\u00e9cision' : accuracy}, ignore_index=True)\n</code></pre>"},{"location":"deep_learning/module2/tp2/#fine-tuning","title":"Fine Tuning","text":"<p>Il fois que l'on pense que l'on a tir\u00e9 tout ce que l'on pouvez extraire des poids que l'on a fig\u00e9, on peut en d\u00e9bloquer tout ou partie, pour qu'ils se sp\u00e9cialisent sur notre probl\u00e8me de classification.</p>"},{"location":"deep_learning/module2/tp2/#degel-dune-partie","title":"D\u00e9gel d'une partie","text":"<pre><code>conv_base.trainable = True\n\nset_trainable = False\n</code></pre> <p>D\u00e9bloquons tous les poids \u00e0 partir de la couche nomm\u00e9e <code>block5_conv1</code>.</p> <pre><code>for layer in conv_base.layers:\n    if layer.name == 'block5_conv1':\n        set_trainable = True\n    if set_trainable:\n        layer.trainable = True\n    else:\n        layer.trainable = False\n\nfor layer in conv_base.layers:\n    print(f'{layer.name}, Trainable : {layer.trainable}')\n</code></pre>"},{"location":"deep_learning/module2/tp2/#degel-complet","title":"D\u00e9gel complet","text":"<pre><code>conv_base.trainable = True\nfor layer in conv_base.layers:\n    print(f'{layer.name}, Trainable : {layer.trainable}')\n</code></pre>"},{"location":"deep_learning/module2/tp2/#lancement-du-fine-tuning","title":"Lancement du fine tuning","text":"<pre><code>vgg_tf.compile(loss = 'categorical_crossentropy',\n              optimizer= tf.keras.optimizers.SGD(lr=0.001),\n              metrics=['accuracy'])\n\nhistory = vgg_tf.fit(X_train, y_train_oh,\n                   epochs = 10,\n                   batch_size=32,\n                   validation_data=(X_valid, y_valid_oh))\n</code></pre> <pre><code>pd.DataFrame(history.history).plot(figsize=(12,8))\nplt.grid(True)\nplt.gca().set_ylim(0,2)\nplt.show()\n</code></pre> <pre><code>loss, accuracy = vgg_tf.evaluate(X_test,\n                                y_test_oh)\n\ncomp = comp.append({'run': 'vgg_tf_tuned', 'Perte' : loss, 'Pr\u00e9cision' : accuracy}, ignore_index=True)\ncomp\n</code></pre>"},{"location":"deep_learning/module3/Module3/","title":"Pratique","text":"(function() {   function addWidgetsRenderer() {     var requireJsScript = document.createElement('script');     requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js';      var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]');     var jupyterWidgetsScript = document.createElement('script');     var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js';     var widgetState;      // Fallback for older version:     try {       widgetState = mimeElement &amp;&amp; JSON.parse(mimeElement.innerHTML);        if (widgetState &amp;&amp; (widgetState.version_major &lt; 2 || !widgetState.version_major)) {         widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js';       }     } catch(e) {}      jupyterWidgetsScript.src = widgetRendererSrc;      document.body.appendChild(requireJsScript);     document.body.appendChild(jupyterWidgetsScript);   }    document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }());"},{"location":"deep_learning/module3/Module3/#module-3-preprocessing-des-donnees-avec-lapi-tfdata","title":"Module 3 : Preprocessing des donn\u00e9es avec l'API tf.data","text":"<p>Pourquoi s'interesser aux pipelines ?</p> <ul> <li>Les mod\u00e8les de DL ont besoins de beaucoup de donn\u00e9es</li> <li>Avant que la donn\u00e9e ne soit envoy\u00e9e au mod\u00e8le, id\u00e9alement elle devrait :</li> <li>\u00eatre m\u00e9lang\u00e9e</li> <li>mise en minibatch</li> <li>les minibatchs devraient \u00eatre disponibles avant la fin de l'\u00e9poque pr\u00e9c\u00e9dente.</li> </ul>"},{"location":"deep_learning/module3/Module3/#import-des-librairies","title":"Import des librairies","text":"<pre><code>import tensorflow as tf\nfrom tensorflow import keras\n\nprint(tf.__version__)\nprint(keras.__version__)\n\nimport os\nimport numpy as np\nimport random\n\n# freeze de l'al\u00e9atoire, pour avoir des exp\u00e9riences reproductibles.\nRANDOM_SEED = 42\n\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\ntf.random.set_seed(RANDOM_SEED)\n</code></pre>      <pre>\n<code>2.2.0-rc3\n2.3.0-tf\n</code>\n</pre>        <pre><code>!nvidia-smi\n</code></pre>      <pre>\n<code>Wed Apr 29 11:34:23 2020       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   41C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code>\n</pre>        <pre><code>from sklearn.model_selection import train_test_split\n\nfrom os import listdir\nfrom os.path import isfile, join\nimport shutil\nimport time\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras import models\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras import optimizers\n</code></pre>     <pre><code>from tensorflow.keras.applications import VGG16\n\ndef vgg_model(input_size, num_classes, lr=0.001, freezed=True):\n\n  conv_base = VGG16(weights='imagenet',\n                    include_top=False,\n                    input_shape=(input_size, input_size, 3))\n\n  #conv_base.summary()\n\n  if freezed:\n      conv_base.trainable = False\n      #for layer in conv_base.layers:\n        #print(f'{layer.name}, Trainable : {layer.trainable}')\n\n  model = models.Sequential([\n      conv_base,\n      Flatten(),\n      Dense(256, activation='relu'),\n      Dense(128,activation='relu'),\n      Dense(num_classes, activation='softmax')\n  ])\n\n  model.compile(loss = 'categorical_crossentropy',\n                optimizer= tf.keras.optimizers.SGD(lr=lr),\n                metrics=['accuracy'])  \n\n  return model\n</code></pre>"},{"location":"deep_learning/module3/Module3/#la-version-classique-imagedatagenerator","title":"La version classique, ImageDataGenerator","text":"<p>Lors du derni\u00e8re, on a vu que les couches convolutives \u00e9taient invariantes par translation, en d'autres termes que la position de l'objet que le CNN doit d\u00e9tecter dans l'image n'est pas importante. Au contraire des couches de neurones denses.</p> <p>Cependant, les couches convolutives ne sont pas invariantes par les autres transformations g\u00e9om\u00e9triques : rotation, dilatation, sym\u00e9trie, modification du contraste, etc.</p> <p>Pour rendre un CNN robuste \u00e0 ces modifications, on applique alors ce que l'on appelle de l'augmentation.</p> <p><code>ImageDataGenerator</code> est la m\u00e9thode de base lorsque que l'on souhaite cr\u00e9er un dataset d'images avec une s\u00e9rie d'augmentations.</p> <p>Ici, avec la commande suivante</p> <pre><code>train_datagen = ImageDataGenerator(\n    rescale=1/255,\n    rotation_range=15,\n    horizontal_flip=True,\n    width_shift_range=0.3,\n    height_shift_range=0.3,\n    zoom_range=0.2,\n    #brightness_range=(0.1, 0.9),\n    shear_range=15\n    )\n</code></pre> <p>on d\u00e9finit un g\u00e9n\u00e9rateur pour le jeu d'entra\u00eenement, qui sera appliqu\u00e9 \u00e0 chaque minibatch.</p> <p>Le principe est le suivant :</p> <ol> <li>On s\u00e9lectionne un minibatch</li> <li>Ce minibatch est alors est alors envoy\u00e9 dans <code>train_datagen</code>, o\u00f9 les op\u00e9rations list\u00e9es seront appliqu\u00e9es, soit de fa\u00e7on certaine comme le <code>rescale</code>, soit avec une certaine probabilit\u00e9.</li> <li>Ce minibatch augment\u00e9 est alors donn\u00e9 pour entra\u00eenement au mod\u00e8le.</li> </ol> <p>Il faut comprendre la chose suivante :</p> <p>L'augmentation des donn\u00e9es se fait \u00e0 la vol\u00e9e, l'int\u00e9grit\u00e9 des donn\u00e9es stock\u00e9es sur votre DD n'est jamais remise en cause.</p>       <p>Appliquons maintenant la librairie <code>ImageDataGenerator</code></p>      <pre><code>from tensorflow.keras.preprocessing.image import ImageDataGenerator\n</code></pre>     <pre><code># set up image augmentation\ntrain_datagen = ImageDataGenerator(\n    rescale=1/255,\n    rotation_range=15,\n    horizontal_flip=True,\n    width_shift_range=0.3,\n    height_shift_range=0.3,\n    zoom_range=0.2,\n    #brightness_range=(0.1, 0.9),\n    shear_range=15\n    )\n\ntest_datagen = ImageDataGenerator(rescale=1/255)\n</code></pre>"},{"location":"deep_learning/module3/Module3/#exemple","title":"Exemple","text":"<p>Prenons le dataset CIFAR10, et voyons un peu ce que font les diff\u00e9rentes transformations.</p>      <pre><code># load data\n(X_train,y_train), (X_test,y_test)  = tf.keras.datasets.cifar10.load_data()\n\nX_train = X_train.reshape(-1, 32, 32, 3).astype('float32')\nX_test = X_test.reshape(-1, 32, 32, 3).astype('float32')\n\nclass_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n\n# define data preparation, mettez les transformations de la cellules pr\u00e9c\u00e9dentes que vous souhaitez observer.\ndatagen = ImageDataGenerator(rescale=1/255,\n                             width_shift_range=0.3,\n                             horizontal_flip=True,\n                             vertical_flip=True)\n# fit parameters from data\ndatagen.fit(X_train)\n\nn_rows = 3\nn_cols = 3\nplt.figure(figsize=(n_cols * 4, n_rows * 4))\n\n# configure batch size and retrieve one batch of images\nfor X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n  for i in range(0, 9):\n    plt.subplot(330 + 1 + i)\n    plt.imshow(X_batch[i].reshape(32, 32, 3))\n    plt.title(class_names[int(y_batch[i])])\n\n  plt.show()\n  break\n</code></pre>"},{"location":"deep_learning/module3/Module3/#augmentation-sur-des-donnees-stockees-en-local","title":"Augmentation sur des donn\u00e9es stock\u00e9es en local","text":"<p>Souvent, les donn\u00e9es ne sont pas stock\u00e9es sous la forme de fichiers plats, mais d\u00e9j\u00e0 r\u00e9pertori\u00e9 dans des dossiers. Dans ce cas l\u00e0 c'est la m\u00e9thode <code>flow_from_directory</code> qui nous permettra de mettre en place le g\u00e9n\u00e9rateur d'augmentations.</p>      <pre><code># Get the flowers' dataset\nflowers = tf.keras.utils.get_file(\n    'flower_photos',\n    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n    untar=True)\n</code></pre>     <pre><code>!ls {flowers}\n</code></pre>      <pre>\n<code>daisy  dandelion  LICENSE.txt  roses  sunflowers  tulips\n</code>\n</pre>        <pre><code>class_names=['daisy', 'dandelion', 'roses',  'sunflowers',  'tulips']\n</code></pre>     <pre><code>train_datagen = ImageDataGenerator(rescale = 1/255,\n                                   rotation_range=45)\n</code></pre>     <pre><code># Verify the shapes yielded by the data generator\ntrain_gen = train_datagen.flow_from_directory(flowers,\n                                              class_mode=\"categorical\",\n                                              target_size=(224, 224),\n                                              batch_size=1,\n                                              color_mode=\"rgb\",\n                                              shuffle=True,\n                                              seed=RANDOM_SEED)\n\nfor data_batch, labels_batch in train_gen:\n  print(f'data batch shape : {data_batch.shape}')\n  print(f'{labels_batch}, labels batch shape : {labels_batch.shape}')\n  break\n</code></pre>      <pre>\n<code>Found 3670 images belonging to 5 classes.\ndata batch shape : (1, 224, 224, 3)\n[[1. 0. 0. 0. 0.]], labels batch shape : (1, 5)\n</code>\n</pre>         <p>Pour voir les modifications apport\u00e9es, on cr\u00e9e des minibatchs de taille 1, et on en s\u00e9lectionne par exemple 9.</p>      <pre><code># create a grid of 3x3 images\nplt.figure(figsize=(9, 9))\nfor i in range(0, 9):\n  X_batch, y_batch  = next(train_gen)\n  plt.subplot(330 + 1 + i)\n  plt.imshow(X_batch[0, :, :, :])\n  plt.axis('off')\n  plt.title(class_names[np.argmax(y_batch[0])], fontsize=12)\n# show the plot\nplt.show()\n</code></pre>               <p>Dans le cas o\u00f9 l'on a un dossier d'entra\u00eenement, un de validation et un de test. Il faut 3 flows diff\u00e9rents.</p> <pre><code>test_datagen = ImageDataGenerator(rescale=1/255)\n\ntrain_gen = train_datagen.flow_from_directory(train_dir,\n                                               target_size=(224,224),\n                                               batch_size=32,\n                                               class_mode='categorical',\n                                               shuffle=True)\n\ntest_gen = test_datagen.flow_from_directory(test_dir,\n                                               target_size=(224,224),\n                                               batch_size=32,\n                                               class_mode='categorical')\n\nval_gen = test_datagen.flow_from_directory(validation_dir,\n                                               target_size=(224,224),\n                                               batch_size=32,\n                                               class_mode='categorical')\n</code></pre>      <pre><code>vgg_tf= vgg_model(input_size = 224,\n                  num_classes= 5)\n\nvgg_tf.summary()\n</code></pre>      <pre>\n<code>Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n58892288/58889256 [==============================] - 1s 0us/step\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nvgg16 (Model)                (None, 7, 7, 512)         14714688  \n_________________________________________________________________\nflatten (Flatten)            (None, 25088)             0         \n_________________________________________________________________\ndense (Dense)                (None, 256)               6422784   \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               32896     \n_________________________________________________________________\ndense_2 (Dense)              (None, 5)                 645       \n=================================================================\nTotal params: 21,171,013\nTrainable params: 6,456,325\nNon-trainable params: 14,714,688\n_________________________________________________________________\n</code>\n</pre>         <p>Dans le cas o\u00f9 les donn\u00e9es sont sotck\u00e9es sous la forme de fichiers plats, Tensorflow sait le nombre d'\u00e9tapes par \u00e9poques \u00e0 effectuer. Avec un g\u00e9n\u00e9rateur possiblement infini, il faut lui pr\u00e9ciser.</p>      <pre><code>from imutils import paths\ntotal_data = len(list(paths.list_images(flowers)))\ntotal_data\ntotal_steps = total_data//32\n</code></pre>     <pre><code>start = time.time()\nhistory = vgg_tf.fit(\n    train_gen,\n    steps_per_epoch = total_steps,\n    epochs=5)\nprint(f\"It took {time.time() - start} seconds\")\n</code></pre>      <pre>\n<code>Epoch 1/5\n114/114 [==============================] - 2s 19ms/step - loss: 1.7583 - accuracy: 0.3158\nEpoch 2/5\n114/114 [==============================] - 2s 19ms/step - loss: 1.6554 - accuracy: 0.2632\nEpoch 3/5\n114/114 [==============================] - 2s 20ms/step - loss: 1.6346 - accuracy: 0.2193\nEpoch 4/5\n114/114 [==============================] - 2s 19ms/step - loss: 1.4662 - accuracy: 0.3596\nEpoch 5/5\n114/114 [==============================] - 2s 20ms/step - loss: 1.3950 - accuracy: 0.3772\nIt took 18.20056986808777 seconds\n</code>\n</pre>         <p>On est capable de faire de l'augmentation, seul probl\u00e8me, on a besoin de d\u00e9finir le nombre d'\u00e9tapes \u00e0 chaque \u00e9poque pour l'entra\u00eenement et la validation. </p>"},{"location":"deep_learning/module3/Module3/#combinaison-avec-tfdata-du-dataset-augmente","title":"Combinaison avec tf.data du dataset augment\u00e9","text":"<pre><code># set up image augmentation\ntrain_datagen = ImageDataGenerator(\n    rescale=1/255,\n    rotation_range=15,\n    horizontal_flip=True,\n    width_shift_range=0.3,\n    height_shift_range=0.3,\n    zoom_range=0.2,\n    #brightness_range=(0.1, 0.9),\n    shear_range=15\n    )\n</code></pre>     <pre><code>ds = tf.data.Dataset.from_generator(\n    lambda: train_datagen.flow_from_directory(flowers,\n                                              class_mode=\"categorical\",\n                                              target_size=(224, 224),\n                                              batch_size=32,\n                                              color_mode=\"rgb\",\n                                              shuffle=True,\n                                              seed=RANDOM_SEED),\n    output_types=(tf.float32, tf.float32),\n    output_shapes = ([None,224,224,3],[None,5])\n)\n</code></pre>      <p>Comparons</p>      <pre><code>vgg_tf= vgg_model(input_size = 224,\n                  num_classes= 5)\n</code></pre>     <pre><code>start = time.time()\nhistory = vgg_tf.fit(ds,\n                     steps_per_epoch = total_steps,\n                     epochs=5)\nprint(f\"It took {time.time() - start} seconds\")\n</code></pre>      <pre>\n<code>Epoch 1/5\nFound 3670 images belonging to 5 classes.\n114/114 [==============================] - 63s 555ms/step - loss: 1.4765 - accuracy: 0.3791\nEpoch 2/5\n114/114 [==============================] - 63s 553ms/step - loss: 1.2427 - accuracy: 0.5236\nEpoch 3/5\n114/114 [==============================] - 63s 554ms/step - loss: 1.0985 - accuracy: 0.5924\nEpoch 4/5\n114/114 [==============================] - 63s 556ms/step - loss: 1.0021 - accuracy: 0.6361\nEpoch 5/5\n114/114 [==============================] - 63s 552ms/step - loss: 0.9494 - accuracy: 0.6528\nIt took 320.4904463291168 seconds\n</code>\n</pre>"},{"location":"deep_learning/module3/Module3/#description-de-lapi-tfdata","title":"Description de l'API tf.data","text":"<p>Lorsque l'on entra\u00eene un r\u00e9seau de neurones, on utilise un algorithme d'optimisation tel que la descente du gradient afin de minimiser la fonction de perte.</p> <p>En utilisant l'API keras, on utilise la m\u00e9thode <code>.fit()</code> pour entra\u00eener le mod\u00e8le. Si le dataset est assez petit, il peut alors \u00ea\u00eatre charg\u00e9 compl\u00e8tement en m\u00e9moire pour l'entra\u00eenement. Cependant si le dataset est trop volumineux pour \u00eatre charg\u00e9 compl\u00e8tement en m\u00e9moire, il devra \u00eatre charg\u00e9 en morceaux, mini-batch par mini-batch, depuis le syst\u00e8me de stockage sur lequel il est.</p> <p>De plus, il est peut \u00eatre n\u00e9c\u00e9ssaire de construire une m\u00e9thode de pr\u00e9processing pour retravailler les donn\u00e9es entrantes.</p> <p>Tensorfow permet de faire cela gr\u00e2\u00e2ce \u00e0 son API <code>tf.data</code></p>"},{"location":"deep_learning/module3/Module3/#lapi-data","title":"L'API Data","text":"<p>Toute l'API se concentre autour du concept de dataset. Dans cette version du dataset, chaque \u00e9l\u00e9ment est directement un tenseur, ce qui permet une meilleure interaction avec Tensorflow.</p> <p>Le but de cette s\u00e9ance sera de voir les diff\u00e9rentes m\u00e9thodes de construction d'un dataset avec cette API.</p>"},{"location":"deep_learning/module3/Module3/#creer-un-dataset-tensorflow-depuis-des-tenseurs-pre-existants","title":"Cr\u00e9er un dataset Tensorflow depuis des tenseurs pr\u00e9-existants","text":"<p>Si les donn\u00e9es existent d\u00e9j\u00e0 sous la forme d'un tenseur, d'une liste Python, d'un tableau Numpy, ou d'une DataFrame Pandas, il est alors simple de construire un dataset via la commande suivante.</p> <p><code>tf.data.Dataset.from_tensor_slices()</code></p>      <pre><code>ls = [1.2, 4, 5, 6, 78, 42, 32, 13]\n</code></pre>     <pre><code>ds = tf.data.Dataset.from_tensor_slices(ls)\n</code></pre>     <pre><code>print(ds)\n</code></pre>      <pre>\n<code>&lt;TensorSliceDataset shapes: (), types: tf.float32&gt;\n</code>\n</pre>        <pre><code>for item in ds:\n  print(item)\n</code></pre>      <pre>\n<code>tf.Tensor(1.2, shape=(), dtype=float32)\ntf.Tensor(4.0, shape=(), dtype=float32)\ntf.Tensor(5.0, shape=(), dtype=float32)\ntf.Tensor(6.0, shape=(), dtype=float32)\ntf.Tensor(78.0, shape=(), dtype=float32)\ntf.Tensor(42.0, shape=(), dtype=float32)\ntf.Tensor(32.0, shape=(), dtype=float32)\ntf.Tensor(13.0, shape=(), dtype=float32)\n</code>\n</pre>        <pre><code>for item in ds.take(1):\n  print(item)\n</code></pre>      <pre>\n<code>tf.Tensor(1.2, shape=(), dtype=float32)\n</code>\n</pre>         <p>Une fois un dataset obtenu, on peut en sortir des batchs via la commande <code>.batch(BATCH_SIZE)</code>.</p>      <pre><code>BATCH_SIZE = 3\n\nds_batch = ds.batch(BATCH_SIZE)\n\nfor i, item in enumerate(ds_batch):\n  print(f'batch {i} : {item}')\n</code></pre>      <pre>\n<code>batch 0 : [1.2 4.  5. ]\nbatch 1 : [ 6. 78. 42.]\nbatch 2 : [32. 13.]\n</code>\n</pre>         <p>On remarque que le dernier batch n'est pas de taille 3, en effet <code>ls</code> comporte 8 \u00e9l\u00e9ments, qui n'est \u00e9videmment pas divisible par 3. Si la taille du batch est importante pour le mod\u00e8le, on peut alors rajouter l'argument <code>drop_remainder=True</code>.</p>      <pre><code>BATCH_SIZE = 3\n\nds_batch = ds.batch(BATCH_SIZE, drop_remainder=True)\n\nfor i, item in enumerate(ds_batch):\n  print(f'batch {i} : {item}')\n</code></pre>      <pre>\n<code>batch 0 : [1.2 4.  5. ]\nbatch 1 : [ 6. 78. 42.]\n</code>\n</pre>"},{"location":"deep_learning/module3/Module3/#combiner-2-tenseurs-en-un-dataset","title":"Combiner 2 tenseurs en un dataset","text":"<p>Souvent, il est possible que l'on est plusieurs datasets, on peut avoir un dataset de features et un dataset de labels. Dans ce cas, on a besoin de les combiner pour pouvoir faire l'entra\u00eenement. </p>      <pre><code>t_x = tf.random.uniform([15, 4], dtype=tf.float32)\nt_y = tf.range(15)\n</code></pre>      <p>On souhaite combiner ces deux tenseurs. Remarquons que pour combiner ces deux tenseurs on doit avoir une correspondance bijective entre les \u00e9l\u00e9ments de ces  tenseurs.</p>      <pre><code>t_x.shape[0] == t_y.shape\n</code></pre>      <pre>\n<code>True</code>\n</pre>        <pre><code>ds_x = tf.data.Dataset.from_tensor_slices(t_x)\nds_y = tf.data.Dataset.from_tensor_slices(t_y)\n\nds = tf.data.Dataset.zip((ds_x, ds_y))\n</code></pre>     <pre><code>for item in ds.take(3):\n  print(f'x : {item[0].numpy()} \\n'\n        f'y : {item[1].numpy()}')\n</code></pre>      <pre>\n<code>x : [0.6645621  0.44100678 0.3528825  0.46448255] \ny : 0\nx : [0.03366041 0.68467236 0.74011743 0.8724445 ] \ny : 1\nx : [0.22632635 0.22319686 0.3103881  0.7223358 ] \ny : 2\n</code>\n</pre>         <p>Remarquez qu'il est aussi possible de le faire via <code>tf.data.Dataset.from_tensor_slices</code>.</p>      <pre><code>ds = tf.data.Dataset.from_tensor_slices((t_x, t_y))\n\nfor item in ds.take(3):\n  print(f'x : {item[0].numpy()} \\n'\n        f'y : {item[1].numpy()}')\n</code></pre>      <pre>\n<code>x : [0.6645621  0.44100678 0.3528825  0.46448255] \ny : 0\nx : [0.03366041 0.68467236 0.74011743 0.8724445 ] \ny : 1\nx : [0.22632635 0.22319686 0.3103881  0.7223358 ] \ny : 2\n</code>\n</pre>"},{"location":"deep_learning/module3/Module3/#shuffle-batch-repeat","title":"Shuffle, Batch, repeat","text":"<p>L'int\u00earet de l'API Data de Tensorflow est qu'une fois que le dataset est construit, il est possible de lui appliquer plusieurs transformations et op\u00e9rations. Nous avons d\u00e9j\u00e0 vu en partie la commande <code>.batch()</code>, nous allons maintenant voir l'interactions avec les autres commandes classiques.</p>"},{"location":"deep_learning/module3/Module3/#shuffle","title":"Shuffle","text":"<pre><code>for item in ds.shuffle(buffer_size=len(t_x), seed = RANDOM_SEED):\n  print(f'x : {item[0].numpy()} y : {item[1].numpy()}')\n</code></pre>      <pre>\n<code>x : [0.23764038 0.7817228  0.9671384  0.06870162] y : 10\nx : [0.03366041 0.68467236 0.74011743 0.8724445 ] y : 1\nx : [0.13318717 0.5480639  0.5746088  0.8996835 ] y : 3\nx : [0.6645621  0.44100678 0.3528825  0.46448255] y : 0\nx : [0.6602763  0.33695042 0.60141766 0.21062577] y : 6\nx : [0.72942245 0.54583454 0.10756552 0.6767061 ] y : 5\nx : [0.7381023  0.32054043 0.6073899  0.46523476] y : 12\nx : [0.81179297 0.5263394  0.494308   0.21612847] y : 8\nx : [0.79873943 0.66028714 0.5871513  0.16461694] y : 11\nx : [0.8527372  0.44062173 0.9485276  0.23752594] y : 7\nx : [0.22632635 0.22319686 0.3103881  0.7223358 ] y : 2\nx : [0.4976915  0.19483674 0.7588748  0.3380444 ] y : 14\nx : [0.00946367 0.5212307  0.6345445  0.1993283 ] y : 4\nx : [0.97803545 0.7223145  0.32347047 0.82577336] y : 13\nx : [0.8457197 0.8718841 0.3083862 0.6868038] y : 9\n</code>\n</pre>         <p><code>.shuffle()</code> a deux arguments, </p> <ol> <li> <p><code>buffer_size</code> qui d\u00e9termine, combien d'\u00e9l\u00e9ments sont tir\u00e9s du dataset avant d'\u00e9tre m\u00e9lang\u00e9s. Ainsi si <code>buffer_size</code> est plus petit que la taille du dataset, il n'est pas enti\u00e8rement m\u00e9lang\u00e9. Pour s'assurer que le dataset est parfaitement m\u00e9lang\u00e9, il suffit de poser <code>buffer_size=len(t_x)</code>.</p> </li> <li> <p><code>seed</code> est le g\u00e9n\u00e9rateur pour le m\u00e9lange, assurant la reproducibilit\u00e9 des r\u00e9sultats. </p> </li> </ol>"},{"location":"deep_learning/module3/Module3/#repeat-batch","title":"Repeat, batch","text":"<p>La commande <code>.repeat()</code> elle est utilis\u00e9e afin de r\u00e9p\u00e9ter le dataset, pour permettre un meilleur comportement al\u00e9atoire lors du batch et du shuffle. </p>      <pre><code>ds_br = ds.batch(4).repeat(2)\n\nfor i,(batch_x, batch_y) in enumerate(ds_br):\n  print(i, batch_x.shape, batch_y.numpy())\n</code></pre>      <pre>\n<code>0 (4, 4) [0 1 2 3]\n1 (4, 4) [4 5 6 7]\n2 (4, 4) [ 8  9 10 11]\n3 (3, 4) [12 13 14]\n4 (4, 4) [0 1 2 3]\n5 (4, 4) [4 5 6 7]\n6 (4, 4) [ 8  9 10 11]\n7 (3, 4) [12 13 14]\n</code>\n</pre>        <pre><code>ds_rb = ds.repeat(2).batch(4)\n\nfor i,(batch_x, batch_y) in enumerate(ds_rb):\n  print(i, batch_x.shape, batch_y.numpy())\n</code></pre>      <pre>\n<code>0 (4, 4) [0 1 2 3]\n1 (4, 4) [4 5 6 7]\n2 (4, 4) [ 8  9 10 11]\n3 (4, 4) [12 13 14  0]\n4 (4, 4) [1 2 3 4]\n5 (4, 4) [5 6 7 8]\n6 (4, 4) [ 9 10 11 12]\n7 (2, 4) [13 14]\n</code>\n</pre>         <p>Remarquez la diff\u00e9rence entre les deux op\u00e9rrations. <code>ds.batch(4).repeat(2)</code> s\u00e9pare d'abord le dataset en batch de taille 4, puis les batchs se r\u00e9p\u00e8tent deux fois. Si lors de la premi\u00e8re fois le dernier batch n'\u00e9tait pas de la bonne taille, alors ce probl\u00e8me se r\u00e9p\u00e8tera.</p> <p>Dans le second cas, <code>ds.repeat(2).batch(4)</code>, ce probl\u00e8me ne peut arriver qu'une seule fois. Ici le dataset est d'abord r\u00e9p\u00e9t\u00e9 2 fois, avant d'\u00eatre divis\u00e9 en batchs.</p> <p>Notez que ce probl\u00e8me n'apparait pas si BATCH_SIZE est un multiple de la taille du dataset. Dans le doute, il est pr\u00e9f\u00e9rable de faire les batchs apr\u00e8s le repeat.</p>      <pre><code>ds_rb = ds.repeat(2).batch(3)\n\nfor i,(batch_x, batch_y) in enumerate(ds_rb):\n  print(i, batch_x.shape, batch_y.numpy())\n</code></pre>      <pre>\n<code>0 (3, 4) [0 1 2]\n1 (3, 4) [3 4 5]\n2 (3, 4) [6 7 8]\n3 (3, 4) [ 9 10 11]\n4 (3, 4) [12 13 14]\n5 (3, 4) [0 1 2]\n6 (3, 4) [3 4 5]\n7 (3, 4) [6 7 8]\n8 (3, 4) [ 9 10 11]\n9 (3, 4) [12 13 14]\n</code>\n</pre>        <pre><code>ds_br = ds.batch(3).repeat(2)\n\nfor i,(batch_x, batch_y) in enumerate(ds_br):\n  print(i, batch_x.shape, batch_y.numpy())\n</code></pre>      <pre>\n<code>0 (3, 4) [0 1 2]\n1 (3, 4) [3 4 5]\n2 (3, 4) [6 7 8]\n3 (3, 4) [ 9 10 11]\n4 (3, 4) [12 13 14]\n5 (3, 4) [0 1 2]\n6 (3, 4) [3 4 5]\n7 (3, 4) [6 7 8]\n8 (3, 4) [ 9 10 11]\n9 (3, 4) [12 13 14]\n</code>\n</pre>         <p>Comme il semble que faire les batchs est plus efficace apr\u00e8s le repeat. Voyons comment le shuffle se comporte uniquement avec le repeat.</p>"},{"location":"deep_learning/module3/Module3/#repeat-shuffle","title":"Repeat, shuffle","text":"<pre><code>ls = [1,2,3]\nds_ls = tf.data.Dataset.from_tensor_slices(ls)\n</code></pre>     <pre><code>ds_rs= ds_ls.repeat(2).shuffle(buffer_size=len(ls), seed = RANDOM_SEED)\n\nfor batch_y in ds_rs:\n  print(batch_y.numpy())\n</code></pre>      <pre>\n<code>2\n1\n1\n2\n3\n3\n</code>\n</pre>        <pre><code>ds_sr= ds_ls.shuffle(buffer_size=len(ls), seed = RANDOM_SEED).repeat(2)\n\n\nfor batch_y in ds_sr:\n  print(batch_y.numpy())\n</code></pre>      <pre>\n<code>2\n1\n3\n2\n3\n1\n</code>\n</pre>         <p>La diff\u00e9rence entre les deux solutions est la suivante :</p> <ol> <li> <p>avec <code>repeat(2).shuffle(buffer_size=len(ls), seed = RANDOM_SEED)</code>, le dataset est r\u00e9p\u00e9t\u00e9 2 fois, puis apr\u00e8s seulement il est m\u00e9lang\u00e9.</p> </li> <li> <p>avec <code>shuffle(buffer_size=len(ls), seed = RANDOM_SEED).repeat(2)</code>, le dataset est d'abord m\u00e9lang\u00e9. Par d\u00e9faut, un <code>repeat()</code> mis apr\u00e8s le <code>shuffle()</code> produit un nouvel ordre apr\u00e8s chaque it\u00e9ration.</p> </li> </ol> <p>Si la premi\u00e8re solution peut sembler mieux, car le m\u00e9lange sera plus g\u00e9n\u00e9ral, elle brouille la notion d'\u00e9poque. Pour rappel, on d\u00e9finit une \u00e9poque par le fait que tous les \u00e9l\u00e9ments du dataset sont pass\u00e9s une fois dans le r\u00e9seau de neurones. Dans la premi\u00e8re solution, il peut se passer beaucoup de temps avant que certains \u00e9l\u00e9ments du dataset apparaissent, par exemple, l'\u00e9l\u00e9ment 3 apparait 2 fois, mais \u00e0 la fin du nouveau dataset.</p> <p>Dans le deuxi\u00e8me cas, comme on a d'abord m\u00e9lang\u00e9, on est s\u00fbr que chaque \u00e9l\u00e9ment du dataset apparaisse assez souvent, une fois par it\u00e9ration du dataset.</p>       <p>Pour r\u00e9sumer, les bonnes pratiques dans la construction pour l'instant sont</p> <ol> <li>Construction du dataset,</li> <li><code>shuffle()</code>,</li> <li><code>repeat()</code>,</li> <li><code>batch()</code>.</li> </ol>"},{"location":"deep_learning/module3/Module3/#les-autres-operations-possibles","title":"Les autres op\u00e9rations possibles","text":"<p>https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/io</p> <p>https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image</p> <p><code>tf.io</code> et <code>tf.image</code> sont deux modeuls compl\u00e9mentaires tr\u00e8s utilis\u00e9s dans la construction des datasets via l'API, <code>tf.io</code> permet de faire l'interface input/output afin de lire les fichiers, et <code>tf.image</code> est sp\u00e9cialis\u00e9 dans le traitement des images. </p>"},{"location":"deep_learning/module3/Module3/#application-avec-cifar-10","title":"Application avec CIFAR 10","text":"<pre><code>(X_train,y_train), (X_test,y_test)  = tf.keras.datasets.cifar10.load_data()\n\n\nX_train = X_train.reshape(-1, 32, 32, 3).astype('float32')\nX_test = X_test.reshape(-1, 32, 32, 3).astype('float32')\n\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, random_state=RANDOM_SEED)\n\nX_test = X_test/255\nX_train = X_train/255\nX_valid = X_valid/255\n\ny_train_oh = tf.keras.utils.to_categorical(y_train, num_classes=10)\ny_test_oh = tf.keras.utils.to_categorical(y_test, num_classes=10)\ny_val_oh = tf.keras.utils.to_categorical(y_valid, num_classes=10)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n</code></pre>     <pre><code>print(X_train.shape)\n</code></pre>      <pre>\n<code>(37500, 32, 32, 3)\n</code>\n</pre>         <p>L'int\u00earet de l'API <code>tf.data</code> est qu'elle permet de tout combiner dans une seule fonction pour des datasets pr\u00eats pour la production.</p>      <pre><code>def train_preprocess(image, label):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n\n    image = tf.image.random_brightness(image, max_delta=32.0 / 255.0)\n    image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n    image = tf.image.random_jpeg_quality(image, 80, 100, seed=RANDOM_SEED)\n    #Make sure the image is still in [0, 1]\n    image = tf.clip_by_value(image, 0.0, 1.0)\n\n    return image, label\n</code></pre>     <pre><code>def create_train_dataset(features, labels, batch=32, repet=1, prefetch=1):\n    dataset = tf.data.Dataset.from_tensor_slices((features,labels))\n    dataset = dataset.cache()\n    dataset = dataset.shuffle(len(features), seed=42)\n    dataset = dataset.repeat(repet)\n    dataset = dataset.map(train_preprocess, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.batch(batch)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset\n\ndef create_test_dataset(features, labels, batch=32, repet=1, prefetch=1):\n    dataset = tf.data.Dataset.from_tensor_slices((features,labels))\n    dataset = dataset.cache()\n    dataset = dataset.shuffle(len(features), seed=42)\n    dataset = dataset.repeat(repet)\n    dataset = dataset.batch(batch)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset\n</code></pre>     <pre><code>ds_train = create_train_dataset(X_train, y_train_oh)\nds_val = create_test_dataset(X_valid, y_val_oh)\nds_test = create_test_dataset(X_test, y_test_oh)\n</code></pre>     <pre><code>vgg_tf= vgg_model(input_size = 32,\n                  num_classes= 10)\n</code></pre>     <pre><code>conv_base = VGG16(weights='imagenet',\n                  include_top=False,\n                  input_shape=(32, 32, 3))\n\n#conv_base.summary()\n\nconv_base.trainable = False\n#for layer in conv_base.layers:\n  #print(f'{layer.name}, Trainable : {layer.trainable}')\n\nmodel = models.Sequential([\n    conv_base,\n    Flatten(),\n    Dense(256, activation='relu'),\n    Dense(128,activation='relu'),\n    Dense(10, activation='softmax')\n])\n\nmodel.compile(loss = 'categorical_crossentropy',\n              optimizer= tf.keras.optimizers.SGD(lr=0.001, momentum=0.9),\n              metrics=['accuracy']) \n</code></pre>     <pre><code>start = time.time()\nhistory = model.fit(ds_train,\n                    epochs=20,\n                    validation_data = ds_val)\n\nprint(f\"It took {time.time() - start} seconds\")\n</code></pre>      <pre>\n<code>Epoch 1/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.7978 - accuracy: 0.3615 - val_loss: 1.6051 - val_accuracy: 0.4278\nEpoch 2/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.5699 - accuracy: 0.4430 - val_loss: 1.5111 - val_accuracy: 0.4652\nEpoch 3/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.5246 - accuracy: 0.4588 - val_loss: 1.4797 - val_accuracy: 0.4774\nEpoch 4/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.4878 - accuracy: 0.4735 - val_loss: 1.4106 - val_accuracy: 0.5051\nEpoch 5/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.4663 - accuracy: 0.4825 - val_loss: 1.3876 - val_accuracy: 0.5154\nEpoch 6/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.4509 - accuracy: 0.4847 - val_loss: 1.3728 - val_accuracy: 0.5188\nEpoch 7/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.4327 - accuracy: 0.4898 - val_loss: 1.3537 - val_accuracy: 0.5282\nEpoch 8/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.4230 - accuracy: 0.4949 - val_loss: 1.3411 - val_accuracy: 0.5267\nEpoch 9/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.4065 - accuracy: 0.5007 - val_loss: 1.3496 - val_accuracy: 0.5262\nEpoch 10/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.4006 - accuracy: 0.5043 - val_loss: 1.3480 - val_accuracy: 0.5228\nEpoch 11/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.3933 - accuracy: 0.5071 - val_loss: 1.3254 - val_accuracy: 0.5336\nEpoch 12/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.3866 - accuracy: 0.5091 - val_loss: 1.3248 - val_accuracy: 0.5389\nEpoch 13/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.3775 - accuracy: 0.5098 - val_loss: 1.3202 - val_accuracy: 0.5361\nEpoch 14/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.3662 - accuracy: 0.5161 - val_loss: 1.3134 - val_accuracy: 0.5350\nEpoch 15/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.3647 - accuracy: 0.5173 - val_loss: 1.2995 - val_accuracy: 0.5418\nEpoch 16/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.3587 - accuracy: 0.5168 - val_loss: 1.3160 - val_accuracy: 0.5334\nEpoch 17/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.3521 - accuracy: 0.5200 - val_loss: 1.2767 - val_accuracy: 0.5497\nEpoch 18/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.3510 - accuracy: 0.5196 - val_loss: 1.2974 - val_accuracy: 0.5387\nEpoch 19/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.3353 - accuracy: 0.5254 - val_loss: 1.2846 - val_accuracy: 0.5496\nEpoch 20/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.3353 - accuracy: 0.5243 - val_loss: 1.2874 - val_accuracy: 0.5442\nIt took 412.23410201072693 seconds\n</code>\n</pre>        <pre><code>pd.DataFrame(history.history).plot(figsize=(12,8))\nplt.grid(True)\nplt.gca().set_ylim(0,2)\nplt.show()\n</code></pre>              <pre><code>set_trainable = False\n\nfor layer in conv_base.layers:\n    if layer.name == 'block5_conv1':\n        set_trainable = True\n    if set_trainable:\n        layer.trainable = True\n    else:\n        layer.trainable = False\n</code></pre>     <pre><code>for layer in conv_base.layers:\n  layer.trainable = True\n</code></pre>     <pre><code>start = time.time()\nhistory = model.fit(ds_train,\n                    epochs=20,\n                    validation_data = ds_val)\n\nprint(f\"It took {time.time() - start} seconds\")\n</code></pre>      <pre>\n<code>Epoch 1/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.2497 - accuracy: 0.5594 - val_loss: 1.3006 - val_accuracy: 0.5446\nEpoch 2/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.2418 - accuracy: 0.5612 - val_loss: 1.2672 - val_accuracy: 0.5544\nEpoch 3/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.2364 - accuracy: 0.5637 - val_loss: 1.2440 - val_accuracy: 0.5590\nEpoch 4/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.2383 - accuracy: 0.5621 - val_loss: 1.2334 - val_accuracy: 0.5662\nEpoch 5/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.2271 - accuracy: 0.5649 - val_loss: 1.2327 - val_accuracy: 0.5642\nEpoch 6/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.2260 - accuracy: 0.5659 - val_loss: 1.2521 - val_accuracy: 0.5574\nEpoch 7/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.2221 - accuracy: 0.5654 - val_loss: 1.2476 - val_accuracy: 0.5609\nEpoch 8/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.2109 - accuracy: 0.5705 - val_loss: 1.2297 - val_accuracy: 0.5685\nEpoch 9/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.2107 - accuracy: 0.5710 - val_loss: 1.2279 - val_accuracy: 0.5666\nEpoch 10/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.2003 - accuracy: 0.5768 - val_loss: 1.2264 - val_accuracy: 0.5674\nEpoch 11/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.2016 - accuracy: 0.5734 - val_loss: 1.2377 - val_accuracy: 0.5602\nEpoch 12/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.1959 - accuracy: 0.5734 - val_loss: 1.2389 - val_accuracy: 0.5632\nEpoch 13/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.1937 - accuracy: 0.5758 - val_loss: 1.2047 - val_accuracy: 0.5771\nEpoch 14/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.1872 - accuracy: 0.5805 - val_loss: 1.2334 - val_accuracy: 0.5636\nEpoch 15/20\n1172/1172 [==============================] - 21s 18ms/step - loss: 1.1930 - accuracy: 0.5781 - val_loss: 1.2163 - val_accuracy: 0.5739\nEpoch 16/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.1805 - accuracy: 0.5824 - val_loss: 1.2221 - val_accuracy: 0.5696\nEpoch 17/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.1762 - accuracy: 0.5821 - val_loss: 1.2099 - val_accuracy: 0.5735\nEpoch 18/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.1742 - accuracy: 0.5803 - val_loss: 1.2120 - val_accuracy: 0.5751\nEpoch 19/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.1707 - accuracy: 0.5842 - val_loss: 1.2072 - val_accuracy: 0.5771\nEpoch 20/20\n1172/1172 [==============================] - 20s 17ms/step - loss: 1.1728 - accuracy: 0.5824 - val_loss: 1.1975 - val_accuracy: 0.5822\nIt took 408.30218720436096 seconds\n</code>\n</pre>        <pre><code>pd.DataFrame(history.history).plot(figsize=(12,8))\nplt.grid(True)\nplt.gca().set_ylim(0,2)\nplt.show()\n</code></pre>"},{"location":"deep_learning/module3/Module3/#annexe-connection-au-drive-import-des-donnees-creation-des-dossiers","title":"Annexe : Connection au drive, import des donn\u00e9es, cr\u00e9ation des dossiers","text":"<pre><code>from google.colab import drive\ndrive.mount('/content/drive')\n</code></pre>      <pre>\n<code>Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&amp;response_type=code&amp;scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n\nEnter your authorization code:\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nMounted at /content/drive\n</code>\n</pre>         <p>Pour ce TP nous utiliserons le dataset d'entra\u00eenement fourni par Kaggle pour la comp\u00e9tition Dogs vs. Cats.</p> <p>https://www.kaggle.com/c/dogs-vs-cats/data</p>      <pre><code>%cd /content/drive/My\\ Drive\nbase_path = '/content/drive/My Drive'\n!pwd\n!ls\n</code></pre>      <pre>\n<code>/content/drive/My Drive\n/content/drive/My Drive\n cats_and_dogs_small  'Colab Notebooks'   config   data_dir\n</code>\n</pre>"},{"location":"deep_learning/module3/Module3/#root-dir","title":"Root dir","text":"<pre><code>!unzip -q train.zip -d data_dir\n</code></pre>     <pre><code>os.rename(os.path.join(base_path, 'data_dir/train'), os.path.join(base_path, 'data_dir/raw_data'))\n</code></pre>     <pre><code>%cd /content/drive/My Drive/data_dir\nbase_dir = '/content/drive/My Drive/data_dir'\nraw_data = os.path.join(base_path, 'data_dir/raw_data')\n!pwd\n</code></pre>      <pre>\n<code>/content/drive/My Drive/data_dir\n/content/drive/My Drive/data_dir\n</code>\n</pre>        <pre><code>!ls\n</code></pre>      <pre>\n<code>raw_data  test    train  validation\n</code>\n</pre>"},{"location":"deep_learning/module3/Module3/#train-test-validation-dir","title":"Train, Test, Validation dir","text":""},{"location":"deep_learning/module3/Module3/#adresse","title":"Adresse","text":"<pre><code>train_dir = os.path.join(base_dir, 'train')\nvalidation_dir = os.path.join(base_dir, 'validation')\ntest_dir = os.path.join(base_dir, 'test')\n</code></pre>"},{"location":"deep_learning/module3/Module3/#mkdir","title":"mkdir","text":"<pre><code>os.mkdir(train_dir)\nos.mkdir(validation_dir)\nos.mkdir(test_dir)\n</code></pre>"},{"location":"deep_learning/module3/Module3/#train-test-validation-subdirs","title":"Train, Test, Validation subdirs","text":""},{"location":"deep_learning/module3/Module3/#adresse_1","title":"Adresse","text":"<pre><code>train_cats_dir = os.path.join(train_dir, 'cats')\ntrain_dogs_dir = os.path.join(train_dir, 'dogs')\n</code></pre>     <pre><code>validation_cats_dir = os.path.join(validation_dir, 'cats')\nvalidation_dogs_dir = os.path.join(validation_dir, 'dogs')\n</code></pre>     <pre><code>test_cats_dir = os.path.join(test_dir, 'cats')\ntest_dogs_dir = os.path.join(test_dir, 'dogs')\n</code></pre>"},{"location":"deep_learning/module3/Module3/#mkdir_1","title":"mkdir","text":"<pre><code>os.mkdir(train_cats_dir)\nos.mkdir(train_dogs_dir)\n</code></pre>     <pre><code>os.mkdir(validation_cats_dir)\nos.mkdir(validation_dogs_dir)\n</code></pre>     <pre><code>os.mkdir(test_cats_dir)\nos.mkdir(test_dogs_dir)\n</code></pre>"},{"location":"deep_learning/module3/Module3/#partition-des-donnees","title":"Partition des donn\u00e9es","text":"<pre><code>images = [f for f in listdir(raw_data) if isfile(join(raw_data, f))]\nimages[:5]\n</code></pre>     <pre><code>len(images)\n</code></pre>      <pre>\n<code>25000</code>\n</pre>        <pre><code>cats = ['cat.{}.jpg'.format(i) for i in range(4000)]\ndogs = ['dog.{}.jpg'.format(i) for i in range(4000)]\nprint(len(cats), len(dogs))\n</code></pre>      <pre>\n<code>4000 4000\n</code>\n</pre>        <pre><code>cats_train, cats_test = train_test_split(cats, test_size=0.25, random_state=RANDOM_SEED)\ncats_train, cats_val = train_test_split(cats_train, test_size=0.25, random_state=RANDOM_SEED)\n</code></pre>     <pre><code>dogs_train, dogs_test = train_test_split(dogs, test_size=0.25, random_state=RANDOM_SEED)\ndogs_train, dogs_val = train_test_split(dogs_train, test_size=0.25, random_state=RANDOM_SEED)\n</code></pre>     <pre><code>def create_ds(new_dir, fnames):\n\n    for fname in fnames:\n        src = os.path.join(raw_data, fname)\n        dst = os.path.join(new_dir, fname)\n        shutil.copyfile(src, dst)\n    print(f'Finished copying files in {new_dir}')\n</code></pre>     <pre><code>create_ds(train_cats_dir, cats_train)\ncreate_ds(train_dogs_dir, dogs_train)\n\ncreate_ds(validation_cats_dir, cats_val)\ncreate_ds(validation_dogs_dir, dogs_val)\n\ncreate_ds(test_cats_dir, cats_test)\ncreate_ds(test_dogs_dir, dogs_test)\n</code></pre>      <pre>\n<code>Finished copying files in /content/drive/My Drive/data_dir/train/cats\nFinished copying files in /content/drive/My Drive/data_dir/train/dogs\nFinished copying files in /content/drive/My Drive/data_dir/validation/cats\nFinished copying files in /content/drive/My Drive/data_dir/validation/dogs\nFinished copying files in /content/drive/My Drive/data_dir/test/cats\nFinished copying files in /content/drive/My Drive/data_dir/test/dogs\n</code>\n</pre>"},{"location":"deep_learning/module3/Module3/#nuke-suppression-des-dossiers","title":"NUKE : suppression des dossiers","text":"<pre><code>shutil.rmtree(train_cats_dir)\nprint(\"File Removed!\")\nshutil.rmtree(train_dogs_dir)\nprint(\"File Removed!\")\nshutil.rmtree(validation_cats_dir)\nprint(\"File Removed!\")\nshutil.rmtree(validation_dogs_dir)\nprint(\"File Removed!\")\nshutil.rmtree(test_cats_dir)\nprint(\"File Removed!\")\nshutil.rmtree(test_dogs_dir)\nprint(\"File Removed!\")\n</code></pre>      <pre>\n<code>File Removed!\nFile Removed!\nFile Removed!\nFile Removed!\nFile Removed!\nFile Removed!\n</code>\n</pre>        <pre><code>shutil.rmtree(train_dir)\nprint(\"File Removed!\")\nshutil.rmtree(validation_dir)\nprint(\"File Removed!\")\nshutil.rmtree(test_dir)\nprint(\"File Removed!\")\n</code></pre>      <pre>\n<code>File Removed!\nFile Removed!\nFile Removed!\n</code>\n</pre>"},{"location":"deep_learning/module3/Module3/#creer-un-dataset-depuis-des-images-dans-un-dossier-tfdatadatasetfrom_tensor_slices","title":"Cr\u00e9er un dataset depuis des images dans un dossier <code>tf.data.Dataset.from_tensor_slices</code>","text":"<p>Cette fois, ci on cr\u00e9e un dataset directement depuis le repertoire <code>raw_data</code>, cr\u00e9er des dossiers s\u00e9par\u00e9s \u00e0 chaque fois peut \u00ea\u00eatre contraignant, surtout si l'on a beaucoup de donn\u00e9es et beaucoup de classes.</p>      <pre><code>cats = [os.path.join(raw_data,'cat.{}.jpg').format(i) for i in range(4000)]\ndogs = [os.path.join(raw_data,'dog.{}.jpg').format(i) for i in range(4000)]\ndata = cats + dogs\n\ndata.sort()  # make sure that the filenames have a fixed order before shuffling\nrandom.seed(RANDOM_SEED)\nrandom.shuffle(data) # shuffles the ordering of filenames (deterministic given the chosen seed)\n\n\ndata[:5]\n</code></pre>      <pre>\n<code>['/content/drive/My Drive/data_dir/raw_data/dog.1179.jpg',\n '/content/drive/My Drive/data_dir/raw_data/cat.2910.jpg',\n '/content/drive/My Drive/data_dir/raw_data/dog.1186.jpg',\n '/content/drive/My Drive/data_dir/raw_data/cat.3931.jpg',\n '/content/drive/My Drive/data_dir/raw_data/cat.1029.jpg']</code>\n</pre>        <pre><code>labels = [1 if 'dog' in os.path.basename(file) else 0 for file in data]\nlabels[:5]\n</code></pre>      <pre>\n<code>[1, 0, 1, 0, 0]</code>\n</pre>        <pre><code>X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.25, random_state=RANDOM_SEED)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=RANDOM_SEED)\n</code></pre>     <pre><code>AUTOTUNE = tf.data.experimental.AUTOTUNE\nNUM_CLASSES=2\n</code></pre>     <pre><code>def parse_image(filename,label):\n\n    # convert the label to one-hot encoding\n    label = tf.one_hot(label, NUM_CLASSES)\n\n    #decode image\n    image = tf.io.read_file(filename)\n    #Don't use tf.image.decode_image, or the output shape will be undefined\n    image = tf.image.decode_jpeg(image)\n    #This will convert to float values in [0, 1]\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, [224, 224])\n    return image, label\n</code></pre>     <pre><code>def train_preprocess(image, label):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n\n    image = tf.image.random_brightness(image, max_delta=32.0 / 255.0)\n    image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n\n    #Make sure the image is still in [0, 1]\n    image = tf.clip_by_value(image, 0.0, 1.0)\n\n    return image, label\n</code></pre>     <pre><code>def create_train_dataset(features, labels, batch=32, repet=1, prefetch=1):\n    dataset = tf.data.Dataset.from_tensor_slices((features,labels))\n    dataset = dataset.shuffle(len(features), seed=RANDOM_SEED)\n    dataset = dataset.repeat(repet)\n    dataset = dataset.map(parse_image, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.map(train_preprocess, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.batch(batch)\n    dataset = dataset.prefetch(prefetch)\n    return dataset\n</code></pre>     <pre><code>def create_test_dataset(features, labels, batch=32, repet=1, prefetch=1):\n    dataset = tf.data.Dataset.from_tensor_slices((features,labels))\n    dataset = dataset.shuffle(len(features), seed=RANDOM_SEED)\n    dataset = dataset.repeat(repet)\n    dataset = dataset.map(parse_image, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.batch(batch)\n    dataset = dataset.prefetch(prefetch)\n    return dataset\n</code></pre>     <pre><code>ds_train = create_train_dataset(X_train, y_train)\nds_val = create_test_dataset(X_val, y_val)\nds_test = create_test_dataset(X_test, y_test)\n</code></pre>     <pre><code>vgg_tf= vgg_model(input_size = 224,\n                  num_classes= 2)\n</code></pre>     <pre><code>start = time.time()\nvgg_tf.fit(ds_train,\n           epochs=5,\n           validation_data = ds_val)\n\nprint(f\"It took {time.time() - start} seconds\")\n</code></pre>      <pre>\n<code>Epoch 1/5\n141/141 [==============================] - 39s 276ms/step - loss: 0.4093 - accuracy: 0.8031 - val_loss: 0.3043 - val_accuracy: 0.8747\nEpoch 2/5\n141/141 [==============================] - 36s 252ms/step - loss: 0.3691 - accuracy: 0.8342 - val_loss: 0.3015 - val_accuracy: 0.8747\nEpoch 3/5\n141/141 [==============================] - 36s 253ms/step - loss: 0.3502 - accuracy: 0.8469 - val_loss: 0.3286 - val_accuracy: 0.8547\nEpoch 4/5\n141/141 [==============================] - 35s 250ms/step - loss: 0.3625 - accuracy: 0.8409 - val_loss: 0.3704 - val_accuracy: 0.8273\nEpoch 5/5\n141/141 [==============================] - 35s 249ms/step - loss: 0.3315 - accuracy: 0.8580 - val_loss: 0.2827 - val_accuracy: 0.8813\nIt took 182.41791415214539 seconds\n</code>\n</pre>"},{"location":"deep_learning/module4/Module4/","title":"Module 4 : Optimisation et r\u00e9gularisation des r\u00e9seaux de neurones","text":"<p>Le but d'un r\u00e9seau de neurones est de cr\u00e9er un mod\u00e8le \\(\\hat{f}\\) minimisant</p> \\[     \\mathbf{E}(\\mathcal{L}_{\\vartheta}(\\hat{y})) \\] <p>o\u00f9 la moyenne est prise sur le dataset d'entra\u00eenement complet, \\(\\mathcal{L}_{\\vartheta}\\) est la fonction de perte choisie pour ce probl\u00e8me d'optimisation, et \\(\\hat{y}\\) est la pr\u00e9diction obtenue par \\(\\hat{f}\\), \\(\\hat{f}(x) = \\hat{y}\\).</p> <p>Entra\u00eener un algorithme de deep learning est co\u00fbteux, en temps de calcul comme en ressource mat\u00e9riel. Beaucoup de techniques ont donc \u00e9t\u00e9 d\u00e9velopp\u00e9es pour r\u00e9duire ces contraintes et faire converger les algorithmes plus rapidement.</p>"},{"location":"deep_learning/module4/Module4/#le-compromis-biais-variance","title":"Le compromis biais-variance","text":""},{"location":"deep_learning/module4/Module4/#le-cas-de-la-regression","title":"Le cas de la r\u00e9gression","text":"<p>Dans un probl\u00e8me de r\u00e9gression classique la fonction de perte utilis\u00e9e et le plus souvent l'Erreur Moyenne Quadratique (Mean Squared Error), d\u00e9finie de la fa\u00e7on suivante.</p> \\[     MSE(\\hat{y}) :=  \\mathbf{E}((\\hat{y}-y)^2) = \\frac{1}{k}\\sum_{i=1}^{k} ||\\hat{y}_{i} - y_{i}||^{2}_{2} \\] <p>Cette formule peut encore se d\u00e9composer de la fa\u00e7on suivante.</p>  <p>D\u00e9composition biais variance</p> \\[     MSE(\\hat{y}) = \\mathbf{Var}(\\hat{y}) + \\mathbf{Biais}(\\hat{y})^2 \\]  <p>\\(\\mathbf{Var}(\\hat{y})\\) est la variance de \\(\\hat{y}\\), elle est d\u00e9finie par la formule suivante.</p> \\[         \\mathbf{Var}(\\hat{y}) = \\mathbf{E}((\\hat{y}- \\mathbf{E}(\\hat{y}))^2) \\] <p>\\(\\mathbf{Biais}(\\hat{y})\\) est le biais de \\(\\hat{y}\\), il est d\u00e9fini par la formule suivante.</p> \\[     \\mathbf{Biais}(\\hat{y}) = \\mathbf{E}(\\hat{y}) - y \\] <p>La variance mesure la dispersion moyenne de notre pr\u00e9diction par rapport \u00e0 la valeur moyenne des pr\u00e9dictions. Une variance importante signifiant que nos valeurs s'\u00e9carterons beaucoup de la moyenne.</p> <p>Le biais lui nous dit de combien en moyenne notre pr\u00e9diction d\u00e9vie de la vraie valeur. Une pr\u00e9diction est alors non biais\u00e9e si \\(\\mathbf{E}(\\mathbf{Biais}(\\hat{y})) = 0\\).</p>   <p>Remarque</p> <p>La d\u00e9composition biais-variance fait normalement intervernir un troisis\u00e8me terme, qui correspond \u00e0 un terme d'erreur. Par simplicit\u00e9, nous avons choisi de ne pas le faire appara\u00eetre.</p>  <p>Le but, dans l'\u00e9valuation d'une r\u00e9gression lin\u00e9aire, est d'essayer de se placer dans le cas id\u00e9al o\u00f9 le biais et la variance sont faibles. Cependant il y a n\u00e9cessairement un compromis \u00e0 faire entre les deux. De fa\u00e7on g\u00e9n\u00e9rale, si la variance baisse au cours du temps, le biais lui augmente.</p>  <p>La combinaison du biais et de la variance permet de quantifier la compl\u00e9xit\u00e9 du mod\u00e8le.</p>  <p>Exemple de sous-apprentissage, apprentissage correct, sur-apprentissage.</p>    <p>Remarque</p> <ol> <li>Un mod\u00e8le avec une variance faible, mais un biais \u00e9lev\u00e9 est en sous-apprentissage, que l'on appelle aussi parfois sur-g\u00e9n\u00e9ralisation.</li> <li>Un mod\u00e8le avec une variance \u00e9lev\u00e9, mais un biais faible, est en sur-apprentissage.</li> </ol>"},{"location":"deep_learning/module4/Module4/#le-cas-de-la-classification","title":"Le cas de la classification","text":"<p>Si l'on ne se place pas dans le cadre d'une r\u00e9gression, mais dans le cadre d'un probl\u00e8me de classification. La question de l'existence d'une d\u00e9composition biais-variance pour une fonction de perte \\(\\mathcal{L}_{\\vartheta}\\) quelconque reste compliqu\u00e9e. Nous avons toutefois les r\u00e9sultats suivants.</p>  <p>Th\u00e9or\u00e8me (Domingos)</p> <p>Dans un probl\u00e8me de classification binaire, si la fonction de perte \\(\\mathcal{L}_{\\vartheta}\\) v\u00e9rifie</p> <ol> <li>\\(\\mathcal{L}_{\\vartheta}(y,y)=0, \\, \\forall y\\)</li> <li>\\(\\mathcal{L}_{\\vartheta}(y_{1},y_{2}) \\neq 0, \\, \\forall y_{1} \\neq y_{2}\\)</li> </ol> <p>Alors</p> \\[     \\mathbf{E}(\\mathcal{L}_{\\vartheta}(\\hat{y})) \\] <p>Admet une d\u00e9compoistion biais-variance.</p>   <p>Pour une classification multiclasse ?</p> <p>Si l'on se place dans le cadre d'un probl\u00e8me de classification multiclasse, la question reste ouverte.</p>"},{"location":"deep_learning/module4/Module4/#earlystopping-regularisation-dropout-et-batchnorm","title":"EarlyStopping, r\u00e9gularisation, Dropout et BatchNorm","text":"<p>A chaque fois, le but de ces m\u00e9thodes est d'att\u00e9nuer les probl\u00e8mes de sur-apprentissage, afin que notre mod\u00e8le soit capable de g\u00e9n\u00e9raliser correctement. De fa\u00e7on usuelle, on y parvient en r\u00e9duisant la compl\u00e9xit\u00e9 du mod\u00e8le, ou en r\u00e9duisant la variance de la pr\u00e9diction. Dans le cadre des r\u00e9seaux de neurones c'est le plus souvent obtenu en rajoutant de l'information, ou en modifiant la fonction de perte.</p>  <p>Attention</p> <p>Avant de passer aux m\u00e9thodes de r\u00e9gularisation des r\u00e9seaux de neurones \u00e0 proprement parler, listons ici les diff\u00e9rentes options disponibles pour \u00e9viter le surapprentissage.</p> <ol> <li> <p>Collecter plus de donn\u00e9es (pas toujours faisable, par exemple en m\u00e9decine), faire de l'augmtentation de donn\u00e9es (attention \u00e0 la faire de fa\u00e7on correcte, par exemple les rotations et symetries en Computer Vision).</p> </li> <li> <p>Choisir une architecture de mod\u00e8le plus petite.</p> </li> <li> <p>Ajouter du bruit.</p> </li> <li> <p>S\u00e9parer en 3 datasets distincts votre jeu de donn\u00e9es : un dataset d'entra\u00eenement, un dataset de validation, et un dataset de test.</p> <ol> <li> <p>Le dataset de validation sera utilis\u00e9 pour mesurer les performances du mod\u00e8le, et le tuning des hyperparam\u00e8tres.</p> </li> <li> <p>Le dataset de test lui donnera une estimation non biais\u00e9e des performances de g\u00e9n\u00e9ralisation du mod\u00e8le.</p> </li> </ol> </li> </ol>"},{"location":"deep_learning/module4/Module4/#earlystopping","title":"EarlyStopping","text":"<p>Lorsque d'un mod\u00e8le s'entra\u00eene trop longtemps, les performances g\u00e9n\u00e9rales du mod\u00e8le sur le dataset de validation ont tendance \u00e0 stagner, alors que celle sur le dataset d'entra\u00eenement peuvent tr\u00e8s bien continuer \u00e0 s'am\u00e9liorer. On est alors dans un cas de sur-apprentissage.</p> <p>L'id\u00e9e est alors de surveiller l'\u00e9cart entre les performances du mod\u00e8le sur le dataset d'entra\u00eenement et celles sur le dataset de validation. La plupart du temps, la m\u00e9trique observ\u00e9e est la valeur de la fonction de perte sur le dataset de validation. Une fois que l'\u00e9cart devient \"trop important\" ou que la m\u00e9trique de validation ne s'am\u00e9liore plus pendant un certain nombre d'\u00e9poques, on ar\u00eatte l'entra\u00eenement.</p> <p>Sur Tensorflow, cela se g\u00e8re via l'API keras avec le callback EarlyStopping.</p> <pre><code>tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                 min_delta=0,\n                                 patience=0,\n                                 verbose=0,\n                                 mode='auto',\n                                 baseline=None,\n                                 restore_best_weights=False\n                                )\n</code></pre> <p>Cette m\u00e9thode n'est plus tr\u00e8s utilis\u00e9e actuellement, les versions modernes de Tensorflow (tout comme Pytorch), nous permettent de sauvegarder les meilleurs mod\u00e8les sans avoir \u00e0 stoper l'entra\u00eenement.</p>"},{"location":"deep_learning/module4/Module4/###","title":"R\u00e9gularisations \\(L_{1}\\) , \\(L_{2}\\)","text":"<p>Les m\u00e9thodes de r\u00e9gularisation \\(L_{1}\\) , \\(L_{2}\\) sont des m\u00e9thodes provenant de l'apprentissage statistique et du Machine Learning classique. On peut trouver la r\u00e9gularisation \\(L_{1}\\) sous le terme de r\u00e9gression LASSO, et la r\u00e9gularisation \\(L_{2}\\) sous le terme de r\u00e9gression Ridge (voire m\u00eame r\u00e9gularisation de Thikonov).</p> <p>Ici, l'id\u00e9e est de contraindre les poids \"\u00e0 \u00eatre petits\", on peut y penser comme l'ajout d'une p\u00e9nalit\u00e9 contre la compl\u00e9xit\u00e9.</p>"},{"location":"deep_learning/module4/Module4/####","title":"R\u00e9gularisation \\(L_{2}\\)","text":"<p>Pla\u00e7ons nous dans le cas d'un mod\u00e8le lin\u00e9aire, tel qu'un Perceptron simple, avec ici une entr\u00e9e de dimension 8 et un seul neurone donnant une pr\u00e9diction \\(\\hat{y}\\), la fonction d'activations \\(\\sigma\\) est ici quelconque, de m\u00eame que la fonction de perte \\(\\mathcal{L}_{\\vartheta}\\), qui peut \u00eatre une MSE, la log-vraissemblance n\u00e9gative, l'entropie crois\u00e9e.</p>  <p>Sans r\u00e9gularisation, la fonction de perte que l'on cherche \u00e0 minimiser est alors d\u00e9finie par</p> \\[         \\mathcal{L}_{\\vartheta} := \\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{L}_{\\vartheta}(y_{i}, \\hat{y_{i}}) \\] <p>Ajouter une r\u00e9gularisation \\(L_{2}\\) revient \u00e0 ajouter une p\u00e9nalit\u00e9 \u00e0 la fonction de perte, la nouvelle fonction de perte consid\u00e9r\u00e9e est alors la suivante.</p> \\[     \\begin{align}         \\mathcal{L}_{\\vartheta} &amp; := \\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{L}_{\\vartheta}(y_{i}, \\hat{y_{i}}) + \\frac{\\lambda}{N}  \\sum_{j=1}^{8} w_{j}^{2} \\\\                             &amp; = \\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{L}_{\\vartheta}(y_{i}, \\hat{y_{i}}) + \\frac{\\lambda}{N}  ||w||^{2}_{2}     \\end{align} \\] <p>Ce que l'on a fait ici consiste \u00e0 rajouter dans la fonction de perte la sommes des poids du r\u00e9seau au carr\u00e9, d'o\u00f9 le terme de r\u00e9gularisation \\(L_{2}\\) puisque ce que l'on a fait ici est de calculer la norme euclidienne (norme \\(L_{2}\\)) du vecteur de poids du r\u00e9seau.</p> <p>Pour minimiser la fonction de perte, il est donc n\u00e9cessaire pour le r\u00e9seau de minimiser ses poids, sans pour autant qu'il soient tous nuls. Si la norme euclidienne du vecteur de poids est nulle, alors tous les poids sont nuls, et donc \\(\\hat{y}\\) ne d\u00e9pendra enti\u00e8rement que du biais, ce qui implique une pr\u00e9diction relativement mauvaise.</p> <p>Ici, \\(\\lambda\\) est un hyperparam\u00e8tre apprenable et \\(N\\) correspond \u00e0 la taille du minibatch dans la descente du gradient stochastique.</p> <p>Dans le cas d'un r\u00e9seau avec plusieurs couches, comme un Perceptron multicouches, on somme d'abord les neurones d'une couche, puis on r\u00e9it\u00e8re cette somme sur chaque couche.</p>  <p>On obtient alors la formules de r\u00e9gularisation suivante,</p> \\[     \\mathcal{L}_{\\vartheta} := \\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{L}_{\\vartheta}(y_{i}, \\hat{y_{i}}) + \\frac{\\lambda}{N} \\sum_{s = 1}^{\\ell} ||\\mathbf{W}_{s}||^{2}_{F} \\] <p>o\u00f9 \\(||\\mathbf{W}_{s}||^{2}_{F}\\) est la norme de Frobenius de la matrice des poids de la couche \\(s\\), \\(\\mathbf{W}_{s}\\), ie la somme du carr\u00e9 de ses \u00e9l\u00e9ments.</p> \\[     ||\\mathbf{W}_{s}||^{2}_{F} := \\sum_{i} \\sum_{j} (w_{i,j}^{s})^{2} \\]"},{"location":"deep_learning/module4/Module4/####","title":"R\u00e9gularisation \\(L_{1}\\)","text":"<p>Dans le cas d'une r\u00e9gularisation \\(L_{1}\\), la norme euclidienne est alors remplac\u00e9e par la somme des valeurs absolues des poids du r\u00e9seau.</p> \\[     \\begin{align}     \\mathcal{L}_{\\vartheta} &amp; := \\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{L}_{\\vartheta}(y_{i}, \\hat{y_{i}}) + \\frac{\\lambda}{N}  \\sum_{j=1}^{8} |w_{j}| \\\\                             &amp; = \\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{L}_{\\vartheta}(y_{i}, \\hat{y_{i}}) + \\frac{\\lambda}{N}  ||w||_{1}     \\end{align} \\] <p>Dans le cas d'un r\u00e9seau avec plusieurs couches, comme un Perceptron multicouches, on somme d'abord les neurones d'une couche, puis on r\u00e9it\u00e8re cette somme sur chaque couche.</p> <p>On obtient alors la formules de r\u00e9gularisation suivante,</p> \\[     \\mathcal{L}_{\\vartheta} := \\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{L}_{\\vartheta}(y_{i}, \\hat{y_{i}}) + \\frac{\\lambda}{N} \\sum_{s = 1}^{\\ell} ||\\mathbf{W}_{s}||_{1} \\] <p>o\u00f9 \\(||\\mathbf{W}_{s}||_{1}\\) est la somme des valeurs absolues de ses \u00e9l\u00e9ments.</p> \\[     ||\\mathbf{W}_{s}||_{1} := \\sum_{i} \\sum_{j} |w_{i,j}^{s}| \\] <p>La r\u00e9gularisation \\(L_{1}\\) encourage les matrices de poids \u00e0 etre creuse, c'est \u00e0 dire \u00e0 avoir beaucoup de z\u00e9ros dans leurs \u00e9l\u00e9ments. En pratique cette m\u00e9thode est peu utilis\u00e9e, du fait que la norme \\(L_{1}\\) n'est pas diff\u00e9rentiable, les r\u00e9seaux r\u00e9gularis\u00e9s avec cette m\u00e9thode sont plus dur \u00e0 optimiser.</p>"},{"location":"deep_learning/module4/Module4/#interpretation-geometrique","title":"Interpr\u00e9tation g\u00e9om\u00e9trique","text":""},{"location":"deep_learning/module4/Module4/#dropout","title":"Dropout","text":""},{"location":"deep_learning/module4/Module4/#phase-dentrainement","title":"Phase d'entra\u00eenement","text":"<p>Le dropout fait partie de l'\u00e9tat de l'art en ce qui concerne les techniques de r\u00e9gularisation des r\u00e9seaux de neurones. Propos\u00e9 par Geoffrey Hinton en 2012 (Improving neural networks by preventing co-adaptation of feature detectors, https://arxiv.org/pdf/1207.0580.pdf) et plus d\u00e9taill\u00e9 en 2014 par Nitish Srivastava et al. (Dropout: A Simple Way to Prevent Neural Networks from Overfitting, http://jmlr.org/papers/v15/srivastava14a.html).</p> <p>La technique de dropout c'est av\u00e9r\u00e9e tr\u00e8s puissante, elle a permis d'am\u00e9liorer la pr\u00e9cision des r\u00e9seaux de neurones faisant partie de l'\u00e9tat de l'art de \\(1-2 \\%\\) simplement en l'ajoutant \u00e0 ces r\u00e9seaux.</p>  <p>Remarque</p> <p>Une am\u00e9lioration de \\(2\\%\\) peut sembler peu, mais si un mod\u00e8le est dej\u00e0 \u00e0 \\(95\\%\\) de pr\u00e9cision, alors une augmentation de \\(2\\%\\) signifie que l'on a r\u00e9duit le taux d'erreur de \\(40\\%\\) (en allant de \\(5\\%\\) d'erreur \u00e0 \\(3\\%\\)) !</p> <p>Si votre mod\u00e8le classifiait mal \\(20000\\) images, cela veut dire que \\(8000\\) nouvelles images sont maintenant correctement classifi\u00e9es.</p>  <p>L'id\u00e9e est simple, \u00e0 chaque \u00e9tape d'entra\u00eenement, tous les neurones (hormis les neurones de sortie) ont une probabilit\u00e9 \\(p\\) d'\u00eatre temporairement d\u00e9sactiv\u00e9, ie ils seront compl\u00e8tement ignor\u00e9 durant cette \u00e9tape, mais ils peuvent tr\u00e8s bien \u00eatre de nouveau actif \u00e0 la prochaine \u00e9tape.</p>  <p>D\u00e9tail de la m\u00e9thode du dropout</p> <p>Le dropout est bas\u00e9 sur les techniques d'\u00e9chantillonage de Bernoulli.</p> <p>Choisissons une couche de neurones \\(\\ell\\), que l'on fixe, et notons \\(y_{i}^{\\ell}\\) la \\(i\\)-i\u00e8me sortie de cette couche. Dans le cadre classe classique d'un r\u00e9seau de neurones, l'\u00e9tape de feedforward pour cette couche est d\u00e9finie par les formules suivantes.</p> \\[     \\begin{align}     z_{i}^{\\ell} &amp; := \\mathbf{y}^{\\ell - 1 } \\mathbf{w}_{i}^{\\ell} + \\mathbf{b}^{\\ell} \\\\     y_{i}^{\\ell} &amp; := \\sigma(z_{i}^{\\ell})     \\end{align} \\] <p>Dans le cadre o\u00f9 l'on rajoute un dropout l'\u00e9tape est modifi\u00e9e comme suit.</p> <ol> <li> <p>On fixe \\(p\\) probabiblit\u00e9 de drop.</p> </li> <li> <p>A chaque \u00e9l\u00e9ment \\(y_{i}^{\\ell}\\) du vecteur \\(\\mathbf{y}^{\\ell}\\), ie les \u00e9l\u00e9ments de sortie de cette couche, on associe une probabilit\u00e9 \\(v_{i}\\) tir\u00e9 suivant une loi uniforme sur l'intervalle \\([0,1]\\).</p> </li> <li> <p>Si pour \\(y_{i}^{\\ell}\\), on a \\(v_{i} &lt; p\\) alors, \\(y_{i}^{\\ell}=0\\). Sinon \\(y_{i}^{\\ell}\\) est laiss\u00e9e tel quel.</p> </li> </ol>   <p>Ensemble des sous r\u00e9seaux possibles via Dropout</p>  <p>Dans l'exemple ici, on peut appliquer deux couches de dropout, une sur la couche d'entr\u00e9e, et une sur la couche cach\u00e9e. Pour l'exemple, fixons \\(p=0,5\\) pour ces deux couches.</p> <p>Dans le premier cas (coin sup\u00e9rieur droit), aucun neurone n'est d\u00e9sactiv\u00e9, ce qui revient \u00e0 dire la probabilit\u00e9 \\(v_{i}\\) \u00e0 chaque fois a \u00e9t\u00e9 sup\u00e9rieur ou \u00e9gale \u00e0 \\(p=0,5\\).</p> <p>Dans le second cas, seul le neurone correspondant \u00e0 l'entr\u00e9e \\(x_{1}\\) a \u00e9t\u00e9 d\u00e9sactiv\u00e9, ce qui veut donc dire que sa probabilit\u00e9 \\(v_{i}\\) devait \u00eatre inf\u00e9rieure strictement \u00e0 \\(0,5\\).</p>  <p>Cela revient donc \u00e0 chaque \u00e9tape d'entra\u00eenement \u00e0 choisir un nouveau sous r\u00e9seau. Pour un entra\u00eenement via SGD avec un minibatch de taille \\(N\\), cela revient donc \u00e0 rajouter une \u00e9tape dans la partie feedforward.</p> <ol> <li>S\u00e9lection d'un Minibatch \\(M\\).</li> <li>D\u00e9sactivation des neurones par Dropout, ie s\u00e9lection d'un sous r\u00e9seau.</li> <li> <p>Pour chaque observation \\(\\mathbf{x}_{i} \\in \\mathbf{R}^{m}\\) dans le minibatch \\(M\\).</p> <ol> <li>Calcul de la pr\u00e9diction \\(\\hat{y}_{i}\\).</li> <li>Calcul de la fonction de perte \\(\\mathcal{L}_{\\vartheta}(\\hat{y}_{i})\\).</li> </ol> </li> <li> <p>Calcul de la perte moyenne \\(\\mathcal{L}_{\\vartheta} = \\frac{1}{N}\\sum_{i=1}^{N}\\mathcal{L}_{\\vartheta}(\\hat{y}_{i})\\).</p> </li> <li>Calcul de \\(\\nabla \\mathcal{L}_{\\vartheta}\\).</li> <li>Mise \u00e0 jour des poids et biais du sous r\u00e9seau par r\u00e9tropropagation.</li> </ol>"},{"location":"deep_learning/module4/Module4/#phase-de-test","title":"Phase de test","text":"<p>Durant la phase de test, le dropout est d\u00e9sactiv\u00e9, ie on consid\u00e8re le r\u00e9seau complet. Il y a cependant une modification apport\u00e9e. Supposons que la probabilit\u00e9 de dropout est de \\(p=0,5\\) pour une couche donn\u00e9e \\(\\ell\\). A cause du dropou, durant la phase d'entra\u00eenement les neurones de la couche suivante \\(\\ell +1\\) n'ont \u00e9t\u00e9 connect\u00e9s en moyenne qu'\u00e0 la moiti\u00e9e des neurones de la couche \\(\\ell\\). Or durant la phase de test ils seront connect\u00e9s \u00e0 2 fois de neurones que ce que les poids n'ont \u00e9t\u00e9 optimis\u00e9s pour. Pour compenser cela, on multiplie par \\(0,5\\) les valeurs de sortie de la couche \\(\\ell\\), apr\u00e8s l'entra\u00eenement.</p> <p>De fa\u00e7on g\u00e9n\u00e9rale, pour une probabilit\u00e9 de dropout de \\(p\\) sur la donn\u00e9e \\(\\ell\\), on multiplie par \\(1-p\\) les valeurs de sortie de la couche \\(\\ell\\), apr\u00e8s l'entra\u00eenement.</p> <p>Dans le cadre de Tensorflow (et Pytorch), la m\u00e9thode d'impl\u00e9mentation choisie est celle dite de l'inverted Dropout, les valeurs de sortie sont multipli\u00e9es par \\(\\frac{1}{1-p}\\) durant la phase d'entra\u00eenement, et non durant la p\u00e9riode de test. Ce qui est moins co\u00fbteux au niveau de la puissance de calcul sur le long terme, si le mod\u00e8le est fortement utilis\u00e9 en production.</p>"},{"location":"deep_learning/module4/Module4/#pourquoi-ca-marche","title":"Pourquoi \u00e7a marche.","text":"<p>Comme un nouveau sous r\u00e9seau \u00e0 chaque nouvelle \u00e9tape d'entra\u00eenement, cela force le r\u00e9seau \u00e0 ne pas se reposer sur un nombre limit\u00e9 de connexions. Le r\u00e9seau consid\u00e9rera un nombre plus importants de connexions et par cons\u00e9quent les poids seront moins concentr\u00e9 sur un petit nombre de neurones.</p> <p>Si l'on d\u00e9signe par \\(D\\) le nombre de neurones qui peuvent \u00eatre susceptibles de dropout dans un r\u00e9seau de neurones, c'est \u00e0 dire le nombre de neurones pr\u00e9sents dans les couche cach\u00e9es, on a alors un total de \\(2^{D}\\) sous r\u00e9seau possibles.</p> <p>Par exemple si le dropout n'est activ\u00e9 que sur une seule couche avec 32 neurones denses, on a quand m\u00eame un nombre de</p> \\[         2^{32} - 1 = 4294967295 \\] <p>sous r\u00e9seaux possibles. Ce qui fait qu'il est virtuellement impossible pour le m\u00eame sous r\u00e9seau d'\u00eatre s\u00e9lectionn\u00e9 deux fois. Si l'entra\u00eenement s'arr\u00eate au bout de \\(10000\\) \u00e9tapes, cela veut dire que l'on a entra\u00een\u00e9 \\(10000\\) sous r\u00e9seaux diff\u00e9rents. Ces sous r\u00e9seaux ne sont pas compl\u00e8tement ind\u00e9pendants car ils partagent beaucoup de leur poids et biais, mais ils sont quand m\u00eame diff\u00e9rents. Le r\u00e9seau de neurones obtenu peut \u00eatre vu comme une moyenne de tous ces sous r\u00e9seaux plus petits.</p> <p>La commande Tensorflow pour rajouter une couche de dropout est la suivante.</p> <pre><code>tf.keras.layers.Dropout(p)\n</code></pre>  <p>Remarque</p> <ol> <li>La probabilit\u00e9 dans l'article d'origine \u00e9tait de \\(p=0,5\\), il est courant qu'elle soit comprise dans l'intervalle \\([0,2; 0,8]\\) aujourd'hui.</li> <li>Si le mod\u00e8le n'est pas sujet au sur-apprentissage, le dropout peut avoir l'effet inverse que celui recherch\u00e9.</li> <li>Dans ce cas l\u00e0, il est conseill\u00e9 d'augmenter la compl\u00e9xit\u00e9 du mod\u00e8le pour le faire rentrer en sur-apprentissage, puis de le r\u00e9-entra\u00eener en ayant ajouter des couches de dropout.</li> <li>Dans le cas o\u00f9 l'on utilise un mod\u00e8le pr\u00e9entra\u00een\u00e9 pour faire du transfert d'apprentissage. il n'est pas possible de rajouter des couches de dropout dans la partie entra\u00een\u00e9e, mais seulement dans la partie classifiante avec les r\u00e9seaux de neurones denses.</li> <li>Dans le cas des CNN, les pixels adjacents sont g\u00e9n\u00e9ralement fortement corr\u00e9l\u00e9s, par cons\u00e9quent l'id\u00e9e que le dropout permet de se d\u00e9barasser de la d\u00e9pendances de certains poids ne marche pas ici. Dans le cadre l\u00e0, l'id\u00e9e est alors de d\u00e9sactiver enti\u00e8rement une feature map, via un 'SpatialDropout'</li> </ol> <pre><code>tf.keras.layers.SpatialDropout2D(p)\n</code></pre>"},{"location":"deep_learning/module4/Module4/#batchnorm","title":"BatchNorm","text":"<p>Il est difficile d'entra\u00eener des r\u00e9seaux de neurones avec des observations ayant un ordre de magnitude important. Pour homog\u00e9n\u00e9iser notre dataset avant l'entra\u00eenement, il est possible de standardiser nos entr\u00e9es en utilisant la transformation suivante.</p> \\[     \\tilde{x}_{i,j} := \\frac{x_{i,j} - \\mu_{j}}{\\sigma_{j}^{2}} \\] <p>o\u00f9</p> <ol> <li>\\(x_{i,j}\\) correspond \u00e0 la \\(j\\)-i\u00e8me feature de l'observation \\(\\mathbf{x}_{i} \\in \\mathbf{R}^{m}\\).</li> <li>\\(\\mu_{j}\\) correspond \u00e0 la moyenne de la \\(j\\)-i\u00e8me feature prise sur l'ensemble du dataset.</li> <li>\\(\\sigma_{j}\\) correspond \u00e0 l'\u00e9cart type de la \\(j\\)-i\u00e8me feature prise sur l'ensemble du dataset.</li> </ol> <p>Dans ce cas l\u00e0, toutes nos features auront une moyenne de \\(0\\) et un \u00e9cart-type de \\(1\\), ie pour chaque feature, \\(95\\%\\) des valeurs seront comprises dans l'intervalle \\([-2,2]\\). Les poids seront alors eux aussi \"normalis\u00e9s\" pour moins refl\u00e9ter l'influence des valeurs importantes.</p> <p>Cependant standardiser les entr\u00e9es du r\u00e9seau ne va affecter que les poids de la premi\u00e8re couche cach\u00e9e. Qu'en est il des autres couches, et de la distribution de leur features ?</p>  <p>De par l'algorithme de SGD et le proc\u00e9d\u00e9 de mise \u00e0 jour des poids et des biais, la distribution des features dans une m\u00eame couche cach\u00e9e peut changer d'une \u00e9tape d'entra\u00eenement \u00e0 l'autre. Ce qui n'est pas optimal pour l'apprentissage du r\u00e9seau.</p> <p>On sait, gr\u00e2ce aux articles de LeCun &amp; al., 1998 (Neural Networks : Tricks of the trade, Springer) et Wiesler &amp; Ney, 2011 (A convergence analysis of log-linera training, Advances in Neural Information Processing Systems), que l'entra\u00eenement d'un r\u00e9seau de neurones converge plus rapidement si les entr\u00e9es du r\u00e9seau sont standardis\u00e9es (moyenne nulle et variance de \\(1\\)), et d\u00e9corr\u00e9l\u00e9es. Cependant le processus de d\u00e9corr\u00e9lation demande demande d'inverser une matrice ce qui est co\u00fbteux en temps de calcul.</p> <p>L'id\u00e9e d\u00e9velopp\u00e9e en 2015 dans un article de Ioffe &amp; Szegedy (Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, https://arxiv.org/abs/1502.03167) est alors de pratiquer l'op\u00e9ration dite de Batch Normalization, ou seule l'op\u00e9ration de standardisation sera appliqu\u00e9e, mais cette fois ci \u00e0 toutes les couches cach\u00e9es du r\u00e9seau.</p>"},{"location":"deep_learning/module4/Module4/#phase-dentrainement_1","title":"Phase d'entra\u00eenement","text":"<p>Pour un entra\u00eenement via SGD avec un minibatch de taille \\(N\\), l'op\u00e9ration de Batch Normalization se d\u00e9roule ainsi</p> <p>Pour chaque feature \\(j \\in \\lbrace 1, \\dots, m \\rbrace\\) dans le minibatch \\(M\\).</p> <ol> <li> <p>Calcul de la moyenne de la \\(j\\)-i\u00e8me feature du batch \\(M\\),</p> \\[     \\mu_{j} := \\frac{1}{N} \\sum_{i=1}^{N} x_{i,j} \\] </li> <li> <p>Calcul de la variance de la \\(j\\)-i\u00e8me feature du batch \\(M\\),</p> \\[     \\sigma_{j}^{2} := \\frac{1}{N} \\sum_{i=1}^{N} (x_{i,j} - \\mu_{j})^{2} \\] </li> <li> <p>Standardisation,</p> \\[     \\tilde{x}_{i,j} := \\frac{x_{i,j} - \\mu_{j}}{\\sqrt{\\sigma_{j}^{2}+ \\varepsilon}} \\quad \\forall i \\] </li> </ol> <p>\\(\\varepsilon\\) \u00e9tant l\u00e0 pour \u00e9viter que l'on divise par z\u00e9ro, en pratique il est de l'ordre de \\(10^{-5}\\).</p> <ol> <li> <p>Ajout des param\u00e8tres,</p> \\[     z_{i,j} := \\gamma_{j} \\tilde{x}_{i,j} + \\beta_{j} \\] </li> </ol> <p>O\u00f9 \\(\\gamma_{j}\\) et \\(\\beta_{j}\\) sont des param\u00e8tres qui sont apprenables par le r\u00e9seau. Ainsi, si la distribution des valeurs \u00e9tait d\u00e9j\u00e0 optimale, la mise \u00e0 jour des poids fera en sorte que l'on obtienne les param\u00e8tres suivants,</p> \\[     \\begin{cases}     \\gamma_{j} &amp; = \\sqrt{\\mathrm{Var}(x_{.,j})} \\\\     \\beta_{j} &amp; = \\mu_{j}     \\end{cases} \\] <p>afin de retrouver la distribution initiale.</p> <p>On a donc une paire de param\u00e8tres \\(\\gamma, \\beta\\) pour chaque features</p> \\[     \\lbrace (\\gamma_{1}, \\beta_{1}), (\\gamma_{2}, \\beta_{2}), \\dots, (\\gamma_{m}, \\beta_{m}) \\rbrace. \\]  <p>Attention</p> <p>Rappelons ici que seuls les param\u00e8tres \\(\\gamma, \\beta\\) sont apprenables par le r\u00e9seau. La moyenne et la variance \\(\\mu, \\sigma\\) sont des statistiques calcul\u00e9es sur le minibatch, donc fixes, leur valeurs peut cependant changer \u00e0 chaque nouveau minibatch.</p>"},{"location":"deep_learning/module4/Module4/#phase-de-test_1","title":"Phase de test","text":"<p>Durant la phase d'entra\u00eenement, la moyenne et la variance sont calcul\u00e9es sur les minibatchs. Or lors de la phase de test ou de production, il est hautement improbable que vous ayez \u00e0 votre disposition un minibatch pour calculer ces statistiques. On souhaite que la pr\u00e9diction ne d\u00e9pende que de l'entr\u00e9e et non pas de statistiques calcul\u00e9es ailleurs.</p> <p>Pour cela, les moyennes et variances calcul\u00e9es sur les minibatch sont sotck\u00e9es en m\u00e9moire afin d'avoir une estimation pr\u00e9cise de la moyenne et de la variance de chaque features sur le dataset complet.</p> <p>La moyenne et la variance sont calcul\u00e9es via des moyennes et variances mouvantes exponentielles pond\u00e9r\u00e9es, en temps r\u00e9el durant la phase d'entra\u00eenement pour pouvoir \u00eatre utilis\u00e9es durant la phase de test.</p> <p>Pour l'\u00e9tape \\(t\\) d'entra\u00eenement, la moyenne et la variance mouvante \\(\\hat{\\mu}_{j}[t]\\), \\(\\hat{\\sigma}_{j}^{2}[t]\\)</p> \\[     \\begin{cases}     \\hat{\\mu}_{j}(t) &amp; = \\hat{\\mu}_{j}(t-1)\\cdot \\mathrm{moment} + (1- \\mathrm{moment})\\cdot \\mu_{j}(t)   \\\\     \\hat{\\sigma}_{j}^{2}(t) &amp; = \\hat{\\sigma}_{j}^{2}(t-1)\\cdot \\mathrm{moment} + (1- \\mathrm{moment})\\cdot \\sigma_{j}^{2}(t)     \\end{cases} \\] <p>Le moment est un nombre r\u00e9el qui permet de contr\u00f4ler la mise \u00e0 jour des ces statistiques, g\u00e9n\u00e9ralement on a \\(\\mathrm{moment} \\simeq 0,1\\).</p> <p>Dans les cas des CNN, le principe est le m\u00eame, sauf qu'ici les features sont remplac\u00e9es par les features maps. Les statistiques moyennes et variances calcul\u00e9es sur le minibatch et les statistiques mouvantes prennent donc \u00e7a en compte.</p>  <p>BacthNorm dans les CNN</p>   <p>Pour chaque feature map \\(c \\in \\lbrace 1, \\dots, r \\rbrace\\) de hauteur \\(H\\) et de largeur \\(W\\), dans le minibatch \\(M\\), on a</p> <ol> <li> <p>Calcul de la moyenne de la \\(c\\)-i\u00e8me feature du batch \\(M\\),</p> \\[     \\mu_{c} = \\frac{1}{NHW} \\sum_{i=1}^{N} \\sum_{j=1}^{H} \\sum_{k=1}^{W} x_{i,j,k,c} \\] </li> <li> <p>Calcul de la variance de la \\(c\\)-i\u00e8me feature du batch \\(M\\),</p> \\[     \\sigma_{c}^2 = \\frac{1}{NHW} \\sum_{i=1}^{N} \\sum_{j=1}^{H} \\sum_{k=1}^{W} (x_{i,j,k,c} - \\mu_{c})^2 \\] </li> <li> <p>Standardisation,</p> \\[     \\tilde{x}_{i,j,k,c} = \\frac{x_{i,j,k,c}-\\mu_{c}}{\\sqrt{\\sigma_{c}^2 + \\epsilon}} \\] </li> <li> <p>Ajout des param\u00e8tres,</p> \\[     z_{i,:,:,c} = \\gamma_c \\tilde{x}_{i,:,:,c} + \\beta_c \\] </li> </ol>"},{"location":"deep_learning/module4/Module4/#pourquoi-ca-marche_1","title":"Pourquoi \u00e7a marche ?","text":"<p>On ne sait pas trop !</p> <p>Le papier d'origine \u00e9mmettait l'hypoth\u00e8se que le principe de Batch Normalization permettait de r\u00e9duire le d\u00e9calage interne covari\u00e9 (ICS : Internal Covariate Shift), ie le fait qu'il y ait un changement de distribution entre l'entr\u00e9e et la sortie d'une couche cach\u00e9e. Hors, aucune preuve formelle ou empirique forte n'a \u00e9t\u00e9 publi\u00e9e.</p> <p>Un nouvel article, 2019 (How Does Batch Normalization Help Optimization?, https://arxiv.org/pdf/1805.11604.pdf) montre que la Batch Normalization permet de lisser la surface de la fonction de perte, ce qui permet une convergence plus rapide, avec des taux d'apprentissage plus \u00e9lev\u00e9.</p> <p>Ce dont l'on est s\u00fbr, c'est que la Batch Normalization aide par rapport aux probl\u00e8mes d'explosion ou de disparition du gradient, qu'il am\u00e9liore la stabilit\u00e9 de l'entra\u00eenement et permet d'augmenter le taux d'apprentissage, et donc de faire converger le mod\u00e8le en moins d'\u00e9poques.</p>"},{"location":"deep_learning/module4/Module4/#les-methodes-dinitialisation-des-poids-xavier-he","title":"Les m\u00e9thodes d'initialisation des poids (Xavier, He)","text":"<p>Maintenant que la distribution de nos features en sortie de chaque couche est optimis\u00e9e gr\u00e2ce \u00e0 l'op\u00e9ration de Batch Normlization, on peut se poser la question de savoir comment optimiser l'initialisation des poids et biais dans nos couches ? On a deux raisons de vouloir optimiser ce d\u00e9part.</p> <ol> <li>Briser la sym\u00e9trie : si l'on initialise tous les poids et biais \u00e0 la m\u00eame constante \\(\\alpha\\), alors tous les neurones d'une m\u00eame couche seront parfaitement identiques, et la mise \u00e0 jour via r\u00e9tropropagation sera la m\u00eame pour tous. Ce qui fait m\u00eame si votre r\u00e9seau de neurones \u00e0 plusieurs couches avec des centaines de neurones dans chaque, une initialisation identique le fera se comporter comme une r\u00e9seau avec un seul neurone par couches.</li> </ol>  <p>Remarque</p> <p>Si les poids doivent \u00eatre initialis\u00e9s de mani\u00e8re al\u00e9atoire pour briser la sym\u00e9trie, il est parfaitement correct d'initialiser tous les biais \u00e0 z\u00e9ro, c'est ce que fait Tensorflow.</p>  <ol> <li>Diminuer les probl\u00e8mes d'explosion ou de disparition du gradient : Regardons le r\u00e9seau suivant.</li> </ol>  <p>Si l'on souhaite mettre \u00e0 jour le poids \\(w_{1,1}^{1}\\), on doit calculer la d\u00e9riv\u00e9e partielle suivante.</p> \\[     \\frac{\\partial \\mathcal{L}_{\\vartheta}}{ \\partial w_{1,1}^{1}} \\] <p>Les poids et neurones intervenant dans cette d\u00e9riv\u00e9e partielle sont ici en rouge, on a alors :</p> \\[     \\begin{align}     \\frac{\\partial \\mathcal{L}_{\\vartheta}}{ \\partial w_{1,1}^{1}} =  &amp; \\frac{\\partial \\mathcal{L}_{\\vartheta}}{\\partial s} \\cdot \\frac{\\partial s}{\\partial h_{1}^{2}} \\cdot \\frac{\\partial h_{1}^{2}}{\\partial h_{1}^{1}} \\cdot \\frac{\\partial h_{1}^{1}}{\\partial w_{1,1}^{1}} \\\\                                                                     + &amp; \\frac{\\partial \\mathcal{L}_{\\vartheta}}{\\partial s} \\cdot \\frac{\\partial s}{\\partial h_{2}^{2}}     \\cdot \\frac{\\partial h_{2}^{2}}{\\partial h_{1}^{1}} \\cdot \\frac{\\partial h_{1}^{1}}{\\partial w_{1,1}^{1}}     \\end{align} \\] <p>Chacune de ses d\u00e9riv\u00e9s implique la d\u00e9rivation d'une fonction d'activation, si la fonction d'activation des couches cach\u00e9es et de la sortie est la sigmo\u00efde</p> \\[     \\sigma(z) := \\frac{1}{1 + \\mathrm{e}^{-z}} \\] <p>alors sa d\u00e9riv\u00e9e est \\(\\sigma'(z) := \\sigma(z)(1-\\sigma(z))\\) et son maximum est en z\u00e9ro avec \\(\\sigma'(0) = 0,25\\). Ce qui veut dire que la valeur de la mise a jour du gradient ne sera pas plus grande que :</p> \\[     (0.25)^{3} \\cdot 2u = 0,015625\\cdot (2u) \\] <p>O\u00f9 \\(2u\\) correspond ici au reste des calculs dans l'addition des d\u00e9riv\u00e9es. Le probl\u00e8me s'aggrave si l'on a un r\u00e9seau de \\(10\\) couches, dans ce cas l\u00e0 :</p> \\[     (0.25)^{10} \\simeq 10^{-6} \\] <p>Traditionnellement, les poids sont initialis\u00e9s soit en effectuant un \u00e9chantillonnage suivant une distribution uniforme \\(\\mathrm{Unif}([0,1])\\), ou \\(\\mathrm{Unif}([-\\frac{1}{2},\\frac{1}{2}])\\), soit en effectuant un \u00e9chantillonnage suivant une distribution normale \\(\\mathcal{N}(0, \\sigma^{2} = 0,01)\\). Cependant ces initilisations ne sont pas optimales. Pour compenser cela on a principalement deux m\u00e9thodes, le choix d\u00e9pendant des fonctions d'activations que vous allez choisir.</p>"},{"location":"deep_learning/module4/Module4/#terminologie","title":"Terminologie","text":"<p>Pour la matrice de poids de la couche \\(\\ell\\), le nombre de neurones dans le couche est not\u00e9 \\(\\mathrm{Fan}_{out}\\), et le nombre de neurones de la couche pr\u00e9c\u00e9dente est not\u00e9 \\(\\mathrm{Fan}_{in}\\). La moyenne des deux se note \\(\\mathrm{Fan}_{avg}\\).</p>"},{"location":"deep_learning/module4/Module4/#initilisation-de-xavier","title":"Initilisation de Xavier","text":"<p>L'initialisation de Xavier, (Understanding the difficulty of training deep feedforward neural networks, http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), est une initilisation sp\u00e9cialis\u00e9e pour la fonction d'activation tangente hyperbolique.</p> \\[         \\tanh(z) := 2 \\sigma(2z) - 1 \\] <p>Le fait est que la fonction \\(\\tanh\\) est l\u00e9g\u00e8rement plus robuste que la fonction sigmo\u00efde en ce qui concerne les probl\u00e8mes de disparition du gradient. Cependant, elle a les m\u00eames probl\u00e8mes de saturation : la fonction \u00e9tant born\u00e9e, le graident est proche de z\u00e9ro pour des valeurs grande en valeur absolue. Le but de l'initialisation de Xavier est alors de modifier la g\u00e9n\u00e9ration initale des poids pour qu'ils restent dans la partie lin\u00e9aire de la fonction \\(\\tanh\\).</p> <p>La m\u00e9thode d'initialisation se fait en deux \u00e9tapes.</p> <ol> <li>On initialise les poids via un \u00e9chantillonnage depuis une distribution Normale ou Uniforme.</li> <li>On corrige la valeur des poids par un facteur pour qu'elle soit proportionnelle au nombre d'inputs de la couche.</li> </ol> <p>Si l'on initialise les poids via un \u00e9chantillonnage depuis une distribution Normale, alors les 2 \u00e9tapes sont les suivantes :</p> <ol> <li>Pour la couche \\(\\ell\\), on initialise \\(\\mathbf{W}_{\\ell, \\mathrm{init}}\\)  en \u00e9chantillonnant depuis une distribution Normale \\(\\mathcal{N}(0, \\sigma^{2} = 1)\\).</li> <li>On multiplie \\(\\mathbf{W}_{\\ell, \\mathrm{init}}\\) par le facteur \\(\\sqrt{\\frac{1}{\\mathrm{Fan}_{avg}}}\\).</li> </ol>  <p>Remarque</p> <p>Cela revient \u00e0 faire directement un \u00e9chantillonnage depuis une distribution Normale</p> \\[     \\mathcal{N}(0, \\sigma = \\sqrt{\\frac{1}{\\mathrm{Fan}_{avg}}}) \\]  <p>Si l'on initialise les poids via un \u00e9chantillonnage depuis une distribution Uniforme, alors les 2 \u00e9tapes sont les suivantes :</p> <ol> <li>Pour la couche \\(\\ell\\), on initialise \\(\\mathbf{W}_{\\ell, \\mathrm{init}}\\) en \u00e9chantillonnant depuis une distribution Uniforme \\(\\mathrm{Unif}([-\\sqrt{3},\\sqrt{3}])\\).</li> <li>On multiplie \\(\\mathbf{W}_{\\ell, \\mathrm{init}}\\) par le facteur \\(\\sqrt{\\frac{1}{\\mathrm{Fan}_{avg}}}\\).</li> </ol>  <p>Remarque</p> <p>Cela revient \u00e0 faire directement un \u00e9chantillonnage depuis une distribution Uniforme</p> \\[     \\mathrm{Unif}([-\\sqrt{\\frac{3}{\\mathrm{Fan}_{avg}}} , \\sqrt{\\frac{3}{\\mathrm{Fan}_{avg}}}]) \\]   <p>Les graphes ci dessus sont tir\u00e9s de l'article d'origine, montrant la diff\u00e9rence au niveau des valeurs que peuvent peuvent prendre la fonction \\(\\tanh\\), en tant que fonction d'activation, puis les valeurs du gradient lors de la r\u00e9tropropagation.</p>"},{"location":"deep_learning/module4/Module4/#initilisation-de-he","title":"Initilisation de He","text":"<p>L'initialisation de Xavier suppose que les fonctions d'activations aient une d\u00e9riv\u00e9e \u00e9gale \u00e0 \\(1\\), et que les sorties de chaque couches sont de moyenne nulle, ce sont des hypoth\u00e8ses raisonnables pour \\(\\tanh\\), mais pas si l'on souhaite utiliser la fonction d'activation \\(\\mathrm{ReLU}\\). De plus, l'initialisation de Xavier n'est pas adapt\u00e9e aux CNN dont l'architecture est tr\u00e8s profonde, comme les architectures faisant parti de l'\u00e9tat de l'art actuel.</p> <p>L'initialisation de He (Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, https://arxiv.org/abs/1502.01852)prend ce changement en compte et est optimis\u00e9e pour la fonction d'activation \\(\\mathrm{ReLU}\\).</p> <p>La m\u00e9thode d'initialisation se fait en deux \u00e9tapes.</p> <ol> <li>On initialise les poids via un \u00e9chantillonnage depuis une distribution Normale ou Uniforme.</li> <li>On corrige la valeur des poids par un facteur pour qu'elle soit proportionnelle au nombre d'inputs de la couche.</li> </ol> <p>Si l'on initialise les poids via un \u00e9chantillonnage depuis une distribution Normale, alors les 2 \u00e9tapes sont les suivantes :</p> <ol> <li>Pour la couche \\(\\ell\\), on initialise \\(\\mathbf{W}_{\\ell, \\mathrm{init}}\\)  en \u00e9chantillonnant depuis une distribution Normale \\(\\mathcal{N}(0, \\sigma^{2} = 1)\\).</li> <li>On multiplie \\(\\mathbf{W}_{\\ell, \\mathrm{init}}\\) par le facteur \\(\\sqrt{\\frac{2}{\\mathrm{Fan}_{in}}}\\).</li> </ol>  <p>Remarque</p> <p>Cela revient \u00e0 faire directement un \u00e9chantillonnage depuis une distribution Normale</p> \\[     \\mathcal{N}(0, \\sigma = \\sqrt{\\frac{2}{\\mathrm{Fan}_{in}}}) \\]  <p>Si l'on initialise les poids via un \u00e9chantillonnage depuis une distribution Uniforme, alors les 2 \u00e9tapes sont les suivantes :</p> <ol> <li>Pour la couche \\(\\ell\\), on initialise \\(\\mathbf{W}_{\\ell, \\mathrm{init}}\\) en \u00e9chantillonnant depuis une distribution Uniforme \\(\\mathrm{Unif}([-\\sqrt{3},\\sqrt{3}])\\).</li> <li>On multiplie \\(\\mathbf{W}_{\\ell, \\mathrm{init}}\\) par le facteur \\(\\sqrt{\\frac{2}{\\mathrm{Fan}_{in}}}\\).</li> </ol>  <p>Remarque</p> <p>Cela revient \u00e0 faire directement un \u00e9chantillonnage depuis une distribution Uniforme</p> \\[     \\mathrm{Unif}([-\\sqrt{\\frac{6}{\\mathrm{Fan}_{in}}} , \\sqrt{\\frac{6}{\\mathrm{Fan}_{in}}}]) \\]   <p>Graphes tir\u00e9s de l'article d'origine, montrant la diff\u00e9rence au niveau des valeurs de la Top-1 erreur de validation. On voit aussi le fait que l'initilisation de He est sp\u00e9cialis\u00e9e pour les CNN, pour un CNN de 30 couches l'initialisation de Xavier ne le fait pas converger.</p>"},{"location":"deep_learning/module4/Module4/#le-cas-des-cnn","title":"Le cas des CNN","text":"<p>Le cas de l'initialisation pour les couches convolutives est particuliers, en effet il n'est pas forc\u00e9ment clair de parler de \\(\\mathrm{Fan}_{in}\\) et de \\(\\mathrm{Fan}_{out}\\) dans une couche convolutive, qui n'est pas du tout structur\u00e9e comme une couche dense.</p> <p>Dans une couche convolutive, les poids que l'on cherche \u00e0 initialiser sont ceux des filtres (kernel), si l'on regarde comment on impl\u00e9mente une couche convolutive en Tensorflow, on a le code suivant.</p> <pre><code>Conv2D(d, (k, k))\n</code></pre> <p>Ici, \\(d\\) correspond au nombre de filtre (qui correspondra aussi au nombre feature maps en sortie de la couche), ses filtres sont de dimension \\((k,k)\\). On a donc \\(k^{2}d\\) poids \u00e0 initialiser, par convention \\(\\mathrm{Fan}_{in}\\) est alors d\u00e9fini comme cela dans le cas d'une couche convolutive pour l'initialisation de He.</p> \\[     \\mathrm{Fan}_{in} := k^{2}d \\] <p>Pour \"He Normal\", on a :</p> \\[     \\mathcal{N}(0, \\sigma = \\sqrt{\\frac{2}{k^{2}d}}). \\] <p>Pour \"He Uniforme\", on a :</p> \\[     \\mathrm{Unif}([-\\sqrt{\\frac{6}{k^{2}d}} , \\sqrt{\\frac{6}{k^{2}d}}]). \\] <p>Par d\u00e9faut Tensorflow utilise l'initialisation de Xavier Uniforme (nomm\u00e9 'glorot_uniform' de par son nom de famille), pour les couches denses comme pour les couches convolutive.</p> <pre><code>tf.keras.layers.Dense(kernel_initializer='glorot_uniform', bias_initializer='zeros')\ntf.keras.layers.Conv2D(kernel_initializer='glorot_uniform', bias_initializer='zeros')\n</code></pre> <p>Pour modifier cela, il suffit de changer la valeur de kernel_initializer.</p> <pre><code>tf.keras.layers.Dense(kernel_initializer='glorot_normal', bias_initializer='zeros')\ntf.keras.layers.Conv2D(kernel_initializer='glorot_normal', bias_initializer='zeros')\n\ntf.keras.layers.Dense(kernel_initializer='he_uniform', bias_initializer='zeros')\ntf.keras.layers.Conv2D(kernel_initializer='he_uniform', bias_initializer='zeros')\n\ntf.keras.layers.Dense(kernel_initializer='he_normal', bias_initializer='zeros')\ntf.keras.layers.Conv2D(kernel_initializer='he_normal', bias_initializer='zeros')\n</code></pre>"},{"location":"deep_learning/module4/Module4/#optimisation-de-la-descente-du-gradient-stochastique","title":"Optimisation de la descente du gradient stochastique","text":"<p>Les techniques que l'on va voir ici ont \u00e9t\u00e9 d\u00e9velopp\u00e9es pour acc\u00e9lerer le processus de SGD, ie acc\u00e9lerer sa convergence, et sa pr\u00e9cision. On se concentrera uniquement sur des techniques du premier ordre ici, qui ne prennent en compte que le gradient. Consid\u00e9rer des techniques d'optimisation du second ordre en consid\u00e9ration les d\u00e9riv\u00e9es secondes et la Hessienne de la fonction de perte n'est pas efficace en Deep Learning du fait de la non convexit\u00e9 de la fonction de perte.</p>"},{"location":"deep_learning/module4/Module4/#learning-rate-decay","title":"Learning rate Decay","text":"<p>Dans le cas du Deep Learning, la m\u00e9thode de descente du gradient la plus commune est la descente du gradient stochastique pas minibatch (MB-SGD). Chaque minibacth peut \u00eatre consid\u00e9r\u00e9 comme un \u00e9chantillon du dataset, qui est lui m\u00eame un \u00e9chantillon de la population totale.</p> <p>Par cons\u00e9quent, le gradient calcul\u00e9 lors de la MB-SGD</p> \\[     \\begin{align}             \\nabla  \\mathcal{L}_{\\vartheta} &amp; := \\nabla ( \\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{L}_{\\vartheta}(\\hat{y}_{i})) \\\\                                      &amp; = \\frac{1}{N} \\sum_{i=1}^{N} \\nabla \\mathcal{L}_{\\vartheta}(\\hat{y}_{i})     \\end{align} \\] <p>est une approximation du gradient</p> \\[     \\begin{align}         \\nabla  \\mathcal{L}_{\\vartheta}^{\\mathrm{tot}} &amp; := \\nabla ( \\frac{1}{n} \\sum_{i=1}^{n} \\mathcal{L}_{\\vartheta}(\\hat{y}_{i})) \\\\                                      &amp; = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla \\mathcal{L}_{\\vartheta}(\\hat{y}_{i})     \\end{align} \\] <p>prenant en compte le dataset complet. En d'autres termes, on a</p> \\[     \\nabla  \\mathcal{L}_{\\vartheta} \\simeq  \\nabla \\mathcal{L}_{\\vartheta}^{\\mathrm{tot}} + \\varepsilon. \\] <p>Ce qui au premier abord peut sembler poser un probl\u00e8me est en fait b\u00e9n\u00e9fique. La fonction de perte \u00e9tant non convexe, l'ajout du bruit peut nous permettre de sortir de minima locaux sous-optimaux.</p> <p>Un des inconv\u00e9nients est n\u00e9anmoins l'existence d'oscillations plus importantes de par la pr\u00e9sence de bruit.</p>   <p>Exemple</p> <p>Haut : Sans bruit, on resterait bloquer dans le premier minimal local observ\u00e9, car le gradient serait nul. La pr\u00e9sence du bruit nous permet de garder \"une certaine inertie pour continuer d'avancer\". Bas : Diff\u00e9rence entre descente du gradient classique (fl\u00eaches vertes), et MB-SGD (fl\u00eaches rouges).</p>    <p>Remarque</p> <p>Un autre avantage de la MB-SGD est sa vitesse de convergence car elle permet de la parall\u00e9lisme.</p>  <p>Pour r\u00e9duire l'effet des oscillations \u00e0 la fin de l'entra\u00eenement, on r\u00e9duit alors le taux d'apprentissage. C'est le principe du Learning Rate Decay.</p>  <p>Attention</p> <p>Dangers du LRD : baisse trop rapide, trop t\u00f4t, qui impacterait la convergence du mod\u00e8le.</p> <p>Bonne pratique : Entra\u00eener le mod\u00e8le sans, puis r\u00e9entra\u00eener avec un LRD.</p>  <p>Les types de LRD les plus connus sont les suivants.</p> <p>Dans chacune des formules suivantes,</p> <ol> <li>\\(\\eta_{0}\\) est le taux d'apprentissage initial,</li> <li>\\(t\\) est l'\u00e9poque,</li> <li>et \\(k\\) est le taux de d\u00e9croissance (hyperparam\u00e8tre).</li> </ol>"},{"location":"deep_learning/module4/Module4/#lrd-exponentiel","title":"LRD exponentiel","text":"\\[     \\eta_{t} := \\eta_{0}\\mathrm{e}^{(-kt)} \\]"},{"location":"deep_learning/module4/Module4/#lrd-inverse","title":"LRD inverse","text":"\\[     \\eta_{t} := \\frac{\\eta_{0}}{1+kt} \\]"},{"location":"deep_learning/module4/Module4/#lrd-divise","title":"LRD divis\u00e9","text":"\\[     \\eta_{t} := \\frac{\\eta_{t-1}}{2} \\] <p>En pratique, il est compliqu\u00e9 de savoir lequel utiliser, l'exp\u00e9rimentation est n\u00e9c\u00e9ssaire.</p> <p>La technique \u00e9voqu\u00e9e \u00e0 l'instant ne fait que modifier le taux d'apprentissage. Pour les suivantes, on modifie directement la m\u00e9thode de mise \u00e0 jour des poids.</p>"},{"location":"deep_learning/module4/Module4/#momentum","title":"Momentum","text":"<p>La MB-SGD ne fait que des mises \u00e0 jours locales ne prenant pas en compte les mises \u00e0 jours pr\u00e9c\u00e9dentes : si \\(||\\nabla  \\mathcal{L}_{\\vartheta}||\\) est petit, alors la mise \u00e0 jour des poids sera petite. Il serait int\u00e9ressant de faire une mise \u00e0 jour \"ayant un spectre un peu plus global\".</p> <p>La m\u00e9thode suivante, propos\u00e9e par Boris Polyak en 1964, et d'utiliser la notion de momentum, connue en physique. L'id\u00e9e est la suivante : on ne souhaite plus uniquement boug\u00e9e dans la direction opppos\u00e9e au gradient, mais aussi bouger dans la direction moyenne des derni\u00e8res mises \u00e0 jours. Cela permet d'amoindrir les oscillations mais aussi d'\u00e9chapper des minima locaux plus facilement.</p> <p>On rappelle que la m\u00e9thode classique de mise \u00e0 jours des poids, de l'\u00e9tape \\(t\\) \u00e0 \\(t+1\\) est donn\u00e9e par la formule suivante.</p> \\[     w_{i,j}^{\\ell}(t+1) := w_{i,j}^{\\ell}(t) - \\eta \\frac{\\partial \\mathcal{L}_{\\vartheta}}{\\partial w_{i,j}^{\\ell}(t)}(\\vartheta) \\] <p>Dans le cas de l'ajout du momentum, la modification est la suivante, on d\u00e9finit la v\u00e9locit\u00e9 \\(\\Delta w_{i,j}^{\\ell}(t)\\) par</p> \\[     \\begin{align}     \\Delta w_{i,j}^{\\ell}(0) &amp; = 0 \\\\     \\Delta w_{i,j}^{\\ell}(t) &amp; := \\alpha \\Delta w_{i,j}^{\\ell}(t-1) - \\eta \\frac{\\partial \\mathcal{L}_{\\vartheta}}{\\partial w_{i,j}^{\\ell}(t)}(\\vartheta)     \\end{align} \\] <p>\\(\\alpha \\Delta w_{i,j}^{\\ell}(t-1)\\) est alors le momentum ajout\u00e9. La mise \u00e0 jour des poids est alors d\u00e9finie par la formule suivante.</p> \\[     w_{i,j}^{\\ell}(t+1) := w_{i,j}^{\\ell}(t) + \\Delta w_{i,j}^{\\ell}(t) \\] <p>Si \\(\\alpha \\in \\mathbf{R}\\) est nul, alors on retombe sur la mise \u00e0 jour classique des poids, \\(\\alpha\\) est un hyperparam\u00e8tre qui est g\u00e9n\u00e9ralement compris entre \\(0,9 \\leq \\alpha \\leq 0,999\\).</p> <p>Dans Tensorflow, ajout\u00e9 le momentum dans la MB-SGD se fait de la fa\u00e7on suivante.</p> <pre><code>tf.keras.optimizers.SGD(lr = 0.001, momentum=0.9)\n</code></pre>"},{"location":"deep_learning/module4/Module4/#nesterov","title":"Nesterov","text":"<p>Une modification du momentum a \u00e9t\u00e9 propos\u00e9e par Yurii Nesterov en 1983, la modification est b\u00e9nigne mais est pourtant presque toujours plus rapide \u00e0 converger que le momentum classique.</p> <p>L'id\u00e9e est qu'au lieu de mesurer le gradient \\(\\nabla  \\mathcal{L}_{\\vartheta}\\) \u00e0 la position actuelle des poids \\(\\vartheta\\), mais un peu en avance, dans la direction du moment, \u00e0 \\(\\vartheta + \\alpha \\Delta w_{i,j}^{\\ell}(t)\\).</p> \\[     \\Delta w_{i,j}^{\\ell}(t) := \\alpha \\Delta w_{i,j}^{\\ell}(t-1) - \\eta \\frac{\\partial \\mathcal{L}_{\\vartheta}}{\\partial w_{i,j}^{\\ell}(t)}(\\vartheta + \\alpha \\Delta w_{i,j}^{\\ell}(t-1)) \\] <p>La mise \u00e0 jour des poids est alors d\u00e9finie par la formule suivante.</p> \\[     w_{i,j}^{\\ell}(t+1) := w_{i,j}^{\\ell}(t) + \\Delta w_{i,j}^{\\ell}(t) \\] <p>Cette modification marche parce qu'en g\u00e9n\u00e9ral, le vecteur moment pointe dans la bonne direction, ie vers l'optimum.</p> <p>Dans Tensorflow, activer Nesterov dans la MB-SGD se fait de la fa\u00e7on suivante.</p> <pre><code>    tf.keras.optimizers.SGD(lr = 0.001, momentum=0.9, nesterov=True)\n</code></pre>"},{"location":"deep_learning/module4/Module4/#taux-dapprentissage-adaptatif","title":"Taux d'apprentissage adaptatif","text":"<p>Les deux m\u00e9thodes les plus populaires de Taux d'apprentissage adaptatif sont les suivantes.</p>"},{"location":"deep_learning/module4/Module4/#rmsprop","title":"RMSProp","text":"<p>Dans le cas de RMSProp (Root Mean Squared (Back)Propgation) propos\u00e9 par Geoffrey Hinton en 2012, le taux d'apprentissage adaptatif est mod\u00e9lis\u00e9 en gardant en m\u00e9moire le carr\u00e9 des gradients pr\u00e9c\u00e9dents. Pour chaque poids, on a la moyenne mouvante exponentielle suivante :</p> \\[     \\begin{align}     \\mathrm{MS}(w_{i,j}^{\\ell})(0) &amp; = 1 \\\\     \\mathrm{MS}(w_{i,j}^{\\ell})(t) &amp;:= \\beta \\mathrm{MS}(w_{i,j}^{\\ell})(t-1) - (1-\\beta) \\left( \\frac{\\partial \\mathcal{L}_{\\vartheta}}{\\partial w_{i,j}^{\\ell}(t)} (\\vartheta) \\right)^{2}     \\end{align} \\] <p>La mise \u00e0 jour des poids est alors donn\u00e9e par la formule suivante.</p> \\[     w_{i,j}^{\\ell}(t+1) := w_{i,j}^{\\ell}(t) - \\frac{\\eta}{\\sqrt{\\mathrm{MS}(w_{i,j}^{\\ell})(t)}+\\varepsilon} \\frac{\\partial \\mathcal{L}_{\\vartheta}}{\\partial w_{i,j}^{\\ell}(t)}(\\vartheta) \\] <p>Le taux d'apprentissage adaptatif pour chaque poids est alors donn\u00e9 par :</p> \\[     \\frac{\\eta}{\\sqrt{\\mathrm{MS}(w_{i,j}^{\\ell})(t)}+\\varepsilon} \\] <p>Le coefficient \\(\\beta\\) est ici un hyperparam\u00e8tre, par d\u00e9faut il est fix\u00e9 \u00e0 0.9 (Valeur propos\u00e9e par Hinton lors de sa pr\u00e9sentation).</p> <pre><code>tf.keras.optimizers.RMSProp(lr = 0.001, rho=0.9)\n</code></pre>"},{"location":"deep_learning/module4/Module4/#adam","title":"Adam","text":"<p>Adam (ADAptative Moment estimation), propos\u00e9 par Kingma, D. P., &amp; Ba, J. L. (2015), combine les id\u00e9es du momentum et de RMSProp.</p> <p>En plus de garder en m\u00e9moire une moyenne exponentielle mouvante du carr\u00e9 des gradients pr\u00e9c\u00e9dents pour chaque poids, comme RMSProp, Adam garde aussi en m\u00e9moire une moyenne exponentielle mouvante des gradients pr\u00e9c\u00e9dents, comme Momentum.</p> <p>Momentum-like terme :</p> \\[     \\begin{align}     m_{i,j}^{\\ell}(0) &amp; = 0 \\\\     m_{i,j}^{\\ell}(t) &amp; := \\beta_{1} m_{i,j}^{\\ell}(t-1) - (1-\\beta_{1}) \\frac{\\partial \\mathcal{L}_{\\vartheta}}{\\partial w_{i,j}^{\\ell}(t)}(\\vartheta)     \\end{align} \\] <p>RMSProp-like terme :</p> \\[     \\begin{align}     r_{i,j}^{\\ell}(0) &amp; = 0 \\\\     r_{i,j}^{\\ell}(t) &amp;:= \\beta_{2} r_{i,j}^{\\ell}(t-1) - (1-\\beta_{2}) \\left( \\frac{\\partial \\mathcal{L}_{\\vartheta}}{\\partial w_{i,j}^{\\ell}(t)}(\\vartheta) \\right)^{2}     \\end{align} \\] <p>Commes les deux initialisation sont \u00e9gales \u00e0 0, les auteurs ont observ\u00e9s que cela pouvait impliqu\u00e9 un biais, pour contrer ce biais, ils normalisent ces valeurs de la fa\u00e7on suivante.</p> \\[     \\begin{align}     \\hat{m}_{i,j}^{\\ell}(t)  &amp; := \\frac{m_{i,j}^{\\ell}(t)}{1- \\beta_{1}^{t}} \\\\     \\hat{r}_{i,j}^{\\ell}(t) &amp;:= \\frac{r_{i,j}^{\\ell}(t)}{1- \\beta_{2}^{t}}     \\end{align} \\] <p>La mise \u00e0 jour des poids est alors donn\u00e9e par la formule suivante.</p> \\[     w_{i,j}^{\\ell}(t+1) := w_{i,j}^{\\ell}(t) - \\eta \\frac{\\hat{m}_{i,j}^{\\ell}(t)}{\\sqrt{\\hat{r}_{i,j}^{\\ell}(t)}+\\varepsilon} \\]"},{"location":"deep_learning/module4/Module4/#comparaison","title":"Comparaison","text":"<p>En 2019 est paru une comparaison de certains des optimiseurs les plus populaires (On Empirical Comparisons of Optimizers for Deep Learning, https://arxiv.org/abs/1910.05446). Les auteurs en ont d\u00e9duit le principe de relations d'inclusion suivante.</p>  <p>D\u00e9finition</p> <p>Etant donn\u00e9es 2 r\u00e8gles de mises \u00e0 jours des poids \\(\\mathcal{M, N}\\) pour une utilisation en tant que m\u00e9thode d'optimisation du premier ordre. On dit que \\(\\mathcal{M}\\) est une sous sp\u00e9cialisation de \\(\\mathcal{N}\\) si, apr\u00e8s un temps d'entra\u00eenement assez long, \\(\\mathcal{N}\\) est capable d'approximer les r\u00e9sultats de \\(\\mathcal{M}\\), quitte \u00e0 modifier les hyperparam\u00e8tres de \\(\\mathcal{N}\\).</p> <p>On note alors \\(\\mathcal{M} \\subseteq \\mathcal{N}\\).</p>  <p>On laisse au lecteur le soin de lire l'article pour obtenir la d\u00e9finition exacte. La chose \u00e0 retenir est la suivante. La m\u00e9thode d'optimisation du premier ordre utilis\u00e9e ici correspond au fait de calculer le gradient de la fonction de perte pour mettre \u00e0 jour les poids, les r\u00e8gles de mises \u00e0 jour elles sont toutes les m\u00e9thodes list\u00e9es plus haut.</p>  <p>Remarque</p> <p>Si 2 r\u00e8gles de mises \u00e0 jours des poids sont en relation d'inclusion, alors la m\u00e9thode la plus g\u00e9n\u00e9rale ne peut jamais \u00eatre la plus mauvaise, peu importe la m\u00e9trique de comparaison utilis\u00e9e, tant que les hyperparam\u00e8tressont suffisemment tun\u00e9s. De plus, cette relation est valable sur :</p> <ul> <li>Le jeu de test,</li> <li>Le jeu de validation,</li> <li>La vitesse d'\u00e9x\u00e9cution.</li> </ul>  <p>On a alors les inclusions suivantes :</p> \\[     \\begin{align}     (1) &amp;\\quad \\mathrm{SGD} \\subseteq \\mathrm{Momentum} \\subseteq \\mathrm{RMSProp} \\\\     (2)&amp; \\quad \\mathrm{SGD} \\subseteq \\mathrm{Momentum} \\subseteq \\mathrm{Adam} \\\\     (3) &amp;\\quad \\mathrm{SGD} \\subseteq \\mathrm{Nesterov}     \\end{align} \\] <p>En d'autres termes, RMSProp ne peut jamais \u00eatre moins performant que Momentum, qui lui m\u00eame ne peut jamais \u00eatre moins performant que SGD.</p>  <p>Attention</p> <p>Cela ne veut absolument pas dire que SGD ne peut jamais \u00eatre meilleur que RMSProp.</p>  <p>Pour la premi\u00e8re liste d'inclusions, cela peut se voir via Tensorflow.</p> <pre><code>tf.keras.optimizers.SGD(lr = 0.001, momentum = 0.0) = tf.keras.optimizers.SGD(lr = 0.001)\n\ntf.keras.optimizers.RMSProp(lr = 0.001, rho=1, momentum = 0.9, epsilon = 0.0) = tf.keras.optimizers.SGD(lr = 0.001, momentum = 0.9)\n</code></pre> <p>De cette article, les auteurs en ressortent alors la bonne pratique suivante.</p> <p>Si l'on peut se permettre plusieurs dizaines de runs diff\u00e9rents pour l'entra\u00eenement, il peut \u00eatre b\u00e9n\u00e9fique de tuner tous les hyperparam\u00e8tres des m\u00e9thodes d'optimisation populaires.</p>"},{"location":"deep_learning/module4/Module4_2/","title":"Pratique","text":"(function() {   function addWidgetsRenderer() {     var requireJsScript = document.createElement('script');     requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js';      var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]');     var jupyterWidgetsScript = document.createElement('script');     var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js';     var widgetState;      // Fallback for older version:     try {       widgetState = mimeElement &amp;&amp; JSON.parse(mimeElement.innerHTML);        if (widgetState &amp;&amp; (widgetState.version_major &lt; 2 || !widgetState.version_major)) {         widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js';       }     } catch(e) {}      jupyterWidgetsScript.src = widgetRendererSrc;      document.body.appendChild(requireJsScript);     document.body.appendChild(jupyterWidgetsScript);   }    document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }());"},{"location":"deep_learning/module4/Module4_2/#tp-module-4-les-methodes-doptimisation-des-reseaux-de-neurones","title":"TP Module 4 : Les m\u00e9thodes d'optimisation des r\u00e9seaux de neurones","text":""},{"location":"deep_learning/module4/Module4_2/#import-libs","title":"Import libs","text":"<pre><code>import tensorflow as tf\nfrom tensorflow import keras\n\nprint(tf.__version__)\nprint(keras.__version__)\n\nimport numpy as np\nimport random\nimport os\nimport datetime\n\n# Load the TensorBoard notebook extension\n%load_ext tensorboard\n\n# freeze de l'al\u00e9atoire, pour avoir des exp\u00e9riences reproductibles.\nRANDOM_SEED = 42\n\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\ntf.random.set_seed(RANDOM_SEED)\n</code></pre>      <pre>\n<code>2.2.0-rc3\n2.3.0-tf\n</code>\n</pre>        <pre><code>!nvidia-smi\n</code></pre>      <pre>\n<code>Wed Apr 22 12:29:55 2020       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   42C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code>\n</pre>        <pre><code>from tensorflow.keras import models\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import MaxPooling2D\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n</code></pre>"},{"location":"deep_learning/module4/Module4_2/#import-dataset","title":"Import dataset","text":"<p>Le but ici sera de voir l'impact des diff\u00e9rentes m\u00e9thodes d'optimisation.</p> <p>Pour se faire on se basera sur le m\u00eame dataset pour toute la suite du TP, avec un entra\u00eenement de \\(20\\) \u00e9poques et une batchsize de \\(64\\).</p>      <pre><code>(X_train,y_train), (X_test,y_test)  = tf.keras.datasets.fashion_mnist.load_data()\n\nprint(X_train.shape, y_train.shape)\n</code></pre>      <pre>\n<code>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n32768/29515 [=================================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26427392/26421880 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n8192/5148 [===============================================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4423680/4422102 [==============================] - 0s 0us/step\n(60000, 28, 28) (60000,)\n</code>\n</pre>        <pre><code>[name for name in dir(keras.datasets) if not name.startswith(\"_\")]\n</code></pre>      <pre>\n<code>['boston_housing',\n 'cifar10',\n 'cifar100',\n 'fashion_mnist',\n 'imdb',\n 'mnist',\n 'reuters']</code>\n</pre>        <pre><code>class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n\nn_rows = 4\nn_cols = 10\nplt.figure(figsize=(n_cols * 2, n_rows * 2))\nfor row in range(n_rows):\n    for col in range(n_cols):\n        index = n_cols * row + col\n        plt.subplot(n_rows, n_cols, index + 1)\n        plt.imshow(X_train[index, : , :, ], cmap=\"binary\", interpolation=\"nearest\")\n        plt.axis('off')\n        plt.title(class_names[y_train[index]], fontsize=12)\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\n</code></pre>              <pre><code>X_train = X_train.reshape(-1, 28, 28, 1).astype('float32')\nX_test = X_test.reshape(-1, 28, 28, 1).astype('float32')\n\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, random_state=RANDOM_SEED)\n\nX_test = X_test/255\nX_train = X_train/255\nX_valid = X_valid/255\n\ny_train_oh = tf.keras.utils.to_categorical(y_train, num_classes=10)\ny_test_oh = tf.keras.utils.to_categorical(y_test, num_classes=10)\ny_valid_oh = tf.keras.utils.to_categorical(y_valid, num_classes=10)\n</code></pre>     <pre><code>comp = pd.DataFrame()\ncomp['run'] = []\ncomp['Perte'] = []\ncomp['Pr\u00e9cision'] = []\n</code></pre>"},{"location":"deep_learning/module4/Module4_2/#cnn-basique","title":"CNN basique","text":"<pre><code>def base_model(optimizer = tf.keras.optimizers.SGD(lr=0.001)):\n  model = keras.models.Sequential([\n                                 Input(shape=(28,28,1)),\n                                 Conv2D(32, (3, 3), activation='relu', padding='same'),\n                                 Conv2D(64, (3, 3), activation='relu', padding='same'),\n                                 MaxPooling2D((2, 2)),\n                                 Conv2D(128, (3, 3), activation='relu', padding='same'),\n                                 Conv2D(128, (3, 3), activation='relu', padding='same'),\n                                 MaxPooling2D((2, 2)),\n                                 Flatten(),\n                                 Dense(256),\n                                 Activation('relu'),\n                                 Dense(128),\n                                 Activation('relu'),\n                                 Dense(10, activation='softmax')\n                                 ])\n\n  model.compile(loss = 'categorical_crossentropy',\n                optimizer=optimizer,\n                metrics=['accuracy'])\n\n  return model\n</code></pre>     <pre><code>model = base_model()\nmodel.summary()\n</code></pre>      <pre>\n<code>Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 28, 28, 32)        320       \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 28, 28, 64)        18496     \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 14, 14, 64)        0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 14, 14, 128)       73856     \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 14, 14, 128)       147584    \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 7, 7, 128)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 6272)              0         \n_________________________________________________________________\ndense (Dense)                (None, 256)               1605888   \n_________________________________________________________________\nactivation (Activation)      (None, 256)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               32896     \n_________________________________________________________________\nactivation_1 (Activation)    (None, 128)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 10)                1290      \n=================================================================\nTotal params: 1,880,330\nTrainable params: 1,880,330\nNon-trainable params: 0\n_________________________________________________________________\n</code>\n</pre>        <pre><code>logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\ntensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n\nhistory = model.fit(X_train, y_train_oh,\n                    epochs = 20,\n                    batch_size=64,\n                    validation_data=(X_valid, y_valid_oh),\n                    callbacks=[tensorboard_callback])\n</code></pre>      <pre>\n<code>Epoch 1/20\n704/704 [==============================] - 12s 17ms/step - loss: 2.2813 - accuracy: 0.2722 - val_loss: 2.2517 - val_accuracy: 0.3401\nEpoch 2/20\n704/704 [==============================] - 12s 17ms/step - loss: 2.0288 - accuracy: 0.4274 - val_loss: 1.3395 - val_accuracy: 0.6017\nEpoch 3/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.9132 - accuracy: 0.6698 - val_loss: 0.8187 - val_accuracy: 0.6958\nEpoch 4/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.7399 - accuracy: 0.7263 - val_loss: 0.8537 - val_accuracy: 0.6747\nEpoch 5/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.6871 - accuracy: 0.7485 - val_loss: 0.9972 - val_accuracy: 0.6795\nEpoch 6/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.6479 - accuracy: 0.7618 - val_loss: 0.7312 - val_accuracy: 0.7510\nEpoch 7/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.6215 - accuracy: 0.7725 - val_loss: 0.6519 - val_accuracy: 0.7645\nEpoch 8/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.5956 - accuracy: 0.7811 - val_loss: 0.8025 - val_accuracy: 0.7280\nEpoch 9/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.5774 - accuracy: 0.7881 - val_loss: 0.8302 - val_accuracy: 0.7277\nEpoch 10/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.5556 - accuracy: 0.7972 - val_loss: 0.6401 - val_accuracy: 0.7703\nEpoch 11/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.5423 - accuracy: 0.8014 - val_loss: 0.6411 - val_accuracy: 0.7772\nEpoch 12/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.5246 - accuracy: 0.8095 - val_loss: 0.5534 - val_accuracy: 0.8013\nEpoch 13/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.5135 - accuracy: 0.8144 - val_loss: 0.6555 - val_accuracy: 0.7576\nEpoch 14/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.5013 - accuracy: 0.8187 - val_loss: 0.6696 - val_accuracy: 0.7626\nEpoch 15/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.4895 - accuracy: 0.8221 - val_loss: 0.4851 - val_accuracy: 0.8303\nEpoch 16/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.4808 - accuracy: 0.8256 - val_loss: 0.7087 - val_accuracy: 0.7554\nEpoch 17/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.4699 - accuracy: 0.8297 - val_loss: 0.4808 - val_accuracy: 0.8287\nEpoch 18/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.4602 - accuracy: 0.8333 - val_loss: 0.5784 - val_accuracy: 0.7922\nEpoch 19/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.4519 - accuracy: 0.8368 - val_loss: 0.6801 - val_accuracy: 0.7607\nEpoch 20/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.4463 - accuracy: 0.8390 - val_loss: 0.4517 - val_accuracy: 0.8411\n</code>\n</pre>        <pre><code>%tensorboard --logdir logs\n</code></pre>     <pre><code>from tensorboard import notebook\nnotebook.list() # View open TensorBoard instances\n</code></pre>      <pre>\n<code>Known TensorBoard instances:\n  - port 6006: logdir logs (started 0:04:21 ago; pid 312)\n</code>\n</pre>        <pre><code># Control TensorBoard display. If no port is provided, \n# the most recently launched TensorBoard is used\nnotebook.display(port=6006, height=1000) \n</code></pre>     <pre><code>from sklearn.metrics import classification_report\n\npredictions = model.predict(X_test, batch_size=64)\n\nprint(classification_report(y_test_oh.argmax(axis=1),\n                            predictions.argmax(axis=1),\n                            target_names=class_names))\n</code></pre>      <pre>\n<code>              precision    recall  f1-score   support\n\n T-shirt/top       0.87      0.65      0.75      1000\n     Trouser       0.98      0.93      0.95      1000\n    Pullover       0.75      0.71      0.73      1000\n       Dress       0.73      0.92      0.81      1000\n        Coat       0.79      0.60      0.68      1000\n      Sandal       0.94      0.96      0.95      1000\n       Shirt       0.53      0.71      0.60      1000\n     Sneaker       0.93      0.92      0.92      1000\n         Bag       0.95      0.95      0.95      1000\n  Ankle boot       0.94      0.95      0.95      1000\n\n    accuracy                           0.83     10000\n   macro avg       0.84      0.83      0.83     10000\nweighted avg       0.84      0.83      0.83     10000\n\n</code>\n</pre>        <pre><code>pd.DataFrame(history.history).plot(figsize=(12,8))\nplt.grid(True)\nplt.gca().set_ylim(0,2)\nplt.show()\n</code></pre>              <pre><code>loss, accuracy = model.evaluate(X_test,\n                                y_test_oh)\n\ncomp = comp.append({'run': 'basic_CNN', 'Perte' : loss, 'Pr\u00e9cision' : accuracy}, ignore_index=True)\nprint(comp)\n</code></pre>      <pre>\n<code>313/313 [==============================] - 1s 3ms/step - loss: 0.4696 - accuracy: 0.8282\n         run     Perte  Pr\u00e9cision\n0  basic_CNN  0.469638     0.8282\n</code>\n</pre>"},{"location":"deep_learning/module4/Module4_2/#regularisation-l_1-l_2","title":"R\u00e9gularisation \\(L_{1}\\), \\(L_{2}\\)","text":"<pre><code>kernel_regularizer=tf.keras.regularizers.l1(0.01)\n</code></pre>     <pre><code>kernel_regularizer=tf.keras.regularizers.l1(0.01)\n</code></pre>"},{"location":"deep_learning/module4/Module4_2/#batchnorm","title":"BatchNorm","text":"<pre><code>model = keras.models.Sequential([\n                                 Input(shape=(28, 28, 1)),\n                                 Conv2D(32, (3, 3), activation='relu', padding='same'),\n                                 Conv2D(64, (3, 3), activation='relu', padding='same',use_bias=False),\n                                 BatchNormalization(),\n                                 MaxPooling2D((2, 2)),\n                                 Conv2D(128, (3, 3), activation='relu', padding='same'),\n                                 Conv2D(128, (3, 3), activation='relu', padding='same',use_bias=False),\n                                 BatchNormalization(),\n                                 MaxPooling2D((2, 2)),\n                                 Flatten(),\n                                 Dense(256),\n                                 Activation('relu'),\n                                 Dense(128),\n                                 Activation('relu'),\n                                 Dense(10, activation='softmax')\n                                 ])\n</code></pre>     <pre><code>model.summary()\n\nmodel.compile(loss = 'categorical_crossentropy',\n              optimizer=tf.keras.optimizers.SGD(lr=0.001),\n              metrics=['accuracy'])\n</code></pre>      <pre>\n<code>Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_4 (Conv2D)            (None, 28, 28, 32)        320       \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 28, 28, 64)        18432     \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 28, 28, 64)        256       \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 14, 14, 64)        0         \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 14, 14, 128)       73856     \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 14, 14, 128)       147456    \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 14, 14, 128)       512       \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 6272)              0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 256)               1605888   \n_________________________________________________________________\nactivation_2 (Activation)    (None, 256)               0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 128)               32896     \n_________________________________________________________________\nactivation_3 (Activation)    (None, 128)               0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                1290      \n=================================================================\nTotal params: 1,880,906\nTrainable params: 1,880,522\nNon-trainable params: 384\n_________________________________________________________________\n</code>\n</pre>        <pre><code>bn1 = model.layers[2]\n[(var.name, var.trainable) for var in bn1.variables]\n</code></pre>      <pre>\n<code>[('batch_normalization_6/gamma:0', True),\n ('batch_normalization_6/beta:0', True),\n ('batch_normalization_6/moving_mean:0', False),\n ('batch_normalization_6/moving_variance:0', False)]</code>\n</pre>        <pre><code>logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\ntensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n\nhistory = model.fit(X_train, y_train_oh,\n                    epochs = 20,\n                    batch_size=64,\n                    validation_data=(X_valid, y_valid_oh),\n                    callbacks=[tensorboard_callback])\n</code></pre>      <pre>\n<code>Epoch 1/20\n704/704 [==============================] - 12s 18ms/step - loss: 0.5792 - accuracy: 0.8023 - val_loss: 0.5035 - val_accuracy: 0.8353\nEpoch 2/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.3677 - accuracy: 0.8712 - val_loss: 0.3615 - val_accuracy: 0.8709\nEpoch 3/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.3160 - accuracy: 0.8882 - val_loss: 0.3245 - val_accuracy: 0.8847\nEpoch 4/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.2830 - accuracy: 0.9002 - val_loss: 0.3664 - val_accuracy: 0.8675\nEpoch 5/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.2602 - accuracy: 0.9089 - val_loss: 0.3474 - val_accuracy: 0.8724\nEpoch 6/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.2390 - accuracy: 0.9157 - val_loss: 0.2883 - val_accuracy: 0.8957\nEpoch 7/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.2242 - accuracy: 0.9219 - val_loss: 0.2869 - val_accuracy: 0.8966\nEpoch 8/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.2103 - accuracy: 0.9268 - val_loss: 0.3699 - val_accuracy: 0.8709\nEpoch 9/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.1958 - accuracy: 0.9315 - val_loss: 0.2748 - val_accuracy: 0.8993\nEpoch 10/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.1852 - accuracy: 0.9350 - val_loss: 0.3592 - val_accuracy: 0.8777\nEpoch 11/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.1737 - accuracy: 0.9402 - val_loss: 0.3234 - val_accuracy: 0.8891\nEpoch 12/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.1633 - accuracy: 0.9440 - val_loss: 0.2610 - val_accuracy: 0.9066\nEpoch 13/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.1535 - accuracy: 0.9479 - val_loss: 0.3963 - val_accuracy: 0.8635\nEpoch 14/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.1447 - accuracy: 0.9512 - val_loss: 0.2610 - val_accuracy: 0.9045\nEpoch 15/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.1357 - accuracy: 0.9552 - val_loss: 0.2761 - val_accuracy: 0.9040\nEpoch 16/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.1272 - accuracy: 0.9590 - val_loss: 0.2643 - val_accuracy: 0.9065\nEpoch 17/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.1200 - accuracy: 0.9614 - val_loss: 0.2600 - val_accuracy: 0.9096\nEpoch 18/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.1128 - accuracy: 0.9644 - val_loss: 0.2628 - val_accuracy: 0.9067\nEpoch 19/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.1055 - accuracy: 0.9674 - val_loss: 0.3656 - val_accuracy: 0.8784\nEpoch 20/20\n704/704 [==============================] - 12s 17ms/step - loss: 0.0993 - accuracy: 0.9695 - val_loss: 0.2529 - val_accuracy: 0.9127\n</code>\n</pre>        <pre><code>pd.DataFrame(history.history).plot(figsize=(12,8))\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()\n</code></pre>              <pre><code>loss, accuracy = model.evaluate(X_test,\n                                y_test_oh)\n\ncomp = comp.append({'run': 'bn_CNN', 'Perte' : loss, 'Pr\u00e9cision' : accuracy}, ignore_index=True)\nprint(comp)\n</code></pre>      <pre>\n<code>313/313 [==============================] - 1s 3ms/step - loss: 0.2919 - accuracy: 0.8975\n         run     Perte  Pr\u00e9cision\n0  basic_CNN  0.469638     0.8282\n1     bn_CNN  0.291927     0.8975\n</code>\n</pre>         <p>La question de savoir si oui ou non la couche de BatchNorm devrait \u00ea\u00eatre plac\u00e9e avant ou apr\u00e8s la fonction d'activation fait toujours d\u00e9bat.</p> <p>Dans l'article d'origine, et les architectures classiques, elle est plac\u00e9e avant.</p>      <pre><code>model = keras.models.Sequential([\n                                 Input(shape=(28, 28, 1)),\n                                 Conv2D(32, (3, 3), activation='relu', padding='same'),\n                                 Conv2D(64, (3, 3), padding='same',use_bias=False),\n                                 BatchNormalization(),\n                                 Activation('relu'),\n                                 MaxPooling2D((2, 2)),\n                                 Conv2D(128, (3, 3), activation='relu', padding='same'),\n                                 Conv2D(128, (3, 3), padding='same',use_bias=False),\n                                 BatchNormalization(),\n                                 Activation('relu'),\n                                 MaxPooling2D((2, 2)),\n                                 Flatten(),\n                                 Dense(256),\n                                 Activation('relu'),\n                                 Dense(128),\n                                 Activation('relu'),\n                                 Dense(10, activation='softmax')\n                                 ])\n</code></pre>     <pre><code>model.summary()\n\nmodel.compile(loss = 'categorical_crossentropy',\n              optimizer=tf.keras.optimizers.SGD(lr=0.001),\n              metrics=['accuracy'])\n</code></pre>      <pre>\n<code>Model: \"sequential_6\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_30 (Conv2D)           (None, 28, 28, 32)        320       \n_________________________________________________________________\nconv2d_31 (Conv2D)           (None, 28, 28, 64)        18432     \n_________________________________________________________________\nbatch_normalization_8 (Batch (None, 28, 28, 64)        256       \n_________________________________________________________________\nactivation_15 (Activation)   (None, 28, 28, 64)        0         \n_________________________________________________________________\nmax_pooling2d_15 (MaxPooling (None, 14, 14, 64)        0         \n_________________________________________________________________\nconv2d_32 (Conv2D)           (None, 14, 14, 128)       73856     \n_________________________________________________________________\nconv2d_33 (Conv2D)           (None, 14, 14, 128)       147456    \n_________________________________________________________________\nbatch_normalization_9 (Batch (None, 14, 14, 128)       512       \n_________________________________________________________________\nactivation_16 (Activation)   (None, 14, 14, 128)       0         \n_________________________________________________________________\nmax_pooling2d_16 (MaxPooling (None, 7, 7, 128)         0         \n_________________________________________________________________\nflatten_6 (Flatten)          (None, 6272)              0         \n_________________________________________________________________\ndense_18 (Dense)             (None, 256)               1605888   \n_________________________________________________________________\nactivation_17 (Activation)   (None, 256)               0         \n_________________________________________________________________\ndense_19 (Dense)             (None, 128)               32896     \n_________________________________________________________________\nactivation_18 (Activation)   (None, 128)               0         \n_________________________________________________________________\ndense_20 (Dense)             (None, 10)                1290      \n=================================================================\nTotal params: 1,880,906\nTrainable params: 1,880,522\nNon-trainable params: 384\n_________________________________________________________________\n</code>\n</pre>        <pre><code>history = model.fit(X_train, y_train_oh,\n                   epochs = 20,\n                   batch_size=64,\n                   validation_data=(X_valid, y_valid_oh))\n</code></pre>      <pre>\n<code>Epoch 1/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.6268 - accuracy: 0.7898 - val_loss: 0.5258 - val_accuracy: 0.8177\nEpoch 2/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.4040 - accuracy: 0.8602 - val_loss: 0.4041 - val_accuracy: 0.8544\nEpoch 3/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.3529 - accuracy: 0.8765 - val_loss: 0.3803 - val_accuracy: 0.8637\nEpoch 4/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.3196 - accuracy: 0.8883 - val_loss: 0.3971 - val_accuracy: 0.8502\nEpoch 5/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2974 - accuracy: 0.8965 - val_loss: 0.4005 - val_accuracy: 0.8489\nEpoch 6/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2782 - accuracy: 0.9032 - val_loss: 0.3119 - val_accuracy: 0.8881\nEpoch 7/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2632 - accuracy: 0.9089 - val_loss: 0.3238 - val_accuracy: 0.8802\nEpoch 8/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2502 - accuracy: 0.9135 - val_loss: 0.3797 - val_accuracy: 0.8636\nEpoch 9/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2380 - accuracy: 0.9186 - val_loss: 0.2796 - val_accuracy: 0.8986\nEpoch 10/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2284 - accuracy: 0.9210 - val_loss: 0.3247 - val_accuracy: 0.8877\nEpoch 11/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2185 - accuracy: 0.9255 - val_loss: 0.3409 - val_accuracy: 0.8800\nEpoch 12/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2090 - accuracy: 0.9276 - val_loss: 0.2644 - val_accuracy: 0.9037\nEpoch 13/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2011 - accuracy: 0.9326 - val_loss: 0.3488 - val_accuracy: 0.8706\nEpoch 14/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.1933 - accuracy: 0.9345 - val_loss: 0.2609 - val_accuracy: 0.9050\nEpoch 15/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.1857 - accuracy: 0.9362 - val_loss: 0.2720 - val_accuracy: 0.9045\nEpoch 16/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.1772 - accuracy: 0.9404 - val_loss: 0.2502 - val_accuracy: 0.9070\nEpoch 17/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.1721 - accuracy: 0.9414 - val_loss: 0.2536 - val_accuracy: 0.9100\nEpoch 18/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.1662 - accuracy: 0.9442 - val_loss: 0.3002 - val_accuracy: 0.8953\nEpoch 19/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.1595 - accuracy: 0.9472 - val_loss: 0.4275 - val_accuracy: 0.8586\nEpoch 20/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.1539 - accuracy: 0.9488 - val_loss: 0.2485 - val_accuracy: 0.9110\n</code>\n</pre>        <pre><code>loss, accuracy = model.evaluate(X_test,\n                                y_test_oh)\n\ncomp = comp.append({'run': 'bn2_CNN', 'Perte' : loss, 'Pr\u00e9cision' : accuracy}, ignore_index=True)\nprint(comp)\n</code></pre>      <pre>\n<code>313/313 [==============================] - 1s 3ms/step - loss: 0.2717 - accuracy: 0.9046\n         run     Perte  Pr\u00e9cision\n0  basic_CNN  0.469638     0.8282\n1     bn_CNN  0.291927     0.8975\n2    bn2_CNN  0.271721     0.9046\n</code>\n</pre>         <p>De fa\u00e7on classique, l'architecture moderne en ce qui concerne les couches convolutives est la suivante :</p> <pre><code>Conv2D(filtre, (kernel_size, kernel_size), padding='same',use_bias=False),\nBatchNormalization(),\nActivation('relu'),\nMaxPooling2D((kernel_size, kernel_size)),\n</code></pre>"},{"location":"deep_learning/module4/Module4_2/#dropout","title":"Dropout","text":"<pre><code>model = keras.models.Sequential([\n                                 Input(shape=(28, 28, 1)),\n                                 Conv2D(32, (3, 3), activation='relu', padding='same'),\n                                 Conv2D(64, (3, 3), activation='relu', padding='same'),\n                                 MaxPooling2D((2, 2)),\n                                 Conv2D(128, (3, 3), activation='relu', padding='same'),\n                                 Conv2D(128, (3, 3), activation='relu', padding='same'),\n                                 MaxPooling2D((2, 2)),\n                                 Flatten(),\n                                 Dense(256),\n                                 Dropout(0.25),\n                                 Activation('relu'),\n                                 Dense(128),\n                                 Dropout(0.25),\n                                 Activation('relu'),\n                                 Dense(10),\n                                 Activation('softmax')\n                                 ])\n</code></pre>     <pre><code>model.summary()\n\nmodel.compile(loss = tf.keras.losses.CategoricalCrossentropy(),\n              optimizer=tf.keras.optimizers.SGD(lr=0.001),\n              metrics=['accuracy'])\n</code></pre>      <pre>\n<code>Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_4 (Conv2D)            (None, 28, 28, 32)        320       \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 28, 28, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 14, 14, 64)        0         \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 14, 14, 128)       73856     \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 14, 14, 128)       147584    \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 6272)              0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 256)               1605888   \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 256)               0         \n_________________________________________________________________\nactivation_2 (Activation)    (None, 256)               0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 128)               32896     \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 128)               0         \n_________________________________________________________________\nactivation_3 (Activation)    (None, 128)               0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                1290      \n_________________________________________________________________\nactivation_4 (Activation)    (None, 10)                0         \n=================================================================\nTotal params: 1,880,330\nTrainable params: 1,880,330\nNon-trainable params: 0\n_________________________________________________________________\n</code>\n</pre>        <pre><code>history = model.fit(X_train, y_train_oh,\n                   epochs = 20,\n                   batch_size=64,\n                   validation_data=(X_valid, y_valid_oh))\n</code></pre>      <pre>\n<code>Epoch 1/20\n704/704 [==============================] - 8s 12ms/step - loss: 2.2616 - accuracy: 0.2487 - val_loss: 2.1819 - val_accuracy: 0.4624\nEpoch 2/20\n704/704 [==============================] - 8s 12ms/step - loss: 1.7692 - accuracy: 0.4208 - val_loss: 1.0943 - val_accuracy: 0.6203\nEpoch 3/20\n704/704 [==============================] - 8s 11ms/step - loss: 1.1026 - accuracy: 0.5836 - val_loss: 0.8671 - val_accuracy: 0.6748\nEpoch 4/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.9212 - accuracy: 0.6552 - val_loss: 0.7761 - val_accuracy: 0.7034\nEpoch 5/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.8448 - accuracy: 0.6886 - val_loss: 0.9521 - val_accuracy: 0.6486\nEpoch 6/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.7920 - accuracy: 0.7104 - val_loss: 0.6777 - val_accuracy: 0.7539\nEpoch 7/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.7587 - accuracy: 0.7256 - val_loss: 0.6720 - val_accuracy: 0.7416\nEpoch 8/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.7186 - accuracy: 0.7380 - val_loss: 0.7903 - val_accuracy: 0.7137\nEpoch 9/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.6936 - accuracy: 0.7513 - val_loss: 0.6363 - val_accuracy: 0.7635\nEpoch 10/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.6714 - accuracy: 0.7574 - val_loss: 0.5762 - val_accuracy: 0.7931\nEpoch 11/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.6538 - accuracy: 0.7611 - val_loss: 0.5720 - val_accuracy: 0.7921\nEpoch 12/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.6329 - accuracy: 0.7705 - val_loss: 0.5508 - val_accuracy: 0.8018\nEpoch 13/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.6165 - accuracy: 0.7768 - val_loss: 0.5731 - val_accuracy: 0.7837\nEpoch 14/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.6003 - accuracy: 0.7841 - val_loss: 0.5497 - val_accuracy: 0.7943\nEpoch 15/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.5880 - accuracy: 0.7877 - val_loss: 0.5172 - val_accuracy: 0.8163\nEpoch 16/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.5769 - accuracy: 0.7917 - val_loss: 0.5066 - val_accuracy: 0.8160\nEpoch 17/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.5662 - accuracy: 0.7942 - val_loss: 0.4985 - val_accuracy: 0.8170\nEpoch 18/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.5553 - accuracy: 0.7999 - val_loss: 0.4834 - val_accuracy: 0.8309\nEpoch 19/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.5469 - accuracy: 0.8033 - val_loss: 0.5423 - val_accuracy: 0.7934\nEpoch 20/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.5345 - accuracy: 0.8060 - val_loss: 0.5119 - val_accuracy: 0.8133\n</code>\n</pre>        <pre><code>pd.DataFrame(history.history).plot(figsize=(12,8))\nplt.grid(True)\nplt.gca().set_ylim(0,2)\nplt.show()\n</code></pre>              <pre><code>loss, accuracy = model.evaluate(X_test,\n                                y_test_oh)\n\ncomp = comp.append({'run': 'drop_CNN', 'Perte' : loss, 'Pr\u00e9cision' : accuracy}, ignore_index=True)\nprint(comp)\n</code></pre>      <pre>\n<code>313/313 [==============================] - 1s 4ms/step - loss: 0.5324 - accuracy: 0.7998\n</code>\n</pre>         <p>Remarque : il n'est pas toujours n\u00e9c\u00e9ssaire ou m\u00ea\u00eame utile de laisser la derni\u00e8re fonction d'activation  <pre><code>Activation('softmax')\n</code></pre> dans le mod\u00e8le.</p> <p>Dans certains cas, il est plus utilse de s'arr\u00ea\u00eater \u00e0 la derni\u00e8re couche dense <pre><code>Dense(num_classes)\n</code></pre> Dans ce cas l\u00e0, la fonction de perte doit \u00eatre modifi\u00e9e en <pre><code>loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True).\n</code></pre></p>"},{"location":"deep_learning/module4/Module4_2/#monte-carlo-dropout","title":"Monte Carlo Dropout","text":"<p>Si l'on se souvient de la partie th\u00e9orique, le dropout est d\u00e9sactiv\u00e9 lorsque l'on fait de l'inf\u00e9rence. Toutefois, il est possible de le r\u00e9activer pour avoir une simulation de Monte Carlo pour la pr\u00e9diction, en moyennant plusieurs pr\u00e9dictions pour la m\u00eame observation.</p>      <pre><code># On fait 100 pr\u00e9dictions sur le dataset de test\ny_probas = np.stack([model(X_test, training=True)\n                     for sample in range(100)])\n# on prend alors la moyenne suivant la premi\u00e8re dimension\ny_proba = y_probas.mean(axis=0)\n</code></pre>      <p>Avant de d\u00e9velopper le code pr\u00e9cedent, voyons une pr\u00e9diction classique sur la premi\u00e8re observation de <code>X_test</code>.</p>      <pre><code>np.round(model.predict(X_test[:1]), 2)\n</code></pre>      <pre>\n<code>array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.26, 0.  , 0.7 ]],\n      dtype=float32)</code>\n</pre>         <p>Le mod\u00e8le est semble certain que cette image appartient \u00e0 la derni\u00e8re classe \"ankle boot\". Mais si l'on r\u00e9active le dropout durant l'inf\u00e9rence, il y a des pr\u00e9dictions, o\u00f9 il n'en est plus aussi certain. Pour la \\(1\\)-\u00e8re pr\u00e9diction, le score pour la \\(9\\)-i\u00e8me classe n'est plus le m\u00eame.</p>      <pre><code>np.round(y_probas[:, :1], 2)\n</code></pre>      <pre>\n<code>array([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.29, 0.  , 0.67]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.12, 0.  , 0.86]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.32, 0.  , 0.42, 0.  , 0.26]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.32, 0.  , 0.56]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.24, 0.  , 0.74]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.18, 0.  , 0.79]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.39, 0.01, 0.49]],\n\n       [[0.01, 0.  , 0.  , 0.  , 0.  , 0.28, 0.  , 0.39, 0.01, 0.29]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.13, 0.  , 0.8 ]],\n\n       [[0.  , 0.  , 0.03, 0.  , 0.  , 0.09, 0.01, 0.45, 0.01, 0.4 ]],\n\n       [[0.01, 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.32, 0.  , 0.52]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.09, 0.  , 0.88]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.29, 0.  , 0.68]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.18, 0.  , 0.82]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.01, 0.5 , 0.  , 0.46]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.18, 0.  , 0.22, 0.  , 0.59]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.52, 0.  , 0.43]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.59, 0.  , 0.38]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.62, 0.  , 0.34]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.58, 0.02, 0.27]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.61, 0.  , 0.35]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.93]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.23, 0.  , 0.27, 0.  , 0.49]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.33, 0.  , 0.62]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.7 , 0.  , 0.28]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.19, 0.  , 0.78]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.43, 0.  , 0.54]],\n\n       [[0.01, 0.  , 0.02, 0.02, 0.  , 0.06, 0.  , 0.6 , 0.  , 0.28]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.02, 0.  , 0.94]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.32, 0.  , 0.6 ]],\n\n       [[0.  , 0.  , 0.14, 0.  , 0.  , 0.06, 0.01, 0.36, 0.06, 0.37]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.16, 0.  , 0.78]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.23, 0.  , 0.76]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.24, 0.  , 0.74]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.25, 0.  , 0.71]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.07, 0.  , 0.92]],\n\n       [[0.  , 0.  , 0.01, 0.  , 0.  , 0.06, 0.  , 0.08, 0.01, 0.83]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.06, 0.01, 0.9 ]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.35, 0.01, 0.55]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.1 , 0.01, 0.82]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.27, 0.  , 0.71]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.59, 0.  , 0.33]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.07, 0.  , 0.85]],\n\n       [[0.01, 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.06, 0.01, 0.85]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.1 , 0.  , 0.89]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.4 , 0.01, 0.53]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.35, 0.  , 0.59]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.1 , 0.  , 0.87]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.32, 0.  , 0.63]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.14, 0.  , 0.82]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.14, 0.  , 0.82]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.28, 0.  , 0.7 ]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.36, 0.  , 0.55]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.52, 0.  , 0.41]],\n\n       [[0.  , 0.  , 0.01, 0.  , 0.  , 0.02, 0.  , 0.06, 0.01, 0.9 ]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.46, 0.  , 0.53]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.12, 0.  , 0.82]],\n\n       [[0.  , 0.  , 0.01, 0.  , 0.  , 0.03, 0.  , 0.52, 0.01, 0.44]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.04, 0.  , 0.87]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.35, 0.  , 0.63]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.62, 0.  , 0.35]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.12, 0.  , 0.78]],\n\n       [[0.  , 0.  , 0.02, 0.01, 0.  , 0.03, 0.01, 0.28, 0.02, 0.63]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.19, 0.  , 0.72]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.44, 0.  , 0.5 ]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.52, 0.  , 0.36]],\n\n       [[0.01, 0.  , 0.01, 0.  , 0.  , 0.13, 0.  , 0.02, 0.  , 0.84]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.35, 0.01, 0.52]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.18, 0.  , 0.74]],\n\n       [[0.  , 0.  , 0.01, 0.  , 0.  , 0.01, 0.01, 0.37, 0.03, 0.57]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.27, 0.01, 0.58]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.13, 0.  , 0.82]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.41, 0.  , 0.56]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.2 , 0.  , 0.79]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.92]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.52, 0.  , 0.45]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.28, 0.  , 0.6 ]],\n\n       [[0.  , 0.  , 0.01, 0.  , 0.  , 0.16, 0.  , 0.13, 0.  , 0.71]],\n\n       [[0.  , 0.  , 0.02, 0.  , 0.  , 0.02, 0.  , 0.15, 0.01, 0.79]],\n\n       [[0.  , 0.  , 0.01, 0.  , 0.  , 0.04, 0.  , 0.19, 0.01, 0.75]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.12, 0.  , 0.87]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.66, 0.  , 0.33]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.42, 0.  , 0.51]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.35, 0.  , 0.2 , 0.  , 0.44]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.08, 0.01, 0.87]],\n\n       [[0.  , 0.  , 0.  , 0.01, 0.  , 0.12, 0.  , 0.24, 0.  , 0.61]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.23, 0.  , 0.76]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.3 , 0.  , 0.68]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.39, 0.  , 0.6 ]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.11, 0.  , 0.87]],\n\n       [[0.  , 0.01, 0.  , 0.  , 0.  , 0.03, 0.  , 0.61, 0.  , 0.35]],\n\n       [[0.  , 0.  , 0.01, 0.  , 0.  , 0.01, 0.  , 0.65, 0.  , 0.32]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.46, 0.  , 0.51]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.23, 0.  , 0.75]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.15, 0.  , 0.17, 0.  , 0.67]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.22, 0.  , 0.76]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.51, 0.07, 0.36]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.17, 0.  , 0.81]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.96]],\n\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.46, 0.03, 0.51]]],\n      dtype=float32)</code>\n</pre>         <p>Si l'on fait la moyenne sur la premi\u00e8re dimension, on voit que le mod\u00e8le n'est plus aussi certain que l'image appartienne \u00e0 la classe \\(9\\).</p>      <pre><code>np.round(y_proba[:1], 2)\n</code></pre>      <pre>\n<code>array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.29, 0.  , 0.64]],\n      dtype=float32)</code>\n</pre>         <p>D'ailleurs, on voit que l'\u00e9cart type pour certaine classe semble assez important. Dans le cas o\u00f9 l'on d\u00e9veloppe des mod\u00e8les pour des syst\u00e8mes critiques (finance, sant\u00e9), c'est important de prendre ces statistiques. </p>      <pre><code>y_std = y_probas.std(axis=0)\nnp.round(y_std[:1], 2)\n</code></pre>      <pre>\n<code>array([[0.  , 0.  , 0.01, 0.  , 0.  , 0.06, 0.  , 0.18, 0.01, 0.19]],\n      dtype=float32)</code>\n</pre>        <pre><code>y_pred = np.argmax(y_proba, axis=1)\ny_pred\n</code></pre>      <pre>\n<code>array([9, 2, 1, ..., 8, 1, 5])</code>\n</pre>         <p>On voit d'ailleurs que en faisant cela, on gagne l\u00e9g\u00e8rement en pr\u00e9cision (j'etais \u00e0 <code>accuracy: 0.7998</code>, lors de ce run).</p>      <pre><code>accuracy = np.sum(y_pred == y_test) / len(y_test)\naccuracy\n</code></pre>      <pre>\n<code>0.8031</code>\n</pre>        <pre><code>class MCDropout(keras.layers.Dropout):\n    def call(self, inputs):\n        return super().call(inputs, training=True)\n</code></pre>"},{"location":"deep_learning/module4/Module4_2/#initilisation-de-xavier-et-de-he","title":"Initilisation de Xavier et de He","text":"<p>Par d\u00e9fault, Keras utilise <code>glorot_uniform</code>.</p>      <pre><code>model = keras.models.Sequential([\n                                 Input(shape=(28, 28, 1)),\n                                 Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\"),\n                                 Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\"),\n                                 MaxPooling2D((2, 2)),\n                                 Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\"),\n                                 Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\"),\n                                 MaxPooling2D((2, 2)),\n                                 Flatten(),\n                                 Dense(256, kernel_initializer=\"he_normal\"),\n                                 Activation('relu'),\n                                 Dense(128, kernel_initializer=\"he_normal\"),\n                                 Activation('relu'),\n                                 Dense(10, activation='softmax')\n                                 ])\n\nmodel.summary()\n\nmodel.compile(loss = 'categorical_crossentropy',\n              optimizer=tf.keras.optimizers.SGD(lr=0.001),\n              metrics=['accuracy'])\n</code></pre>      <pre>\n<code>Model: \"sequential_10\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_46 (Conv2D)           (None, 28, 28, 32)        320       \n_________________________________________________________________\nconv2d_47 (Conv2D)           (None, 28, 28, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_23 (MaxPooling (None, 14, 14, 64)        0         \n_________________________________________________________________\nconv2d_48 (Conv2D)           (None, 14, 14, 128)       73856     \n_________________________________________________________________\nconv2d_49 (Conv2D)           (None, 14, 14, 128)       147584    \n_________________________________________________________________\nmax_pooling2d_24 (MaxPooling (None, 7, 7, 128)         0         \n_________________________________________________________________\nflatten_10 (Flatten)         (None, 6272)              0         \n_________________________________________________________________\ndense_30 (Dense)             (None, 256)               1605888   \n_________________________________________________________________\nactivation_25 (Activation)   (None, 256)               0         \n_________________________________________________________________\ndense_31 (Dense)             (None, 128)               32896     \n_________________________________________________________________\nactivation_26 (Activation)   (None, 128)               0         \n_________________________________________________________________\ndense_32 (Dense)             (None, 10)                1290      \n=================================================================\nTotal params: 1,880,330\nTrainable params: 1,880,330\nNon-trainable params: 0\n_________________________________________________________________\n</code>\n</pre>        <pre><code>history = model.fit(X_train, y_train_oh,\n                   epochs = 20,\n                   batch_size=64,\n                   validation_data=(X_valid, y_valid_oh))\n</code></pre>      <pre>\n<code>Epoch 1/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.9376 - accuracy: 0.7017 - val_loss: 0.8291 - val_accuracy: 0.7122\nEpoch 2/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.5761 - accuracy: 0.7957 - val_loss: 0.6167 - val_accuracy: 0.7787\nEpoch 3/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.5070 - accuracy: 0.8194 - val_loss: 0.5451 - val_accuracy: 0.8119\nEpoch 4/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.4684 - accuracy: 0.8348 - val_loss: 0.5113 - val_accuracy: 0.8159\nEpoch 5/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.4397 - accuracy: 0.8440 - val_loss: 0.7333 - val_accuracy: 0.7603\nEpoch 6/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.4164 - accuracy: 0.8517 - val_loss: 0.4440 - val_accuracy: 0.8388\nEpoch 7/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.3999 - accuracy: 0.8582 - val_loss: 0.4860 - val_accuracy: 0.8289\nEpoch 8/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.3859 - accuracy: 0.8648 - val_loss: 0.5647 - val_accuracy: 0.8170\nEpoch 9/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.3722 - accuracy: 0.8696 - val_loss: 0.3680 - val_accuracy: 0.8666\nEpoch 10/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.3602 - accuracy: 0.8736 - val_loss: 0.3614 - val_accuracy: 0.8700\nEpoch 11/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.3507 - accuracy: 0.8770 - val_loss: 0.4713 - val_accuracy: 0.8329\nEpoch 12/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.3408 - accuracy: 0.8793 - val_loss: 0.3694 - val_accuracy: 0.8679\nEpoch 13/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.3336 - accuracy: 0.8826 - val_loss: 0.4376 - val_accuracy: 0.8385\nEpoch 14/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.3274 - accuracy: 0.8843 - val_loss: 0.4386 - val_accuracy: 0.8431\nEpoch 15/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.3193 - accuracy: 0.8873 - val_loss: 0.3434 - val_accuracy: 0.8777\nEpoch 16/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.3133 - accuracy: 0.8890 - val_loss: 0.3469 - val_accuracy: 0.8743\nEpoch 17/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.3082 - accuracy: 0.8922 - val_loss: 0.3196 - val_accuracy: 0.8871\nEpoch 18/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.3027 - accuracy: 0.8933 - val_loss: 0.4351 - val_accuracy: 0.8531\nEpoch 19/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2970 - accuracy: 0.8960 - val_loss: 0.5215 - val_accuracy: 0.8220\nEpoch 20/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2927 - accuracy: 0.8971 - val_loss: 0.3065 - val_accuracy: 0.8895\n</code>\n</pre>        <pre><code>pd.DataFrame(history.history).plot(figsize=(12,8))\nplt.grid(True)\nplt.gca().set_ylim(0,2)\nplt.show()\n</code></pre>              <pre><code>loss, accuracy = model.evaluate(X_test,\n                                y_test_oh)\n\ncomp = comp.append({'run': 'he_CNN', 'Perte' : loss, 'Pr\u00e9cision' : accuracy}, ignore_index=True)\nprint(comp)\n</code></pre>      <pre>\n<code>313/313 [==============================] - 1s 4ms/step - loss: 0.3282 - accuracy: 0.8817\n         run     Perte  Pr\u00e9cision\n0  basic_CNN  0.469638     0.8282\n1     bn_CNN  0.291927     0.8975\n2    bn2_CNN  0.271721     0.9046\n3   drop_CNN  0.497585     0.8187\n4     he_CNN  0.328177     0.8817\n</code>\n</pre>"},{"location":"deep_learning/module4/Module4_2/#optimisation-de-la-sgd","title":"Optimisation de la SGD","text":"<pre><code>[name for name in dir(keras.optimizers) if not name.startswith(\"_\")]\n</code></pre>      <pre>\n<code>['Adadelta',\n 'Adagrad',\n 'Adam',\n 'Adamax',\n 'Ftrl',\n 'Nadam',\n 'Optimizer',\n 'RMSprop',\n 'SGD',\n 'deserialize',\n 'get',\n 'schedules',\n 'serialize']</code>\n</pre>"},{"location":"deep_learning/module4/Module4_2/#lrd","title":"LRD","text":"<pre><code>def exponential_decay(lr0, s):\n  def exponential_decay_fn(epoch):\n    return lr0 * 0.1**(epoch / s)\n  return exponential_decay_fn\n\nexponential_decay_fn = exponential_decay(lr0=0.001,s=20)\n</code></pre>     <pre><code>rng = [i for i in range(100)]\ny = [exponential_decay_fn(x) for x in rng]\nplt.plot(rng, [exponential_decay_fn(x) for x in rng])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Learning Rate\")\nprint(y[0], y[-1])\n</code></pre>     <pre><code>lr_callback = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn, verbose=True)\n</code></pre>"},{"location":"deep_learning/module4/Module4_2/#momentum","title":"Momentum","text":"<pre><code>optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)\n\nmodel = base_model(optimizer = optimizer)\n\nhistory = model.fit(X_train, y_train_oh,\n                   epochs = 20,\n                   batch_size=64,\n                   validation_data=(X_valid, y_valid_oh))\n</code></pre>      <pre>\n<code>Epoch 1/20\n704/704 [==============================] - 8s 11ms/step - loss: 1.0444 - accuracy: 0.6311 - val_loss: 0.7022 - val_accuracy: 0.7455\nEpoch 2/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.5792 - accuracy: 0.7872 - val_loss: 0.5715 - val_accuracy: 0.7930\nEpoch 3/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.4858 - accuracy: 0.8243 - val_loss: 0.4480 - val_accuracy: 0.8421\nEpoch 4/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.4330 - accuracy: 0.8420 - val_loss: 0.4316 - val_accuracy: 0.8421\nEpoch 5/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.4002 - accuracy: 0.8547 - val_loss: 0.3932 - val_accuracy: 0.8547\nEpoch 6/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.3704 - accuracy: 0.8641 - val_loss: 0.3739 - val_accuracy: 0.8673\nEpoch 7/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.3496 - accuracy: 0.8721 - val_loss: 0.3562 - val_accuracy: 0.8701\nEpoch 8/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.3295 - accuracy: 0.8794 - val_loss: 0.3587 - val_accuracy: 0.8668\nEpoch 9/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.3139 - accuracy: 0.8833 - val_loss: 0.3395 - val_accuracy: 0.8777\nEpoch 10/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2992 - accuracy: 0.8905 - val_loss: 0.3219 - val_accuracy: 0.8833\nEpoch 11/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2871 - accuracy: 0.8945 - val_loss: 0.3207 - val_accuracy: 0.8809\nEpoch 12/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2777 - accuracy: 0.8969 - val_loss: 0.2976 - val_accuracy: 0.8944\nEpoch 13/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2629 - accuracy: 0.9030 - val_loss: 0.2999 - val_accuracy: 0.8936\nEpoch 14/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2529 - accuracy: 0.9064 - val_loss: 0.3000 - val_accuracy: 0.8899\nEpoch 15/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2465 - accuracy: 0.9086 - val_loss: 0.2856 - val_accuracy: 0.8971\nEpoch 16/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2376 - accuracy: 0.9117 - val_loss: 0.2839 - val_accuracy: 0.8968\nEpoch 17/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2288 - accuracy: 0.9150 - val_loss: 0.2913 - val_accuracy: 0.8975\nEpoch 18/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2223 - accuracy: 0.9172 - val_loss: 0.2755 - val_accuracy: 0.9008\nEpoch 19/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2154 - accuracy: 0.9207 - val_loss: 0.2833 - val_accuracy: 0.8977\nEpoch 20/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2071 - accuracy: 0.9232 - val_loss: 0.2737 - val_accuracy: 0.9009\n</code>\n</pre>        <pre><code>loss, accuracy = model.evaluate(X_test,\n                                y_test_oh)\n\ncomp = comp.append({'run': 'momentum_CNN', 'Perte' : loss, 'Pr\u00e9cision' : accuracy}, ignore_index=True)\nprint(comp)\n</code></pre>      <pre>\n<code>313/313 [==============================] - 1s 4ms/step - loss: 0.2860 - accuracy: 0.8988\n            run     Perte  Pr\u00e9cision\n0     basic_CNN  0.469638     0.8282\n1        bn_CNN  0.291927     0.8975\n2       bn2_CNN  0.271721     0.9046\n3      drop_CNN  0.497585     0.8187\n4        he_CNN  0.328177     0.8817\n5  momentum_CNN  0.286005     0.8988\n</code>\n</pre>"},{"location":"deep_learning/module4/Module4_2/#nesterov","title":"Nesterov","text":"<pre><code>optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\n\nmodel = base_model(optimizer = optimizer)\n\nhistory = model.fit(X_train, y_train_oh,\n                   epochs = 20,\n                   batch_size=64,\n                   validation_data=(X_valid, y_valid_oh))\n</code></pre>      <pre>\n<code>Epoch 1/20\n704/704 [==============================] - 8s 11ms/step - loss: 1.1436 - accuracy: 0.6146 - val_loss: 0.8565 - val_accuracy: 0.6578\nEpoch 2/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.5587 - accuracy: 0.7964 - val_loss: 0.6265 - val_accuracy: 0.7754\nEpoch 3/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.4691 - accuracy: 0.8315 - val_loss: 0.4570 - val_accuracy: 0.8379\nEpoch 4/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.4151 - accuracy: 0.8498 - val_loss: 0.4584 - val_accuracy: 0.8299\nEpoch 5/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.3811 - accuracy: 0.8628 - val_loss: 0.4670 - val_accuracy: 0.8242\nEpoch 6/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.3538 - accuracy: 0.8719 - val_loss: 0.3482 - val_accuracy: 0.8751\nEpoch 7/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.3340 - accuracy: 0.8784 - val_loss: 0.3706 - val_accuracy: 0.8655\nEpoch 8/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.3146 - accuracy: 0.8867 - val_loss: 0.4191 - val_accuracy: 0.8507\nEpoch 9/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2984 - accuracy: 0.8911 - val_loss: 0.3340 - val_accuracy: 0.8815\nEpoch 10/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2862 - accuracy: 0.8955 - val_loss: 0.3138 - val_accuracy: 0.8869\nEpoch 11/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2735 - accuracy: 0.9001 - val_loss: 0.3191 - val_accuracy: 0.8863\nEpoch 12/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2645 - accuracy: 0.9021 - val_loss: 0.2970 - val_accuracy: 0.8955\nEpoch 13/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2520 - accuracy: 0.9074 - val_loss: 0.2954 - val_accuracy: 0.8971\nEpoch 14/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2436 - accuracy: 0.9106 - val_loss: 0.2855 - val_accuracy: 0.8978\nEpoch 15/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2349 - accuracy: 0.9136 - val_loss: 0.2758 - val_accuracy: 0.8995\nEpoch 16/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2244 - accuracy: 0.9176 - val_loss: 0.2763 - val_accuracy: 0.9023\nEpoch 17/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2181 - accuracy: 0.9193 - val_loss: 0.2817 - val_accuracy: 0.9017\nEpoch 18/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2117 - accuracy: 0.9224 - val_loss: 0.2909 - val_accuracy: 0.8964\nEpoch 19/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2040 - accuracy: 0.9258 - val_loss: 0.2865 - val_accuracy: 0.8957\nEpoch 20/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.1961 - accuracy: 0.9279 - val_loss: 0.2796 - val_accuracy: 0.8991\n</code>\n</pre>        <pre><code>loss, accuracy = model.evaluate(X_test,\n                                y_test_oh)\n\ncomp = comp.append({'run': 'nesterov_CNN', 'Perte' : loss, 'Pr\u00e9cision' : accuracy}, ignore_index=True)\nprint(comp)\n</code></pre>      <pre>\n<code>313/313 [==============================] - 1s 4ms/step - loss: 0.2970 - accuracy: 0.8942\n            run     Perte  Pr\u00e9cision\n0     basic_CNN  0.469638     0.8282\n1        bn_CNN  0.291927     0.8975\n2       bn2_CNN  0.271721     0.9046\n3      drop_CNN  0.497585     0.8187\n4        he_CNN  0.328177     0.8817\n5  momentum_CNN  0.286005     0.8988\n6  nesterov_CNN  0.296967     0.8942\n</code>\n</pre>"},{"location":"deep_learning/module4/Module4_2/#rmsprop","title":"RMSProp","text":"<pre><code>optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)\n\nmodel = base_model(optimizer = optimizer)\n\nhistory = model.fit(X_train, y_train_oh,\n                   epochs = 20,\n                   batch_size=64,\n                   validation_data=(X_valid, y_valid_oh))\n</code></pre>      <pre>\n<code>Epoch 1/20\n704/704 [==============================] - 8s 12ms/step - loss: 0.4338 - accuracy: 0.8420 - val_loss: 0.4256 - val_accuracy: 0.8446\nEpoch 2/20\n704/704 [==============================] - 8s 12ms/step - loss: 0.2444 - accuracy: 0.9113 - val_loss: 0.2724 - val_accuracy: 0.9021\nEpoch 3/20\n704/704 [==============================] - 8s 12ms/step - loss: 0.1981 - accuracy: 0.9287 - val_loss: 0.2215 - val_accuracy: 0.9189\nEpoch 4/20\n704/704 [==============================] - 8s 12ms/step - loss: 0.1690 - accuracy: 0.9393 - val_loss: 0.2297 - val_accuracy: 0.9157\nEpoch 5/20\n704/704 [==============================] - 8s 12ms/step - loss: 0.1486 - accuracy: 0.9460 - val_loss: 0.2638 - val_accuracy: 0.9139\nEpoch 6/20\n704/704 [==============================] - 9s 12ms/step - loss: 0.1311 - accuracy: 0.9536 - val_loss: 0.2867 - val_accuracy: 0.9127\nEpoch 7/20\n704/704 [==============================] - 8s 12ms/step - loss: 0.1175 - accuracy: 0.9585 - val_loss: 0.2541 - val_accuracy: 0.9213\nEpoch 8/20\n704/704 [==============================] - 8s 12ms/step - loss: 0.1087 - accuracy: 0.9622 - val_loss: 0.4688 - val_accuracy: 0.9194\nEpoch 9/20\n704/704 [==============================] - 8s 12ms/step - loss: 0.1003 - accuracy: 0.9652 - val_loss: 0.3328 - val_accuracy: 0.9269\nEpoch 10/20\n704/704 [==============================] - 8s 12ms/step - loss: 0.0962 - accuracy: 0.9668 - val_loss: 0.4793 - val_accuracy: 0.9161\nEpoch 11/20\n704/704 [==============================] - 8s 12ms/step - loss: 0.0906 - accuracy: 0.9695 - val_loss: 0.3913 - val_accuracy: 0.9321\nEpoch 12/20\n704/704 [==============================] - 8s 12ms/step - loss: 0.0894 - accuracy: 0.9706 - val_loss: 0.3182 - val_accuracy: 0.9229\nEpoch 13/20\n704/704 [==============================] - 8s 12ms/step - loss: 0.0881 - accuracy: 0.9705 - val_loss: 0.4237 - val_accuracy: 0.9284\nEpoch 14/20\n704/704 [==============================] - 8s 12ms/step - loss: 0.0834 - accuracy: 0.9727 - val_loss: 0.5571 - val_accuracy: 0.9282\nEpoch 15/20\n704/704 [==============================] - 8s 12ms/step - loss: 0.0817 - accuracy: 0.9743 - val_loss: 0.9855 - val_accuracy: 0.9236\nEpoch 16/20\n704/704 [==============================] - 8s 12ms/step - loss: 0.0817 - accuracy: 0.9750 - val_loss: 0.3483 - val_accuracy: 0.9257\nEpoch 17/20\n704/704 [==============================] - 8s 12ms/step - loss: 0.0835 - accuracy: 0.9747 - val_loss: 0.5525 - val_accuracy: 0.9230\nEpoch 18/20\n704/704 [==============================] - 8s 12ms/step - loss: 0.0858 - accuracy: 0.9732 - val_loss: 0.3046 - val_accuracy: 0.9223\nEpoch 19/20\n704/704 [==============================] - 8s 12ms/step - loss: 0.0793 - accuracy: 0.9755 - val_loss: 0.4696 - val_accuracy: 0.9265\nEpoch 20/20\n704/704 [==============================] - 8s 12ms/step - loss: 0.0825 - accuracy: 0.9747 - val_loss: 0.5379 - val_accuracy: 0.9200\n</code>\n</pre>        <pre><code>loss, accuracy = model.evaluate(X_test,\n                                y_test_oh)\n\ncomp = comp.append({'run': 'RMS_CNN', 'Perte' : loss, 'Pr\u00e9cision' : accuracy}, ignore_index=True)\nprint(comp)\n</code></pre>      <pre>\n<code>313/313 [==============================] - 1s 3ms/step - loss: 0.6125 - accuracy: 0.9108\n            run     Perte  Pr\u00e9cision\n0     basic_CNN  0.469638     0.8282\n1        bn_CNN  0.291927     0.8975\n2       bn2_CNN  0.271721     0.9046\n3      drop_CNN  0.497585     0.8187\n4        he_CNN  0.328177     0.8817\n5  momentum_CNN  0.286005     0.8988\n6  nesterov_CNN  0.296967     0.8942\n7       RMS_CNN  0.612474     0.9108\n</code>\n</pre>"},{"location":"deep_learning/module4/Module4_2/#adam","title":"Adam","text":"<pre><code>optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n\nmodel = base_model(optimizer = optimizer)\n\nhistory = model.fit(X_train, y_train_oh,\n                   epochs = 20,\n                   batch_size=64,\n                   validation_data=(X_valid, y_valid_oh))\n</code></pre>      <pre>\n<code>Epoch 1/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.4215 - accuracy: 0.8475 - val_loss: 0.2967 - val_accuracy: 0.8911\nEpoch 2/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2484 - accuracy: 0.9099 - val_loss: 0.2287 - val_accuracy: 0.9171\nEpoch 3/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.1963 - accuracy: 0.9278 - val_loss: 0.2135 - val_accuracy: 0.9187\nEpoch 4/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.1601 - accuracy: 0.9408 - val_loss: 0.2227 - val_accuracy: 0.9195\nEpoch 5/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.1320 - accuracy: 0.9501 - val_loss: 0.2325 - val_accuracy: 0.9224\nEpoch 6/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.1037 - accuracy: 0.9602 - val_loss: 0.2395 - val_accuracy: 0.9205\nEpoch 7/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0832 - accuracy: 0.9691 - val_loss: 0.2608 - val_accuracy: 0.9260\nEpoch 8/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0642 - accuracy: 0.9770 - val_loss: 0.2667 - val_accuracy: 0.9267\nEpoch 9/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0513 - accuracy: 0.9810 - val_loss: 0.3028 - val_accuracy: 0.9234\nEpoch 10/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0434 - accuracy: 0.9842 - val_loss: 0.2953 - val_accuracy: 0.9257\nEpoch 11/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0370 - accuracy: 0.9870 - val_loss: 0.3495 - val_accuracy: 0.9269\nEpoch 12/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0355 - accuracy: 0.9873 - val_loss: 0.3201 - val_accuracy: 0.9251\nEpoch 13/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0265 - accuracy: 0.9910 - val_loss: 0.3979 - val_accuracy: 0.9211\nEpoch 14/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0255 - accuracy: 0.9915 - val_loss: 0.4307 - val_accuracy: 0.9284\nEpoch 15/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0256 - accuracy: 0.9910 - val_loss: 0.4208 - val_accuracy: 0.9189\nEpoch 16/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0282 - accuracy: 0.9905 - val_loss: 0.3714 - val_accuracy: 0.9237\nEpoch 17/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0189 - accuracy: 0.9936 - val_loss: 0.4384 - val_accuracy: 0.9300\nEpoch 18/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0178 - accuracy: 0.9933 - val_loss: 0.4159 - val_accuracy: 0.9265\nEpoch 19/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0229 - accuracy: 0.9922 - val_loss: 0.3970 - val_accuracy: 0.9282\nEpoch 20/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0219 - accuracy: 0.9926 - val_loss: 0.4309 - val_accuracy: 0.9231\n</code>\n</pre>        <pre><code>loss, accuracy = model.evaluate(X_test,\n                                y_test_oh)\n\ncomp = comp.append({'run': 'Adam_CNN', 'Perte' : loss, 'Pr\u00e9cision' : accuracy}, ignore_index=True)\nprint(comp)\n</code></pre>      <pre>\n<code>313/313 [==============================] - 1s 4ms/step - loss: 0.5018 - accuracy: 0.9172\n            run     Perte  Pr\u00e9cision\n0     basic_CNN  0.469638     0.8282\n1        bn_CNN  0.291927     0.8975\n2       bn2_CNN  0.271721     0.9046\n3      drop_CNN  0.497585     0.8187\n4        he_CNN  0.328177     0.8817\n5  momentum_CNN  0.286005     0.8988\n6  nesterov_CNN  0.296967     0.8942\n7       RMS_CNN  0.612474     0.9108\n8      Adam_CNN  0.501753     0.9172\n</code>\n</pre>"},{"location":"deep_learning/module4/Module4_2/#batchnorm-he-adam","title":"BatchNorm + He + Adam","text":"<pre><code>model = keras.models.Sequential([\n                                 Input(shape=(28, 28, 1)),\n                                 Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\"),\n                                 Conv2D(64, (3, 3), padding='same',use_bias=False,  kernel_initializer=\"he_normal\"),\n                                 BatchNormalization(),\n                                 MaxPooling2D((2, 2)),\n                                 Activation('relu'),\n                                 Conv2D(128, (3, 3), activation='relu', padding='same',  kernel_initializer=\"he_normal\"),\n                                 Conv2D(128, (3, 3), padding='same',use_bias=False,  kernel_initializer=\"he_normal\"),\n                                 BatchNormalization(),\n                                 MaxPooling2D((2, 2)),\n                                 Activation('relu'),\n                                 Flatten(),\n                                 Dense(256,  kernel_initializer=\"he_normal\"),\n                                 Activation('relu'),\n                                 Dense(128,  kernel_initializer=\"he_normal\"),\n                                 Activation('relu'),\n                                 Dense(10, activation='softmax',  kernel_initializer=\"he_normal\")\n                                 ])\n\nmodel.compile(loss = 'categorical_crossentropy',\n              optimizer=tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999),\n              metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train_oh,\n                   epochs = 20,\n                   batch_size=64,\n                   validation_data=(X_valid, y_valid_oh))\n</code></pre>      <pre>\n<code>Epoch 1/20\n704/704 [==============================] - 8s 12ms/step - loss: 0.4872 - accuracy: 0.8331 - val_loss: 0.3376 - val_accuracy: 0.8719\nEpoch 2/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.2807 - accuracy: 0.8990 - val_loss: 0.2903 - val_accuracy: 0.8987\nEpoch 3/20\n704/704 [==============================] - 8s 12ms/step - loss: 0.2239 - accuracy: 0.9176 - val_loss: 0.2428 - val_accuracy: 0.9114\nEpoch 4/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.1906 - accuracy: 0.9300 - val_loss: 0.2191 - val_accuracy: 0.9189\nEpoch 5/20\n704/704 [==============================] - 8s 12ms/step - loss: 0.1688 - accuracy: 0.9381 - val_loss: 0.2799 - val_accuracy: 0.9035\nEpoch 6/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.1460 - accuracy: 0.9471 - val_loss: 0.2471 - val_accuracy: 0.9162\nEpoch 7/20\n704/704 [==============================] - 8s 12ms/step - loss: 0.1245 - accuracy: 0.9546 - val_loss: 0.2247 - val_accuracy: 0.9223\nEpoch 8/20\n704/704 [==============================] - 8s 12ms/step - loss: 0.1045 - accuracy: 0.9620 - val_loss: 0.2052 - val_accuracy: 0.9289\nEpoch 9/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0879 - accuracy: 0.9678 - val_loss: 0.2891 - val_accuracy: 0.9127\nEpoch 10/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0720 - accuracy: 0.9737 - val_loss: 0.2324 - val_accuracy: 0.9281\nEpoch 11/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0601 - accuracy: 0.9784 - val_loss: 0.2793 - val_accuracy: 0.9283\nEpoch 12/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0480 - accuracy: 0.9830 - val_loss: 0.2750 - val_accuracy: 0.9293\nEpoch 13/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0399 - accuracy: 0.9854 - val_loss: 0.2626 - val_accuracy: 0.9333\nEpoch 14/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0368 - accuracy: 0.9859 - val_loss: 0.2972 - val_accuracy: 0.9299\nEpoch 15/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0356 - accuracy: 0.9871 - val_loss: 0.2719 - val_accuracy: 0.9324\nEpoch 16/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0281 - accuracy: 0.9899 - val_loss: 0.3408 - val_accuracy: 0.9231\nEpoch 17/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0249 - accuracy: 0.9912 - val_loss: 0.3601 - val_accuracy: 0.9246\nEpoch 18/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0269 - accuracy: 0.9900 - val_loss: 0.3535 - val_accuracy: 0.9291\nEpoch 19/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0186 - accuracy: 0.9935 - val_loss: 0.3342 - val_accuracy: 0.9309\nEpoch 20/20\n704/704 [==============================] - 8s 11ms/step - loss: 0.0218 - accuracy: 0.9926 - val_loss: 0.3540 - val_accuracy: 0.9288\n</code>\n</pre>        <pre><code>pd.DataFrame(history.history).plot(figsize=(12,8))\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()\n</code></pre>              <pre><code>loss, accuracy = model.evaluate(X_test,\n                                y_test_oh)\n\ncomp = comp.append({'run': 'Bn_He_Adam_CNN', 'Perte' : loss, 'Pr\u00e9cision' : accuracy}, ignore_index=True)\nprint(comp)\n</code></pre>      <pre>\n<code>313/313 [==============================] - 1s 4ms/step - loss: 0.4062 - accuracy: 0.9219\n</code>\n</pre>"},{"location":"deep_learning/module4/Module4_2/#keras-tuner-et-hiplot","title":"Keras Tuner et HiPlotnum_filters_1 (Choice)num_filters_2 (Choice)units (Int)learning_rate (Float)Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:Hyperparameters:","text":"<p>Savoir </p> <ul> <li>quel optimiseur utiliser, </li> <li>quel taux d'apprentissage fixer, </li> <li>quel taux de dropout fixer, </li> <li>le nombre de neurones denses, </li> <li>ou de filtres dans une couche convolutive, </li> </ul> <p>tout cela peut \u00eatre submergeant.</p> <p>On pout toutefois en automatiser une certaine partie, en esp\u00e9rant que le choix fait sera alors optimal.</p>      <pre><code>!pip install hiplot\n</code></pre>     <pre><code>!pip install keras-tuner\n</code></pre>     <pre><code>from kerastuner import HyperModel\nfrom kerastuner.tuners import Hyperband\n</code></pre>      <p>L'objet de base dans Keras-Tuner est la classe <code>HyperModel</code>, et sa variable hyperparam\u00e8tre d\u00e9fini par la <code>hp</code>. On a plusieurs fa\u00e7on de l'appeler.</p> <ul> <li> <p>Faire un choix entre plusieurs possibilit\u00e9s d\u00e9j\u00e0 fix\u00e9es, avec une valeur par d\u00e9faut. <pre><code>filters=hp.Choice('num_filters_1',\n                  values=[32, 64, 128],\n                  default=64,\n                  )\n</code></pre></p> </li> <li> <p>D\u00e9finir un intervalle de r\u00e9els \\([\\mathrm{min \\_ value}, \\mathrm{max\\_value}]\\) avec un incr\u00e9ment <code>step</code>, et une valeur par d\u00e9faut. <pre><code>rate=hp.Float('dropout_1',\n               min_value=0.0,\n               max_value=0.5,\n               default=0.25,\n               step=0.05\n               )\n</code></pre></p> </li> <li> <p>D\u00e9finir un intervalle d'entiers \\([\\mathrm{min\\_value}, \\mathrm{max\\_value}]\\) avec un incr\u00e9ment <code>step</code>, et une valeur par d\u00e9faut. <pre><code>units=hp.Int('units',\n              min_value=32,\n              max_value=512,\n              step=32,\n              default=128\n              )\n</code></pre></p> </li> </ul>      <pre><code>class CNNHyperModel(HyperModel):\n\n    def __init__(self, input_shape, num_classes):\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n\n    def build(self, hp):\n        model = tf.keras.Sequential([\n                                     Conv2D(hp.Choice('num_filters_1',\n                                                      values=[32, 64],\n                                                      default=64\n                                                      ), \n                                            (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\"),\n\n                                     Conv2D(hp.Choice('num_filters_2',\n                                                      values=[32, 64],\n                                                      default=64\n                                                      ), \n                                            (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\"),\n\n                                     MaxPooling2D((2, 2)),\n                                     Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\"),\n                                     Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\"),\n                                     MaxPooling2D((2, 2)),\n\n                                     Flatten(),\n                                     Dense(hp.Int('units',\n                                                  min_value=32,\n                                                  max_value=512,\n                                                  step=32,\n                                                  default=256\n                                                  ), \n                                           kernel_initializer=\"he_normal\"),\n\n                                     Activation('relu'),\n                                     Dense(128, kernel_initializer=\"he_normal\"),\n                                     Activation('relu'),\n\n                                     Dense(self.num_classes, \n                                           activation='softmax')\n                                     ])\n\n\n        model.compile(\n            optimizer=tf.keras.optimizers.Adam(\n                hp.Float(\n                    'learning_rate',\n                    min_value=1e-4,\n                    max_value=1e-2,\n                    sampling='LOG',\n                    default=1e-3\n                )\n            ),\n            loss='categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        return model\n\nhypermodel = CNNHyperModel(input_shape=(28, 28, 1), num_classes=10)\n</code></pre>      <p>Une fois notre hypermod\u00e8le d\u00e9fini, on instancie un <code>tuner</code> en d\u00e9finissant : - quelle m\u00e9thode d'optimisation on utilisera( ici <code>RandomSearch</code>), - L'objectif d'optimisation, - Le repertoire <code>directory</code> o\u00f9 seront stock\u00e9es les r\u00e9sultats.</p>      <pre><code>from kerastuner.tuners import RandomSearch\n\ntuner = RandomSearch(\n    hypermodel,\n    objective='val_accuracy',\n    seed=RANDOM_SEED,\n    max_trials=20,\n    directory='random_search',\n    project_name='fashion_mnist'\n)\n</code></pre>      <ul> <li> <p>Le param\u00e8tre <code>objective</code> est la fonction \u00e0 optimiser. Le tuner d\u00e9duit s'il s'agit d'un probl\u00e8me de maximisation ou de minimisation en fonction de sa valeur.</p> </li> <li> <p>Ensuite, la variable <code>max_trials</code> repr\u00e9sente le nombre de combinaisons d'hyperparam\u00e8tres qui seront test\u00e9es par le tuner, tandis que la variable <code>execution_per_trial</code> est le nombre de mod\u00e8les qui devraient \u00eatre construits et adapt\u00e9s \u00e0 chaque essai pour des raisons de robustesse.</p> </li> <li> <p><code>project_name</code> lui est le nom du dossier dans lequel les donn\u00e9es de visualisation seront enregistr\u00e9es</p> </li> </ul> <p>Le nombre de tuners, ie de m\u00e9thode de recherche pr\u00e9d\u00e9finies, est au nombre de 3 (https://keras-team.github.io/keras-tuner/documentation/tuners/), on </p> <ul> <li><code>RandomSearch</code>, qui effectue une recherche al\u00e9atoire,</li> <li><code>BayesianOptimization</code>, pour un optimisation Bayesienne,</li> <li><code>Hyperband</code>, qui utilise l'algorithme HyperBand, m\u00e9thode d'optimisation via les bandits manchots \u00e0 \\(n\\) bras.</li> </ul> <p>L'espace des hyperparam\u00e8tres est alors visualisable via la commande suivante.</p>      <pre><code>tuner.search_space_summary()\n</code></pre>      Search space summary      |-Default search space size: 4           |-default: 64      |-ordered: True      |-values: [32, 64]           |-default: 64      |-ordered: True      |-values: [32, 64]           |-default: 256      |-max_value: 512      |-min_value: 32      |-sampling: None      |-step: 32           |-default: 0.001      |-max_value: 0.01      |-min_value: 0.0001      |-sampling: log      |-step: None         <p>De la m\u00eame fa\u00e7on que l'on entra\u00eene un mod\u00e8le via la commande <code>.fit()</code>, on lance la recherche des hyperparam\u00e8tres optimaux via la commance <code>.search()</code>.</p>      <pre><code>num_of_epochs = 10\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\ntuner.search(X_train,\n             y_train_oh,\n             epochs=num_of_epochs,\n             batch_size=64,\n             validation_data=(X_valid, y_valid_oh),\n             callbacks=[callback])\n</code></pre>      <pre>\n<code>Epoch 1/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.4168 - accuracy: 0.8494 - val_loss: 0.2970 - val_accuracy: 0.8934\nEpoch 2/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.2591 - accuracy: 0.9068 - val_loss: 0.2509 - val_accuracy: 0.9111\nEpoch 3/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.2048 - accuracy: 0.9256 - val_loss: 0.2268 - val_accuracy: 0.9153\nEpoch 4/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.1666 - accuracy: 0.9387 - val_loss: 0.2306 - val_accuracy: 0.9139\nEpoch 5/10\n704/704 [==============================] - 7s 11ms/step - loss: 0.1309 - accuracy: 0.9516 - val_loss: 0.2405 - val_accuracy: 0.9124\nEpoch 6/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.1030 - accuracy: 0.9617 - val_loss: 0.2344 - val_accuracy: 0.9180\n</code>\n</pre>     Trial complete     Trial summary      |-Trial ID: ad549b3b4b8b6172b82ea67ee5fad689      |-Score: 0.9179999828338623      |-Best step: 0           |-learning_rate: 0.0003497444477863496      |-num_filters_1: 32      |-num_filters_2: 32      |-units: 224     <pre>\n<code>Epoch 1/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.4549 - accuracy: 0.8366 - val_loss: 0.3342 - val_accuracy: 0.8806\nEpoch 2/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.2940 - accuracy: 0.8945 - val_loss: 0.2841 - val_accuracy: 0.8981\nEpoch 3/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.2436 - accuracy: 0.9111 - val_loss: 0.2525 - val_accuracy: 0.9068\nEpoch 4/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.2096 - accuracy: 0.9232 - val_loss: 0.2480 - val_accuracy: 0.9079\nEpoch 5/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.1848 - accuracy: 0.9315 - val_loss: 0.2710 - val_accuracy: 0.8982\nEpoch 6/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.1603 - accuracy: 0.9408 - val_loss: 0.2396 - val_accuracy: 0.9149\nEpoch 7/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.1393 - accuracy: 0.9492 - val_loss: 0.2223 - val_accuracy: 0.9186\nEpoch 8/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.1185 - accuracy: 0.9577 - val_loss: 0.2200 - val_accuracy: 0.9227\nEpoch 9/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.1012 - accuracy: 0.9627 - val_loss: 0.2714 - val_accuracy: 0.9123\nEpoch 10/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.0859 - accuracy: 0.9691 - val_loss: 0.2429 - val_accuracy: 0.9205\n</code>\n</pre>     Trial complete     Trial summary      |-Trial ID: 8279e19274848d0d493e7c40cac4713c      |-Score: 0.9226666688919067      |-Best step: 0           |-learning_rate: 0.00013607174450468629      |-num_filters_1: 32      |-num_filters_2: 64      |-units: 288     <pre>\n<code>Epoch 1/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.4173 - accuracy: 0.8495 - val_loss: 0.2987 - val_accuracy: 0.8889\nEpoch 2/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.2552 - accuracy: 0.9057 - val_loss: 0.2484 - val_accuracy: 0.9107\nEpoch 3/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.2046 - accuracy: 0.9243 - val_loss: 0.2410 - val_accuracy: 0.9133\nEpoch 4/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.1704 - accuracy: 0.9368 - val_loss: 0.2299 - val_accuracy: 0.9167\nEpoch 5/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.1447 - accuracy: 0.9464 - val_loss: 0.2436 - val_accuracy: 0.9174\nEpoch 6/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.1228 - accuracy: 0.9547 - val_loss: 0.2702 - val_accuracy: 0.9129\nEpoch 7/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.1011 - accuracy: 0.9632 - val_loss: 0.2520 - val_accuracy: 0.9222\n</code>\n</pre>     Trial complete     Trial summary      |-Trial ID: 9fa074003b81dc588b82694213d0eff8      |-Score: 0.9222000241279602      |-Best step: 0           |-learning_rate: 0.001715074355925934      |-num_filters_1: 64      |-num_filters_2: 32      |-units: 512     <pre>\n<code>Epoch 1/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.4543 - accuracy: 0.8377 - val_loss: 0.3411 - val_accuracy: 0.8769\nEpoch 2/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.2938 - accuracy: 0.8938 - val_loss: 0.2950 - val_accuracy: 0.8941\nEpoch 3/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.2420 - accuracy: 0.9123 - val_loss: 0.2572 - val_accuracy: 0.9071\nEpoch 4/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.2049 - accuracy: 0.9256 - val_loss: 0.2439 - val_accuracy: 0.9099\nEpoch 5/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.1768 - accuracy: 0.9360 - val_loss: 0.2637 - val_accuracy: 0.9029\nEpoch 6/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.1513 - accuracy: 0.9450 - val_loss: 0.2431 - val_accuracy: 0.9137\nEpoch 7/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.1290 - accuracy: 0.9537 - val_loss: 0.2405 - val_accuracy: 0.9125\nEpoch 8/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.1094 - accuracy: 0.9614 - val_loss: 0.2179 - val_accuracy: 0.9225\nEpoch 9/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.0903 - accuracy: 0.9676 - val_loss: 0.2728 - val_accuracy: 0.9088\nEpoch 10/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.0733 - accuracy: 0.9739 - val_loss: 0.2495 - val_accuracy: 0.9229\n</code>\n</pre>     Trial complete     Trial summary      |-Trial ID: ad4279e13d037a77eb612f9367544ae9      |-Score: 0.9228666424751282      |-Best step: 0           |-learning_rate: 0.00012165541012624621      |-num_filters_1: 32      |-num_filters_2: 32      |-units: 480     <pre>\n<code>Epoch 1/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.3942 - accuracy: 0.8578 - val_loss: 0.2787 - val_accuracy: 0.8969\nEpoch 2/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.2472 - accuracy: 0.9094 - val_loss: 0.2325 - val_accuracy: 0.9128\nEpoch 3/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.1933 - accuracy: 0.9278 - val_loss: 0.2098 - val_accuracy: 0.9207\nEpoch 4/10\n704/704 [==============================] - 7s 11ms/step - loss: 0.1571 - accuracy: 0.9410 - val_loss: 0.2531 - val_accuracy: 0.9041\nEpoch 5/10\n704/704 [==============================] - 7s 10ms/step - loss: 0.1258 - accuracy: 0.9524 - val_loss: 0.2383 - val_accuracy: 0.9174\nEpoch 6/10\n704/704 [==============================] - 7s 11ms/step - loss: 0.0988 - accuracy: 0.9629 - val_loss: 0.2387 - val_accuracy: 0.9184\n</code>\n</pre>     Trial complete     Trial summary      |-Trial ID: 0d06d8240a239bf91872c87cdfffd081      |-Score: 0.9206666946411133      |-Best step: 0           |-learning_rate: 0.0009749332269823054      |-num_filters_1: 32      |-num_filters_2: 32      |-units: 192     <pre>\n<code>Epoch 1/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.4211 - accuracy: 0.8496 - val_loss: 0.3052 - val_accuracy: 0.8866\nEpoch 2/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.2540 - accuracy: 0.9084 - val_loss: 0.2378 - val_accuracy: 0.9118\nEpoch 3/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.2002 - accuracy: 0.9262 - val_loss: 0.2149 - val_accuracy: 0.9195\nEpoch 4/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.1636 - accuracy: 0.9400 - val_loss: 0.2114 - val_accuracy: 0.9220\nEpoch 5/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.1310 - accuracy: 0.9507 - val_loss: 0.2307 - val_accuracy: 0.9196\nEpoch 6/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.1057 - accuracy: 0.9606 - val_loss: 0.2163 - val_accuracy: 0.9247\nEpoch 7/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.0809 - accuracy: 0.9695 - val_loss: 0.2183 - val_accuracy: 0.9293\n</code>\n</pre>     Trial complete     Trial summary      |-Trial ID: 888c4d90d81bb8a15db36715a329abe5      |-Score: 0.9293333292007446      |-Best step: 0           |-learning_rate: 0.0006752863927347823      |-num_filters_1: 32      |-num_filters_2: 64      |-units: 256     <pre>\n<code>Epoch 1/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.4065 - accuracy: 0.8545 - val_loss: 0.2793 - val_accuracy: 0.8965\nEpoch 2/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.2476 - accuracy: 0.9104 - val_loss: 0.2366 - val_accuracy: 0.9121\nEpoch 3/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.2009 - accuracy: 0.9257 - val_loss: 0.2230 - val_accuracy: 0.9177\nEpoch 4/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.1716 - accuracy: 0.9367 - val_loss: 0.2302 - val_accuracy: 0.9198\nEpoch 5/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.1524 - accuracy: 0.9441 - val_loss: 0.2771 - val_accuracy: 0.9065\nEpoch 6/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.1278 - accuracy: 0.9526 - val_loss: 0.2299 - val_accuracy: 0.9222\n</code>\n</pre>     Trial complete     Trial summary      |-Trial ID: 3e5e38f8970e988856d96a81dd0cb384      |-Score: 0.9222000241279602      |-Best step: 0           |-learning_rate: 0.0023360487671257027      |-num_filters_1: 32      |-num_filters_2: 32      |-units: 384     <pre>\n<code>Epoch 1/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.4540 - accuracy: 0.8346 - val_loss: 0.3190 - val_accuracy: 0.8851\nEpoch 2/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.2785 - accuracy: 0.8996 - val_loss: 0.2580 - val_accuracy: 0.9079\nEpoch 3/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.2256 - accuracy: 0.9175 - val_loss: 0.2406 - val_accuracy: 0.9104\nEpoch 4/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.1921 - accuracy: 0.9306 - val_loss: 0.2271 - val_accuracy: 0.9160\nEpoch 5/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.1627 - accuracy: 0.9394 - val_loss: 0.2319 - val_accuracy: 0.9171\nEpoch 6/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.1382 - accuracy: 0.9488 - val_loss: 0.2194 - val_accuracy: 0.9218\nEpoch 7/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.1126 - accuracy: 0.9588 - val_loss: 0.2146 - val_accuracy: 0.9239\nEpoch 8/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.0916 - accuracy: 0.9668 - val_loss: 0.2299 - val_accuracy: 0.9251\nEpoch 9/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.0734 - accuracy: 0.9732 - val_loss: 0.3007 - val_accuracy: 0.9144\nEpoch 10/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.0611 - accuracy: 0.9776 - val_loss: 0.2920 - val_accuracy: 0.9189\n</code>\n</pre>     Trial complete     Trial summary      |-Trial ID: 8b8f262dc35d2c9bec969b0c7126eb8d      |-Score: 0.9251333475112915      |-Best step: 0           |-learning_rate: 0.00036276965095376624      |-num_filters_1: 32      |-num_filters_2: 64      |-units: 64     <pre>\n<code>Epoch 1/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.4737 - accuracy: 0.8366 - val_loss: 0.3217 - val_accuracy: 0.8825\nEpoch 2/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.2988 - accuracy: 0.8895 - val_loss: 0.3082 - val_accuracy: 0.8886\nEpoch 3/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.2627 - accuracy: 0.9015 - val_loss: 0.2628 - val_accuracy: 0.9033\nEpoch 4/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.2367 - accuracy: 0.9128 - val_loss: 0.2531 - val_accuracy: 0.9063\nEpoch 5/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.2206 - accuracy: 0.9181 - val_loss: 0.2952 - val_accuracy: 0.8993\nEpoch 6/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.2109 - accuracy: 0.9216 - val_loss: 0.2781 - val_accuracy: 0.8996\nEpoch 7/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.1948 - accuracy: 0.9280 - val_loss: 0.2531 - val_accuracy: 0.9113\nEpoch 8/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.1826 - accuracy: 0.9320 - val_loss: 0.2671 - val_accuracy: 0.9127\nEpoch 9/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.1887 - accuracy: 0.9316 - val_loss: 0.2836 - val_accuracy: 0.9081\nEpoch 10/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.1691 - accuracy: 0.9387 - val_loss: 0.2690 - val_accuracy: 0.9123\n</code>\n</pre>     Trial complete     Trial summary      |-Trial ID: e63dadbd65de215a3097342f65d4ca6b      |-Score: 0.9126666784286499      |-Best step: 0           |-learning_rate: 0.003964886305018653      |-num_filters_1: 32      |-num_filters_2: 64      |-units: 224     <pre>\n<code>Epoch 1/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.4152 - accuracy: 0.8523 - val_loss: 0.2942 - val_accuracy: 0.8907\nEpoch 2/10\n704/704 [==============================] - 7s 11ms/step - loss: 0.2526 - accuracy: 0.9078 - val_loss: 0.2415 - val_accuracy: 0.9092\nEpoch 3/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.1968 - accuracy: 0.9270 - val_loss: 0.2263 - val_accuracy: 0.9163\nEpoch 4/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.1632 - accuracy: 0.9407 - val_loss: 0.2260 - val_accuracy: 0.9151\nEpoch 5/10\n704/704 [==============================] - 7s 11ms/step - loss: 0.1297 - accuracy: 0.9519 - val_loss: 0.2575 - val_accuracy: 0.9107\nEpoch 6/10\n704/704 [==============================] - 7s 11ms/step - loss: 0.1058 - accuracy: 0.9604 - val_loss: 0.2581 - val_accuracy: 0.9118\nEpoch 7/10\n704/704 [==============================] - 7s 11ms/step - loss: 0.0838 - accuracy: 0.9694 - val_loss: 0.2470 - val_accuracy: 0.9201\n</code>\n</pre>     Trial complete     Trial summary      |-Trial ID: c5a660c5b9f51d34c7738dac5b546771      |-Score: 0.9201333522796631      |-Best step: 0           |-learning_rate: 0.0010277342442932575      |-num_filters_1: 32      |-num_filters_2: 32      |-units: 160     <pre>\n<code>Epoch 1/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.4259 - accuracy: 0.8470 - val_loss: 0.3122 - val_accuracy: 0.8875\nEpoch 2/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.2672 - accuracy: 0.9037 - val_loss: 0.2500 - val_accuracy: 0.9089\nEpoch 3/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.2149 - accuracy: 0.9222 - val_loss: 0.2419 - val_accuracy: 0.9112\nEpoch 4/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.1790 - accuracy: 0.9345 - val_loss: 0.2294 - val_accuracy: 0.9140\nEpoch 5/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.1480 - accuracy: 0.9460 - val_loss: 0.2373 - val_accuracy: 0.9131\nEpoch 6/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.1224 - accuracy: 0.9550 - val_loss: 0.2487 - val_accuracy: 0.9127\nEpoch 7/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.0965 - accuracy: 0.9646 - val_loss: 0.2306 - val_accuracy: 0.9225\n</code>\n</pre>     Trial complete     Trial summary      |-Trial ID: 0bf67cf27d543ea733d47f39075cf186      |-Score: 0.9224666953086853      |-Best step: 0           |-learning_rate: 0.00025071332274895686      |-num_filters_1: 32      |-num_filters_2: 64      |-units: 384     <pre>\n<code>Epoch 1/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.4525 - accuracy: 0.8384 - val_loss: 0.3305 - val_accuracy: 0.8824\nEpoch 2/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.2939 - accuracy: 0.8955 - val_loss: 0.2951 - val_accuracy: 0.8953\nEpoch 3/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.2412 - accuracy: 0.9121 - val_loss: 0.2610 - val_accuracy: 0.9045\nEpoch 4/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.2081 - accuracy: 0.9243 - val_loss: 0.2400 - val_accuracy: 0.9121\nEpoch 5/10\n704/704 [==============================] - 7s 11ms/step - loss: 0.1785 - accuracy: 0.9348 - val_loss: 0.2951 - val_accuracy: 0.8959\nEpoch 6/10\n704/704 [==============================] - 7s 11ms/step - loss: 0.1545 - accuracy: 0.9441 - val_loss: 0.2476 - val_accuracy: 0.9129\nEpoch 7/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.1317 - accuracy: 0.9523 - val_loss: 0.2356 - val_accuracy: 0.9157\nEpoch 8/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.1106 - accuracy: 0.9619 - val_loss: 0.2424 - val_accuracy: 0.9171\nEpoch 9/10\n704/704 [==============================] - 7s 11ms/step - loss: 0.0930 - accuracy: 0.9662 - val_loss: 0.2706 - val_accuracy: 0.9117\nEpoch 10/10\n704/704 [==============================] - 7s 11ms/step - loss: 0.0744 - accuracy: 0.9736 - val_loss: 0.2584 - val_accuracy: 0.9219\n</code>\n</pre>     Trial complete     Trial summary      |-Trial ID: 6a1861dcde046721d62be37072693fe1      |-Score: 0.9218666553497314      |-Best step: 0           |-learning_rate: 0.00014527180385656086      |-num_filters_1: 32      |-num_filters_2: 32      |-units: 224     <pre>\n<code>Epoch 1/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.5618 - accuracy: 0.8244 - val_loss: 0.3947 - val_accuracy: 0.8569\nEpoch 2/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.3342 - accuracy: 0.8777 - val_loss: 0.3224 - val_accuracy: 0.8801\nEpoch 3/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.2999 - accuracy: 0.8881 - val_loss: 0.2986 - val_accuracy: 0.8887\nEpoch 4/10\n704/704 [==============================] - 7s 11ms/step - loss: 0.2818 - accuracy: 0.8957 - val_loss: 0.2983 - val_accuracy: 0.8874\nEpoch 5/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.2721 - accuracy: 0.8990 - val_loss: 0.3596 - val_accuracy: 0.8762\nEpoch 6/10\n704/704 [==============================] - 7s 11ms/step - loss: 0.2709 - accuracy: 0.8992 - val_loss: 0.3247 - val_accuracy: 0.8817\nEpoch 7/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.2687 - accuracy: 0.9011 - val_loss: 0.3011 - val_accuracy: 0.8930\n</code>\n</pre>     Trial complete     Trial summary      |-Trial ID: c284d54100b20a32b6aa92ddafcb7c29      |-Score: 0.8930000066757202      |-Best step: 0           |-learning_rate: 0.006678619769811224      |-num_filters_1: 32      |-num_filters_2: 32      |-units: 224     <pre>\n<code>Epoch 1/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.4399 - accuracy: 0.8412 - val_loss: 0.3216 - val_accuracy: 0.8828\nEpoch 2/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.2793 - accuracy: 0.8999 - val_loss: 0.2713 - val_accuracy: 0.9025\nEpoch 3/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.2268 - accuracy: 0.9173 - val_loss: 0.2415 - val_accuracy: 0.9102\nEpoch 4/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.1911 - accuracy: 0.9298 - val_loss: 0.2375 - val_accuracy: 0.9106\nEpoch 5/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.1609 - accuracy: 0.9408 - val_loss: 0.2751 - val_accuracy: 0.9025\nEpoch 6/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.1366 - accuracy: 0.9488 - val_loss: 0.2382 - val_accuracy: 0.9157\nEpoch 7/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.1105 - accuracy: 0.9593 - val_loss: 0.2396 - val_accuracy: 0.9187\n</code>\n</pre>     Trial complete     Trial summary      |-Trial ID: 932a4440c0a7e4b8e0f629c87a4f1789      |-Score: 0.918666660785675      |-Best step: 0           |-learning_rate: 0.0002449483373216793      |-num_filters_1: 32      |-num_filters_2: 32      |-units: 224     <pre>\n<code>Epoch 1/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.4237 - accuracy: 0.8497 - val_loss: 0.2860 - val_accuracy: 0.8974\nEpoch 2/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.2604 - accuracy: 0.9061 - val_loss: 0.2438 - val_accuracy: 0.9117\nEpoch 3/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.2069 - accuracy: 0.9225 - val_loss: 0.2246 - val_accuracy: 0.9173\nEpoch 4/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.1730 - accuracy: 0.9359 - val_loss: 0.2231 - val_accuracy: 0.9191\nEpoch 5/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.1485 - accuracy: 0.9436 - val_loss: 0.2548 - val_accuracy: 0.9167\nEpoch 6/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.1267 - accuracy: 0.9520 - val_loss: 0.2448 - val_accuracy: 0.9167\nEpoch 7/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.1045 - accuracy: 0.9610 - val_loss: 0.2277 - val_accuracy: 0.9247\n</code>\n</pre>     Trial complete     Trial summary      |-Trial ID: 0c621859edde556edb80c57cf0c8266b      |-Score: 0.9247333407402039      |-Best step: 0           |-learning_rate: 0.001453129317743269      |-num_filters_1: 64      |-num_filters_2: 64      |-units: 96     <pre>\n<code>Epoch 1/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.5476 - accuracy: 0.8286 - val_loss: 0.3346 - val_accuracy: 0.8709\nEpoch 2/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.3074 - accuracy: 0.8883 - val_loss: 0.3067 - val_accuracy: 0.8888\nEpoch 3/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.2723 - accuracy: 0.9013 - val_loss: 0.3265 - val_accuracy: 0.8818\nEpoch 4/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.2559 - accuracy: 0.9060 - val_loss: 0.2848 - val_accuracy: 0.8953\nEpoch 5/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.2425 - accuracy: 0.9114 - val_loss: 0.2858 - val_accuracy: 0.8973\nEpoch 6/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.2384 - accuracy: 0.9137 - val_loss: 0.2825 - val_accuracy: 0.9000\nEpoch 7/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.2212 - accuracy: 0.9194 - val_loss: 0.3535 - val_accuracy: 0.8835\nEpoch 8/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.2159 - accuracy: 0.9218 - val_loss: 0.3222 - val_accuracy: 0.8905\nEpoch 9/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.2137 - accuracy: 0.9227 - val_loss: 0.3173 - val_accuracy: 0.9015\n</code>\n</pre>     Trial complete     Trial summary      |-Trial ID: c54d5d40604c49d5387cec2703cf3750      |-Score: 0.9014666676521301      |-Best step: 0           |-learning_rate: 0.005704251931830293      |-num_filters_1: 32      |-num_filters_2: 64      |-units: 480     <pre>\n<code>Epoch 1/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.4362 - accuracy: 0.8432 - val_loss: 0.3162 - val_accuracy: 0.8859\nEpoch 2/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.2736 - accuracy: 0.9010 - val_loss: 0.2606 - val_accuracy: 0.9046\nEpoch 3/10\n704/704 [==============================] - 8s 11ms/step - loss: 0.2196 - accuracy: 0.9194 - val_loss: 0.2443 - val_accuracy: 0.9090\nEpoch 4/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.1840 - accuracy: 0.9330 - val_loss: 0.2176 - val_accuracy: 0.9209\nEpoch 5/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.1509 - accuracy: 0.9437 - val_loss: 0.2442 - val_accuracy: 0.9140\nEpoch 6/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.1269 - accuracy: 0.9519 - val_loss: 0.2352 - val_accuracy: 0.9165\nEpoch 7/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.1004 - accuracy: 0.9631 - val_loss: 0.2251 - val_accuracy: 0.9240\n</code>\n</pre>     Trial complete     Trial summary      |-Trial ID: 822f72617459e63dd80979cd498fca21      |-Score: 0.9240000247955322      |-Best step: 0           |-learning_rate: 0.0003623815491328145      |-num_filters_1: 64      |-num_filters_2: 32      |-units: 96     <pre>\n<code>Epoch 1/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.4912 - accuracy: 0.8245 - val_loss: 0.3394 - val_accuracy: 0.8806\nEpoch 2/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.3076 - accuracy: 0.8901 - val_loss: 0.3014 - val_accuracy: 0.8927\nEpoch 3/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.2532 - accuracy: 0.9088 - val_loss: 0.2668 - val_accuracy: 0.9023\nEpoch 4/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.2165 - accuracy: 0.9220 - val_loss: 0.2599 - val_accuracy: 0.9049\nEpoch 5/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.1894 - accuracy: 0.9314 - val_loss: 0.2814 - val_accuracy: 0.8993\nEpoch 6/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.1650 - accuracy: 0.9392 - val_loss: 0.2432 - val_accuracy: 0.9114\nEpoch 7/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.1430 - accuracy: 0.9474 - val_loss: 0.2406 - val_accuracy: 0.9134\nEpoch 8/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.1213 - accuracy: 0.9561 - val_loss: 0.2293 - val_accuracy: 0.9198\nEpoch 9/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.1034 - accuracy: 0.9616 - val_loss: 0.2832 - val_accuracy: 0.9069\nEpoch 10/10\n704/704 [==============================] - 8s 12ms/step - loss: 0.0859 - accuracy: 0.9685 - val_loss: 0.2390 - val_accuracy: 0.9210\n</code>\n</pre>     Trial complete     Trial summary      |-Trial ID: 63e62b8aedcfffad94d17a4acb85075b      |-Score: 0.9210000038146973      |-Best step: 0           |-learning_rate: 0.00011510936029076842      |-num_filters_1: 64      |-num_filters_2: 32      |-units: 256     <pre>\n<code>Epoch 1/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.4175 - accuracy: 0.8509 - val_loss: 0.3105 - val_accuracy: 0.8843\nEpoch 2/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.2631 - accuracy: 0.9051 - val_loss: 0.2478 - val_accuracy: 0.9088\nEpoch 3/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.2082 - accuracy: 0.9236 - val_loss: 0.2309 - val_accuracy: 0.9131\nEpoch 4/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.1709 - accuracy: 0.9375 - val_loss: 0.2193 - val_accuracy: 0.9191\nEpoch 5/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.1375 - accuracy: 0.9492 - val_loss: 0.2253 - val_accuracy: 0.9181\nEpoch 6/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.1088 - accuracy: 0.9598 - val_loss: 0.2426 - val_accuracy: 0.9189\nEpoch 7/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.0838 - accuracy: 0.9689 - val_loss: 0.2514 - val_accuracy: 0.9168\n</code>\n</pre>     Trial complete     Trial summary      |-Trial ID: ff63b04ae658949175a2fa79197ae8a3      |-Score: 0.9191333055496216      |-Best step: 0           |-learning_rate: 0.0003021384939866327      |-num_filters_1: 32      |-num_filters_2: 64      |-units: 416     <pre>\n<code>Epoch 1/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.4488 - accuracy: 0.8391 - val_loss: 0.3204 - val_accuracy: 0.8839\nEpoch 2/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.2875 - accuracy: 0.8973 - val_loss: 0.2801 - val_accuracy: 0.8988\nEpoch 3/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.2347 - accuracy: 0.9142 - val_loss: 0.2502 - val_accuracy: 0.9074\nEpoch 4/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.2011 - accuracy: 0.9279 - val_loss: 0.2469 - val_accuracy: 0.9082\nEpoch 5/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.1717 - accuracy: 0.9368 - val_loss: 0.2625 - val_accuracy: 0.9028\nEpoch 6/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.1460 - accuracy: 0.9471 - val_loss: 0.2437 - val_accuracy: 0.9103\nEpoch 7/10\n704/704 [==============================] - 9s 13ms/step - loss: 0.1233 - accuracy: 0.9546 - val_loss: 0.2213 - val_accuracy: 0.9224\nEpoch 8/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.1042 - accuracy: 0.9624 - val_loss: 0.2427 - val_accuracy: 0.9189\nEpoch 9/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.0852 - accuracy: 0.9692 - val_loss: 0.2911 - val_accuracy: 0.9087\nEpoch 10/10\n704/704 [==============================] - 9s 12ms/step - loss: 0.0690 - accuracy: 0.9748 - val_loss: 0.2558 - val_accuracy: 0.9183\n</code>\n</pre>     Trial complete     Trial summary      |-Trial ID: 9e654a297fcccf88ea1724cb684f6542      |-Score: 0.9223999977111816      |-Best step: 0           |-learning_rate: 0.00014948684422968163      |-num_filters_1: 32      |-num_filters_2: 64      |-units: 288     <pre>\n<code>INFO:tensorflow:Oracle triggered exit\n</code>\n</pre>        <pre><code>tuner.results_summary()\n</code></pre>      Results summary      |-Results in random_search/fashion_mnist      |-Showing 10 best trials      |-Objective(name='val_accuracy', direction='max')     Trial summary      |-Trial ID: 888c4d90d81bb8a15db36715a329abe5      |-Score: 0.9293333292007446      |-Best step: 0           |-learning_rate: 0.0006752863927347823      |-num_filters_1: 32      |-num_filters_2: 64      |-units: 256     Trial summary      |-Trial ID: 8b8f262dc35d2c9bec969b0c7126eb8d      |-Score: 0.9251333475112915      |-Best step: 0           |-learning_rate: 0.00036276965095376624      |-num_filters_1: 32      |-num_filters_2: 64      |-units: 64     Trial summary      |-Trial ID: 0c621859edde556edb80c57cf0c8266b      |-Score: 0.9247333407402039      |-Best step: 0           |-learning_rate: 0.001453129317743269      |-num_filters_1: 64      |-num_filters_2: 64      |-units: 96     Trial summary      |-Trial ID: 822f72617459e63dd80979cd498fca21      |-Score: 0.9240000247955322      |-Best step: 0           |-learning_rate: 0.0003623815491328145      |-num_filters_1: 64      |-num_filters_2: 32      |-units: 96     Trial summary      |-Trial ID: ad4279e13d037a77eb612f9367544ae9      |-Score: 0.9228666424751282      |-Best step: 0           |-learning_rate: 0.00012165541012624621      |-num_filters_1: 32      |-num_filters_2: 32      |-units: 480     Trial summary      |-Trial ID: 8279e19274848d0d493e7c40cac4713c      |-Score: 0.9226666688919067      |-Best step: 0           |-learning_rate: 0.00013607174450468629      |-num_filters_1: 32      |-num_filters_2: 64      |-units: 288     Trial summary      |-Trial ID: 0bf67cf27d543ea733d47f39075cf186      |-Score: 0.9224666953086853      |-Best step: 0           |-learning_rate: 0.00025071332274895686      |-num_filters_1: 32      |-num_filters_2: 64      |-units: 384     Trial summary      |-Trial ID: 9e654a297fcccf88ea1724cb684f6542      |-Score: 0.9223999977111816      |-Best step: 0           |-learning_rate: 0.00014948684422968163      |-num_filters_1: 32      |-num_filters_2: 64      |-units: 288     Trial summary      |-Trial ID: 9fa074003b81dc588b82694213d0eff8      |-Score: 0.9222000241279602      |-Best step: 0           |-learning_rate: 0.001715074355925934      |-num_filters_1: 64      |-num_filters_2: 32      |-units: 512     Trial summary      |-Trial ID: 3e5e38f8970e988856d96a81dd0cb384      |-Score: 0.9222000241279602      |-Best step: 0           |-learning_rate: 0.0023360487671257027      |-num_filters_1: 32      |-num_filters_2: 32      |-units: 384        <pre><code>best_model = tuner.get_best_models(num_models=1)[0]\n</code></pre>     <pre><code>loss, accuracy = best_model.evaluate(X_test,\n                                     y_test_oh)\n\ncomp = comp.append({'run': 'Optim_CNN', 'Perte' : loss, 'Pr\u00e9cision' : accuracy}, ignore_index=True)\nprint(comp)\n</code></pre>      <pre>\n<code>313/313 [==============================] - 1s 4ms/step - loss: 0.2551 - accuracy: 0.9246\n               run     Perte  Pr\u00e9cision\n0        basic_CNN  0.469638     0.8282\n1           bn_CNN  0.291927     0.8975\n2          bn2_CNN  0.271721     0.9046\n3         drop_CNN  0.497585     0.8187\n4           he_CNN  0.328177     0.8817\n5     momentum_CNN  0.286005     0.8988\n6     nesterov_CNN  0.296967     0.8942\n7          RMS_CNN  0.612474     0.9108\n8         Adam_CNN  0.501753     0.9172\n9   Bn_He_Adam_CNN  0.395749     0.9219\n10       Optim_CNN  0.255052     0.9246\n11      Optim2_CNN  0.584973     0.9308\n12       Optim_CNN  0.255052     0.9246\n</code>\n</pre>        <pre><code>import json\n\nvis_data = []\nrootdir = 'random_search/fashion_mnist'\nfor subdirs, dirs, files in os.walk(rootdir):\n    for file in files:\n        if file.endswith(\"trial.json\"):\n          with open(subdirs + '/' + file, 'r') as json_file:\n            data = json_file.read()\n          vis_data.append(json.loads(data))\n</code></pre>     <pre><code>import hiplot as hip\n\ndata = [{'num_filters_1': vis_data[idx]['hyperparameters']['values']['num_filters_1'],\n         'num_filters_2': vis_data[idx]['hyperparameters']['values']['num_filters_2'],\n         'units': vis_data[idx]['hyperparameters']['values']['units'], \n         'learning_rate': vis_data[idx]['hyperparameters']['values']['learning_rate'], \n         'loss': vis_data[idx]['metrics']['metrics']['loss']['observations'][0]['value'],  \n         'val_loss': vis_data[idx]['metrics']['metrics']['val_loss']['observations'][0]['value'], \n         'accuracy': vis_data[idx]['metrics']['metrics']['accuracy']['observations'][0]['value'],\n         'val_accuracy': vis_data[idx]['metrics']['metrics']['val_accuracy']['observations'][0]['value']} for idx in range(num_of_epochs)]\n\nhip.Experiment.from_iterable(data).display()\n</code></pre>     <pre><code>lr0 = tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values['learning_rate']\nprint(lr0)\n</code></pre>      <pre>\n<code>0.0006752863927347823\n</code>\n</pre>        <pre><code>def exponential_decay(lr0,step):\n    def exponential_decay_fn(epoch):\n        return lr0*0.1**(epoch/step)\n    return exponential_decay_fn\n\nexponential_decay_fn = exponential_decay(lr0 = lr0, step = 20)\n\ncallbacks_fit = [keras.callbacks.EarlyStopping(\n                 # Stop training when `val_loss` is no longer improving\n                 monitor='val_loss',\n                 # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n                 min_delta=1e-2,\n                 # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n                 patience=10,\n                 verbose=1),\n                 keras.callbacks.LearningRateScheduler(exponential_decay_fn),\n                 keras.callbacks.ModelCheckpoint('./weights.{epoch:02d}-.hdf5',\n                                                 monitor='val_loss',\n                                                 verbose=1,\n                                                 save_best_only=True,\n                                                 save_weights_only=False,\n                                                 mode='auto',\n                                                 save_freq='epoch')]\n</code></pre>     <pre><code>history = best_model.fit(X_train,\n                         y_train_oh,\n                         validation_data=(X_valid, y_valid_oh),\n                         batch_size=64,\n                         epochs=100,\n                         initial_epoch=10,\n                         callbacks=callbacks_fit,\n                         verbose=2)\n</code></pre>     <pre><code>loss, accuracy = best_model.evaluate(X_test,\n                                     y_test_oh)\n\ncomp = comp.append({'run': 'Optim2_CNN', 'Perte' : loss, 'Pr\u00e9cision' : accuracy}, ignore_index=True)\nprint(comp)\n</code></pre>      <pre>\n<code>313/313 [==============================] - 1s 4ms/step - loss: 0.5850 - accuracy: 0.9308\n               run     Perte  Pr\u00e9cision\n0        basic_CNN  0.469638     0.8282\n1           bn_CNN  0.291927     0.8975\n2          bn2_CNN  0.271721     0.9046\n3         drop_CNN  0.497585     0.8187\n4           he_CNN  0.328177     0.8817\n5     momentum_CNN  0.286005     0.8988\n6     nesterov_CNN  0.296967     0.8942\n7          RMS_CNN  0.612474     0.9108\n8         Adam_CNN  0.501753     0.9172\n9   Bn_He_Adam_CNN  0.395749     0.9219\n10       Optim_CNN  0.255052     0.9246\n11      Optim2_CNN  0.584973     0.9308\n</code>\n</pre>"},{"location":"deep_learning/module4/Module4_2/#comparaison","title":"Comparaison","text":"<pre><code># import plotly.express as px\n# fig = px.scatter(comp, \n#                 x=\"Pr\u00e9cision\", \n#                 y=\"Perte\",\n#                 color=\"run\")\n# fig.show()\n</code></pre>          if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'};    <pre><code>        &lt;script type=\"text/javascript\"&gt;\n\n                window.PLOTLYENV=window.PLOTLYENV || {};\n\n            if (document.getElementById(\"d3a19371-4742-4764-85ae-aa0c67d28833\")) {\n                Plotly.newPlot(\n                    'd3a19371-4742-4764-85ae-aa0c67d28833',\n                    [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"run=basic_CNN&lt;br&gt;Pr\\u00e9cision=%{x}&lt;br&gt;Perte=%{y}\", \"legendgroup\": \"run=basic_CNN\", \"marker\": {\"color\": \"#636efa\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"run=basic_CNN\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0.8281999826431274], \"xaxis\": \"x\", \"y\": [0.46963778138160706], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"run=bn_CNN&lt;br&gt;Pr\\u00e9cision=%{x}&lt;br&gt;Perte=%{y}\", \"legendgroup\": \"run=bn_CNN\", \"marker\": {\"color\": \"#EF553B\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"run=bn_CNN\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0.8974999785423279], \"xaxis\": \"x\", \"y\": [0.29192668199539185], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"run=bn2_CNN&lt;br&gt;Pr\\u00e9cision=%{x}&lt;br&gt;Perte=%{y}\", \"legendgroup\": \"run=bn2_CNN\", \"marker\": {\"color\": \"#00cc96\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"run=bn2_CNN\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0.9046000242233276], \"xaxis\": \"x\", \"y\": [0.27172139286994934], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"run=drop_CNN&lt;br&gt;Pr\\u00e9cision=%{x}&lt;br&gt;Perte=%{y}\", \"legendgroup\": \"run=drop_CNN\", \"marker\": {\"color\": \"#ab63fa\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"run=drop_CNN\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0.8187000155448914], \"xaxis\": \"x\", \"y\": [0.49758535623550415], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"run=he_CNN&lt;br&gt;Pr\\u00e9cision=%{x}&lt;br&gt;Perte=%{y}\", \"legendgroup\": \"run=he_CNN\", \"marker\": {\"color\": \"#FFA15A\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"run=he_CNN\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0.8816999793052673], \"xaxis\": \"x\", \"y\": [0.32817697525024414], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"run=momentum_CNN&lt;br&gt;Pr\\u00e9cision=%{x}&lt;br&gt;Perte=%{y}\", \"legendgroup\": \"run=momentum_CNN\", \"marker\": {\"color\": \"#19d3f3\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"run=momentum_CNN\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0.8988000154495239], \"xaxis\": \"x\", \"y\": [0.2860051989555359], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"run=nesterov_CNN&lt;br&gt;Pr\\u00e9cision=%{x}&lt;br&gt;Perte=%{y}\", \"legendgroup\": \"run=nesterov_CNN\", \"marker\": {\"color\": \"#FF6692\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"run=nesterov_CNN\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0.8942000269889832], \"xaxis\": \"x\", \"y\": [0.29696694016456604], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"run=RMS_CNN&lt;br&gt;Pr\\u00e9cision=%{x}&lt;br&gt;Perte=%{y}\", \"legendgroup\": \"run=RMS_CNN\", \"marker\": {\"color\": \"#B6E880\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"run=RMS_CNN\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0.9107999801635742], \"xaxis\": \"x\", \"y\": [0.6124737858772278], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"run=Adam_CNN&lt;br&gt;Pr\\u00e9cision=%{x}&lt;br&gt;Perte=%{y}\", \"legendgroup\": \"run=Adam_CNN\", \"marker\": {\"color\": \"#FF97FF\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"run=Adam_CNN\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0.9172000288963318], \"xaxis\": \"x\", \"y\": [0.5017529129981995], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"run=Bn_He_Adam_CNN&lt;br&gt;Pr\\u00e9cision=%{x}&lt;br&gt;Perte=%{y}\", \"legendgroup\": \"run=Bn_He_Adam_CNN\", \"marker\": {\"color\": \"#FECB52\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"run=Bn_He_Adam_CNN\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0.9218999743461609], \"xaxis\": \"x\", \"y\": [0.39574897289276123], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"run=Optim_CNN&lt;br&gt;Pr\\u00e9cision=%{x}&lt;br&gt;Perte=%{y}\", \"legendgroup\": \"run=Optim_CNN\", \"marker\": {\"color\": \"#636efa\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"run=Optim_CNN\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0.9246000051498413, 0.9246000051498413], \"xaxis\": \"x\", \"y\": [0.25505155324935913, 0.25505155324935913], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"run=Optim2_CNN&lt;br&gt;Pr\\u00e9cision=%{x}&lt;br&gt;Perte=%{y}\", \"legendgroup\": \"run=Optim2_CNN\", \"marker\": {\"color\": \"#EF553B\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"run=Optim2_CNN\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0.9308000206947327], \"xaxis\": \"x\", \"y\": [0.5849732756614685], \"yaxis\": \"y\"}],\n                    {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Pr\\u00e9cision\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Perte\"}}},\n                    {\"responsive\": true}\n                ).then(function(){\n</code></pre> <p>var gd = document.getElementById('d3a19371-4742-4764-85ae-aa0c67d28833'); var x = new MutationObserver(function (mutations, observer) {{         var display = window.getComputedStyle(gd).display;         if (!display || display === 'none') {{             console.log([gd, 'removed!']);             Plotly.purge(gd);             observer.disconnect();         }} }});</p> <p>// Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{     x.observe(notebookContainer, {childList: true}); }}</p> <p>// Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{     x.observe(outputEl, {childList: true}); }}</p> <pre><code>                    })\n            };\n\n        &lt;/script&gt;\n</code></pre>"},{"location":"deep_learning/module5/Module5/","title":"Module 5 : Personnaliser son r\u00e9seau de neurones","text":""},{"location":"deep_learning/module5/Module5/#les-nouvelles-architectures-et-les-nouvelles-operations-associees","title":"Les nouvelles architectures et les nouvelles op\u00e9rations associ\u00e9es","text":""},{"location":"deep_learning/module5/Module5/#conv-1-times-1","title":"Conv \\(1 \\times 1\\)","text":"<p>Le probl\u00e8me des architectures classiques telles que VGGNet provient en fait de la couche classifiante dense \u00e0 la fin du mod\u00e8le. C'est cette partie l\u00e0 qui a fortement tendance au sur-apprentissage.</p> <p>L'id\u00e9e est alors ne plus mettre de couche dense, ou le moins possible.</p>  <p>Citation</p> <p>In Convolutional Nets, there is no such thing as \"fully-connected layers\". There are only convolution layers with \\(1 \\times 1\\) convolution kernels and a full connection table.</p> <p>It's a too-rarely-understood fact that ConvNets don't need to have a fixed-size input. You can train them on inputs that happen to produce a single output vector (with no spatial extent), and then apply them to larger images. Instead of a single output vector, you then get a spatial map of output vectors. Each vector sees input windows at different locations on the input.</p> <p>In that scenario, the \"fully connected layers\" really act as \\(1 \\times 1\\) convolutions.</p> <p>Yann LeCun</p>  <p>En d'autres termes, pour remplacer les r\u00e9seaux de neurones denses, on utilise des couches convolutives avec des noyaux de taille \\(1 \\times 1\\).</p> <ol> <li> <p>Ce type de convolution ne peut pas capturer de features g\u00e9om\u00e9triques, puisque le noyau regarde un pixel \u00e0 la fois, les dimensions g\u00e9om\u00e9triques des features maps ne sont donc pas affect\u00e9es par cette convolution.</p> </li> <li> <p>Cependant, ces convolutions sont capables de capturer les features spatiales sur plusieurs canaux.</p> </li> <li> <p>Elles permettent de r\u00e9duire le nombre de features maps, et agissent donc comme un \"goul\u00f4t d'\u00e9tranglement\" qui permet d'augmenter la vitesse de calcul.</p> </li> </ol>   <p>Application d'une convolution \\(1\\times1\\)</p>  <p>Notons \\(p_{i,j}(F_{k})\\) le pixel \u00e0 la coordon\u00e9e \\((i,j)\\) dans la feature map \\(F_{k}\\).</p> <p>Chacun des pixels obtenus dans la feature map en sortie est alors une combinaison lin\u00e9aire des pixels aux m\u00eames coordonn\u00e9es dans les features maps d'entr\u00e9e. Les coefficients de la combinaison lin\u00e9aire \u00e9tant appris par le r\u00e9seau et les m\u00eames pour tous les pixels de la feature map de sorite, ce sont les coefficients \\((w_{1}^{1}, w_{2}^{1}, w_{3}^{1})\\) du filtre de la convolution \\(1\\times1\\).</p>  <p>Dans les architectures modernes, les couches denses sont le plus souvent r\u00e9duit au strict minimum, une unique couche en sortie avec le nombre de classes d\u00e9sir\u00e9es.</p>"},{"location":"deep_learning/module5/Module5/#globalavgpooling","title":"GlobalAvgPooling","text":"<p>Pour r\u00e9duire les dimensions des features, il est aussi possible de passer par les op\u00e9rations de pooling global. Un GlobalAvgPooling ou un GlobalMaxPooling calcul la statistique demand\u00e9e sur la feature map compl\u00e8te.</p>  <p>Global Average Pooling</p>   <p>Bien que cela soit assez destructeur, les op\u00e9rations globales de pooling, et surtout GlobalAvgPooling, permettent d'obtenir des propri\u00e9t\u00e9s de localisation int\u00e9ressante de la part des CNNs.</p>"},{"location":"deep_learning/module5/Module5/#resnet","title":"ResNet","text":"<p>Les mod\u00e8les ayant des architectures de plus en plus profondes, il est n\u00e9c\u00e9ssaire de comprendre comment le fait d'ajouter de nouvelles couches peut augmenter la compl\u00e9xit\u00e9 du mod\u00e8le et le rendre plus expressif. Rendre un mod\u00e8le plus expressif \u00e9tant plus important que de le rendre unique, il faut \u00eatre s\u00fbr que les couches que nous rajoutons soient utiles.</p> <p>Consid\u00e9rons \\(\\mathcal{F}\\), la classe des fonctions quu'une architecture sp\u00e9cifique peut approximer, ie, pour tout fonction \\(f\\) appartenant \u00e0 \\(\\mathcal{F}\\), il existe des param\u00e8tres \\(\\vartheta\\) qui peuvent \u00eatre obtenus sur un dataset particulier permettant \u00e0 cette architecture d'approximer \\(f\\).</p> <p>Si \\(f^{\\ast}\\) est la vraie fonction recherch\u00e9e et qu'elle est dans \\(\\mathcal{F}\\), on est alors en bonne position pour l'approximer, mais c'est rarement le cas.</p> <p>On peut alors consid\u00e9rer \\(f^{\\ast}_{\\mathcal{F}}\\), la meilleure approximation possible de \\(f^{\\ast}\\) dans \\(\\mathcal{F}\\), ie</p> \\[     f^{\\ast}_{\\mathcal{F}} := \\underset{f \\in \\mathcal{F}}{\\text{argmin}} \\mathcal{L}_{\\vartheta}(y, \\hat{y}, f) \\] <p>Supposons que l'on d\u00e9finisse une architecture plus complexe aboutissant \u00e0 une nouvelle classe \\(\\mathcal{F}'\\). Si \\(\\mathcal{F} \\not\\subset \\mathcal{F}'\\), il n'y a aucune garantie que \\(f^{\\ast}_{\\mathcal{F}'}\\) soit mieux ou plus facile \u00e0 trouver que \\(f^{\\ast}_{\\mathcal{F}}\\).</p>  <p>Remarque</p> <p>C'est la situation rencontr\u00e9e par He &amp; al. dans l'article Deep Residual Learning for Image Recognition : rajouter plus de couches de fa\u00e7on na\u00efve ne fait qu'empirer le probl\u00e8me d'approximation.</p>  <p>L'id\u00e9e de l'article est alors la suivante : tout bloc de couches rajout\u00e9es devrait \u00eatre capable d'approximer l'identit\u00e9 \\(f(x)=x\\). Ainsi, s'i n'apporte rien de plus et que l'architecture \u00e9tait d\u00e9j\u00e0 optimale, le rajout de ces couches n'aura que peu d'impact. C'est le principe des blocs r\u00e9siduels.</p> <p>Voici ce qu'ils disent dans l'article.</p>  <p>R\u00e9sum\u00e9</p> <ol> <li> <p>Driven by the significance of depth, a question arises : Is learning better networks as easy as stacking more layers ? An obstacle to answering this question was the notorious problem of vanishing/exploding gradient [...]. This problem, however, has been largely addressed by normalized initilization and intermediate normalization layers. (ie BatchNorm et initialisation des poids)</p> </li> <li> <p>When deeper networks are able to start converging, a degradation problem has been exposed : : with the network depth increasing, accuracy gets saturated (...) and then degrades rapidly. Unexpectedly, such a degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error.</p> </li> <li> <p>We show that :</p> <ol> <li> <p>Our extremely deep residual nets are esay to optimize, but the counterpart \"plain\" nets (that simply stacks layers) exhibit higher training error when the depth increases.</p> </li> <li> <p>Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks.</p> </li> </ol> </li> <li> <p>Our 152-layers residual net is the deeper network ever presented on ImageNet (2015), while still having lower complexity than VGG nets.</p> </li> <li> <p>The degradation problem suggests that the solvers (ie weights optimization) might have difficulties in approximating identity mappings by multiple nonlinear layers.</p> </li> </ol>"},{"location":"deep_learning/module5/Module5/#architecture","title":"Architecture","text":""},{"location":"deep_learning/module5/Module5/#bloc-de-base","title":"Bloc de base","text":""},{"location":"deep_learning/module5/Module5/#bloc-identite","title":"Bloc identit\u00e9","text":""},{"location":"deep_learning/module5/Module5/#bloc-projection","title":"Bloc projection","text":""},{"location":"deep_learning/module5/Module5/#bloc-resnet","title":"Bloc ResNet","text":""},{"location":"deep_learning/module5/Module5/#densenet","title":"DenseNet","text":"<p>ResNet a radicalement chang\u00e9 la vision que l'on avait de la construction des r\u00e9seaux de neurones profonds.</p> <p>DenseNet, introduit par Huang &amp; al. dans Densely Connected Convolutional Networks est l'extension logique de ResNet. La diff\u00e9rence majeure avec ResNet est que DenseNet d\u00e9compose la fonction approxim\u00e9e en termes de compl\u00e9xit\u00e9 croissante non plus en utilisant l'addiction, mais la concat\u00e9nation cette fois ci.</p> <p>Voici ce qu'ils disent dans l'article.</p>  <p>R\u00e9sum\u00e9</p> <ol> <li> <p>As CNNs become increasingly deep, a new research problem emerges : as information about the input of gradient passes through many layers, it can vanish and \"wash out\" by the time it reaches the end (or beginning) of the network.</p> </li> <li> <p>to ensure maximum information flow between layers in the network, we connect all layers (with matching feature maps sizes) directly with each other.</p> </li> <li> <p>Crucially, in contrast to ResNets, we never combine features through summation before they are passed into a layer, instead, we combine features by concatenating them.</p> </li> <li> <p>the final classifier makes a decision based on all features maps in the network.</p> </li> </ol>"},{"location":"deep_learning/module5/Module5/#architecture_1","title":"Architecture","text":""},{"location":"deep_learning/module5/Module5/#bloc-dense","title":"Bloc dense","text":""},{"location":"deep_learning/module5/Module5/#bloc-transition","title":"Bloc transition","text":""},{"location":"deep_learning/module5/Module5_2/","title":"Pratique","text":"(function() {   function addWidgetsRenderer() {     var requireJsScript = document.createElement('script');     requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js';      var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]');     var jupyterWidgetsScript = document.createElement('script');     var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js';     var widgetState;      // Fallback for older version:     try {       widgetState = mimeElement &amp;&amp; JSON.parse(mimeElement.innerHTML);        if (widgetState &amp;&amp; (widgetState.version_major &lt; 2 || !widgetState.version_major)) {         widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js';       }     } catch(e) {}      jupyterWidgetsScript.src = widgetRendererSrc;      document.body.appendChild(requireJsScript);     document.body.appendChild(jupyterWidgetsScript);   }    document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }());"},{"location":"deep_learning/module5/Module5_2/#tp-module-5-personnaliser-son-reseau-de-neurones","title":"TP Module 5 : Personnaliser son r\u00e9seau de neurones","text":"<pre><code>import tensorflow as tf\nfrom tensorflow import keras\n\nprint(tf.__version__)\nprint(keras.__version__)\n\n# Splitting\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nimport os\nimport numpy as np\n\n# freeze de l'al\u00e9atoire, pour avoir des exp\u00e9riences reproductibles.\nRANDOM_SEED = 42\n\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\ntf.random.set_seed(RANDOM_SEED)\n</code></pre>      <pre>\n<code>2.2.0\n2.3.0-tf\n</code>\n</pre>        <pre><code>from tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import ReLU\nfrom tensorflow.keras.layers import Add\nfrom tensorflow.keras.layers import MaxPool2D\nfrom tensorflow.keras.layers import GlobalAvgPool2D\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import Flatten\n</code></pre>     <pre><code>!nvidia-smi\n</code></pre>      <pre>\n<code>Mon May 18 12:33:57 2020       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   34C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code>\n</pre>"},{"location":"deep_learning/module5/Module5_2/#import-dataset","title":"Import Dataset","text":"<pre><code>(X_train,y_train), (X_test,y_test)  = tf.keras.datasets.cifar100.load_data()\n\nprint(X_train.shape, y_train.shape)\n</code></pre>      <pre>\n<code>Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n169009152/169001437 [==============================] - 5s 0us/step\n(50000, 32, 32, 3) (50000, 1)\n</code>\n</pre>        <pre><code>X_train = X_train.reshape(-1, 32, 32, 3).astype('float32')\nX_test = X_test.reshape(-1, 32, 32, 3).astype('float32')\n\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, random_state=RANDOM_SEED)\n\nX_test = X_test/255\nX_train = X_train/255\nX_valid = X_valid/255\n\ny_train_oh = tf.keras.utils.to_categorical(y_train, num_classes=100)\ny_test_oh = tf.keras.utils.to_categorical(y_test, num_classes=100)\ny_valid_oh = tf.keras.utils.to_categorical(y_valid, num_classes=100)\n</code></pre>     <pre><code>AUTOTUNE = tf.data.experimental.AUTOTUNE\n\ndef train_preprocess(image, label):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n\n    #image = tf.image.random_brightness(image, max_delta=32.0 / 255.0)\n    #image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n\n    #Make sure the image is still in [0, 1]\n    image = tf.clip_by_value(image, 0.0, 1.0)\n\n    return image, label\n\ndef create_train_dataset(features, labels, batch=64, repet=1, prefetch=1):\n    dataset = tf.data.Dataset.from_tensor_slices((features,labels))\n    dataset = dataset.shuffle(len(features), seed=RANDOM_SEED)\n    dataset = dataset.repeat(repet)\n    dataset = dataset.map(train_preprocess, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.batch(batch)\n    dataset = dataset.prefetch(prefetch)\n    return dataset\n\ndef create_test_dataset(features, labels, batch=64, repet=1, prefetch=1):\n    dataset = tf.data.Dataset.from_tensor_slices((features,labels))\n    dataset = dataset.shuffle(len(features), seed=RANDOM_SEED)\n    dataset = dataset.repeat(repet)\n    dataset = dataset.batch(batch)\n    dataset = dataset.prefetch(prefetch)\n    return dataset\n\nds_train = create_train_dataset(X_train, y_train_oh)\nds_val = create_test_dataset(X_valid, y_valid_oh)\nds_test = create_test_dataset(X_test, y_test_oh)\n</code></pre>"},{"location":"deep_learning/module5/Module5_2/#les-architectures-modernes","title":"Les architectures modernes","text":"<p>In Convolutional Nets, there is no such thing as \"fully-connected layers\". There are only convolution layers with \\(1 \\times 1\\) convolution kernels and a full connection table.</p> <p>It's a too-rarely-understood fact that ConvNets don't need to have a fixed-size input. You can train them on inputs that happen to produce a single output vector (with no spatial extent), and then apply them to larger images. Instead of a single output vector, you then get a spatial map of output vectors. Each vector sees input windows at different locations on the input.</p> <p>In that scenario, the \"fully connected layers\" really act as \\(1 \\times 1\\) convolutions.</p> <p>Yann LeCun</p>"},{"location":"deep_learning/module5/Module5_2/#resnet","title":"ResNet","text":""},{"location":"deep_learning/module5/Module5_2/#idee","title":"Id\u00e9e","text":"<ul> <li> <p>Driven by the significance of depth, a question arises : Is learning better networks as easy as stacking more layers ? An obstacle to answering this question was the notorious problem of vanishing/exploding gradient [...]. This problem, however, has been largely addressed by normalized initilization and intermediate normalization layers. (ie BatchNorm et initialisation des poids)</p> </li> <li> <p>When deeper networks are able to start converging, a degradation problem has been exposed : : with the network depth increasing, accuracy gets saturated (...) and then degrades rapidly. Unexpectedly, such a degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error.</p> </li> <li> <p>We show that :</p> </li> <li>Our extremely deep residual nets are esay to optimize, but the counterpart \"plain\" nets (that simply stacks layers) exhibit higher training error when the depth increases.</li> <li> <p>Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks.</p> </li> <li> <p>Our 152-layers residual net is the deeper network ever presented on ImageNet (2015), while still having lower complexity than VGG nets.</p> </li> <li> <p>The degradation problem suggests that the solvers (ie weights optimization) might have difficulties in approximating identity mappings by multiple nonlinear layers.</p> </li> </ul>"},{"location":"deep_learning/module5/Module5_2/#definition-des-briques-de-bases","title":"D\u00e9finition des briques de bases","text":"<ul> <li>We adopt batch normalization (BN) right after each convolution and before activation.</li> </ul>      <pre><code>def conv_batchnorm_relu(x, filters, kernel_size, strides):\n  x = Conv2D(filters=filters,\n             kernel_size=kernel_size,\n             strides=strides,\n             padding='same',\n             kernel_initializer=\"he_normal\",\n             use_bias = False)(x)\n  x = BatchNormalization()(x)\n  x = ReLU()(x)\n  return x\n</code></pre>"},{"location":"deep_learning/module5/Module5_2/#bloc-identite","title":"Bloc Identit\u00e9","text":"<ul> <li> <p>The three layers are \\(1 \\times 1\\), \\(3 \\times 3\\), and \\(1 \\times 1\\) convolutions, where the \\(1 \\times 1\\) layers are responsible for reducing and then increasing (restoring) dimensions, leaving the \\(3 \\times 3\\) layer a bottleneck with smaller input/output dimensions.</p> </li> <li> <p>\\(50\\)-layer ResNet: We replace each \\(2\\)-layer block in the \\(34\\)-layer net with this \\(3\\)-layer bottleneck block, resulting in a \\(50\\)-layer ResNet (Table 1). We use option B for increasing dimensions (ie projection blocks).</p> </li> </ul>      <pre><code>def identity_block(tensor, filters):\n  x = conv_batchnorm_relu(tensor,\n                          filters=filters,\n                          kernel_size=1,\n                          strides=1)\n  #print(x.shape)\n  x = conv_batchnorm_relu(x,\n                          filters=filters,\n                          kernel_size=3,\n                          strides=1)\n  #print(x.shape)\n  x = Conv2D(filters=4*filters,\n             kernel_size=1, \n             strides=1,\n             kernel_initializer=\"he_normal\")(x)  # notice: filters=4*filters\n  #print(x.shape)\n  x = BatchNormalization()(x)\n\n  x = Add()([x, tensor])\n  x = ReLU()(x)\n  return x\n</code></pre>"},{"location":"deep_learning/module5/Module5_2/#bloc-projection","title":"Bloc Projection","text":"<ul> <li> <p>The projection shortcut in Eqn.(2) is used to match dimensions (done by \\(1 \\times 1\\) convolutions). For both options, when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2</p> </li> <li> <p>[...] Projection shortcuts are used for increasing dimensions, and other shortcuts are identity;</p> </li> </ul>      <pre><code>def projection_block(tensor, filters, strides):\n  # left stream\n  x = conv_batchnorm_relu(tensor, \n                          filters=filters, \n                          kernel_size=1,\n                          strides=strides)\n  x = conv_batchnorm_relu(x,\n                          filters=filters,\n                          kernel_size=3,\n                          strides=1)\n  x = Conv2D(filters=4*filters,\n             kernel_size=1,\n             strides=1,\n             kernel_initializer=\"he_normal\")(x)  # notice: filters=4*filters\n  x = BatchNormalization()(x)\n\n  # right stream\n  shortcut = Conv2D(filters=4*filters,\n                    kernel_size=1,\n                    strides=strides,\n                    kernel_initializer=\"he_normal\")(tensor)  # notice: filters=4*filters\n  shortcut = BatchNormalization()(shortcut)\n\n  x = Add()([x, shortcut])\n  x = ReLU()(x)\n  return x\n</code></pre>"},{"location":"deep_learning/module5/Module5_2/#bloc-resnet","title":"Bloc ResNet","text":"<ul> <li>Donwsampling is performed by conv3_1, conv4_1, and conv5_1 with a stride of 2.</li> </ul>      <pre><code>def resnet_block(x, filters, reps, strides):\n  x = projection_block(x, filters=filters, strides=strides)\n  for _ in range(reps-1):\n    x = identity_block(x, filters=filters)\n  return x\n</code></pre>     <pre><code>input = Input(shape=(32, 32, 3))\n\nx = conv_batchnorm_relu(input, filters=64, kernel_size=7, strides=2)  # [3]: 7x7, 64, strides 2\nx = MaxPool2D(pool_size=3, strides=2, padding='same')(x)  # [3]: 3x3 max pool, strides 2\n\nx = resnet_block(x, filters=64, reps=3, strides=1)\nx = resnet_block(x, filters=128, reps=4, strides=2)  # strides=2 ([2]: conv3_1)\nx = resnet_block(x, filters=256, reps=6, strides=2)  # strides=2 ([2]: conv4_1)\nx = resnet_block(x, filters=512, reps=3, strides=2)  # strides=2 ([2]: conv5_1)\n\nx = GlobalAvgPool2D()(x)  # [3]: average pool *it is not written any pool size so we use Global\nx = Dense(100)(x)\noutput = Activation('softmax')(x)  # [3]: 1000-d fc, softmax\n\nfrom tensorflow.keras import Model\n\nmodel = Model(input, output)\n</code></pre>     <pre><code>model.summary()\n</code></pre>      <pre>\n<code>Model: \"model_2\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_4 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n__________________________________________________________________________________________________\nconv2d_167 (Conv2D)             (None, 16, 16, 64)   9408        input_4[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_166 (BatchN (None, 16, 16, 64)   256         conv2d_167[0][0]                 \n__________________________________________________________________________________________________\nre_lu_152 (ReLU)                (None, 16, 16, 64)   0           batch_normalization_166[0][0]    \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 64)     0           re_lu_152[0][0]                  \n__________________________________________________________________________________________________\nconv2d_168 (Conv2D)             (None, 8, 8, 64)     4096        max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nbatch_normalization_167 (BatchN (None, 8, 8, 64)     256         conv2d_168[0][0]                 \n__________________________________________________________________________________________________\nre_lu_153 (ReLU)                (None, 8, 8, 64)     0           batch_normalization_167[0][0]    \n__________________________________________________________________________________________________\nconv2d_169 (Conv2D)             (None, 8, 8, 64)     36864       re_lu_153[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_168 (BatchN (None, 8, 8, 64)     256         conv2d_169[0][0]                 \n__________________________________________________________________________________________________\nre_lu_154 (ReLU)                (None, 8, 8, 64)     0           batch_normalization_168[0][0]    \n__________________________________________________________________________________________________\nconv2d_170 (Conv2D)             (None, 8, 8, 256)    16640       re_lu_154[0][0]                  \n__________________________________________________________________________________________________\nconv2d_171 (Conv2D)             (None, 8, 8, 256)    16640       max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nbatch_normalization_169 (BatchN (None, 8, 8, 256)    1024        conv2d_170[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_170 (BatchN (None, 8, 8, 256)    1024        conv2d_171[0][0]                 \n__________________________________________________________________________________________________\nadd_50 (Add)                    (None, 8, 8, 256)    0           batch_normalization_169[0][0]    \n                                                                 batch_normalization_170[0][0]    \n__________________________________________________________________________________________________\nre_lu_155 (ReLU)                (None, 8, 8, 256)    0           add_50[0][0]                     \n__________________________________________________________________________________________________\nconv2d_172 (Conv2D)             (None, 8, 8, 64)     16384       re_lu_155[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_171 (BatchN (None, 8, 8, 64)     256         conv2d_172[0][0]                 \n__________________________________________________________________________________________________\nre_lu_156 (ReLU)                (None, 8, 8, 64)     0           batch_normalization_171[0][0]    \n__________________________________________________________________________________________________\nconv2d_173 (Conv2D)             (None, 8, 8, 64)     36864       re_lu_156[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_172 (BatchN (None, 8, 8, 64)     256         conv2d_173[0][0]                 \n__________________________________________________________________________________________________\nre_lu_157 (ReLU)                (None, 8, 8, 64)     0           batch_normalization_172[0][0]    \n__________________________________________________________________________________________________\nconv2d_174 (Conv2D)             (None, 8, 8, 256)    16640       re_lu_157[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_173 (BatchN (None, 8, 8, 256)    1024        conv2d_174[0][0]                 \n__________________________________________________________________________________________________\nadd_51 (Add)                    (None, 8, 8, 256)    0           batch_normalization_173[0][0]    \n                                                                 re_lu_155[0][0]                  \n__________________________________________________________________________________________________\nre_lu_158 (ReLU)                (None, 8, 8, 256)    0           add_51[0][0]                     \n__________________________________________________________________________________________________\nconv2d_175 (Conv2D)             (None, 8, 8, 64)     16384       re_lu_158[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_174 (BatchN (None, 8, 8, 64)     256         conv2d_175[0][0]                 \n__________________________________________________________________________________________________\nre_lu_159 (ReLU)                (None, 8, 8, 64)     0           batch_normalization_174[0][0]    \n__________________________________________________________________________________________________\nconv2d_176 (Conv2D)             (None, 8, 8, 64)     36864       re_lu_159[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_175 (BatchN (None, 8, 8, 64)     256         conv2d_176[0][0]                 \n__________________________________________________________________________________________________\nre_lu_160 (ReLU)                (None, 8, 8, 64)     0           batch_normalization_175[0][0]    \n__________________________________________________________________________________________________\nconv2d_177 (Conv2D)             (None, 8, 8, 256)    16640       re_lu_160[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_176 (BatchN (None, 8, 8, 256)    1024        conv2d_177[0][0]                 \n__________________________________________________________________________________________________\nadd_52 (Add)                    (None, 8, 8, 256)    0           batch_normalization_176[0][0]    \n                                                                 re_lu_158[0][0]                  \n__________________________________________________________________________________________________\nre_lu_161 (ReLU)                (None, 8, 8, 256)    0           add_52[0][0]                     \n__________________________________________________________________________________________________\nconv2d_178 (Conv2D)             (None, 4, 4, 128)    32768       re_lu_161[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_177 (BatchN (None, 4, 4, 128)    512         conv2d_178[0][0]                 \n__________________________________________________________________________________________________\nre_lu_162 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_177[0][0]    \n__________________________________________________________________________________________________\nconv2d_179 (Conv2D)             (None, 4, 4, 128)    147456      re_lu_162[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_178 (BatchN (None, 4, 4, 128)    512         conv2d_179[0][0]                 \n__________________________________________________________________________________________________\nre_lu_163 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_178[0][0]    \n__________________________________________________________________________________________________\nconv2d_180 (Conv2D)             (None, 4, 4, 512)    66048       re_lu_163[0][0]                  \n__________________________________________________________________________________________________\nconv2d_181 (Conv2D)             (None, 4, 4, 512)    131584      re_lu_161[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_179 (BatchN (None, 4, 4, 512)    2048        conv2d_180[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_180 (BatchN (None, 4, 4, 512)    2048        conv2d_181[0][0]                 \n__________________________________________________________________________________________________\nadd_53 (Add)                    (None, 4, 4, 512)    0           batch_normalization_179[0][0]    \n                                                                 batch_normalization_180[0][0]    \n__________________________________________________________________________________________________\nre_lu_164 (ReLU)                (None, 4, 4, 512)    0           add_53[0][0]                     \n__________________________________________________________________________________________________\nconv2d_182 (Conv2D)             (None, 4, 4, 128)    65536       re_lu_164[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_181 (BatchN (None, 4, 4, 128)    512         conv2d_182[0][0]                 \n__________________________________________________________________________________________________\nre_lu_165 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_181[0][0]    \n__________________________________________________________________________________________________\nconv2d_183 (Conv2D)             (None, 4, 4, 128)    147456      re_lu_165[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_182 (BatchN (None, 4, 4, 128)    512         conv2d_183[0][0]                 \n__________________________________________________________________________________________________\nre_lu_166 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_182[0][0]    \n__________________________________________________________________________________________________\nconv2d_184 (Conv2D)             (None, 4, 4, 512)    66048       re_lu_166[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_183 (BatchN (None, 4, 4, 512)    2048        conv2d_184[0][0]                 \n__________________________________________________________________________________________________\nadd_54 (Add)                    (None, 4, 4, 512)    0           batch_normalization_183[0][0]    \n                                                                 re_lu_164[0][0]                  \n__________________________________________________________________________________________________\nre_lu_167 (ReLU)                (None, 4, 4, 512)    0           add_54[0][0]                     \n__________________________________________________________________________________________________\nconv2d_185 (Conv2D)             (None, 4, 4, 128)    65536       re_lu_167[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_184 (BatchN (None, 4, 4, 128)    512         conv2d_185[0][0]                 \n__________________________________________________________________________________________________\nre_lu_168 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_184[0][0]    \n__________________________________________________________________________________________________\nconv2d_186 (Conv2D)             (None, 4, 4, 128)    147456      re_lu_168[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_185 (BatchN (None, 4, 4, 128)    512         conv2d_186[0][0]                 \n__________________________________________________________________________________________________\nre_lu_169 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_185[0][0]    \n__________________________________________________________________________________________________\nconv2d_187 (Conv2D)             (None, 4, 4, 512)    66048       re_lu_169[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_186 (BatchN (None, 4, 4, 512)    2048        conv2d_187[0][0]                 \n__________________________________________________________________________________________________\nadd_55 (Add)                    (None, 4, 4, 512)    0           batch_normalization_186[0][0]    \n                                                                 re_lu_167[0][0]                  \n__________________________________________________________________________________________________\nre_lu_170 (ReLU)                (None, 4, 4, 512)    0           add_55[0][0]                     \n__________________________________________________________________________________________________\nconv2d_188 (Conv2D)             (None, 4, 4, 128)    65536       re_lu_170[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_187 (BatchN (None, 4, 4, 128)    512         conv2d_188[0][0]                 \n__________________________________________________________________________________________________\nre_lu_171 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_187[0][0]    \n__________________________________________________________________________________________________\nconv2d_189 (Conv2D)             (None, 4, 4, 128)    147456      re_lu_171[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_188 (BatchN (None, 4, 4, 128)    512         conv2d_189[0][0]                 \n__________________________________________________________________________________________________\nre_lu_172 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_188[0][0]    \n__________________________________________________________________________________________________\nconv2d_190 (Conv2D)             (None, 4, 4, 512)    66048       re_lu_172[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_189 (BatchN (None, 4, 4, 512)    2048        conv2d_190[0][0]                 \n__________________________________________________________________________________________________\nadd_56 (Add)                    (None, 4, 4, 512)    0           batch_normalization_189[0][0]    \n                                                                 re_lu_170[0][0]                  \n__________________________________________________________________________________________________\nre_lu_173 (ReLU)                (None, 4, 4, 512)    0           add_56[0][0]                     \n__________________________________________________________________________________________________\nconv2d_191 (Conv2D)             (None, 2, 2, 256)    131072      re_lu_173[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_190 (BatchN (None, 2, 2, 256)    1024        conv2d_191[0][0]                 \n__________________________________________________________________________________________________\nre_lu_174 (ReLU)                (None, 2, 2, 256)    0           batch_normalization_190[0][0]    \n__________________________________________________________________________________________________\nconv2d_192 (Conv2D)             (None, 2, 2, 256)    589824      re_lu_174[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_191 (BatchN (None, 2, 2, 256)    1024        conv2d_192[0][0]                 \n__________________________________________________________________________________________________\nre_lu_175 (ReLU)                (None, 2, 2, 256)    0           batch_normalization_191[0][0]    \n__________________________________________________________________________________________________\nconv2d_193 (Conv2D)             (None, 2, 2, 1024)   263168      re_lu_175[0][0]                  \n__________________________________________________________________________________________________\nconv2d_194 (Conv2D)             (None, 2, 2, 1024)   525312      re_lu_173[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_192 (BatchN (None, 2, 2, 1024)   4096        conv2d_193[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_193 (BatchN (None, 2, 2, 1024)   4096        conv2d_194[0][0]                 \n__________________________________________________________________________________________________\nadd_57 (Add)                    (None, 2, 2, 1024)   0           batch_normalization_192[0][0]    \n                                                                 batch_normalization_193[0][0]    \n__________________________________________________________________________________________________\nre_lu_176 (ReLU)                (None, 2, 2, 1024)   0           add_57[0][0]                     \n__________________________________________________________________________________________________\nconv2d_195 (Conv2D)             (None, 2, 2, 256)    262144      re_lu_176[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_194 (BatchN (None, 2, 2, 256)    1024        conv2d_195[0][0]                 \n__________________________________________________________________________________________________\nre_lu_177 (ReLU)                (None, 2, 2, 256)    0           batch_normalization_194[0][0]    \n__________________________________________________________________________________________________\nconv2d_196 (Conv2D)             (None, 2, 2, 256)    589824      re_lu_177[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_195 (BatchN (None, 2, 2, 256)    1024        conv2d_196[0][0]                 \n__________________________________________________________________________________________________\nre_lu_178 (ReLU)                (None, 2, 2, 256)    0           batch_normalization_195[0][0]    \n__________________________________________________________________________________________________\nconv2d_197 (Conv2D)             (None, 2, 2, 1024)   263168      re_lu_178[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_196 (BatchN (None, 2, 2, 1024)   4096        conv2d_197[0][0]                 \n__________________________________________________________________________________________________\nadd_58 (Add)                    (None, 2, 2, 1024)   0           batch_normalization_196[0][0]    \n                                                                 re_lu_176[0][0]                  \n__________________________________________________________________________________________________\nre_lu_179 (ReLU)                (None, 2, 2, 1024)   0           add_58[0][0]                     \n__________________________________________________________________________________________________\nconv2d_198 (Conv2D)             (None, 2, 2, 256)    262144      re_lu_179[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_197 (BatchN (None, 2, 2, 256)    1024        conv2d_198[0][0]                 \n__________________________________________________________________________________________________\nre_lu_180 (ReLU)                (None, 2, 2, 256)    0           batch_normalization_197[0][0]    \n__________________________________________________________________________________________________\nconv2d_199 (Conv2D)             (None, 2, 2, 256)    589824      re_lu_180[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_198 (BatchN (None, 2, 2, 256)    1024        conv2d_199[0][0]                 \n__________________________________________________________________________________________________\nre_lu_181 (ReLU)                (None, 2, 2, 256)    0           batch_normalization_198[0][0]    \n__________________________________________________________________________________________________\nconv2d_200 (Conv2D)             (None, 2, 2, 1024)   263168      re_lu_181[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_199 (BatchN (None, 2, 2, 1024)   4096        conv2d_200[0][0]                 \n__________________________________________________________________________________________________\nadd_59 (Add)                    (None, 2, 2, 1024)   0           batch_normalization_199[0][0]    \n                                                                 re_lu_179[0][0]                  \n__________________________________________________________________________________________________\nre_lu_182 (ReLU)                (None, 2, 2, 1024)   0           add_59[0][0]                     \n__________________________________________________________________________________________________\nconv2d_201 (Conv2D)             (None, 2, 2, 256)    262144      re_lu_182[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_200 (BatchN (None, 2, 2, 256)    1024        conv2d_201[0][0]                 \n__________________________________________________________________________________________________\nre_lu_183 (ReLU)                (None, 2, 2, 256)    0           batch_normalization_200[0][0]    \n__________________________________________________________________________________________________\nconv2d_202 (Conv2D)             (None, 2, 2, 256)    589824      re_lu_183[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_201 (BatchN (None, 2, 2, 256)    1024        conv2d_202[0][0]                 \n__________________________________________________________________________________________________\nre_lu_184 (ReLU)                (None, 2, 2, 256)    0           batch_normalization_201[0][0]    \n__________________________________________________________________________________________________\nconv2d_203 (Conv2D)             (None, 2, 2, 1024)   263168      re_lu_184[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_202 (BatchN (None, 2, 2, 1024)   4096        conv2d_203[0][0]                 \n__________________________________________________________________________________________________\nadd_60 (Add)                    (None, 2, 2, 1024)   0           batch_normalization_202[0][0]    \n                                                                 re_lu_182[0][0]                  \n__________________________________________________________________________________________________\nre_lu_185 (ReLU)                (None, 2, 2, 1024)   0           add_60[0][0]                     \n__________________________________________________________________________________________________\nconv2d_204 (Conv2D)             (None, 2, 2, 256)    262144      re_lu_185[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_203 (BatchN (None, 2, 2, 256)    1024        conv2d_204[0][0]                 \n__________________________________________________________________________________________________\nre_lu_186 (ReLU)                (None, 2, 2, 256)    0           batch_normalization_203[0][0]    \n__________________________________________________________________________________________________\nconv2d_205 (Conv2D)             (None, 2, 2, 256)    589824      re_lu_186[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_204 (BatchN (None, 2, 2, 256)    1024        conv2d_205[0][0]                 \n__________________________________________________________________________________________________\nre_lu_187 (ReLU)                (None, 2, 2, 256)    0           batch_normalization_204[0][0]    \n__________________________________________________________________________________________________\nconv2d_206 (Conv2D)             (None, 2, 2, 1024)   263168      re_lu_187[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_205 (BatchN (None, 2, 2, 1024)   4096        conv2d_206[0][0]                 \n__________________________________________________________________________________________________\nadd_61 (Add)                    (None, 2, 2, 1024)   0           batch_normalization_205[0][0]    \n                                                                 re_lu_185[0][0]                  \n__________________________________________________________________________________________________\nre_lu_188 (ReLU)                (None, 2, 2, 1024)   0           add_61[0][0]                     \n__________________________________________________________________________________________________\nconv2d_207 (Conv2D)             (None, 2, 2, 256)    262144      re_lu_188[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_206 (BatchN (None, 2, 2, 256)    1024        conv2d_207[0][0]                 \n__________________________________________________________________________________________________\nre_lu_189 (ReLU)                (None, 2, 2, 256)    0           batch_normalization_206[0][0]    \n__________________________________________________________________________________________________\nconv2d_208 (Conv2D)             (None, 2, 2, 256)    589824      re_lu_189[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_207 (BatchN (None, 2, 2, 256)    1024        conv2d_208[0][0]                 \n__________________________________________________________________________________________________\nre_lu_190 (ReLU)                (None, 2, 2, 256)    0           batch_normalization_207[0][0]    \n__________________________________________________________________________________________________\nconv2d_209 (Conv2D)             (None, 2, 2, 1024)   263168      re_lu_190[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_208 (BatchN (None, 2, 2, 1024)   4096        conv2d_209[0][0]                 \n__________________________________________________________________________________________________\nadd_62 (Add)                    (None, 2, 2, 1024)   0           batch_normalization_208[0][0]    \n                                                                 re_lu_188[0][0]                  \n__________________________________________________________________________________________________\nre_lu_191 (ReLU)                (None, 2, 2, 1024)   0           add_62[0][0]                     \n__________________________________________________________________________________________________\nconv2d_210 (Conv2D)             (None, 1, 1, 512)    524288      re_lu_191[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_209 (BatchN (None, 1, 1, 512)    2048        conv2d_210[0][0]                 \n__________________________________________________________________________________________________\nre_lu_192 (ReLU)                (None, 1, 1, 512)    0           batch_normalization_209[0][0]    \n__________________________________________________________________________________________________\nconv2d_211 (Conv2D)             (None, 1, 1, 512)    2359296     re_lu_192[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_210 (BatchN (None, 1, 1, 512)    2048        conv2d_211[0][0]                 \n__________________________________________________________________________________________________\nre_lu_193 (ReLU)                (None, 1, 1, 512)    0           batch_normalization_210[0][0]    \n__________________________________________________________________________________________________\nconv2d_212 (Conv2D)             (None, 1, 1, 2048)   1050624     re_lu_193[0][0]                  \n__________________________________________________________________________________________________\nconv2d_213 (Conv2D)             (None, 1, 1, 2048)   2099200     re_lu_191[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_211 (BatchN (None, 1, 1, 2048)   8192        conv2d_212[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_212 (BatchN (None, 1, 1, 2048)   8192        conv2d_213[0][0]                 \n__________________________________________________________________________________________________\nadd_63 (Add)                    (None, 1, 1, 2048)   0           batch_normalization_211[0][0]    \n                                                                 batch_normalization_212[0][0]    \n__________________________________________________________________________________________________\nre_lu_194 (ReLU)                (None, 1, 1, 2048)   0           add_63[0][0]                     \n__________________________________________________________________________________________________\nconv2d_214 (Conv2D)             (None, 1, 1, 512)    1048576     re_lu_194[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_213 (BatchN (None, 1, 1, 512)    2048        conv2d_214[0][0]                 \n__________________________________________________________________________________________________\nre_lu_195 (ReLU)                (None, 1, 1, 512)    0           batch_normalization_213[0][0]    \n__________________________________________________________________________________________________\nconv2d_215 (Conv2D)             (None, 1, 1, 512)    2359296     re_lu_195[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_214 (BatchN (None, 1, 1, 512)    2048        conv2d_215[0][0]                 \n__________________________________________________________________________________________________\nre_lu_196 (ReLU)                (None, 1, 1, 512)    0           batch_normalization_214[0][0]    \n__________________________________________________________________________________________________\nconv2d_216 (Conv2D)             (None, 1, 1, 2048)   1050624     re_lu_196[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_215 (BatchN (None, 1, 1, 2048)   8192        conv2d_216[0][0]                 \n__________________________________________________________________________________________________\nadd_64 (Add)                    (None, 1, 1, 2048)   0           batch_normalization_215[0][0]    \n                                                                 re_lu_194[0][0]                  \n__________________________________________________________________________________________________\nre_lu_197 (ReLU)                (None, 1, 1, 2048)   0           add_64[0][0]                     \n__________________________________________________________________________________________________\nconv2d_217 (Conv2D)             (None, 1, 1, 512)    1048576     re_lu_197[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_216 (BatchN (None, 1, 1, 512)    2048        conv2d_217[0][0]                 \n__________________________________________________________________________________________________\nre_lu_198 (ReLU)                (None, 1, 1, 512)    0           batch_normalization_216[0][0]    \n__________________________________________________________________________________________________\nconv2d_218 (Conv2D)             (None, 1, 1, 512)    2359296     re_lu_198[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_217 (BatchN (None, 1, 1, 512)    2048        conv2d_218[0][0]                 \n__________________________________________________________________________________________________\nre_lu_199 (ReLU)                (None, 1, 1, 512)    0           batch_normalization_217[0][0]    \n__________________________________________________________________________________________________\nconv2d_219 (Conv2D)             (None, 1, 1, 2048)   1050624     re_lu_199[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_218 (BatchN (None, 1, 1, 2048)   8192        conv2d_219[0][0]                 \n__________________________________________________________________________________________________\nadd_65 (Add)                    (None, 1, 1, 2048)   0           batch_normalization_218[0][0]    \n                                                                 re_lu_197[0][0]                  \n__________________________________________________________________________________________________\nre_lu_200 (ReLU)                (None, 1, 1, 2048)   0           add_65[0][0]                     \n__________________________________________________________________________________________________\nglobal_average_pooling2d_3 (Glo (None, 2048)         0           re_lu_200[0][0]                  \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 10)           20490       global_average_pooling2d_3[0][0] \n__________________________________________________________________________________________________\nactivation_2 (Activation)       (None, 10)           0           dense[0][0]                      \n==================================================================================================\nTotal params: 23,600,586\nTrainable params: 23,547,466\nNon-trainable params: 53,120\n__________________________________________________________________________________________________\n</code>\n</pre>        <pre><code>model.compile(loss = 'categorical_crossentropy',\n              optimizer=tf.keras.optimizers.Adam(lr=0.001),\n              metrics=['accuracy'])\n</code></pre>     <pre><code>import time\nstart = time.time()\nhistory = model.fit(ds_train,\n                    epochs=20,\n                    validation_data = ds_val)\n\nprint(f\"It took {time.time() - start} seconds\")\n</code></pre>      <pre>\n<code>Epoch 1/20\n1172/1172 [==============================] - 57s 49ms/step - loss: 0.9570 - accuracy: 0.6647 - val_loss: 1.0660 - val_accuracy: 0.6246\nEpoch 2/20\n1172/1172 [==============================] - 57s 48ms/step - loss: 0.9468 - accuracy: 0.6715 - val_loss: 0.9648 - val_accuracy: 0.6563\nEpoch 3/20\n1172/1172 [==============================] - 57s 48ms/step - loss: 0.9169 - accuracy: 0.6779 - val_loss: 0.9009 - val_accuracy: 0.6806\nEpoch 4/20\n1172/1172 [==============================] - 57s 48ms/step - loss: 0.8700 - accuracy: 0.6936 - val_loss: 0.9810 - val_accuracy: 0.6605\nEpoch 5/20\n1172/1172 [==============================] - 58s 49ms/step - loss: 0.8400 - accuracy: 0.7034 - val_loss: 1.0241 - val_accuracy: 0.6444\nEpoch 6/20\n1172/1172 [==============================] - 57s 49ms/step - loss: 0.8027 - accuracy: 0.7189 - val_loss: 0.9003 - val_accuracy: 0.6813\nEpoch 7/20\n1172/1172 [==============================] - 58s 49ms/step - loss: 0.7728 - accuracy: 0.7295 - val_loss: 1.0182 - val_accuracy: 0.6477\nEpoch 8/20\n1172/1172 [==============================] - 57s 49ms/step - loss: 0.7569 - accuracy: 0.7333 - val_loss: 1.1326 - val_accuracy: 0.6317\nEpoch 9/20\n1172/1172 [==============================] - 57s 49ms/step - loss: 0.7486 - accuracy: 0.7394 - val_loss: 0.9695 - val_accuracy: 0.6631\nEpoch 10/20\n1172/1172 [==============================] - 57s 49ms/step - loss: 0.7180 - accuracy: 0.7495 - val_loss: 0.8269 - val_accuracy: 0.7190\nEpoch 11/20\n1172/1172 [==============================] - 57s 49ms/step - loss: 0.6806 - accuracy: 0.7623 - val_loss: 0.8275 - val_accuracy: 0.7151\nEpoch 12/20\n1172/1172 [==============================] - 57s 49ms/step - loss: 0.6671 - accuracy: 0.7681 - val_loss: 0.8477 - val_accuracy: 0.7069\nEpoch 13/20\n1172/1172 [==============================] - 57s 49ms/step - loss: 0.6447 - accuracy: 0.7735 - val_loss: 0.8543 - val_accuracy: 0.7118\nEpoch 14/20\n1172/1172 [==============================] - 57s 48ms/step - loss: 0.6236 - accuracy: 0.7800 - val_loss: 1.1812 - val_accuracy: 0.6317\nEpoch 15/20\n1172/1172 [==============================] - 57s 48ms/step - loss: 0.6177 - accuracy: 0.7875 - val_loss: 1.0423 - val_accuracy: 0.6597\nEpoch 16/20\n1172/1172 [==============================] - 56s 48ms/step - loss: 0.6007 - accuracy: 0.7906 - val_loss: 0.8287 - val_accuracy: 0.7252\nEpoch 17/20\n1172/1172 [==============================] - 56s 48ms/step - loss: 0.5753 - accuracy: 0.7982 - val_loss: 0.7585 - val_accuracy: 0.7417\nEpoch 18/20\n1172/1172 [==============================] - 56s 48ms/step - loss: 0.5758 - accuracy: 0.7979 - val_loss: 0.7436 - val_accuracy: 0.7445\nEpoch 19/20\n1172/1172 [==============================] - 57s 48ms/step - loss: 0.5455 - accuracy: 0.8107 - val_loss: 0.7485 - val_accuracy: 0.7460\nEpoch 20/20\n1172/1172 [==============================] - 57s 48ms/step - loss: 0.5747 - accuracy: 0.7996 - val_loss: 0.7514 - val_accuracy: 0.7444\nIt took 1148.7555301189423 seconds\n</code>\n</pre>        <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\npd.DataFrame(history.history).plot(figsize=(12,8))\nplt.grid(True)\nplt.gca().set_ylim(0,2)\nplt.show()\n</code></pre>"},{"location":"deep_learning/module5/Module5_2/#densenet","title":"DenseNet","text":""},{"location":"deep_learning/module5/Module5_2/#idee_1","title":"Id\u00e9e","text":"<ul> <li> <p>As CNNs become increasingly deep, a new research problem emerges : as  information about the input of gradient passes through many layers, it can vanish and \"wash out\" by the time it reaches the end (or beginning) of the network.</p> </li> <li> <p>to ensure maximum information flow between layers in the network, we connect all layers (with matching feature maps sizes) directly with each other.</p> </li> <li> <p>Crucially, in contrast to ResNets, we never combine features through summation before they are passed into a layer, instead, we combine features by concatenating them.</p> </li> <li> <p>the final classifier makes a decision based on all features maps in the network.</p> </li> </ul>"},{"location":"deep_learning/module5/Module5_2/#definition-des-briques-de-bases_1","title":"D\u00e9finition des briques de bases","text":"<ul> <li> <p>To facilitate down-sampling in our architecture we divide the network into multiple densely connected dense blocks.</p> </li> <li> <p>We refer to layers between blocks as transition blocks. </p> </li> </ul> <p>\\(\\implies\\) Le mod\u00e8le est donc articul\u00e9 autour d'une architecture qui alterne deux types de blocs :</p> <ul> <li>Les blocs dits \"denses\",</li> <li>Les blocs dits de \"transitions\". </li> </ul>"},{"location":"deep_learning/module5/Module5_2/#composite-function-dense-blocks","title":"Composite function &amp; dense blocks","text":"<p>De l'article, nous tirons les indications suivantes.</p> <ul> <li> <p>The network comprises \\(L\\) layers, each of which implements a non-linear transformation \\(H_{\\ell}(-)\\), where \\(\\ell\\) indexes the layer. [...] We denote the output of the \\(\\ell^{th}\\) layer as \\(x_{\\ell}\\).</p> </li> <li> <p>Consequenttly, the \\(\\ell^{th}\\) layer receives the feature maps of all preceding layers, \\(x_{0}, \\dots, x_{\\ell-1}\\) as input :</p> </li> </ul> \\[x_{\\ell} = H_{\\ell}([x_{0},x_{1} \\dots, x_{\\ell-1}])\\] <ul> <li>We define \\(H_{\\ell}\\) as a compite function of the consecutive operations :</li> <li>BN-ReLU-Conv(\\(1 \\times 1\\))-BN-ReLU-Conv(\\(3 \\times 3\\)), </li> <li>Each Conv(3 \\(\\times\\) 3) produces \\(k\\) features maps, [...] we let each Conv(1 \\(\\times\\) 1) produce \\(4k\\) feature maps.<ul> <li>We refer to the hyperparameter \\(k\\) as the growth rate of the network.</li> <li>The growth rate for all networks is \\(k=32\\)</li> </ul> </li> <li>For convolutionnal layers with kernel size \\(3 \\times 3\\), each side of the inputs is zero-padded  by one pixel to keep the feature-map size fixed.</li> <li>We adopt the weight initialization introduced by [10] (ie He)</li> </ul>      <pre><code>def bn_relu_conv(tensor, k, kernel_size):\n  x = BatchNormalization()(tensor)\n  x = ReLU()(x)\n  x = Conv2D(filters=k,\n             kernel_size=kernel_size,\n             strides=(1,1),\n             padding='same',\n             kernel_initializer='he_normal',\n             use_bias=False)(x)\n  return x\n</code></pre>      <p>Les blocs denses ont un sch\u00e9ma r\u00e9p\u00e9titif.</p>      <pre><code>from tensorflow.keras.layers import Concatenate\n\ndef dense_block(tensor, k, reps):\n  for _ in range(reps):\n    x = bn_relu_conv(tensor, 4*k, 1)\n    x = bn_relu_conv(x, k, 3)\n\n    tensor = Concatenate()([x, tensor])  # le tenseur d'input en entr\u00e9e par d\u00e9finition\n\n  return tensor\n</code></pre>"},{"location":"deep_learning/module5/Module5_2/#transition-function","title":"Transition function","text":"<ul> <li>If a dense blocks contains m feature-maps, we let the following transition layer generate \\(\\lfloor \\theta m \\rfloor\\) output feature maps, where \\(0 &lt; \\theta \\leq 1\\) is referred as the compression factor.</li> <li>We set \\(\\theta = 0.5\\) in our experiment.</li> <li>We use BN-ReLU-Conv(\\(1 \\times 1\\)) followed by \\(2 \\times 2\\) average pooling as transition layers between two contiguous dense blocks.</li> </ul> <p>Pour avoir acc\u00e8s au nombre de feature maps, on a besoin d'utiliser la commande <code>tf.keras.backend.int_shape(x)[-1]</code>.</p>      <pre><code>from tensorflow.keras.layers import AvgPool2D\n\ndef transition_layer(x, theta):\n    f = int(tf.keras.backend.int_shape(x)[-1] * theta)\n    x = bn_relu_conv(x, f, 1)\n    x = AvgPool2D(pool_size=2, strides=2, padding='same')(x)\n    return x\n</code></pre>      <ul> <li>The initial convolution layer comprises \\(2k\\) convolutions of size \\(7\\times7\\) with stride \\(2\\). </li> <li>At the end of the last dense blocks, a global average pooling is performed and then a softmax classfier is attached.</li> </ul>      <pre><code>from tensorflow.keras import Model\n\ndef DenseNet(img_shape, k, theta, repetitions, small=True, include_top=True, num_classes=10):\n\n  input = Input(img_shape)\n\n  if small:\n    x = Conv2D(filters = 16,\n               kernel_size = 7,\n               strides=2,\n               padding='same',\n               kernel_initializer='he_normal')(input)\n  else:\n    x = Conv2D(filters = 2*k,\n               kernel_size = 7,\n               strides=2,\n               padding='same',\n               kernel_initializer='he_normal')(input)\n    x = MaxPool2D(pool_size = 3,\n                  strides = 2,\n                  padding='same')(x)\n\n  for reps in repetitions:\n      d = dense_block(x, k, reps)\n      x = transition_layer(d, theta)\n\n  if include_top:\n    x = GlobalAvgPool2D()(d)\n    x = Dense(num_classes)(x)\n    output = Activation('softmax')(x)\n  else:\n    output = GlobalAvgPool2D()(d)\n\n  model = Model(input, output)\n\n  return model\n</code></pre>     <pre><code>img_shape = 32, 32, 3\nk = 32\ntheta = 0.5\nrepetitions = 6, 12, 24, 16\nnum_classes = 100\n\nmodel = DenseNet(img_shape, k, theta, repetitions, num_classes=num_classes)\n</code></pre>     <pre><code>model.summary()\n</code></pre>      <pre>\n<code>Model: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_4 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n__________________________________________________________________________________________________\nconv2d_123 (Conv2D)             (None, 16, 16, 16)   2368        input_4[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_120 (BatchN (None, 16, 16, 16)   64          conv2d_123[0][0]                 \n__________________________________________________________________________________________________\nre_lu_120 (ReLU)                (None, 16, 16, 16)   0           batch_normalization_120[0][0]    \n__________________________________________________________________________________________________\nconv2d_124 (Conv2D)             (None, 16, 16, 128)  2048        re_lu_120[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_121 (BatchN (None, 16, 16, 128)  512         conv2d_124[0][0]                 \n__________________________________________________________________________________________________\nre_lu_121 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_121[0][0]    \n__________________________________________________________________________________________________\nconv2d_125 (Conv2D)             (None, 16, 16, 32)   36864       re_lu_121[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_58 (Concatenate)    (None, 16, 16, 48)   0           conv2d_123[0][0]                 \n                                                                 conv2d_125[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_122 (BatchN (None, 16, 16, 48)   192         concatenate_58[0][0]             \n__________________________________________________________________________________________________\nre_lu_122 (ReLU)                (None, 16, 16, 48)   0           batch_normalization_122[0][0]    \n__________________________________________________________________________________________________\nconv2d_126 (Conv2D)             (None, 16, 16, 128)  6144        re_lu_122[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_123 (BatchN (None, 16, 16, 128)  512         conv2d_126[0][0]                 \n__________________________________________________________________________________________________\nre_lu_123 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_123[0][0]    \n__________________________________________________________________________________________________\nconv2d_127 (Conv2D)             (None, 16, 16, 32)   36864       re_lu_123[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_59 (Concatenate)    (None, 16, 16, 80)   0           concatenate_58[0][0]             \n                                                                 conv2d_127[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_124 (BatchN (None, 16, 16, 80)   320         concatenate_59[0][0]             \n__________________________________________________________________________________________________\nre_lu_124 (ReLU)                (None, 16, 16, 80)   0           batch_normalization_124[0][0]    \n__________________________________________________________________________________________________\nconv2d_128 (Conv2D)             (None, 16, 16, 128)  10240       re_lu_124[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_125 (BatchN (None, 16, 16, 128)  512         conv2d_128[0][0]                 \n__________________________________________________________________________________________________\nre_lu_125 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_125[0][0]    \n__________________________________________________________________________________________________\nconv2d_129 (Conv2D)             (None, 16, 16, 32)   36864       re_lu_125[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_60 (Concatenate)    (None, 16, 16, 112)  0           concatenate_59[0][0]             \n                                                                 conv2d_129[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_126 (BatchN (None, 16, 16, 112)  448         concatenate_60[0][0]             \n__________________________________________________________________________________________________\nre_lu_126 (ReLU)                (None, 16, 16, 112)  0           batch_normalization_126[0][0]    \n__________________________________________________________________________________________________\nconv2d_130 (Conv2D)             (None, 16, 16, 128)  14336       re_lu_126[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_127 (BatchN (None, 16, 16, 128)  512         conv2d_130[0][0]                 \n__________________________________________________________________________________________________\nre_lu_127 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_127[0][0]    \n__________________________________________________________________________________________________\nconv2d_131 (Conv2D)             (None, 16, 16, 32)   36864       re_lu_127[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_61 (Concatenate)    (None, 16, 16, 144)  0           concatenate_60[0][0]             \n                                                                 conv2d_131[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_128 (BatchN (None, 16, 16, 144)  576         concatenate_61[0][0]             \n__________________________________________________________________________________________________\nre_lu_128 (ReLU)                (None, 16, 16, 144)  0           batch_normalization_128[0][0]    \n__________________________________________________________________________________________________\nconv2d_132 (Conv2D)             (None, 16, 16, 128)  18432       re_lu_128[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_129 (BatchN (None, 16, 16, 128)  512         conv2d_132[0][0]                 \n__________________________________________________________________________________________________\nre_lu_129 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_129[0][0]    \n__________________________________________________________________________________________________\nconv2d_133 (Conv2D)             (None, 16, 16, 32)   36864       re_lu_129[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_62 (Concatenate)    (None, 16, 16, 176)  0           concatenate_61[0][0]             \n                                                                 conv2d_133[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_130 (BatchN (None, 16, 16, 176)  704         concatenate_62[0][0]             \n__________________________________________________________________________________________________\nre_lu_130 (ReLU)                (None, 16, 16, 176)  0           batch_normalization_130[0][0]    \n__________________________________________________________________________________________________\nconv2d_134 (Conv2D)             (None, 16, 16, 128)  22528       re_lu_130[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_131 (BatchN (None, 16, 16, 128)  512         conv2d_134[0][0]                 \n__________________________________________________________________________________________________\nre_lu_131 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_131[0][0]    \n__________________________________________________________________________________________________\nconv2d_135 (Conv2D)             (None, 16, 16, 32)   36864       re_lu_131[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_63 (Concatenate)    (None, 16, 16, 208)  0           concatenate_62[0][0]             \n                                                                 conv2d_135[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_132 (BatchN (None, 16, 16, 208)  832         concatenate_63[0][0]             \n__________________________________________________________________________________________________\nre_lu_132 (ReLU)                (None, 16, 16, 208)  0           batch_normalization_132[0][0]    \n__________________________________________________________________________________________________\nconv2d_136 (Conv2D)             (None, 16, 16, 104)  21632       re_lu_132[0][0]                  \n__________________________________________________________________________________________________\naverage_pooling2d_4 (AveragePoo (None, 8, 8, 104)    0           conv2d_136[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_133 (BatchN (None, 8, 8, 104)    416         average_pooling2d_4[0][0]        \n__________________________________________________________________________________________________\nre_lu_133 (ReLU)                (None, 8, 8, 104)    0           batch_normalization_133[0][0]    \n__________________________________________________________________________________________________\nconv2d_137 (Conv2D)             (None, 8, 8, 128)    13312       re_lu_133[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_134 (BatchN (None, 8, 8, 128)    512         conv2d_137[0][0]                 \n__________________________________________________________________________________________________\nre_lu_134 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_134[0][0]    \n__________________________________________________________________________________________________\nconv2d_138 (Conv2D)             (None, 8, 8, 32)     36864       re_lu_134[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_64 (Concatenate)    (None, 8, 8, 136)    0           average_pooling2d_4[0][0]        \n                                                                 conv2d_138[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_135 (BatchN (None, 8, 8, 136)    544         concatenate_64[0][0]             \n__________________________________________________________________________________________________\nre_lu_135 (ReLU)                (None, 8, 8, 136)    0           batch_normalization_135[0][0]    \n__________________________________________________________________________________________________\nconv2d_139 (Conv2D)             (None, 8, 8, 128)    17408       re_lu_135[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_136 (BatchN (None, 8, 8, 128)    512         conv2d_139[0][0]                 \n__________________________________________________________________________________________________\nre_lu_136 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_136[0][0]    \n__________________________________________________________________________________________________\nconv2d_140 (Conv2D)             (None, 8, 8, 32)     36864       re_lu_136[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_65 (Concatenate)    (None, 8, 8, 168)    0           concatenate_64[0][0]             \n                                                                 conv2d_140[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_137 (BatchN (None, 8, 8, 168)    672         concatenate_65[0][0]             \n__________________________________________________________________________________________________\nre_lu_137 (ReLU)                (None, 8, 8, 168)    0           batch_normalization_137[0][0]    \n__________________________________________________________________________________________________\nconv2d_141 (Conv2D)             (None, 8, 8, 128)    21504       re_lu_137[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_138 (BatchN (None, 8, 8, 128)    512         conv2d_141[0][0]                 \n__________________________________________________________________________________________________\nre_lu_138 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_138[0][0]    \n__________________________________________________________________________________________________\nconv2d_142 (Conv2D)             (None, 8, 8, 32)     36864       re_lu_138[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_66 (Concatenate)    (None, 8, 8, 200)    0           concatenate_65[0][0]             \n                                                                 conv2d_142[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_139 (BatchN (None, 8, 8, 200)    800         concatenate_66[0][0]             \n__________________________________________________________________________________________________\nre_lu_139 (ReLU)                (None, 8, 8, 200)    0           batch_normalization_139[0][0]    \n__________________________________________________________________________________________________\nconv2d_143 (Conv2D)             (None, 8, 8, 128)    25600       re_lu_139[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_140 (BatchN (None, 8, 8, 128)    512         conv2d_143[0][0]                 \n__________________________________________________________________________________________________\nre_lu_140 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_140[0][0]    \n__________________________________________________________________________________________________\nconv2d_144 (Conv2D)             (None, 8, 8, 32)     36864       re_lu_140[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_67 (Concatenate)    (None, 8, 8, 232)    0           concatenate_66[0][0]             \n                                                                 conv2d_144[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_141 (BatchN (None, 8, 8, 232)    928         concatenate_67[0][0]             \n__________________________________________________________________________________________________\nre_lu_141 (ReLU)                (None, 8, 8, 232)    0           batch_normalization_141[0][0]    \n__________________________________________________________________________________________________\nconv2d_145 (Conv2D)             (None, 8, 8, 128)    29696       re_lu_141[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_142 (BatchN (None, 8, 8, 128)    512         conv2d_145[0][0]                 \n__________________________________________________________________________________________________\nre_lu_142 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_142[0][0]    \n__________________________________________________________________________________________________\nconv2d_146 (Conv2D)             (None, 8, 8, 32)     36864       re_lu_142[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_68 (Concatenate)    (None, 8, 8, 264)    0           concatenate_67[0][0]             \n                                                                 conv2d_146[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_143 (BatchN (None, 8, 8, 264)    1056        concatenate_68[0][0]             \n__________________________________________________________________________________________________\nre_lu_143 (ReLU)                (None, 8, 8, 264)    0           batch_normalization_143[0][0]    \n__________________________________________________________________________________________________\nconv2d_147 (Conv2D)             (None, 8, 8, 128)    33792       re_lu_143[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_144 (BatchN (None, 8, 8, 128)    512         conv2d_147[0][0]                 \n__________________________________________________________________________________________________\nre_lu_144 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_144[0][0]    \n__________________________________________________________________________________________________\nconv2d_148 (Conv2D)             (None, 8, 8, 32)     36864       re_lu_144[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_69 (Concatenate)    (None, 8, 8, 296)    0           concatenate_68[0][0]             \n                                                                 conv2d_148[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_145 (BatchN (None, 8, 8, 296)    1184        concatenate_69[0][0]             \n__________________________________________________________________________________________________\nre_lu_145 (ReLU)                (None, 8, 8, 296)    0           batch_normalization_145[0][0]    \n__________________________________________________________________________________________________\nconv2d_149 (Conv2D)             (None, 8, 8, 128)    37888       re_lu_145[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_146 (BatchN (None, 8, 8, 128)    512         conv2d_149[0][0]                 \n__________________________________________________________________________________________________\nre_lu_146 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_146[0][0]    \n__________________________________________________________________________________________________\nconv2d_150 (Conv2D)             (None, 8, 8, 32)     36864       re_lu_146[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_70 (Concatenate)    (None, 8, 8, 328)    0           concatenate_69[0][0]             \n                                                                 conv2d_150[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_147 (BatchN (None, 8, 8, 328)    1312        concatenate_70[0][0]             \n__________________________________________________________________________________________________\nre_lu_147 (ReLU)                (None, 8, 8, 328)    0           batch_normalization_147[0][0]    \n__________________________________________________________________________________________________\nconv2d_151 (Conv2D)             (None, 8, 8, 128)    41984       re_lu_147[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_148 (BatchN (None, 8, 8, 128)    512         conv2d_151[0][0]                 \n__________________________________________________________________________________________________\nre_lu_148 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_148[0][0]    \n__________________________________________________________________________________________________\nconv2d_152 (Conv2D)             (None, 8, 8, 32)     36864       re_lu_148[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_71 (Concatenate)    (None, 8, 8, 360)    0           concatenate_70[0][0]             \n                                                                 conv2d_152[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_149 (BatchN (None, 8, 8, 360)    1440        concatenate_71[0][0]             \n__________________________________________________________________________________________________\nre_lu_149 (ReLU)                (None, 8, 8, 360)    0           batch_normalization_149[0][0]    \n__________________________________________________________________________________________________\nconv2d_153 (Conv2D)             (None, 8, 8, 128)    46080       re_lu_149[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_150 (BatchN (None, 8, 8, 128)    512         conv2d_153[0][0]                 \n__________________________________________________________________________________________________\nre_lu_150 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_150[0][0]    \n__________________________________________________________________________________________________\nconv2d_154 (Conv2D)             (None, 8, 8, 32)     36864       re_lu_150[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_72 (Concatenate)    (None, 8, 8, 392)    0           concatenate_71[0][0]             \n                                                                 conv2d_154[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_151 (BatchN (None, 8, 8, 392)    1568        concatenate_72[0][0]             \n__________________________________________________________________________________________________\nre_lu_151 (ReLU)                (None, 8, 8, 392)    0           batch_normalization_151[0][0]    \n__________________________________________________________________________________________________\nconv2d_155 (Conv2D)             (None, 8, 8, 128)    50176       re_lu_151[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_152 (BatchN (None, 8, 8, 128)    512         conv2d_155[0][0]                 \n__________________________________________________________________________________________________\nre_lu_152 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_152[0][0]    \n__________________________________________________________________________________________________\nconv2d_156 (Conv2D)             (None, 8, 8, 32)     36864       re_lu_152[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_73 (Concatenate)    (None, 8, 8, 424)    0           concatenate_72[0][0]             \n                                                                 conv2d_156[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_153 (BatchN (None, 8, 8, 424)    1696        concatenate_73[0][0]             \n__________________________________________________________________________________________________\nre_lu_153 (ReLU)                (None, 8, 8, 424)    0           batch_normalization_153[0][0]    \n__________________________________________________________________________________________________\nconv2d_157 (Conv2D)             (None, 8, 8, 128)    54272       re_lu_153[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_154 (BatchN (None, 8, 8, 128)    512         conv2d_157[0][0]                 \n__________________________________________________________________________________________________\nre_lu_154 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_154[0][0]    \n__________________________________________________________________________________________________\nconv2d_158 (Conv2D)             (None, 8, 8, 32)     36864       re_lu_154[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_74 (Concatenate)    (None, 8, 8, 456)    0           concatenate_73[0][0]             \n                                                                 conv2d_158[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_155 (BatchN (None, 8, 8, 456)    1824        concatenate_74[0][0]             \n__________________________________________________________________________________________________\nre_lu_155 (ReLU)                (None, 8, 8, 456)    0           batch_normalization_155[0][0]    \n__________________________________________________________________________________________________\nconv2d_159 (Conv2D)             (None, 8, 8, 128)    58368       re_lu_155[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_156 (BatchN (None, 8, 8, 128)    512         conv2d_159[0][0]                 \n__________________________________________________________________________________________________\nre_lu_156 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_156[0][0]    \n__________________________________________________________________________________________________\nconv2d_160 (Conv2D)             (None, 8, 8, 32)     36864       re_lu_156[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_75 (Concatenate)    (None, 8, 8, 488)    0           concatenate_74[0][0]             \n                                                                 conv2d_160[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_157 (BatchN (None, 8, 8, 488)    1952        concatenate_75[0][0]             \n__________________________________________________________________________________________________\nre_lu_157 (ReLU)                (None, 8, 8, 488)    0           batch_normalization_157[0][0]    \n__________________________________________________________________________________________________\nconv2d_161 (Conv2D)             (None, 8, 8, 244)    119072      re_lu_157[0][0]                  \n__________________________________________________________________________________________________\naverage_pooling2d_5 (AveragePoo (None, 4, 4, 244)    0           conv2d_161[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_158 (BatchN (None, 4, 4, 244)    976         average_pooling2d_5[0][0]        \n__________________________________________________________________________________________________\nre_lu_158 (ReLU)                (None, 4, 4, 244)    0           batch_normalization_158[0][0]    \n__________________________________________________________________________________________________\nconv2d_162 (Conv2D)             (None, 4, 4, 128)    31232       re_lu_158[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_159 (BatchN (None, 4, 4, 128)    512         conv2d_162[0][0]                 \n__________________________________________________________________________________________________\nre_lu_159 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_159[0][0]    \n__________________________________________________________________________________________________\nconv2d_163 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_159[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_76 (Concatenate)    (None, 4, 4, 276)    0           average_pooling2d_5[0][0]        \n                                                                 conv2d_163[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_160 (BatchN (None, 4, 4, 276)    1104        concatenate_76[0][0]             \n__________________________________________________________________________________________________\nre_lu_160 (ReLU)                (None, 4, 4, 276)    0           batch_normalization_160[0][0]    \n__________________________________________________________________________________________________\nconv2d_164 (Conv2D)             (None, 4, 4, 128)    35328       re_lu_160[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_161 (BatchN (None, 4, 4, 128)    512         conv2d_164[0][0]                 \n__________________________________________________________________________________________________\nre_lu_161 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_161[0][0]    \n__________________________________________________________________________________________________\nconv2d_165 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_161[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_77 (Concatenate)    (None, 4, 4, 308)    0           concatenate_76[0][0]             \n                                                                 conv2d_165[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_162 (BatchN (None, 4, 4, 308)    1232        concatenate_77[0][0]             \n__________________________________________________________________________________________________\nre_lu_162 (ReLU)                (None, 4, 4, 308)    0           batch_normalization_162[0][0]    \n__________________________________________________________________________________________________\nconv2d_166 (Conv2D)             (None, 4, 4, 128)    39424       re_lu_162[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_163 (BatchN (None, 4, 4, 128)    512         conv2d_166[0][0]                 \n__________________________________________________________________________________________________\nre_lu_163 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_163[0][0]    \n__________________________________________________________________________________________________\nconv2d_167 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_163[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_78 (Concatenate)    (None, 4, 4, 340)    0           concatenate_77[0][0]             \n                                                                 conv2d_167[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_164 (BatchN (None, 4, 4, 340)    1360        concatenate_78[0][0]             \n__________________________________________________________________________________________________\nre_lu_164 (ReLU)                (None, 4, 4, 340)    0           batch_normalization_164[0][0]    \n__________________________________________________________________________________________________\nconv2d_168 (Conv2D)             (None, 4, 4, 128)    43520       re_lu_164[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_165 (BatchN (None, 4, 4, 128)    512         conv2d_168[0][0]                 \n__________________________________________________________________________________________________\nre_lu_165 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_165[0][0]    \n__________________________________________________________________________________________________\nconv2d_169 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_165[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_79 (Concatenate)    (None, 4, 4, 372)    0           concatenate_78[0][0]             \n                                                                 conv2d_169[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_166 (BatchN (None, 4, 4, 372)    1488        concatenate_79[0][0]             \n__________________________________________________________________________________________________\nre_lu_166 (ReLU)                (None, 4, 4, 372)    0           batch_normalization_166[0][0]    \n__________________________________________________________________________________________________\nconv2d_170 (Conv2D)             (None, 4, 4, 128)    47616       re_lu_166[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_167 (BatchN (None, 4, 4, 128)    512         conv2d_170[0][0]                 \n__________________________________________________________________________________________________\nre_lu_167 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_167[0][0]    \n__________________________________________________________________________________________________\nconv2d_171 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_167[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_80 (Concatenate)    (None, 4, 4, 404)    0           concatenate_79[0][0]             \n                                                                 conv2d_171[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_168 (BatchN (None, 4, 4, 404)    1616        concatenate_80[0][0]             \n__________________________________________________________________________________________________\nre_lu_168 (ReLU)                (None, 4, 4, 404)    0           batch_normalization_168[0][0]    \n__________________________________________________________________________________________________\nconv2d_172 (Conv2D)             (None, 4, 4, 128)    51712       re_lu_168[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_169 (BatchN (None, 4, 4, 128)    512         conv2d_172[0][0]                 \n__________________________________________________________________________________________________\nre_lu_169 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_169[0][0]    \n__________________________________________________________________________________________________\nconv2d_173 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_169[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_81 (Concatenate)    (None, 4, 4, 436)    0           concatenate_80[0][0]             \n                                                                 conv2d_173[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_170 (BatchN (None, 4, 4, 436)    1744        concatenate_81[0][0]             \n__________________________________________________________________________________________________\nre_lu_170 (ReLU)                (None, 4, 4, 436)    0           batch_normalization_170[0][0]    \n__________________________________________________________________________________________________\nconv2d_174 (Conv2D)             (None, 4, 4, 128)    55808       re_lu_170[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_171 (BatchN (None, 4, 4, 128)    512         conv2d_174[0][0]                 \n__________________________________________________________________________________________________\nre_lu_171 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_171[0][0]    \n__________________________________________________________________________________________________\nconv2d_175 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_171[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_82 (Concatenate)    (None, 4, 4, 468)    0           concatenate_81[0][0]             \n                                                                 conv2d_175[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_172 (BatchN (None, 4, 4, 468)    1872        concatenate_82[0][0]             \n__________________________________________________________________________________________________\nre_lu_172 (ReLU)                (None, 4, 4, 468)    0           batch_normalization_172[0][0]    \n__________________________________________________________________________________________________\nconv2d_176 (Conv2D)             (None, 4, 4, 128)    59904       re_lu_172[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_173 (BatchN (None, 4, 4, 128)    512         conv2d_176[0][0]                 \n__________________________________________________________________________________________________\nre_lu_173 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_173[0][0]    \n__________________________________________________________________________________________________\nconv2d_177 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_173[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_83 (Concatenate)    (None, 4, 4, 500)    0           concatenate_82[0][0]             \n                                                                 conv2d_177[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_174 (BatchN (None, 4, 4, 500)    2000        concatenate_83[0][0]             \n__________________________________________________________________________________________________\nre_lu_174 (ReLU)                (None, 4, 4, 500)    0           batch_normalization_174[0][0]    \n__________________________________________________________________________________________________\nconv2d_178 (Conv2D)             (None, 4, 4, 128)    64000       re_lu_174[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_175 (BatchN (None, 4, 4, 128)    512         conv2d_178[0][0]                 \n__________________________________________________________________________________________________\nre_lu_175 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_175[0][0]    \n__________________________________________________________________________________________________\nconv2d_179 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_175[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_84 (Concatenate)    (None, 4, 4, 532)    0           concatenate_83[0][0]             \n                                                                 conv2d_179[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_176 (BatchN (None, 4, 4, 532)    2128        concatenate_84[0][0]             \n__________________________________________________________________________________________________\nre_lu_176 (ReLU)                (None, 4, 4, 532)    0           batch_normalization_176[0][0]    \n__________________________________________________________________________________________________\nconv2d_180 (Conv2D)             (None, 4, 4, 128)    68096       re_lu_176[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_177 (BatchN (None, 4, 4, 128)    512         conv2d_180[0][0]                 \n__________________________________________________________________________________________________\nre_lu_177 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_177[0][0]    \n__________________________________________________________________________________________________\nconv2d_181 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_177[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_85 (Concatenate)    (None, 4, 4, 564)    0           concatenate_84[0][0]             \n                                                                 conv2d_181[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_178 (BatchN (None, 4, 4, 564)    2256        concatenate_85[0][0]             \n__________________________________________________________________________________________________\nre_lu_178 (ReLU)                (None, 4, 4, 564)    0           batch_normalization_178[0][0]    \n__________________________________________________________________________________________________\nconv2d_182 (Conv2D)             (None, 4, 4, 128)    72192       re_lu_178[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_179 (BatchN (None, 4, 4, 128)    512         conv2d_182[0][0]                 \n__________________________________________________________________________________________________\nre_lu_179 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_179[0][0]    \n__________________________________________________________________________________________________\nconv2d_183 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_179[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_86 (Concatenate)    (None, 4, 4, 596)    0           concatenate_85[0][0]             \n                                                                 conv2d_183[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_180 (BatchN (None, 4, 4, 596)    2384        concatenate_86[0][0]             \n__________________________________________________________________________________________________\nre_lu_180 (ReLU)                (None, 4, 4, 596)    0           batch_normalization_180[0][0]    \n__________________________________________________________________________________________________\nconv2d_184 (Conv2D)             (None, 4, 4, 128)    76288       re_lu_180[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_181 (BatchN (None, 4, 4, 128)    512         conv2d_184[0][0]                 \n__________________________________________________________________________________________________\nre_lu_181 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_181[0][0]    \n__________________________________________________________________________________________________\nconv2d_185 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_181[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_87 (Concatenate)    (None, 4, 4, 628)    0           concatenate_86[0][0]             \n                                                                 conv2d_185[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_182 (BatchN (None, 4, 4, 628)    2512        concatenate_87[0][0]             \n__________________________________________________________________________________________________\nre_lu_182 (ReLU)                (None, 4, 4, 628)    0           batch_normalization_182[0][0]    \n__________________________________________________________________________________________________\nconv2d_186 (Conv2D)             (None, 4, 4, 128)    80384       re_lu_182[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_183 (BatchN (None, 4, 4, 128)    512         conv2d_186[0][0]                 \n__________________________________________________________________________________________________\nre_lu_183 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_183[0][0]    \n__________________________________________________________________________________________________\nconv2d_187 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_183[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_88 (Concatenate)    (None, 4, 4, 660)    0           concatenate_87[0][0]             \n                                                                 conv2d_187[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_184 (BatchN (None, 4, 4, 660)    2640        concatenate_88[0][0]             \n__________________________________________________________________________________________________\nre_lu_184 (ReLU)                (None, 4, 4, 660)    0           batch_normalization_184[0][0]    \n__________________________________________________________________________________________________\nconv2d_188 (Conv2D)             (None, 4, 4, 128)    84480       re_lu_184[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_185 (BatchN (None, 4, 4, 128)    512         conv2d_188[0][0]                 \n__________________________________________________________________________________________________\nre_lu_185 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_185[0][0]    \n__________________________________________________________________________________________________\nconv2d_189 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_185[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_89 (Concatenate)    (None, 4, 4, 692)    0           concatenate_88[0][0]             \n                                                                 conv2d_189[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_186 (BatchN (None, 4, 4, 692)    2768        concatenate_89[0][0]             \n__________________________________________________________________________________________________\nre_lu_186 (ReLU)                (None, 4, 4, 692)    0           batch_normalization_186[0][0]    \n__________________________________________________________________________________________________\nconv2d_190 (Conv2D)             (None, 4, 4, 128)    88576       re_lu_186[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_187 (BatchN (None, 4, 4, 128)    512         conv2d_190[0][0]                 \n__________________________________________________________________________________________________\nre_lu_187 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_187[0][0]    \n__________________________________________________________________________________________________\nconv2d_191 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_187[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_90 (Concatenate)    (None, 4, 4, 724)    0           concatenate_89[0][0]             \n                                                                 conv2d_191[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_188 (BatchN (None, 4, 4, 724)    2896        concatenate_90[0][0]             \n__________________________________________________________________________________________________\nre_lu_188 (ReLU)                (None, 4, 4, 724)    0           batch_normalization_188[0][0]    \n__________________________________________________________________________________________________\nconv2d_192 (Conv2D)             (None, 4, 4, 128)    92672       re_lu_188[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_189 (BatchN (None, 4, 4, 128)    512         conv2d_192[0][0]                 \n__________________________________________________________________________________________________\nre_lu_189 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_189[0][0]    \n__________________________________________________________________________________________________\nconv2d_193 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_189[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_91 (Concatenate)    (None, 4, 4, 756)    0           concatenate_90[0][0]             \n                                                                 conv2d_193[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_190 (BatchN (None, 4, 4, 756)    3024        concatenate_91[0][0]             \n__________________________________________________________________________________________________\nre_lu_190 (ReLU)                (None, 4, 4, 756)    0           batch_normalization_190[0][0]    \n__________________________________________________________________________________________________\nconv2d_194 (Conv2D)             (None, 4, 4, 128)    96768       re_lu_190[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_191 (BatchN (None, 4, 4, 128)    512         conv2d_194[0][0]                 \n__________________________________________________________________________________________________\nre_lu_191 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_191[0][0]    \n__________________________________________________________________________________________________\nconv2d_195 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_191[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_92 (Concatenate)    (None, 4, 4, 788)    0           concatenate_91[0][0]             \n                                                                 conv2d_195[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_192 (BatchN (None, 4, 4, 788)    3152        concatenate_92[0][0]             \n__________________________________________________________________________________________________\nre_lu_192 (ReLU)                (None, 4, 4, 788)    0           batch_normalization_192[0][0]    \n__________________________________________________________________________________________________\nconv2d_196 (Conv2D)             (None, 4, 4, 128)    100864      re_lu_192[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_193 (BatchN (None, 4, 4, 128)    512         conv2d_196[0][0]                 \n__________________________________________________________________________________________________\nre_lu_193 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_193[0][0]    \n__________________________________________________________________________________________________\nconv2d_197 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_193[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_93 (Concatenate)    (None, 4, 4, 820)    0           concatenate_92[0][0]             \n                                                                 conv2d_197[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_194 (BatchN (None, 4, 4, 820)    3280        concatenate_93[0][0]             \n__________________________________________________________________________________________________\nre_lu_194 (ReLU)                (None, 4, 4, 820)    0           batch_normalization_194[0][0]    \n__________________________________________________________________________________________________\nconv2d_198 (Conv2D)             (None, 4, 4, 128)    104960      re_lu_194[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_195 (BatchN (None, 4, 4, 128)    512         conv2d_198[0][0]                 \n__________________________________________________________________________________________________\nre_lu_195 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_195[0][0]    \n__________________________________________________________________________________________________\nconv2d_199 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_195[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_94 (Concatenate)    (None, 4, 4, 852)    0           concatenate_93[0][0]             \n                                                                 conv2d_199[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_196 (BatchN (None, 4, 4, 852)    3408        concatenate_94[0][0]             \n__________________________________________________________________________________________________\nre_lu_196 (ReLU)                (None, 4, 4, 852)    0           batch_normalization_196[0][0]    \n__________________________________________________________________________________________________\nconv2d_200 (Conv2D)             (None, 4, 4, 128)    109056      re_lu_196[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_197 (BatchN (None, 4, 4, 128)    512         conv2d_200[0][0]                 \n__________________________________________________________________________________________________\nre_lu_197 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_197[0][0]    \n__________________________________________________________________________________________________\nconv2d_201 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_197[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_95 (Concatenate)    (None, 4, 4, 884)    0           concatenate_94[0][0]             \n                                                                 conv2d_201[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_198 (BatchN (None, 4, 4, 884)    3536        concatenate_95[0][0]             \n__________________________________________________________________________________________________\nre_lu_198 (ReLU)                (None, 4, 4, 884)    0           batch_normalization_198[0][0]    \n__________________________________________________________________________________________________\nconv2d_202 (Conv2D)             (None, 4, 4, 128)    113152      re_lu_198[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_199 (BatchN (None, 4, 4, 128)    512         conv2d_202[0][0]                 \n__________________________________________________________________________________________________\nre_lu_199 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_199[0][0]    \n__________________________________________________________________________________________________\nconv2d_203 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_199[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_96 (Concatenate)    (None, 4, 4, 916)    0           concatenate_95[0][0]             \n                                                                 conv2d_203[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_200 (BatchN (None, 4, 4, 916)    3664        concatenate_96[0][0]             \n__________________________________________________________________________________________________\nre_lu_200 (ReLU)                (None, 4, 4, 916)    0           batch_normalization_200[0][0]    \n__________________________________________________________________________________________________\nconv2d_204 (Conv2D)             (None, 4, 4, 128)    117248      re_lu_200[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_201 (BatchN (None, 4, 4, 128)    512         conv2d_204[0][0]                 \n__________________________________________________________________________________________________\nre_lu_201 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_201[0][0]    \n__________________________________________________________________________________________________\nconv2d_205 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_201[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_97 (Concatenate)    (None, 4, 4, 948)    0           concatenate_96[0][0]             \n                                                                 conv2d_205[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_202 (BatchN (None, 4, 4, 948)    3792        concatenate_97[0][0]             \n__________________________________________________________________________________________________\nre_lu_202 (ReLU)                (None, 4, 4, 948)    0           batch_normalization_202[0][0]    \n__________________________________________________________________________________________________\nconv2d_206 (Conv2D)             (None, 4, 4, 128)    121344      re_lu_202[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_203 (BatchN (None, 4, 4, 128)    512         conv2d_206[0][0]                 \n__________________________________________________________________________________________________\nre_lu_203 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_203[0][0]    \n__________________________________________________________________________________________________\nconv2d_207 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_203[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_98 (Concatenate)    (None, 4, 4, 980)    0           concatenate_97[0][0]             \n                                                                 conv2d_207[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_204 (BatchN (None, 4, 4, 980)    3920        concatenate_98[0][0]             \n__________________________________________________________________________________________________\nre_lu_204 (ReLU)                (None, 4, 4, 980)    0           batch_normalization_204[0][0]    \n__________________________________________________________________________________________________\nconv2d_208 (Conv2D)             (None, 4, 4, 128)    125440      re_lu_204[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_205 (BatchN (None, 4, 4, 128)    512         conv2d_208[0][0]                 \n__________________________________________________________________________________________________\nre_lu_205 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_205[0][0]    \n__________________________________________________________________________________________________\nconv2d_209 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_205[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_99 (Concatenate)    (None, 4, 4, 1012)   0           concatenate_98[0][0]             \n                                                                 conv2d_209[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_206 (BatchN (None, 4, 4, 1012)   4048        concatenate_99[0][0]             \n__________________________________________________________________________________________________\nre_lu_206 (ReLU)                (None, 4, 4, 1012)   0           batch_normalization_206[0][0]    \n__________________________________________________________________________________________________\nconv2d_210 (Conv2D)             (None, 4, 4, 506)    512072      re_lu_206[0][0]                  \n__________________________________________________________________________________________________\naverage_pooling2d_6 (AveragePoo (None, 2, 2, 506)    0           conv2d_210[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_207 (BatchN (None, 2, 2, 506)    2024        average_pooling2d_6[0][0]        \n__________________________________________________________________________________________________\nre_lu_207 (ReLU)                (None, 2, 2, 506)    0           batch_normalization_207[0][0]    \n__________________________________________________________________________________________________\nconv2d_211 (Conv2D)             (None, 2, 2, 128)    64768       re_lu_207[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_208 (BatchN (None, 2, 2, 128)    512         conv2d_211[0][0]                 \n__________________________________________________________________________________________________\nre_lu_208 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_208[0][0]    \n__________________________________________________________________________________________________\nconv2d_212 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_208[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_100 (Concatenate)   (None, 2, 2, 538)    0           average_pooling2d_6[0][0]        \n                                                                 conv2d_212[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_209 (BatchN (None, 2, 2, 538)    2152        concatenate_100[0][0]            \n__________________________________________________________________________________________________\nre_lu_209 (ReLU)                (None, 2, 2, 538)    0           batch_normalization_209[0][0]    \n__________________________________________________________________________________________________\nconv2d_213 (Conv2D)             (None, 2, 2, 128)    68864       re_lu_209[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_210 (BatchN (None, 2, 2, 128)    512         conv2d_213[0][0]                 \n__________________________________________________________________________________________________\nre_lu_210 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_210[0][0]    \n__________________________________________________________________________________________________\nconv2d_214 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_210[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_101 (Concatenate)   (None, 2, 2, 570)    0           concatenate_100[0][0]            \n                                                                 conv2d_214[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_211 (BatchN (None, 2, 2, 570)    2280        concatenate_101[0][0]            \n__________________________________________________________________________________________________\nre_lu_211 (ReLU)                (None, 2, 2, 570)    0           batch_normalization_211[0][0]    \n__________________________________________________________________________________________________\nconv2d_215 (Conv2D)             (None, 2, 2, 128)    72960       re_lu_211[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_212 (BatchN (None, 2, 2, 128)    512         conv2d_215[0][0]                 \n__________________________________________________________________________________________________\nre_lu_212 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_212[0][0]    \n__________________________________________________________________________________________________\nconv2d_216 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_212[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_102 (Concatenate)   (None, 2, 2, 602)    0           concatenate_101[0][0]            \n                                                                 conv2d_216[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_213 (BatchN (None, 2, 2, 602)    2408        concatenate_102[0][0]            \n__________________________________________________________________________________________________\nre_lu_213 (ReLU)                (None, 2, 2, 602)    0           batch_normalization_213[0][0]    \n__________________________________________________________________________________________________\nconv2d_217 (Conv2D)             (None, 2, 2, 128)    77056       re_lu_213[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_214 (BatchN (None, 2, 2, 128)    512         conv2d_217[0][0]                 \n__________________________________________________________________________________________________\nre_lu_214 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_214[0][0]    \n__________________________________________________________________________________________________\nconv2d_218 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_214[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_103 (Concatenate)   (None, 2, 2, 634)    0           concatenate_102[0][0]            \n                                                                 conv2d_218[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_215 (BatchN (None, 2, 2, 634)    2536        concatenate_103[0][0]            \n__________________________________________________________________________________________________\nre_lu_215 (ReLU)                (None, 2, 2, 634)    0           batch_normalization_215[0][0]    \n__________________________________________________________________________________________________\nconv2d_219 (Conv2D)             (None, 2, 2, 128)    81152       re_lu_215[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_216 (BatchN (None, 2, 2, 128)    512         conv2d_219[0][0]                 \n__________________________________________________________________________________________________\nre_lu_216 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_216[0][0]    \n__________________________________________________________________________________________________\nconv2d_220 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_216[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_104 (Concatenate)   (None, 2, 2, 666)    0           concatenate_103[0][0]            \n                                                                 conv2d_220[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_217 (BatchN (None, 2, 2, 666)    2664        concatenate_104[0][0]            \n__________________________________________________________________________________________________\nre_lu_217 (ReLU)                (None, 2, 2, 666)    0           batch_normalization_217[0][0]    \n__________________________________________________________________________________________________\nconv2d_221 (Conv2D)             (None, 2, 2, 128)    85248       re_lu_217[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_218 (BatchN (None, 2, 2, 128)    512         conv2d_221[0][0]                 \n__________________________________________________________________________________________________\nre_lu_218 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_218[0][0]    \n__________________________________________________________________________________________________\nconv2d_222 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_218[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_105 (Concatenate)   (None, 2, 2, 698)    0           concatenate_104[0][0]            \n                                                                 conv2d_222[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_219 (BatchN (None, 2, 2, 698)    2792        concatenate_105[0][0]            \n__________________________________________________________________________________________________\nre_lu_219 (ReLU)                (None, 2, 2, 698)    0           batch_normalization_219[0][0]    \n__________________________________________________________________________________________________\nconv2d_223 (Conv2D)             (None, 2, 2, 128)    89344       re_lu_219[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_220 (BatchN (None, 2, 2, 128)    512         conv2d_223[0][0]                 \n__________________________________________________________________________________________________\nre_lu_220 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_220[0][0]    \n__________________________________________________________________________________________________\nconv2d_224 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_220[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_106 (Concatenate)   (None, 2, 2, 730)    0           concatenate_105[0][0]            \n                                                                 conv2d_224[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_221 (BatchN (None, 2, 2, 730)    2920        concatenate_106[0][0]            \n__________________________________________________________________________________________________\nre_lu_221 (ReLU)                (None, 2, 2, 730)    0           batch_normalization_221[0][0]    \n__________________________________________________________________________________________________\nconv2d_225 (Conv2D)             (None, 2, 2, 128)    93440       re_lu_221[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_222 (BatchN (None, 2, 2, 128)    512         conv2d_225[0][0]                 \n__________________________________________________________________________________________________\nre_lu_222 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_222[0][0]    \n__________________________________________________________________________________________________\nconv2d_226 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_222[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_107 (Concatenate)   (None, 2, 2, 762)    0           concatenate_106[0][0]            \n                                                                 conv2d_226[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_223 (BatchN (None, 2, 2, 762)    3048        concatenate_107[0][0]            \n__________________________________________________________________________________________________\nre_lu_223 (ReLU)                (None, 2, 2, 762)    0           batch_normalization_223[0][0]    \n__________________________________________________________________________________________________\nconv2d_227 (Conv2D)             (None, 2, 2, 128)    97536       re_lu_223[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_224 (BatchN (None, 2, 2, 128)    512         conv2d_227[0][0]                 \n__________________________________________________________________________________________________\nre_lu_224 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_224[0][0]    \n__________________________________________________________________________________________________\nconv2d_228 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_224[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_108 (Concatenate)   (None, 2, 2, 794)    0           concatenate_107[0][0]            \n                                                                 conv2d_228[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_225 (BatchN (None, 2, 2, 794)    3176        concatenate_108[0][0]            \n__________________________________________________________________________________________________\nre_lu_225 (ReLU)                (None, 2, 2, 794)    0           batch_normalization_225[0][0]    \n__________________________________________________________________________________________________\nconv2d_229 (Conv2D)             (None, 2, 2, 128)    101632      re_lu_225[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_226 (BatchN (None, 2, 2, 128)    512         conv2d_229[0][0]                 \n__________________________________________________________________________________________________\nre_lu_226 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_226[0][0]    \n__________________________________________________________________________________________________\nconv2d_230 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_226[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_109 (Concatenate)   (None, 2, 2, 826)    0           concatenate_108[0][0]            \n                                                                 conv2d_230[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_227 (BatchN (None, 2, 2, 826)    3304        concatenate_109[0][0]            \n__________________________________________________________________________________________________\nre_lu_227 (ReLU)                (None, 2, 2, 826)    0           batch_normalization_227[0][0]    \n__________________________________________________________________________________________________\nconv2d_231 (Conv2D)             (None, 2, 2, 128)    105728      re_lu_227[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_228 (BatchN (None, 2, 2, 128)    512         conv2d_231[0][0]                 \n__________________________________________________________________________________________________\nre_lu_228 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_228[0][0]    \n__________________________________________________________________________________________________\nconv2d_232 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_228[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_110 (Concatenate)   (None, 2, 2, 858)    0           concatenate_109[0][0]            \n                                                                 conv2d_232[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_229 (BatchN (None, 2, 2, 858)    3432        concatenate_110[0][0]            \n__________________________________________________________________________________________________\nre_lu_229 (ReLU)                (None, 2, 2, 858)    0           batch_normalization_229[0][0]    \n__________________________________________________________________________________________________\nconv2d_233 (Conv2D)             (None, 2, 2, 128)    109824      re_lu_229[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_230 (BatchN (None, 2, 2, 128)    512         conv2d_233[0][0]                 \n__________________________________________________________________________________________________\nre_lu_230 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_230[0][0]    \n__________________________________________________________________________________________________\nconv2d_234 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_230[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_111 (Concatenate)   (None, 2, 2, 890)    0           concatenate_110[0][0]            \n                                                                 conv2d_234[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_231 (BatchN (None, 2, 2, 890)    3560        concatenate_111[0][0]            \n__________________________________________________________________________________________________\nre_lu_231 (ReLU)                (None, 2, 2, 890)    0           batch_normalization_231[0][0]    \n__________________________________________________________________________________________________\nconv2d_235 (Conv2D)             (None, 2, 2, 128)    113920      re_lu_231[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_232 (BatchN (None, 2, 2, 128)    512         conv2d_235[0][0]                 \n__________________________________________________________________________________________________\nre_lu_232 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_232[0][0]    \n__________________________________________________________________________________________________\nconv2d_236 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_232[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_112 (Concatenate)   (None, 2, 2, 922)    0           concatenate_111[0][0]            \n                                                                 conv2d_236[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_233 (BatchN (None, 2, 2, 922)    3688        concatenate_112[0][0]            \n__________________________________________________________________________________________________\nre_lu_233 (ReLU)                (None, 2, 2, 922)    0           batch_normalization_233[0][0]    \n__________________________________________________________________________________________________\nconv2d_237 (Conv2D)             (None, 2, 2, 128)    118016      re_lu_233[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_234 (BatchN (None, 2, 2, 128)    512         conv2d_237[0][0]                 \n__________________________________________________________________________________________________\nre_lu_234 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_234[0][0]    \n__________________________________________________________________________________________________\nconv2d_238 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_234[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_113 (Concatenate)   (None, 2, 2, 954)    0           concatenate_112[0][0]            \n                                                                 conv2d_238[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_235 (BatchN (None, 2, 2, 954)    3816        concatenate_113[0][0]            \n__________________________________________________________________________________________________\nre_lu_235 (ReLU)                (None, 2, 2, 954)    0           batch_normalization_235[0][0]    \n__________________________________________________________________________________________________\nconv2d_239 (Conv2D)             (None, 2, 2, 128)    122112      re_lu_235[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_236 (BatchN (None, 2, 2, 128)    512         conv2d_239[0][0]                 \n__________________________________________________________________________________________________\nre_lu_236 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_236[0][0]    \n__________________________________________________________________________________________________\nconv2d_240 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_236[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_114 (Concatenate)   (None, 2, 2, 986)    0           concatenate_113[0][0]            \n                                                                 conv2d_240[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_237 (BatchN (None, 2, 2, 986)    3944        concatenate_114[0][0]            \n__________________________________________________________________________________________________\nre_lu_237 (ReLU)                (None, 2, 2, 986)    0           batch_normalization_237[0][0]    \n__________________________________________________________________________________________________\nconv2d_241 (Conv2D)             (None, 2, 2, 128)    126208      re_lu_237[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_238 (BatchN (None, 2, 2, 128)    512         conv2d_241[0][0]                 \n__________________________________________________________________________________________________\nre_lu_238 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_238[0][0]    \n__________________________________________________________________________________________________\nconv2d_242 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_238[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_115 (Concatenate)   (None, 2, 2, 1018)   0           concatenate_114[0][0]            \n                                                                 conv2d_242[0][0]                 \n__________________________________________________________________________________________________\nglobal_average_pooling2d_1 (Glo (None, 1018)         0           concatenate_115[0][0]            \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 100)          101900      global_average_pooling2d_1[0][0] \n__________________________________________________________________________________________________\nactivation_1 (Activation)       (None, 100)          0           dense_1[0][0]                    \n==================================================================================================\nTotal params: 6,965,604\nTrainable params: 6,886,220\nNon-trainable params: 79,384\n__________________________________________________________________________________________________\n</code>\n</pre>        <pre><code>model.compile(loss = 'categorical_crossentropy',\n              optimizer=tf.keras.optimizers.Adam(lr=0.001),\n              metrics=['accuracy'])\n</code></pre>     <pre><code>import time\nstart = time.time()\nhistory = model.fit(ds_train,\n                    epochs=20,\n                    validation_data = ds_val)\n\nprint(f\"It took {time.time() - start} seconds\")\n</code></pre>      <pre>\n<code>Epoch 1/20\n586/586 [==============================] - 54s 93ms/step - loss: 3.1179 - accuracy: 0.2295 - val_loss: 3.1247 - val_accuracy: 0.2200\nEpoch 2/20\n586/586 [==============================] - 55s 93ms/step - loss: 2.7529 - accuracy: 0.2975 - val_loss: 9.7801 - val_accuracy: 0.1771\nEpoch 3/20\n586/586 [==============================] - 54s 93ms/step - loss: 2.4882 - accuracy: 0.3499 - val_loss: 2.8652 - val_accuracy: 0.2881\nEpoch 4/20\n586/586 [==============================] - 54s 92ms/step - loss: 2.3228 - accuracy: 0.3846 - val_loss: 2.9444 - val_accuracy: 0.2735\nEpoch 5/20\n586/586 [==============================] - 54s 92ms/step - loss: 2.1534 - accuracy: 0.4218 - val_loss: 5.3858 - val_accuracy: 0.3058\nEpoch 6/20\n586/586 [==============================] - 54s 92ms/step - loss: 1.9807 - accuracy: 0.4586 - val_loss: 2.4927 - val_accuracy: 0.3708\nEpoch 7/20\n586/586 [==============================] - 54s 92ms/step - loss: 1.8318 - accuracy: 0.4897 - val_loss: 2.4684 - val_accuracy: 0.3822\nEpoch 8/20\n586/586 [==============================] - 54s 92ms/step - loss: 1.7204 - accuracy: 0.5164 - val_loss: 2.2386 - val_accuracy: 0.4179\nEpoch 9/20\n586/586 [==============================] - 53s 91ms/step - loss: 1.5877 - accuracy: 0.5470 - val_loss: 2.6878 - val_accuracy: 0.3539\nEpoch 10/20\n586/586 [==============================] - 53s 91ms/step - loss: 1.4851 - accuracy: 0.5734 - val_loss: 2.2689 - val_accuracy: 0.4295\nEpoch 11/20\n586/586 [==============================] - 53s 91ms/step - loss: 1.3668 - accuracy: 0.6050 - val_loss: 2.2262 - val_accuracy: 0.4541\nEpoch 12/20\n586/586 [==============================] - 53s 91ms/step - loss: 1.2730 - accuracy: 0.6249 - val_loss: 2.0790 - val_accuracy: 0.4722\nEpoch 13/20\n586/586 [==============================] - 53s 91ms/step - loss: 1.1463 - accuracy: 0.6619 - val_loss: 2.1492 - val_accuracy: 0.4834\nEpoch 14/20\n586/586 [==============================] - 53s 91ms/step - loss: 1.0477 - accuracy: 0.6844 - val_loss: 2.5350 - val_accuracy: 0.4299\nEpoch 15/20\n586/586 [==============================] - 53s 91ms/step - loss: 0.9707 - accuracy: 0.7059 - val_loss: 2.0789 - val_accuracy: 0.5034\nEpoch 16/20\n586/586 [==============================] - 53s 90ms/step - loss: 0.8857 - accuracy: 0.7306 - val_loss: 2.1846 - val_accuracy: 0.4991\nEpoch 17/20\n586/586 [==============================] - 53s 91ms/step - loss: 0.7917 - accuracy: 0.7553 - val_loss: 4.4854 - val_accuracy: 0.4355\nEpoch 18/20\n586/586 [==============================] - 53s 91ms/step - loss: 0.7192 - accuracy: 0.7772 - val_loss: 2.3988 - val_accuracy: 0.4859\nEpoch 19/20\n586/586 [==============================] - 53s 91ms/step - loss: 0.6658 - accuracy: 0.7915 - val_loss: 2.4001 - val_accuracy: 0.4908\nEpoch 20/20\n586/586 [==============================] - 54s 92ms/step - loss: 0.6068 - accuracy: 0.8085 - val_loss: 3.2709 - val_accuracy: 0.3686\nIt took 1086.7563781738281 seconds\n</code>\n</pre>        <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\npd.DataFrame(history.history).plot(figsize=(12,8))\nplt.grid(True)\nplt.gca().set_ylim(0,2)\nplt.show()\n</code></pre>              <pre><code>model.evaluate(ds_test)\n</code></pre>      <pre>\n<code>157/157 [==============================] - 5s 33ms/step - loss: 3.3091 - accuracy: 0.3688\n</code>\n</pre>     <pre>\n<code>[3.3090856075286865, 0.36880001425743103]</code>\n</pre>"},{"location":"deep_learning/module5/Module5_2/#customiser-ce-que-se-passe-dans-fit","title":"Customiser ce que se passe dans <code>fit()</code>","text":""},{"location":"deep_learning/module5/Module5_2/#les-etapes-cachees-dans-fit","title":"Les \u00e9tapes cach\u00e9es dans <code>.fit()</code>","text":"<pre><code>class CustomModel(keras.Model):\n\n  def train_step(self, data):\n    x, y = data\n\n    with tf.GradientTape() as tape:\n      y_pred = self(x, training=True)\n      loss = self.compiled_loss(y, y_pred,\n                                regularization_losses=self.losses)\n\n    trainable_vars = self.trainable_variables\n    gradients = tape.gradient(loss, trainable_vars)\n    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n    self.compiled_metrics.update_state(y, y_pred)\n    return {m.name: m.result() for m in self.metrics}\n</code></pre> <ol> <li>On r\u00e9cup\u00e8re les donn\u00e9es du minibatch :</li> </ol> <pre><code>  def train_step(self, data):\n    x, y = data\n</code></pre> <p>Le type de donn\u00e9es que vous r\u00e9cup\u00e9rez d\u00e9pend \u00e9videmment du type de mod\u00e8le que vous entra\u00eenez et donc des donn\u00e9es que vous passez dans <code>.fit()</code>.</p> <p><pre><code>    with tf.GradientTape() as tape:\n</code></pre> <code>tf.GradientTape()</code> est la m\u00e9thode de Tensorflow pour diff\u00e9rentier les fonctions, i.e. calculer des d\u00e9riv\u00e9es et des d\u00e9riv\u00e9es partielles. Qui dit d\u00e9riv\u00e9es partielles, dit \u00e9tapes de mises \u00e0 jours des poids. </p> <ol> <li> <p>On calcule la pr\u00e9diction sur le minibatch : $$(\\hat{y}{1}, \\dots, \\hat{y}) =( f(x_{1}), \\dots, f(x_{N})) $$ <pre><code>      y_pred = self(x, training=True)\n</code></pre></p> </li> <li> <p>Pour chaque \\(\\hat{y}_{i}\\), on calcule l'erreur faite via la fonction de perte \\(\\mathcal{L}_{\\vartheta}(y_{i},\\hat{y}_{i})\\), et on en d\u00e9duit l'erreur moyenne sur le minibatch.</p> </li> </ol> \\[\\mathcal{L}_{\\vartheta} = \\frac{1}{N}\\sum_{i=1}^{N}\\mathcal{L}_{\\vartheta}(y_{i},\\hat{y}_{i})\\] <pre><code>      loss = self.compiled_loss(y, y_pred,\n                                regularization_losses=self.losses)\n</code></pre> <p>On rappelle que la fonction de perte est d\u00e9finie dans la <code>.compile()</code>. </p> <ol> <li>On calcule alors le gradient pour chaque param\u00e8tre dans \\(\\vartheta\\), i.e.</li> </ol> <p>\\(\\(\\nabla \\mathcal{L}_{\\vartheta}\\)\\) <pre><code>    trainable_vars = self.trainable_variables\n    gradients = tape.gradient(loss, trainable_vars)\n</code></pre></p> <ol> <li>On met alors \u00e0 jour les param\u00e8tres :</li> </ol> <p>$$ w_{i} \\leftarrow w_{i} - \\eta \\frac{\\partial \\mathcal{L}{\\vartheta}}{\\partial w}(\\vartheta), \\ b_{i} \\leftarrow b_{i} - \\eta \\frac{\\partial \\mathcal{L}{\\vartheta}}{\\partial b}(\\vartheta).$$ <pre><code>    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n</code></pre></p> <ol> <li>On met alors \u00e0 jour les m\u00e9triques (loss, accuracy, ...) et on renvoit un dictionnaire contenant ces mise \u00e0 jours.</li> </ol> <pre><code>    self.compiled_metrics.update_state(y, y_pred)\n    return {m.name: m.result() for m in self.metrics}\n</code></pre>       <p>Voyons un peu comment se comportent ses diff\u00e9rentes \u00e9tapes.</p>       <p>Inutile de lancer \u00e7a sur un vrai mod\u00e8le, plusieurs centaines de couches et millions de param\u00e8tres. Nous voulons d'abord juste voir comme nt cela fonctionne. Cr\u00e9ons donc un dataset et un mod\u00e8le compl\u00e8tement na\u00eff.</p>      <pre><code>class CustomModel(keras.Model):\n\n  def train_step(self, data):\n    print()\n    print(f\"----Etape: {self.step_counter}\")\n    self.step_counter += 1\n\n    x, y = data\n    print(f'D\u00e9but du train : {x.shape}, {y.shape}')\n\n    with tf.GradientTape() as tape:\n      print(f'Start GradientTape step {x.shape}')\n      y_pred = self(x, training=True)\n      print(f'Prediction done {y_pred.shape}')\n      loss = self.compiled_loss(y, y_pred,\n                                regularization_losses=self.losses)\n      print(f'loss {loss}')\n\n    trainable_vars = self.trainable_variables\n    gradients = tape.gradient(loss, trainable_vars)\n    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n    self.compiled_metrics.update_state(y, y_pred)\n    return {m.name: m.result() for m in self.metrics}\n</code></pre>     <pre><code>#Cr\u00e9ons un dataset dummy\nt_x = tf.random.uniform([30, 4], dtype=tf.float32)\nt_y = tf.range(30)\n\nds_x = tf.data.Dataset.from_tensor_slices(t_x)\nds_y = tf.data.Dataset.from_tensor_slices(t_y)\n\nds = tf.data.Dataset.zip((ds_x, ds_y))\n\nds = ds.batch(3)\n\nfrom tensorflow.keras import Model\n# Dummy model\ninput = Input(shape=(4,))\n\nx = Dense(32)(input)\noutput = Dense(1)(x)\n\nmodel = CustomModel(input,x)\n\nmodel.compile(loss = 'mean_absolute_error',\n              optimizer=tf.keras.optimizers.Adam(lr=0.001),\n              run_eagerly=True)\nmodel.step_counter = 0\nmodel.summary()\n</code></pre>      <pre>\n<code>Model: \"custom_model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 4)]               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 32)                160       \n=================================================================\nTotal params: 160\nTrainable params: 160\nNon-trainable params: 0\n_________________________________________________________________\n</code>\n</pre>        <pre><code>model.fit(ds,\n          epochs=1,\n          verbose=0)\n</code></pre>      <pre>\n<code>\n----Etape: 0\nStart train step (None, 4), (None,)\nStart GradientTape step (None, 4)\nPrediction done (None, 32)\nloss Tensor(\"mean_absolute_error/weighted_loss/value:0\", shape=(), dtype=float32)\n\n----Etape: 1\nStart train step (None, 4), (None,)\nStart GradientTape step (None, 4)\nPrediction done (None, 32)\nloss Tensor(\"mean_absolute_error/weighted_loss/value:0\", shape=(), dtype=float32)\n</code>\n</pre>         <p>Pas tr\u00e8s concluant hein ? on ne voit m\u00eame pas toutes les \u00e9tapes. C'est parce que Python est un langage lent, la plupart des structure internes de Tensorflow sont cod\u00e9s dans un langage beaucoup plus rapide tel que le C ou le C++.</p> <p>Ce que l'on \u00e9crit n'est dont pas toujours ce que l'on obtient vraiment. Pour que <code>tf.keras</code> fasse les instructions de fa\u00e7on s\u00e9quentielle, on complie le mod\u00e8le en rajoutant l'option :</p> <pre><code>run_eagerly=True\n</code></pre>      <pre><code>model.compile(loss = 'mean_absolute_error',\n              optimizer=tf.keras.optimizers.Adam(lr=0.001),\n              run_eagerly=True)\nmodel.step_counter = 0\nmodel.summary()\n\nmodel.fit(ds,\n          epochs=1,\n          verbose=0)\n</code></pre>      <pre>\n<code>Model: \"custom_model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 4)]               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 32)                160       \n=================================================================\nTotal params: 160\nTrainable params: 160\nNon-trainable params: 0\n_________________________________________________________________\n\n----Etape: 0\nStart train step (3, 4), (3,)\nStart GradientTape step (3, 4)\nPrediction done (3, 32)\nloss 1.0454397201538086\n\n----Etape: 1\nStart train step (3, 4), (3,)\nStart GradientTape step (3, 4)\nPrediction done (3, 32)\nloss 3.949005126953125\n\n----Etape: 2\nStart train step (3, 4), (3,)\nStart GradientTape step (3, 4)\nPrediction done (3, 32)\nloss 6.941404342651367\n\n----Etape: 3\nStart train step (3, 4), (3,)\nStart GradientTape step (3, 4)\nPrediction done (3, 32)\nloss 9.953435897827148\n\n----Etape: 4\nStart train step (3, 4), (3,)\nStart GradientTape step (3, 4)\nPrediction done (3, 32)\nloss 12.954936027526855\n\n----Etape: 5\nStart train step (3, 4), (3,)\nStart GradientTape step (3, 4)\nPrediction done (3, 32)\nloss 15.921150207519531\n\n----Etape: 6\nStart train step (3, 4), (3,)\nStart GradientTape step (3, 4)\nPrediction done (3, 32)\nloss 18.93268394470215\n\n----Etape: 7\nStart train step (3, 4), (3,)\nStart GradientTape step (3, 4)\nPrediction done (3, 32)\nloss 21.93629264831543\n\n----Etape: 8\nStart train step (3, 4), (3,)\nStart GradientTape step (3, 4)\nPrediction done (3, 32)\nloss 24.937978744506836\n\n----Etape: 9\nStart train step (3, 4), (3,)\nStart GradientTape step (3, 4)\nPrediction done (3, 32)\nloss 27.9443302154541\n</code>\n</pre>     <pre>\n<code>&lt;tensorflow.python.keras.callbacks.History at 0x7f258a26f278&gt;</code>\n</pre>"},{"location":"deep_learning/module5/Module5_2/#prise-en-main-de-gradienttape","title":"Prise en main de <code>GradientTape</code>","text":"<p><code>GradientTape</code> enregistre les op\u00e9rations qui sont faites dans un graphe (voir rappel du module 1 sur comment une fonction peut se d\u00e9finir comme une graphe), afin de calculer la diff\u00e9rentielle de cette fonction.</p> <p>Prenons un exemple simple, la fonction \\(f(x) = x^{2}\\), on souhaite calculer sa d\u00e9riv\u00e9e en 3. Les formules classiques d'analyse diff\u00e9rentielle nous donnent alors \\(f'(x)=2x\\) et donc \\(f'(3)=6\\).</p> <p>Avec <code>GradientTape</code>, on fait comme cela.</p>      <pre><code>x = tf.constant(3.0)\nwith tf.GradientTape() as tape:\n  tape.watch(x)\n  y = x * x\n\ndy_dx = tape.gradient(y, x)\n</code></pre>     <pre><code>dy_dx\n</code></pre>      <p>Dans le cas de <code>tf.keras</code>, la partie <code>tape.watch(x)</code> qui nous dit par rapport \u00e0 quelle variable nous allons d\u00e9river n'est pas n\u00e9cessaire, <code>tf.keras</code> sait tr\u00e8s bien quels sont les param\u00e8tres dans le r\u00e9seau de neurones que nous entra\u00eenons.</p>       <p>On peut \u00e9videmment le combiner pour calculer des d\u00e9riv\u00e9es secondes.</p>      <pre><code>x = tf.constant(3.0)\nwith tf.GradientTape() as tape:\n  tape.watch(x)\n  with tf.GradientTape() as tape2:\n    tape2.watch(x)\n    y = x * x\n  dy_dx = tape2.gradient(y, x)     \nd2y_dx2 = tape.gradient(dy_dx, x)\n</code></pre>     <pre><code>dy_dx\n</code></pre>     <pre><code>d2y_dx2\n</code></pre>"},{"location":"deep_learning/module5/Module5_2/#we-need-to-go-deeper","title":"We need to go deeper","text":"<p>Dans les modules suivants nous verrons comment modifier la m\u00e9thode  <code>.fit()</code> pour qu'elle corresponde \u00e0 l'entra\u00eenement que l'on souhaite, par exemple lorque l'on entra\u00eenera un autoencodeur variationnel.</p> <p>Il faut aussi noter que l'on peut compl\u00e8tement \u00e9crire sa boucle d'entra\u00eenement sans passer en aucune fa\u00e7on par la m\u00e9thode <code>.fit()</code>. Ce qui est pratique, voire m\u00eame n\u00e9c\u00e9ssaire si l'on souhaite impl\u00e9menter certaines bonnes pratiques lors de l'entra\u00eenement de certains mod\u00e8les comme les GAN.</p>       <pre><code>epochs = ...\nloss_fn = tf.keras.losses.[..]\nmetric_fn = tf.keras.metrics.[...]\noptimizer = tf.keras.optimizers.[...]\n\n@tf.function\ndef train_step(x, y):\n  with tf.GradientTape() as tape:\n    #pr\u00e9diction sur le minibatch\n    y_pred = model(x, training=True)\n    #calcul de la fonction de perte moyenne sur le minibatch\n    loss_value = loss_fn(y, y_pred)\n\n  # calcul des gradients et retropropagation\n  grads = tape.gradient(loss_value, model.trainable_weights)\n  optimizer.apply_gradients(zip(grads, model.trainable_weights))\n\n  # mise \u00e0 jour des m\u00e9triques\n  metric_fn.update_state(y, y_pred)\n\n  return loss_value\n\n@tf.function\ndef test_step(x, y):\n  y_pred = model(x, training=False)\n  loss_value = loss_fn(y, y_pred)\n  metric_fn.update_state(y, y_pred)\n\n  return loss_value\n\nfor epoch in range(epochs):\n  print(f\"\\nD\u00e9but de l'\u00e9poque {epoch+1},\")\n  start_time = time.time()\n\n  # it\u00e9ration sur les minibatchs du dataset\n  for step, (x_batch_train, y_batch_train) in enumerate(ds):\n    loss_value = train_step(x_batch_train, y_batch_train)\n\n    # Log tous les 10 batches\n    if step % 10 == 0:\n      print(f\"Loss sur le batch \u00e0 l'\u00e9tape {step} : {float(loss_value):.4f}\")\n\n  # Affichage des m\u00e9triques \u00e0 la fin de l'\u00e9poque\n  metric = metric_fn.result()\n  print(f\"M\u00e9trique pour l'\u00e9poque : {float(metric):.4f} \\n\")\n\n  # Reset de la m\u00e9trique \u00e0 la fin de chaque \u00e9poque\n  metric_fn.reset_states()\n\n  # validation loop \u00e0 la fin de chaque \u00e9poque\n  for x_batch_val, y_batch_val in ds_val:\n    val_loss = test_step(x_batch_val, y_batch_val)\n\n  val_metric = metric_fn.result()\n  metric_fn.reset_states()\n  print()\n  print(f\"Loss de validation : {float(val_loss):.4f}\")\n  print(f\"M\u00e9trique de validation : {float(val_metric):.4f}\")\n  print(f\"Dur\u00e9e de l'\u00e9poque: {time.time() - start_time:.2fs}\")\n</code></pre>       <p>D\u00e9taillons les parties importantes.</p>       <ul> <li>On lance une boucle <code>for</code> pour it\u00e9rer sur les \u00e9poques.</li> <li>Pour chaque \u00e9poque, on ouvre une autre boucle <code>for</code>  pour it\u00e9rer sur les batchs du dataset.</li> <li>Pour chaque batch, on ouvre un <code>GradientTape()</code>, o\u00f9 l'on calcule l'\u00e9tape de feedforward.</li> <li>Une fois fini, on calcule le gradient par rapport aux poids du mod\u00e8le et l'on met \u00e0 jour ces poids.</li> </ul>       <p>D\u00e9taillons plus. Il est \u00e0 noter ici que pour cette boucle, nous avons d\u00e9j\u00e0 acc\u00e8s aux batchs. Notre dataset est donc d\u00e9j\u00e0 sous la forme tensorielle via, l'API <code>tf.data.Dataset</code>.</p> <p>Premi\u00e8rement, on fixe les variables : le nombre d'\u00e9poques, et les diff\u00e9rents fonctions que l'on utilisera.  <pre><code>epochs = ...\nloss_fn = tf.keras.losses.[..]\nmetric_fn = tf.keras.metrics.[...]\noptimizer = tf.keras.optimizers.[...]\n</code></pre></p> <p>On lance alors la boucle principale sur le nombre d'\u00e9poque. <pre><code>for epoch in range(epochs):\n  print(f\"\\nD\u00e9but de l'\u00e9poque {epoch+1},\")\n  start_time = time.time()\n</code></pre></p> <p>Pour chaque batch, on lance alors l'\u00e9tape d'entra\u00eenement (on revient dessus plus tard) et on affiche la perte disons par exemple tous les 10 minibatchs. <pre><code>for step, (x_batch_train, y_batch_train) in enumerate(ds):\n    loss_value = train_step(x_batch_train, y_batch_train)\n\n    # Log tous les 10 batches\n    if step % 10 == 0:\n      print(f\"Loss sur le batch \u00e0 l'\u00e9tape {step} : {float(loss_value):.4f}\")\n</code></pre></p> <p>Une fois que tous les minibatchs sont pass\u00e9s, l'\u00e9poque est finie. On affiche alors la m\u00e9trique moyenne obtenue \u00e0 la fin. <pre><code>  # Affichage des m\u00e9triques \u00e0 la fin de l'\u00e9poque\n  metric = metric_fn.result()\n  print(f\"M\u00e9trique pour l'\u00e9poque : {float(metric):.4f} \\n\")\n</code></pre></p> <p>On remet \u00e0 z\u00e9ro la m\u00e9trique pour le d\u00e9but de la nouvelle \u00e9poque. <pre><code>  # Reset de la m\u00e9trique \u00e0 la fin de chaque \u00e9poque\n  metric_fn.reset_states()\n</code></pre></p> <p>Si on souhaite  avoir un dataset de validation, c'est ici que \u00e7a se passe. Comme pour <code>train_step</code>, on y revient bient\u00f4t. <pre><code>  # validation loop \u00e0 la fin de chaque \u00e9poque\n  for x_batch_val, y_batch_val in ds_val:\n    val_loss = test_step(x_batch_val, y_batch_val)\n</code></pre></p> <p>On affiche les m\u00e9triques de validation. <pre><code>  val_metric = metric_fn.result()\n  metric_fn.reset_states()\n  print()\n  print(f\"Loss de validation : {float(val_loss):.4f}\")\n  print(f\"M\u00e9trique de validation : {float(val_metric):.4f}\")\n  print(f\"Dur\u00e9e de l'\u00e9poque: {time.time() - start_time:.2fs}\")\n</code></pre></p> <p>Comme expliqu\u00e9 plus haut, <code>tf.keras</code> a en fait deux modes de fonctionnement, et le fonctionnement de base est celui dit <code>eager mode</code>, ce qui fait que les instructions donn\u00e9es dans une fonctions d\u00e9finie \u00e0 la main, comme ici pour <code>def train_step</code>, seront ex\u00e9cut\u00e9es les unes \u00e0 la suites des autres, ce qui est long.</p> <p>Le d\u00e9corateur <code>@tf.function</code> permet de transformer toute fonction n'yant come variable que des tenseurs en un graphe statique. Il n'est pas n\u00e9cessaire d'en savoir plus sur ces fameux graphes, la seule chose \u00e0 savoir est que cela augmente la vitesse \u00e0 laquelle les op\u00e9rations sont faites dans la fonction.</p> <p>En dehors de cela, la fonction <code>train_step</code> est exactement la m\u00eame que celle d\u00e9finie dans <code>.fit()</code>, de la m\u00eame fa\u00e7on pour la fonction de validation.</p> <pre><code>@tf.function\ndef train_step(x, y):\n  with tf.GradientTape() as tape:\n    #pr\u00e9diction sur le minibatch\n    y_pred = model(x, training=True)\n    #calcul de la fonction de perte moyenne sur le minibatch\n    loss_value = loss_fn(y, y_pred)\n\n  # calcul des gradients et retropropagation\n  grads = tape.gradient(loss_value, model.trainable_weights)\n  optimizer.apply_gradients(zip(grads, model.trainable_weights))\n\n  # mise \u00e0 jour des m\u00e9triques\n  metric_fn.update_state(y, y_pred)\n\n  return loss_value\n</code></pre> <pre><code>@tf.function\ndef test_step(x, y):\n  y_pred = model(x, training=False)\n  loss_value = loss_fn(y, y_pred)\n  metric_fn.update_state(y, y_pred)\n\n  return loss_value\n</code></pre>"},{"location":"deep_learning/module5/Module5_2/#callbacks","title":"Callbacks","text":"<p>En plus des arguments classiques tel que <code>epochs</code> ou <code>validation_data</code>, la m\u00e9thode <code>.fit()</code> accepte aussi l'argument <code>callbacks</code>. Les callbacks permettent de fournir une liste d'intruction que <code>tf.keras</code> \u00e0 certains moment pr\u00e9cis de l'entra\u00eenement : </p> <ul> <li>au d\u00e9but (\u00e0 la fin) de l'\u00e9poque actuelle,</li> <li>au d\u00e9but (\u00e0 la fin de l'\u00e9tape de minibatch actuelle,</li> <li>au d\u00e9but (\u00e0 la fin) de l'entra\u00eenement.</li> </ul> <p>Nous avons d\u00e9j\u00e0 vu le callback <code>LearningRateScheduler</code> dans le module suivant lorsque nous parlions du Learninf Rate Decay. L'id\u00e9e est ici de passer en revue les plus importants, et de voir la structure interne d'un callback pour en \u00e9crire un nous m\u00eame.</p>"},{"location":"deep_learning/module5/Module5_2/#les-callbacks-importants","title":"Les callbacks importants","text":""},{"location":"deep_learning/module5/Module5_2/#tfkerascallbacksearlystopping","title":"<code>tf.keras.callbacks.EarlyStopping</code>","text":"<p><code>EarlyStopping</code> permet \u00e0 <code>tf.keras</code> d'aretter de lui m\u00eame l'entra\u00eenement. On lui passe la m\u00e9trique que l'on souhaite monitorer, et les cri\u00e8res d'arr\u00eats.</p> <p>On l'appelle vec les param\u00e8tres suivants.</p> <pre><code>keras.callbacks.EarlyStopping(monitor='val_loss',\n                              min_delta=1e-2,\n                              patience=10,\n                              verbose=1)\n</code></pre> <p>D\u00e9taillons.</p> <p><pre><code>keras.callbacks.EarlyStopping(monitor='val_loss',\n</code></pre> D\u00e9termine la m\u00e9trique \u00e0 surveillez, principalement la <code>val_loss</code>, mais on peut aussi monitorer <code>val_acc</code>, ou les m\u00e9triques d'entra\u00eenement. Ici, l'entra\u00eenement s'arette lorsque <code>val_loss</code> ne s'am\u00e9liore plus.</p> <pre><code>                 min_delta=1e-2,\n</code></pre> <p>On pr\u00e9cise le \"ne s'am\u00e9liore plus\" : on dit que la val_loss ne s'am\u00e9liore plus si</p> \\[\\mathrm{val \\_loss}(t+1) - \\mathrm{val\\_loss}(t) \\leq 10^{-2}.\\] <pre><code>                 patience=10,\n                 verbose=1)\n</code></pre> <p>Si pendant 10 \u00e9poques \u00e7a ne s'am\u00e9liore pas, on ar\u00eatte l'entra\u00eenement. </p>"},{"location":"deep_learning/module5/Module5_2/#tfkerascallbacksmodelcheckpoint","title":"<code>tf.keras.callbacks.ModelCheckpoint</code>","text":"<p><code>ModelCheckpoint</code> permet lui de faire des sauvegarde r\u00e9guli\u00e8re du mod\u00e8le suivant un crit\u00e8re de m\u00e9trique. Une nouvelle version du mod\u00e8le ne sera enr\u00e9gistr\u00e9e que si la m\u00e9trique s'est am\u00e9lior\u00e9e.</p>       <pre><code>keras.callbacks.ModelCheckpoint('./weights.{epoch:02d}-{val_loss:02d}.h5',\n                                                 monitor='val_loss',\n                                                 verbose=1,\n                                                 save_best_only=True,\n                                                 save_weights_only=False,\n                                                 mode='auto',\n                                                 save_freq='epoch')\n</code></pre>"},{"location":"deep_learning/module5/Module5_2/#tfkerascallbackslearningratescheduler","title":"<code>tf.keras.callbacks.LearningRateScheduler</code>","text":"<pre><code>def exponential_decay(lr0,step):\n    def exponential_decay_fn(epoch):\n        return lr0*0.1**(epoch/step)\n    return exponential_decay_fn\n\nexponential_decay_fn = exponential_decay(lr0 = lr0, step = 20)\n\nkeras.callbacks.LearningRateScheduler(exponential_decay_fn)\n</code></pre>"},{"location":"deep_learning/module5/Module5_2/#tfkerascallbackstensorboard","title":"<code>tf.keras.callbacks.TensorBoard</code>","text":"<pre><code>logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\ntensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n</code></pre>       <pre><code>%tensorboard --logdir logs\n</code></pre>       <pre><code>from tensorboard import notebook\nnotebook.list() # View open TensorBoard instances\n#Control TensorBoard display. If no port is provided, \n#the most recently launched TensorBoard is used\nnotebook.display(port=6006, height=1000)\n</code></pre>       <pre><code>callbacks_fit = [keras.callbacks.EarlyStopping(\n                 # Stop training when `val_loss` is no longer improving\n                 monitor='val_loss',\n                 # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n                 min_delta=1e-2,\n                 # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n                 patience=10,\n                 verbose=1),\n                 keras.callbacks.LearningRateScheduler(exponential_decay_fn),\n                 keras.callbacks.ModelCheckpoint('./weights.{epoch:02d}-.hdf5',\n                                                 monitor='val_loss',\n                                                 verbose=1,\n                                                 save_best_only=True,\n                                                 save_weights_only=False,\n                                                 mode='auto',\n                                                 save_freq='epoch')]\n</code></pre>"},{"location":"deep_learning/module5/Module5_2/#structure-interne-dun-callback","title":"Structure interne d'un callback","text":"<p>Ce sont tous des sousclasses de la classe <code>tf.keras.callbacks.Callback</code>.</p> <p>On peut les passer en liste lorsque l'on fait appel \u00e0 l'une des 3 commandes suivantes.</p> <pre><code>   model.fit()\n   model.evaluate()\n   model.predict()\n</code></pre>       <p>Un callback fait une action particuli\u00e8re avec une \u00e9tape particuli\u00e8re, pour d\u00e9finir cette \u00e9tape on a les m\u00e9thodes suivantes.</p> <ul> <li> <p>M\u00e9thodes globales <pre><code>on_(train|test|predict)_begin(self, logs=None)\n</code></pre> <pre><code>on_(train|test|predict)_end(self, logs=None)\n</code></pre></p> </li> <li> <p>Batch-level m\u00e9thodes <pre><code>on_(train|test|predict)_batch_begin(self, batch, logs=None)\n</code></pre> <pre><code>on_(train|test|predict)_batch_end(self, batch, logs=None)\n</code></pre> Ici, logs est un dictionnaire contenant les diff\u00e9rentes m\u00e9triques.</p> </li> <li> <p>Epoch-level m\u00e9thodes (uniquement durant lentra\u00eenement) <pre><code>on_epoch_begin(self, epoch, logs=None)\n</code></pre> <pre><code>on_epoch_end(self, epoch, logs=None)\n</code></pre></p> </li> </ul>      <pre><code>#Cr\u00e9ons un dataset dummy\nt_x = tf.random.uniform([30, 4], dtype=tf.float32)\nt_y = tf.range(30)\n\nds_x = tf.data.Dataset.from_tensor_slices(t_x)\nds_y = tf.data.Dataset.from_tensor_slices(t_y)\n\nds = tf.data.Dataset.zip((ds_x, ds_y))\n\nds = ds.batch(3)\n\nfrom tensorflow.keras import Model\n# Dummy model\ninput = Input(shape=(4,))\n\nx = Dense(32)(input)\noutput = Dense(1)(x)\n\nmodel = Model(input,x)\n\nmodel.compile(loss = 'mean_absolute_error',\n              optimizer=tf.keras.optimizers.Adam(lr=0.001),\n              metrics=['mean_absolute_error'])\n\nmodel.summary()\n</code></pre>      <pre>\n<code>Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 4)]               0         \n_________________________________________________________________\ndense (Dense)                (None, 32)                160       \n=================================================================\nTotal params: 160\nTrainable params: 160\nNon-trainable params: 0\n_________________________________________________________________\n</code>\n</pre>        <pre><code>class CustomCallback(keras.callbacks.Callback):\n    def on_train_begin(self, logs=None):\n        keys = list(logs.keys())\n        print(f\"D\u00e9but de l'entra\u00eenement, les cl\u00e9s du log sont: {keys}\")\n\n    def on_train_end(self, logs=None):\n        keys = list(logs.keys())\n        print(f\"Fin de l'entra\u00eenement, les cl\u00e9s du log sont: {keys}\")\n\n    def on_epoch_begin(self, epoch, logs=None):\n        keys = list(logs.keys())\n        print(f\"D\u00e9but de l'\u00e9poque {epoch}, les cl\u00e9s du log sont: {keys}\")\n\n    def on_epoch_end(self, epoch, logs=None):\n        keys = list(logs.keys())\n        print(f\"Fin de l'\u00e9poque {epoch}, les cl\u00e9s du log sont: {keys}\")\n\n    def on_train_batch_begin(self, batch, logs=None):\n        keys = list(logs.keys())\n        print(f\"Entra\u00eenement : d\u00e9but du batch {batch}, les cl\u00e9s du log sont: {keys}\")\n\n    def on_train_batch_end(self, batch, logs=None):\n        keys = list(logs.keys())\n        print(f\"Entra\u00eenement : fin du batch {batch}, les cl\u00e9s du log sont: {keys}\")\n</code></pre>     <pre><code>history = model.fit(ds,\n                    epochs=1,\n                    verbose=0,\n                    callbacks=[CustomCallback()])\n</code></pre>      <pre>\n<code>D\u00e9but de l'entra\u00eenement, les cl\u00e9s du log sont: []\nD\u00e9but de l'\u00e9poque 0, les cl\u00e9s du log sont: []\nEntra\u00eenement : d\u00e9but du batch 0, les cl\u00e9s du log sont: []\nEntra\u00eenement : fin du batch 0, les cl\u00e9s du log sont: ['loss', 'mean_absolute_error']\nEntra\u00eenement : d\u00e9but du batch 1, les cl\u00e9s du log sont: []\nEntra\u00eenement : fin du batch 1, les cl\u00e9s du log sont: ['loss', 'mean_absolute_error']\nEntra\u00eenement : d\u00e9but du batch 2, les cl\u00e9s du log sont: []\nEntra\u00eenement : fin du batch 2, les cl\u00e9s du log sont: ['loss', 'mean_absolute_error']\nEntra\u00eenement : d\u00e9but du batch 3, les cl\u00e9s du log sont: []\nEntra\u00eenement : fin du batch 3, les cl\u00e9s du log sont: ['loss', 'mean_absolute_error']\nEntra\u00eenement : d\u00e9but du batch 4, les cl\u00e9s du log sont: []\nEntra\u00eenement : fin du batch 4, les cl\u00e9s du log sont: ['loss', 'mean_absolute_error']\nEntra\u00eenement : d\u00e9but du batch 5, les cl\u00e9s du log sont: []\nEntra\u00eenement : fin du batch 5, les cl\u00e9s du log sont: ['loss', 'mean_absolute_error']\nEntra\u00eenement : d\u00e9but du batch 6, les cl\u00e9s du log sont: []\nEntra\u00eenement : fin du batch 6, les cl\u00e9s du log sont: ['loss', 'mean_absolute_error']\nEntra\u00eenement : d\u00e9but du batch 7, les cl\u00e9s du log sont: []\nEntra\u00eenement : fin du batch 7, les cl\u00e9s du log sont: ['loss', 'mean_absolute_error']\nEntra\u00eenement : d\u00e9but du batch 8, les cl\u00e9s du log sont: []\nEntra\u00eenement : fin du batch 8, les cl\u00e9s du log sont: ['loss', 'mean_absolute_error']\nEntra\u00eenement : d\u00e9but du batch 9, les cl\u00e9s du log sont: []\nEntra\u00eenement : fin du batch 9, les cl\u00e9s du log sont: ['loss', 'mean_absolute_error']\nFin de l'\u00e9poque 0, les cl\u00e9s du log sont: ['loss', 'mean_absolute_error']\nFin de l'entra\u00eenement, les cl\u00e9s du log sont: []\n</code>\n</pre>         <p>Allons plus dans le d\u00e9tail et voyons par exemple de quelles fa\u00e7ons sont calcul\u00e9es les m\u00e9triques donn\u00e9es par <code>tf.keras</code>. </p>      <pre><code>class LossCallback(tf.keras.callbacks.Callback):\n\n    def on_train_batch_end(self, batch, logs):\n        print(f'Batch {batch}, la perte est de {logs[\"loss\"]:.2f}.\\n')\n\n    def on_epoch_end(self, epoch, logs):\n        print(f'La perte moyenne pour l\u00e9poque {epoch} est {logs[\"loss\"]:.2f} \\n')\n\ncb = LossCallback()\n</code></pre>     <pre><code>history = model.fit(ds,\n                    epochs=1,\n                    verbose=0,\n                    callbacks=[cb])\n</code></pre>      <pre>\n<code>Batch 0, la perte est de 1.01.\n\nBatch 1, la perte est de 2.46.\n\nBatch 2, la perte est de 3.93.\n\nBatch 3, la perte est de 5.41.\n\nBatch 4, la perte est de 6.90.\n\nBatch 5, la perte est de 8.40.\n\nBatch 6, la perte est de 9.89.\n\nBatch 7, la perte est de 11.39.\n\nBatch 8, la perte est de 12.89.\n\nBatch 9, la perte est de 14.39.\n\nLa perte moyenne pour l\u00e9poque 0 est 14.39 \n\n</code>\n</pre>         <p>La fa\u00e7on dont <code>tf.keras</code> calcule la fonction de perte \u00e0 la fin de de chaque \u00e9poque est donc en faisant une moyenne : \u00e0 la fin de l'\u00e9poque, la m\u00e9trique de perte donn\u00e9e est la perte moyenne sur un minibatch, et \u00e0 la fin de chaque batch, la m\u00e9trique de perte donn\u00e9e est la perte moyenne mouvante.</p> <p>Si l'on souhaite avoir la perte moyenne sur une observation, ou la valeur de la perte sur chaque batch, comment faire ?</p>       \\[\\mathrm{AvgLoss}_{n+1} := \\frac{\\mathrm{Loss}_{n+1}+n\\cdot \\mathrm{AvgLoss}_{n}}{n+1}\\] \\[(n+1)\\cdot\\mathrm{AvgLoss}_{n+1} -n\\cdot \\mathrm{AvgLoss}_{n} = \\mathrm{Loss}_{n+1}\\]      <pre><code>class LossCallback(tf.keras.callbacks.Callback):\n\n    def __init__(self, L = 0):\n        self.L = L\n\n    def on_train_batch_end(self, batch, logs):\n      if batch == 0:\n        print(f'Batch {batch}, loss is {logs[\"loss\"]:.2f}.\\n')\n        #logs['loss'] gives the running avg mean, not the mean of the minibatch\n      else:\n        print(f'Batch {batch}, loss is {(batch+1)*logs[\"loss\"]-self.L:.2f}.\\n')\n        print(f'Batch {batch}, running loss is {logs[\"loss\"]:.2f}.\\n')\n      self.L = (batch+1)*logs[\"loss\"]\n\n    def on_epoch_end(self, epoch, logs):\n        print(f'Avg loss on {epoch} is {logs[\"loss\"]:.2f} \\n')\n\ncb = LossCallback()\n\nmodel.fit(ds,\n          epochs=1,\n          verbose=0,\n          callbacks=[cb])\n</code></pre>      <pre>\n<code>Batch 0, loss is 0.95.\n\nBatch 1, loss is 3.78.\n\nBatch 1, running loss is 2.36.\n\nBatch 2, loss is 6.73.\n\nBatch 2, running loss is 3.82.\n\nBatch 3, loss is 9.71.\n\nBatch 3, running loss is 5.29.\n\nBatch 4, loss is 12.72.\n\nBatch 4, running loss is 6.78.\n\nBatch 5, loss is 15.75.\n\nBatch 5, running loss is 8.27.\n\nBatch 6, loss is 18.74.\n\nBatch 6, running loss is 9.77.\n\nBatch 7, loss is 21.78.\n\nBatch 7, running loss is 11.27.\n\nBatch 8, loss is 24.76.\n\nBatch 8, running loss is 12.77.\n\nBatch 9, loss is 27.69.\n\nBatch 9, running loss is 14.26.\n\nAvg loss on 0 is 14.26 \n\n</code>\n</pre>     <pre>\n<code>&lt;tensorflow.python.keras.callbacks.History at 0x7f258a3c1da0&gt;</code>\n</pre>        <pre><code>class PrintValTrainRatioCallback(tf.keras.callbacks.Callback):\n\n  def on_epoch_end(self, epoch, logs):\n    print(f'\\n Validation-Train Ratio : {logs[\"val_loss\"]/logs[\"loss\"]:.2f}')\n\ncb = PrintValTrainRatioCallback()\n</code></pre>"},{"location":"deep_learning/module5/Module5_2/#annexe","title":"Annexe","text":"<pre><code>import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Dense\n\nimport random\n\nprint(tf.__version__)\nprint(keras.__version__)\n\nRANDOM_SEED = 42\n\n\nrandom.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)\n\n\n\n#dummy dataset\nt_x = tf.random.uniform([30, 4], dtype=tf.float32)\nt_y = tf.range(30)\nds_x = tf.data.Dataset.from_tensor_slices(t_x)\nds_y = tf.data.Dataset.from_tensor_slices(t_y)\nds = tf.data.Dataset.zip((ds_x, ds_y))\nds = ds.batch(3)\n\nclass LossCallback(tf.keras.callbacks.Callback):\n\n    def on_train_batch_end(self, batch, logs):\n        print(f'Batch {batch}, loss is {logs[\"loss\"]:.2f}.\\n')\n        #logs['loss'] gives the running avg mean, not the mean of the minibatch\n\n    def on_epoch_end(self, epoch, logs):\n        print(f'Avg loss on {epoch} is {logs[\"loss\"]:.2f} \\n')\n\ncb = LossCallback()\n\nfrom tensorflow.keras import Model\n\ninput = Input(shape=(4,))\n\nx = Dense(2)(input)\nX = Dense(1)(x)\n\nmodel = Model(input,x)\n\nmodel.compile(loss = 'mean_absolute_error',\n              optimizer=tf.keras.optimizers.SGD())\n\nhistory = model.fit(ds,\n                    epochs=1,\n                    verbose=0,\n                    callbacks=[cb])\n</code></pre>      <pre>\n<code>2.2.0\n2.3.0-tf\nBatch 0, loss is 0.83.\n\nBatch 1, loss is 2.00.\n\nBatch 2, loss is 3.40.\n\nBatch 3, loss is 4.83.\n\nBatch 4, loss is 6.27.\n\nBatch 5, loss is 7.75.\n\nBatch 6, loss is 9.21.\n\nBatch 7, loss is 10.71.\n\nBatch 8, loss is 12.20.\n\nBatch 9, loss is 13.67.\n\nAvg loss on 0 is 13.67 \n\n</code>\n</pre>"},{"location":"deep_learning/module6/Module6_2/","title":"Pratique","text":"(function() {   function addWidgetsRenderer() {     var requireJsScript = document.createElement('script');     requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js';      var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]');     var jupyterWidgetsScript = document.createElement('script');     var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js';     var widgetState;      // Fallback for older version:     try {       widgetState = mimeElement &amp;&amp; JSON.parse(mimeElement.innerHTML);        if (widgetState &amp;&amp; (widgetState.version_major &lt; 2 || !widgetState.version_major)) {         widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js';       }     } catch(e) {}      jupyterWidgetsScript.src = widgetRendererSrc;      document.body.appendChild(requireJsScript);     document.body.appendChild(jupyterWidgetsScript);   }    document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }());"},{"location":"deep_learning/module6/Module6_2/#tp-module-6-la-segmentation","title":"TP Module 6 : La segmentation","text":""},{"location":"deep_learning/module6/Module6_2/#import-libs","title":"Import libs","text":"<pre><code>import tensorflow as tf\nfrom tensorflow import keras\n\nprint(tf.__version__)\nprint(keras.__version__)\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nimport os\nimport numpy as np\n\n# freeze de l'al\u00e9atoire, pour avoir des exp\u00e9riences reproductibles.\nRANDOM_SEED = 42\n\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\ntf.random.set_seed(RANDOM_SEED)\n</code></pre>      <pre>\n<code>2.2.0\n2.3.0-tf\n</code>\n</pre>        <pre><code>!nvidia-smi\n</code></pre>      <pre>\n<code>Wed May 27 13:55:24 2020       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   46C    P0    33W / 250W |    357MiB / 16280MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n</code>\n</pre>        <pre><code>from tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import ReLU\nfrom tensorflow.keras.layers import Add\nfrom tensorflow.keras.layers import MaxPool2D\nfrom tensorflow.keras.layers import GlobalAvgPool2D\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import Concatenate\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Conv2DTranspose\n\nfrom tensorflow.keras import Model\n</code></pre>"},{"location":"deep_learning/module6/Module6_2/#les-datasets-les-metriques-appropries","title":"Les datasets &amp; les m\u00e9triques appropri\u00e9s","text":""},{"location":"deep_learning/module6/Module6_2/#nucleus","title":"Nucleus","text":"<pre><code>!unzip -q nucleus.zip -d nucleus_dir\n</code></pre>     <pre><code>x_train_dir = '/content/nucleus_dir/data/train/'\ny_train_dir = '/content/nucleus_dir/data/train/solutions/'\n\nx_test_dir = '/content/nucleus_dir/data/test/'\ny_test_dir = '/content/nucleus_dir/data/test/solutions/'\n\ninput_size = (224, 224)\n\nimport glob\nfrom PIL import Image\n\ndef load_image(path): \n  return np.asarray(Image.open(path).resize(input_size, Image.NEAREST))\n\ndef load_dir(dir):\n  img_paths = glob.glob(dir+'*.png')\n  img_arr = np.array([load_image(p) for p in img_paths])\n  return img_arr\n</code></pre>      <p>Seul le jeu d'entra\u00eenement et le jeu de test ont les masques. On tuilisera donc le jeu de test comme jeu de validation.</p>      <pre><code>x_train = load_dir(x_train_dir)\nprint('x_train shape', x_train.shape)\ny_train = load_dir(y_train_dir)\nprint('y_train shape', y_train.shape)\n\nx_test = load_dir(x_test_dir)\nprint('x_test shape', x_test.shape)\ny_test = load_dir(y_test_dir)\nprint('y_test shape', y_test.shape)\n</code></pre>      <pre>\n<code>x_train shape (555, 224, 224)\ny_train shape (555, 224, 224)\nx_test shape (42, 224, 224)\ny_test shape (42, 224, 224)\n</code>\n</pre>        <pre><code>x_train = tf.reshape(x_train, (-1, 224,224,1))\ny_train = tf.reshape(y_train, (-1, 224,224,1))\nx_test = tf.reshape(x_test, (-1, 224,224,1))\ny_test = tf.reshape(y_test, (-1, 224,224,1))\n\nx_train = x_train/ 255\ny_train = y_train/ 255\nx_test = x_test/ 255\ny_test = y_test/ 255\n</code></pre>     <pre><code>ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size=len(x_train)).batch(16)\nds_val = tf.data.Dataset.from_tensor_slices((x_test, y_test)).shuffle(buffer_size=len(x_test)).batch(16)\n</code></pre>      <p>Voyons \u00e0 quoi cela ressemble.</p>      <pre><code>%pylab inline\n\nobs = 1\n\nfigsize(15,15)\nsubplot(1, 2, 1)\nimshow(x_train[obs, :, :, 0])\nsubplot(1, 2, 2)\nimshow(y_train[obs, :, :, 0])\n</code></pre>      <pre>\n<code>Populating the interactive namespace from numpy and matplotlib\n</code>\n</pre>     <pre>\n<code>&lt;matplotlib.image.AxesImage at 0x7f656fb63b70&gt;</code>\n</pre>"},{"location":"deep_learning/module6/Module6_2/#camvid","title":"CamVid","text":"<p>Le dataset pr\u00e9sent\u00e9 ici est le dataset commun\u00e9ment nomm\u00e9 CamVid, pour Cambridge-driving Labeled Video Database, c'est un dataset souvent utilis\u00e9 pour le benchmark de mod\u00e8le en segmentation s\u00e9mantique.</p> <p>Le Dataset se compose de photos prises du point de vue d'un conducteur de voiture, ce qui en fait un bon point de d\u00e9part pour les mod\u00e8les par exemple utilis\u00e9s dans les voitures autonomes.</p> <p>Attention ne vous leurrez pas, les voitures autonomes par exemple chez Tesla, fonctionnent gr\u00e2ce \u00e0 la combinaison de 90 mod\u00e8les de deep learning diff\u00e9rents, le tout cumulant 75000h d'entra\u00eenements au minimum. Mais cela reste un point de d\u00e9part.</p>      <pre><code>!git clone https://github.com/alexgkendall/SegNet-Tutorial.git\n\n!mkdir /data\n!mkdir /data/datasets/\n!mv /content/SegNet-Tutorial/ /data/datasets/\n</code></pre>      <pre>\n<code>fatal: destination path 'SegNet-Tutorial' already exists and is not an empty directory.\nmkdir: cannot create directory \u2018/data\u2019: File exists\nmkdir: cannot create directory \u2018/data/datasets/\u2019: File exists\nmv: cannot move '/content/SegNet-Tutorial/' to '/data/datasets/SegNet-Tutorial': Directory not empty\n</code>\n</pre>        <pre><code>input_size = (224, 224)\n</code></pre>     <pre><code>base = '/data/datasets/SegNet-Tutorial/CamVid/'\n\nx_train = load_dir(base+'train/')\nprint('x_train shape', x_train.shape)\ny_train = load_dir(base+'trainannot/')\nprint('y_train shape', y_train.shape)\n\nx_val = load_dir(base+'val/')\nprint('x_val shape', x_val.shape)\ny_val = load_dir(base+'valannot/')\nprint('y_val shape', y_val.shape)\n\nx_test = load_dir(base+'test/')\nprint('x_test shape', x_test.shape)\ny_test = load_dir(base+'testannot/')\nprint('y_test shape', y_test.shape)\n</code></pre>      <pre>\n<code>x_train shape (367, 224, 224, 3)\ny_train shape (367, 224, 224)\nx_val shape (101, 224, 224, 3)\ny_val shape (101, 224, 224)\nx_test shape (233, 224, 224, 3)\ny_test shape (233, 224, 224)\n</code>\n</pre>         <p>La Dataset contient au total \\(701\\) images annot\u00e9es \u00e0 la main.  Les annotations dans y correspondent \u00e0 un masque : chaque pixel \u00e0 une classe qui lui est d\u00e9fini, il y a en tout 32 classes dans ce dataset.  Les classes sont les suivantes :</p> <ul> <li>VegetationMisc</li> <li>SignSymbol</li> <li>Column_Pole</li> <li>Child</li> <li>Truck_Bus</li> <li>Train</li> <li>MotorcycleScooter</li> <li>Archway</li> <li>LaneMkgsNonDriv</li> <li>RoadShoulder</li> <li>Sky</li> <li>Misc_Text</li> <li>Tree</li> <li>Road</li> <li>OtherMoving</li> <li>LaneMkgsDriv</li> <li>Building</li> <li>ParkingBlock</li> <li>Wall</li> <li>SUVPickupTruck</li> <li>Animal</li> <li>Fence</li> <li>Pedestrian</li> <li>CartLuggagePram</li> <li>Car</li> <li>Tunnel</li> <li>Bicyclist</li> <li>Bridge</li> <li>TrafficLight</li> <li>Sidewalk</li> <li>TrafficCone</li> <li>Void</li> </ul> <p>et chaque classe est r\u00e9pr\u00e9sent\u00e9e par une combinaison RGB diff\u00e9rente :</p> <pre><code>192 192 0     VegetationMisc\n192 128 128 SignSymbol\n192 192 128 Column_Pole\n192 128 64   Child\n192 128 192 Truck_Bus\n192 64 128   Train\n192 0 192     MotorcycleScooter\n192 0 128     Archway\n192 0 64       LaneMkgsNonDriv\n128 128 192 RoadShoulder\n128 128 128 Sky\n128 128 64   Misc_Text\n128 128 0     Tree\n128 64 128   Road\n128 64 64     OtherMoving\n128 0 192     LaneMkgsDriv\n128 0 0     Building\n64 192 128  ParkingBlock\n64 192 0       Wall\n64 128 192   SUVPickupTruck\n64 128 64     Animal\n64 64 128     Fence\n64 64 0     Pedestrian\n64 0 192       CartLuggagePram\n64 0 128       Car\n64 0 64     Tunnel\n0 128 192     Bicyclist\n0 128 64       Bridge\n0 64 64     TrafficLight\n0 0 192     Sidewalk\n0 0 64       TrafficCone\n0 0 0         Void\n</code></pre>       <p>Les classes sont dans l'ordre et commencent \u00e0 \\(1\\). Par exemple, la classe \"Sky\" est repr\u00e9sent\u00e9e dans le masque par l'entier \\(11\\).</p>       <p>Voyons \u00e0 quoi cela ressemble.</p>      <pre><code>%pylab inline\n\nobs = 350\n\nfigsize(15,15)\nsubplot(1, 2, 1)\nimshow(x_train[obs])\nsubplot(1, 2, 2)\nimshow(y_train[obs, :, :])\n</code></pre>      <pre>\n<code>Populating the interactive namespace from numpy and matplotlib\n</code>\n</pre>     <pre>\n<code>&lt;matplotlib.image.AxesImage at 0x7f6507e15f98&gt;</code>\n</pre>              <p>Le but d'un mod\u00e8le de segmentation s\u00e9mantique est donc de pr\u00e9dire la classe de chaque pixels. Finalisons nos datasets.</p>      <pre><code>x_train = x_train / 255\nx_val = x_val / 255\nx_test = x_test / 255\n</code></pre>     <pre><code>ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size=len(x_train)).batch(3)\nds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val)).shuffle(buffer_size=len(x_val)).batch(3)\nds_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).shuffle(buffer_size=len(x_test)).batch(3)\n</code></pre>"},{"location":"deep_learning/module6/Module6_2/#iou-metrique","title":"IoU m\u00e9trique","text":"<p>Concernant les m\u00e9triques sp\u00e9cifiques au probl\u00e8mes de la segmentation, nous en avaons en particuliers deux : </p> <ul> <li> <p>Dice Coefficient : \\(\\(\\text{DC} = \\frac{2 TP}{2 TP + FP + FN} = \\frac{2|X \\cap Y|}{|X| + |Y|}\\)\\)</p> </li> <li> <p>Intersection over Union : \\(\\(\\text{IoU} = \\frac{TP}{TP + FP + FN} = \\frac{|X \\cap Y|}{|X| + |Y| - |X \\cap Y|}\\)\\)</p> </li> </ul> <p>Par d\u00e9finition, on voit que \\(\\text{DC} \\geq \\text{IoU}\\).</p>"},{"location":"deep_learning/module6/Module6_2/#la-convolution-transposee","title":"La convolution transpos\u00e9e","text":"<p>G\u00e9n\u00e9rons une feature map al\u00e9atoire.</p>      <pre><code>fm  = np.random.random(1*256*256*3).astype('float32')\nfm = tf.reshape(fm, (-1,256,256,3))\nfm.shape\n</code></pre>      <pre>\n<code>TensorShape([1, 256, 256, 3])</code>\n</pre>         <p>Une convolution classique, avec un <code>stride=1</code>, et un <code>padding='same'</code> ne change pas la dimension de <code>fm</code>.</p>      <pre><code>conv = Conv2D(3, kernel_size=3, strides=1, padding='same')\nfm_out = conv(fm)\nfm_out.shape\n</code></pre>      <pre>\n<code>TensorShape([1, 256, 256, 3])</code>\n</pre>         <p>Mais une convolution avec un <code>stride=1</code>, <code>padding='same'</code> elle change la dimension de <code>fm</code>, en la divisant par 2.</p> <p>Dans le cas o\u00f9 <code>padding=same</code>, la dimension de sortie \\(o\\) est compl\u00e8tement d\u00e9termin\u00e9e par le stride et la dimension d'entr\u00e9e \\(i\\), avec la formule suivante.</p> \\[o = \\left\\lceil \\frac{i}{s} \\right\\rceil\\]      <pre><code>conv2 = Conv2D(3, kernel_size=3, strides=2, padding='same')\nfm_out2 = conv2(fm)\nfm_out2.shape\n</code></pre>      <pre>\n<code>TensorShape([1, 128, 128, 3])</code>\n</pre>         <p>Pour la convolution transpos\u00e9e, comme cette op\u00e9ration est la contrepartie arri\u00e8re d'une convolution normale, cela signifie que la dimension de sortie d'une convolution normale correspond \u00e0 la dimension d'entr\u00e9e de la convolution transpos\u00e9e. En d'autres termes, alors que la forme de sortie de <code>Conv2D()</code> est divis\u00e9e par le stride, la dimension de sortie de <code>Conv2DTranspose()</code> est multipli\u00e9e par celui-ci.</p> \\[o=s\\cdot i\\]      <pre><code>convT = Conv2DTranspose(3, kernel_size=3, strides=2, padding='same')\nfm2 = convT(fm_out2)\nfm2.shape\n</code></pre>      <pre>\n<code>TensorShape([1, 256, 256, 3])</code>\n</pre>         <p>Attention, comme dit prc\u00e9demment, on ne r\u00e9cup\u00e8re que la dimension, on ne r\u00e9cup\u00e8re \u00e9videmment pas les valeurs initiales de <code>fm</code>.</p>      <pre><code>np.sum((fm==fm2).numpy(), axis=(0,1,2,3))\n</code></pre>      <pre>\n<code>0</code>\n</pre>"},{"location":"deep_learning/module6/Module6_2/#unet","title":"Unet","text":"<p>Unet est l'un des premiers r\u00e9seaux de neurones g\u00e9n\u00e9ralisant les \"skip connections\" de l'architecture ResNet pour la segmentation s\u00e9mantique. Il a \u00e9t\u00e9 \u00e0 la base d\u00e9velopp\u00e9 pour des probl\u00e8mes de m\u00e9decines, ie donner une classification des diff\u00e9rents types de cellules dans une boite de Petri. L'article d'origine n'avait un jeu de donn\u00e9e que d'une cinquantaine d'images de boites de Petri diff\u00e9rentes, et pourtant il a largement surpass\u00e9 l'\u00e9tat de l'art comtemporain.</p>"},{"location":"deep_learning/module6/Module6_2/#idee","title":"Id\u00e9e","text":"<ul> <li> <p>However, in many visual tasks, especially in biomedical image processing, the desired output should include localization, i.e. a class label is supposed to be assigned to each pixel. Moreover, thousands of training images are usually beyond reach in biomedial tasks.</p> </li> <li> <p>The main idea [...] is to supplement a usual contracting network by successive layers, where pooling operators are replaced by upsampling operators. Hence these layers increase the resolution of the output. In order to localize, high resolution features from the contracting path are combined with the upsampled output.</p> </li> <li> <p>The network does not have any fully connected layers and only uses [..] convolution [...].</p> </li> </ul>       <ul> <li>It consists of a contracting path (left side) and an expensive path (right side)</li> </ul>"},{"location":"deep_learning/module6/Module6_2/#les-briques-de-bases","title":"Les briques de bases","text":""},{"location":"deep_learning/module6/Module6_2/#downsampling","title":"Downsampling","text":"<ul> <li> <p>The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two \\(3 \\times 3\\) convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a \\(2 \\times 2\\) max pooling operation with strides \\(2\\). for downsampling.</p> </li> <li> <p>At each downsampling step we double the number of features channels.</p> </li> </ul>      <pre><code>def conv_relu(tensor, filters):\n  x = Conv2D(filters,\n             kernel_size=(3,3),\n             strides=(1,1),\n             padding='same',\n             kernel_initializer='he_normal')(tensor)\n  x = ReLU()(x)\n  x = Conv2D(filters,\n             kernel_size=(3,3),\n             strides=(1,1),\n             padding='same',\n             kernel_initializer='he_normal')(x)\n  x = ReLU()(x)\n\n  return x\n</code></pre>     <pre><code>def downsample(tensor):\n  x = MaxPool2D(pool_size=(2,2),\n                strides=(2,2))(tensor)\n\n  return x\n</code></pre>"},{"location":"deep_learning/module6/Module6_2/#upsampling","title":"Upsampling","text":"<ul> <li> <p>Every step in the expansive path consists of an upsampling of the feature map followed by a \\(2 \\times 2\\) convolution (\"up-convolution\") that halves the number of feature channels.</p> </li> <li> <p>A concatenation with the correspondingly cropped feature map from the contracting path, and two \\(3 \\times 3\\) convolutions, each followed by a ReLU.</p> </li> </ul>      <pre><code>def upsample(tensor, filters):\n  x = Conv2DTranspose(filters = filters,\n                      kernel_size = 2,\n                      strides = (2, 2),\n                      padding = \"same\",\n                      kernel_initializer = 'he_normal')(tensor)\n\n  return x \n</code></pre>"},{"location":"deep_learning/module6/Module6_2/#final-layer","title":"Final layer","text":"<ul> <li> <p>At the final layer a \\(1 \\times 1\\) convolution is used to map each \\(64\\)-component feature vector to the desired number of classes.</p> </li> <li> <p>In total the network has \\(23\\) convolutional layers.</p> </li> </ul>"},{"location":"deep_learning/module6/Module6_2/#mise-en-place-du-modele","title":"Mise en place du mod\u00e8le","text":"<pre><code>def get_unet(filters, num_classes, img_shape):\n\n  growth_rate = [filters, 2*filters, 4*filters, 8*filters, 16*filters, 8*filters, 4*filters, 2*filters, filters]\n  nb_blocks = 4\n  upsampling_fm = []\n\n  # input block\n  input = Input(shape=img_shape)\n  x = input\n\n  #downsampling blocks\n  for i in range(nb_blocks):\n    x = conv_relu(x, growth_rate[i])\n    upsampling_fm.append(x)\n    x = downsample(x)\n\n  # Bottleneck\n  upsampling = upsampling_fm[::-1]\n  x = conv_relu(x, growth_rate[4])\n\n  #upsampling path\n  for i in range(nb_blocks):\n    x = upsample(x, growth_rate[5+i])\n    x = Concatenate()([x, upsampling[i]])\n    x = conv_relu(x, growth_rate[i])\n\n  outputs =  Conv2D(num_classes,\n                    kernel_size=(1,1),\n                    strides=(1,1),\n                    kernel_initializer='he_normal',\n                    activation='sigmoid')(x) \n\n  model = Model(input, outputs)\n\n  return model\n</code></pre>     <pre><code>filters = 32\nnum_classes = 1\nimg_shape = (224, 224, 1)\n\nmodel = get_unet(filters, num_classes, img_shape)\nmodel.summary()\n</code></pre>      <pre>\n<code>Model: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            [(None, 224, 224, 1) 0                                            \n__________________________________________________________________________________________________\nconv2d_19 (Conv2D)              (None, 224, 224, 32) 320         input_2[0][0]                    \n__________________________________________________________________________________________________\nre_lu_18 (ReLU)                 (None, 224, 224, 32) 0           conv2d_19[0][0]                  \n__________________________________________________________________________________________________\nconv2d_20 (Conv2D)              (None, 224, 224, 32) 9248        re_lu_18[0][0]                   \n__________________________________________________________________________________________________\nre_lu_19 (ReLU)                 (None, 224, 224, 32) 0           conv2d_20[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_4 (MaxPooling2D)  (None, 112, 112, 32) 0           re_lu_19[0][0]                   \n__________________________________________________________________________________________________\nconv2d_21 (Conv2D)              (None, 112, 112, 64) 18496       max_pooling2d_4[0][0]            \n__________________________________________________________________________________________________\nre_lu_20 (ReLU)                 (None, 112, 112, 64) 0           conv2d_21[0][0]                  \n__________________________________________________________________________________________________\nconv2d_22 (Conv2D)              (None, 112, 112, 64) 36928       re_lu_20[0][0]                   \n__________________________________________________________________________________________________\nre_lu_21 (ReLU)                 (None, 112, 112, 64) 0           conv2d_22[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_5 (MaxPooling2D)  (None, 56, 56, 64)   0           re_lu_21[0][0]                   \n__________________________________________________________________________________________________\nconv2d_23 (Conv2D)              (None, 56, 56, 128)  73856       max_pooling2d_5[0][0]            \n__________________________________________________________________________________________________\nre_lu_22 (ReLU)                 (None, 56, 56, 128)  0           conv2d_23[0][0]                  \n__________________________________________________________________________________________________\nconv2d_24 (Conv2D)              (None, 56, 56, 128)  147584      re_lu_22[0][0]                   \n__________________________________________________________________________________________________\nre_lu_23 (ReLU)                 (None, 56, 56, 128)  0           conv2d_24[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_6 (MaxPooling2D)  (None, 28, 28, 128)  0           re_lu_23[0][0]                   \n__________________________________________________________________________________________________\nconv2d_25 (Conv2D)              (None, 28, 28, 256)  295168      max_pooling2d_6[0][0]            \n__________________________________________________________________________________________________\nre_lu_24 (ReLU)                 (None, 28, 28, 256)  0           conv2d_25[0][0]                  \n__________________________________________________________________________________________________\nconv2d_26 (Conv2D)              (None, 28, 28, 256)  590080      re_lu_24[0][0]                   \n__________________________________________________________________________________________________\nre_lu_25 (ReLU)                 (None, 28, 28, 256)  0           conv2d_26[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_7 (MaxPooling2D)  (None, 14, 14, 256)  0           re_lu_25[0][0]                   \n__________________________________________________________________________________________________\nconv2d_27 (Conv2D)              (None, 14, 14, 512)  1180160     max_pooling2d_7[0][0]            \n__________________________________________________________________________________________________\nre_lu_26 (ReLU)                 (None, 14, 14, 512)  0           conv2d_27[0][0]                  \n__________________________________________________________________________________________________\nconv2d_28 (Conv2D)              (None, 14, 14, 512)  2359808     re_lu_26[0][0]                   \n__________________________________________________________________________________________________\nre_lu_27 (ReLU)                 (None, 14, 14, 512)  0           conv2d_28[0][0]                  \n__________________________________________________________________________________________________\nconv2d_transpose_4 (Conv2DTrans (None, 28, 28, 256)  524544      re_lu_27[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_4 (Concatenate)     (None, 28, 28, 512)  0           conv2d_transpose_4[0][0]         \n                                                                 re_lu_25[0][0]                   \n__________________________________________________________________________________________________\nconv2d_29 (Conv2D)              (None, 28, 28, 32)   147488      concatenate_4[0][0]              \n__________________________________________________________________________________________________\nre_lu_28 (ReLU)                 (None, 28, 28, 32)   0           conv2d_29[0][0]                  \n__________________________________________________________________________________________________\nconv2d_30 (Conv2D)              (None, 28, 28, 32)   9248        re_lu_28[0][0]                   \n__________________________________________________________________________________________________\nre_lu_29 (ReLU)                 (None, 28, 28, 32)   0           conv2d_30[0][0]                  \n__________________________________________________________________________________________________\nconv2d_transpose_5 (Conv2DTrans (None, 56, 56, 128)  16512       re_lu_29[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_5 (Concatenate)     (None, 56, 56, 256)  0           conv2d_transpose_5[0][0]         \n                                                                 re_lu_23[0][0]                   \n__________________________________________________________________________________________________\nconv2d_31 (Conv2D)              (None, 56, 56, 64)   147520      concatenate_5[0][0]              \n__________________________________________________________________________________________________\nre_lu_30 (ReLU)                 (None, 56, 56, 64)   0           conv2d_31[0][0]                  \n__________________________________________________________________________________________________\nconv2d_32 (Conv2D)              (None, 56, 56, 64)   36928       re_lu_30[0][0]                   \n__________________________________________________________________________________________________\nre_lu_31 (ReLU)                 (None, 56, 56, 64)   0           conv2d_32[0][0]                  \n__________________________________________________________________________________________________\nconv2d_transpose_6 (Conv2DTrans (None, 112, 112, 64) 16448       re_lu_31[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_6 (Concatenate)     (None, 112, 112, 128 0           conv2d_transpose_6[0][0]         \n                                                                 re_lu_21[0][0]                   \n__________________________________________________________________________________________________\nconv2d_33 (Conv2D)              (None, 112, 112, 128 147584      concatenate_6[0][0]              \n__________________________________________________________________________________________________\nre_lu_32 (ReLU)                 (None, 112, 112, 128 0           conv2d_33[0][0]                  \n__________________________________________________________________________________________________\nconv2d_34 (Conv2D)              (None, 112, 112, 128 147584      re_lu_32[0][0]                   \n__________________________________________________________________________________________________\nre_lu_33 (ReLU)                 (None, 112, 112, 128 0           conv2d_34[0][0]                  \n__________________________________________________________________________________________________\nconv2d_transpose_7 (Conv2DTrans (None, 224, 224, 32) 16416       re_lu_33[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_7 (Concatenate)     (None, 224, 224, 64) 0           conv2d_transpose_7[0][0]         \n                                                                 re_lu_19[0][0]                   \n__________________________________________________________________________________________________\nconv2d_35 (Conv2D)              (None, 224, 224, 256 147712      concatenate_7[0][0]              \n__________________________________________________________________________________________________\nre_lu_34 (ReLU)                 (None, 224, 224, 256 0           conv2d_35[0][0]                  \n__________________________________________________________________________________________________\nconv2d_36 (Conv2D)              (None, 224, 224, 256 590080      re_lu_34[0][0]                   \n__________________________________________________________________________________________________\nre_lu_35 (ReLU)                 (None, 224, 224, 256 0           conv2d_36[0][0]                  \n__________________________________________________________________________________________________\nconv2d_37 (Conv2D)              (None, 224, 224, 1)  257         re_lu_35[0][0]                   \n==================================================================================================\nTotal params: 6,659,969\nTrainable params: 6,659,969\nNon-trainable params: 0\n__________________________________________________________________________________________________\n</code>\n</pre>        <pre><code>img = tf.reshape(x_train[0, :, :], (-1, 224,224, 1))\np = model(img)\np.shape\n</code></pre>      <pre>\n<code>TensorShape([1, 224, 224, 1])</code>\n</pre>        <pre><code>model.compile(loss='binary_crossentropy',\n              optimizer=keras.optimizers.RMSprop(),\n              metrics=[\"accuracy\"])\n</code></pre>     <pre><code>history = model.fit(x_train, y_train,\n                    epochs=100,\n                    batch_size=16,\n                    validation_split=0.1)\n</code></pre>      <pre>\n<code>Epoch 1/100\n32/32 [==============================] - 21s 658ms/step - loss: 0.0083 - accuracy: 0.9965 - val_loss: 0.0279 - val_accuracy: 0.9912\nEpoch 2/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0092 - accuracy: 0.9961 - val_loss: 0.0309 - val_accuracy: 0.9905\nEpoch 3/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0126 - accuracy: 0.9952 - val_loss: 0.0240 - val_accuracy: 0.9924\nEpoch 4/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0077 - accuracy: 0.9967 - val_loss: 0.0328 - val_accuracy: 0.9918\nEpoch 5/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0082 - accuracy: 0.9965 - val_loss: 0.0578 - val_accuracy: 0.9924\nEpoch 6/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0077 - accuracy: 0.9967 - val_loss: 0.0270 - val_accuracy: 0.9910\nEpoch 7/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0073 - accuracy: 0.9969 - val_loss: 0.0460 - val_accuracy: 0.9921\nEpoch 8/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0077 - accuracy: 0.9969 - val_loss: 0.0288 - val_accuracy: 0.9918\nEpoch 9/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0074 - accuracy: 0.9970 - val_loss: 0.0276 - val_accuracy: 0.9919\nEpoch 10/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0063 - accuracy: 0.9973 - val_loss: 0.0567 - val_accuracy: 0.9914\nEpoch 11/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0070 - accuracy: 0.9971 - val_loss: 0.0334 - val_accuracy: 0.9916\nEpoch 12/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0748 - accuracy: 0.9909 - val_loss: 0.0275 - val_accuracy: 0.9916\nEpoch 13/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0067 - accuracy: 0.9974 - val_loss: 0.0314 - val_accuracy: 0.9910\nEpoch 14/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0047 - accuracy: 0.9980 - val_loss: 0.0327 - val_accuracy: 0.9921\nEpoch 15/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0062 - accuracy: 0.9974 - val_loss: 0.0323 - val_accuracy: 0.9898\nEpoch 16/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0058 - accuracy: 0.9975 - val_loss: 0.0432 - val_accuracy: 0.9918\nEpoch 17/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0060 - accuracy: 0.9975 - val_loss: 0.0425 - val_accuracy: 0.9927\nEpoch 18/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0057 - accuracy: 0.9976 - val_loss: 0.0459 - val_accuracy: 0.9917\nEpoch 19/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0057 - accuracy: 0.9976 - val_loss: 0.0292 - val_accuracy: 0.9901\nEpoch 20/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0055 - accuracy: 0.9977 - val_loss: 0.0505 - val_accuracy: 0.9923\nEpoch 21/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0054 - accuracy: 0.9977 - val_loss: 0.0648 - val_accuracy: 0.9917\nEpoch 22/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0055 - accuracy: 0.9977 - val_loss: 0.0443 - val_accuracy: 0.9922\nEpoch 23/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0050 - accuracy: 0.9979 - val_loss: 0.0522 - val_accuracy: 0.9909\nEpoch 24/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0051 - accuracy: 0.9978 - val_loss: 0.0456 - val_accuracy: 0.9927\nEpoch 25/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0048 - accuracy: 0.9979 - val_loss: 0.0429 - val_accuracy: 0.9921\nEpoch 26/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0048 - accuracy: 0.9980 - val_loss: 0.0403 - val_accuracy: 0.9913\nEpoch 27/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0045 - accuracy: 0.9981 - val_loss: 0.0598 - val_accuracy: 0.9917\nEpoch 28/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0046 - accuracy: 0.9981 - val_loss: 0.0416 - val_accuracy: 0.9904\nEpoch 29/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0043 - accuracy: 0.9982 - val_loss: 0.0554 - val_accuracy: 0.9918\nEpoch 30/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0430 - accuracy: 0.9935 - val_loss: 0.0317 - val_accuracy: 0.9923\nEpoch 31/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0036 - accuracy: 0.9984 - val_loss: 0.0376 - val_accuracy: 0.9924\nEpoch 32/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0036 - accuracy: 0.9984 - val_loss: 0.0435 - val_accuracy: 0.9906\nEpoch 33/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0042 - accuracy: 0.9983 - val_loss: 0.0421 - val_accuracy: 0.9918\nEpoch 34/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0038 - accuracy: 0.9984 - val_loss: 0.0480 - val_accuracy: 0.9921\nEpoch 35/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0044 - accuracy: 0.9982 - val_loss: 0.0341 - val_accuracy: 0.9916\nEpoch 36/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0045 - accuracy: 0.9982 - val_loss: 0.0343 - val_accuracy: 0.9915\nEpoch 37/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0035 - accuracy: 0.9985 - val_loss: 0.0464 - val_accuracy: 0.9913\nEpoch 38/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0036 - accuracy: 0.9985 - val_loss: 0.0425 - val_accuracy: 0.9918\nEpoch 39/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0040 - accuracy: 0.9983 - val_loss: 0.0527 - val_accuracy: 0.9919\nEpoch 40/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0034 - accuracy: 0.9985 - val_loss: 0.0477 - val_accuracy: 0.9911\nEpoch 41/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0036 - accuracy: 0.9985 - val_loss: 0.0555 - val_accuracy: 0.9921\nEpoch 42/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0037 - accuracy: 0.9985 - val_loss: 0.0412 - val_accuracy: 0.9921\nEpoch 43/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0032 - accuracy: 0.9986 - val_loss: 0.0457 - val_accuracy: 0.9919\nEpoch 44/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0034 - accuracy: 0.9986 - val_loss: 0.0423 - val_accuracy: 0.9906\nEpoch 45/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0034 - accuracy: 0.9986 - val_loss: 0.0396 - val_accuracy: 0.9886\nEpoch 46/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0032 - accuracy: 0.9987 - val_loss: 0.0425 - val_accuracy: 0.9924\nEpoch 47/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0031 - accuracy: 0.9987 - val_loss: 0.0511 - val_accuracy: 0.9918\nEpoch 48/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0031 - accuracy: 0.9987 - val_loss: 0.0469 - val_accuracy: 0.9918\nEpoch 49/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0030 - accuracy: 0.9987 - val_loss: 0.0455 - val_accuracy: 0.9917\nEpoch 50/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0031 - accuracy: 0.9987 - val_loss: 0.0468 - val_accuracy: 0.9919\nEpoch 51/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0029 - accuracy: 0.9988 - val_loss: 0.0427 - val_accuracy: 0.9915\nEpoch 52/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0028 - accuracy: 0.9988 - val_loss: 0.0473 - val_accuracy: 0.9919\nEpoch 53/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0028 - accuracy: 0.9988 - val_loss: 0.0476 - val_accuracy: 0.9915\nEpoch 54/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0026 - accuracy: 0.9989 - val_loss: 0.0480 - val_accuracy: 0.9918\nEpoch 55/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0028 - accuracy: 0.9988 - val_loss: 0.0499 - val_accuracy: 0.9923\nEpoch 56/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0026 - accuracy: 0.9989 - val_loss: 0.0580 - val_accuracy: 0.9921\nEpoch 57/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0026 - accuracy: 0.9989 - val_loss: 0.0525 - val_accuracy: 0.9915\nEpoch 58/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0025 - accuracy: 0.9990 - val_loss: 0.0514 - val_accuracy: 0.9917\nEpoch 59/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0024 - accuracy: 0.9990 - val_loss: 0.0551 - val_accuracy: 0.9916\nEpoch 60/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0026 - accuracy: 0.9989 - val_loss: 0.0591 - val_accuracy: 0.9922\nEpoch 61/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0021 - accuracy: 0.9991 - val_loss: 0.0569 - val_accuracy: 0.9919\nEpoch 62/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0023 - accuracy: 0.9990 - val_loss: 0.0702 - val_accuracy: 0.9918\nEpoch 63/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0023 - accuracy: 0.9990 - val_loss: 0.0481 - val_accuracy: 0.9921\nEpoch 64/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0021 - accuracy: 0.9991 - val_loss: 0.0641 - val_accuracy: 0.9922\nEpoch 65/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0020 - accuracy: 0.9991 - val_loss: 0.0672 - val_accuracy: 0.9919\nEpoch 66/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0021 - accuracy: 0.9991 - val_loss: 0.0704 - val_accuracy: 0.9919\nEpoch 67/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0023 - accuracy: 0.9991 - val_loss: 0.0510 - val_accuracy: 0.9921\nEpoch 68/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0017 - accuracy: 0.9993 - val_loss: 0.0788 - val_accuracy: 0.9918\nEpoch 69/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0019 - accuracy: 0.9992 - val_loss: 0.0563 - val_accuracy: 0.9921\nEpoch 70/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0017 - accuracy: 0.9993 - val_loss: 0.0687 - val_accuracy: 0.9923\nEpoch 71/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0019 - accuracy: 0.9992 - val_loss: 0.0530 - val_accuracy: 0.9921\nEpoch 72/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0017 - accuracy: 0.9993 - val_loss: 0.0545 - val_accuracy: 0.9921\nEpoch 73/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0017 - accuracy: 0.9993 - val_loss: 0.0623 - val_accuracy: 0.9912\nEpoch 74/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0017 - accuracy: 0.9993 - val_loss: 0.0633 - val_accuracy: 0.9922\nEpoch 75/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.0539 - val_accuracy: 0.9919\nEpoch 76/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.0644 - val_accuracy: 0.9922\nEpoch 77/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.0682 - val_accuracy: 0.9911\nEpoch 78/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.0599 - val_accuracy: 0.9920\nEpoch 79/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.0688 - val_accuracy: 0.9920\nEpoch 80/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.0666 - val_accuracy: 0.9918\nEpoch 81/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 0.0717 - val_accuracy: 0.9919\nEpoch 82/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 0.0657 - val_accuracy: 0.9919\nEpoch 83/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 0.0644 - val_accuracy: 0.9926\nEpoch 84/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0012 - accuracy: 0.9995 - val_loss: 0.0669 - val_accuracy: 0.9923\nEpoch 85/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0012 - accuracy: 0.9995 - val_loss: 0.0674 - val_accuracy: 0.9918\nEpoch 86/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0012 - accuracy: 0.9995 - val_loss: 0.0548 - val_accuracy: 0.9921\nEpoch 87/100\n32/32 [==============================] - 21s 656ms/step - loss: 9.9234e-04 - accuracy: 0.9996 - val_loss: 0.0708 - val_accuracy: 0.9919\nEpoch 88/100\n32/32 [==============================] - 21s 656ms/step - loss: 9.9478e-04 - accuracy: 0.9996 - val_loss: 0.0810 - val_accuracy: 0.9918\nEpoch 89/100\n32/32 [==============================] - 21s 656ms/step - loss: 0.0010 - accuracy: 0.9996 - val_loss: 0.0798 - val_accuracy: 0.9922\nEpoch 90/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.0602 - val_accuracy: 0.9916\nEpoch 91/100\n32/32 [==============================] - 21s 656ms/step - loss: 6.5260e-04 - accuracy: 0.9998 - val_loss: 0.0677 - val_accuracy: 0.9922\nEpoch 92/100\n32/32 [==============================] - 21s 657ms/step - loss: 7.2608e-04 - accuracy: 0.9997 - val_loss: 0.0695 - val_accuracy: 0.9919\nEpoch 93/100\n32/32 [==============================] - 21s 656ms/step - loss: 9.5700e-04 - accuracy: 0.9996 - val_loss: 0.0624 - val_accuracy: 0.9919\nEpoch 94/100\n32/32 [==============================] - 21s 656ms/step - loss: 9.3425e-04 - accuracy: 0.9997 - val_loss: 0.0735 - val_accuracy: 0.9920\nEpoch 95/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.0589 - val_accuracy: 0.9919\nEpoch 96/100\n32/32 [==============================] - 21s 657ms/step - loss: 7.5235e-04 - accuracy: 0.9997 - val_loss: 0.0711 - val_accuracy: 0.9920\nEpoch 97/100\n32/32 [==============================] - 21s 657ms/step - loss: 7.0484e-04 - accuracy: 0.9997 - val_loss: 0.0701 - val_accuracy: 0.9920\nEpoch 98/100\n32/32 [==============================] - 21s 656ms/step - loss: 8.2732e-04 - accuracy: 0.9997 - val_loss: 0.0761 - val_accuracy: 0.9919\nEpoch 99/100\n32/32 [==============================] - 21s 657ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.0624 - val_accuracy: 0.9922\nEpoch 100/100\n32/32 [==============================] - 21s 656ms/step - loss: 6.2872e-04 - accuracy: 0.9998 - val_loss: 0.0829 - val_accuracy: 0.9923\n</code>\n</pre>        <pre><code>%pylab inline\nfigsize(10, 5)\n\nsubplot(1, 2, 1)\nplot(history.history['accuracy'], label='accuracy')\nplot(history.history['val_accuracy'], label='validation accuracy')\nlegend()\n\nsubplot(1, 2, 2)\nplot(history.history['loss'], label='loss')\nplot(history.history['val_loss'], label='validation loss')\n\nlegend()\nshow()\n</code></pre>      <pre>\n<code>Populating the interactive namespace from numpy and matplotlib\n</code>\n</pre>             <pre><code>model.save('unet_nucleus_100_epochs.h5')\n</code></pre>     <pre><code>img = tf.reshape(x_train[num_obs], (-1, 224, 224))\np = model.predict(img)\np.shape\n</code></pre>      <pre>\n<code>(1, 224, 224, 1)</code>\n</pre>"},{"location":"deep_learning/module6/Module6_2/#prediction","title":"Pr\u00e9diction","text":"<pre><code>%pylab inline\nnum_obs=13\n\nimg = tf.reshape(x_test[num_obs], (-1, 224, 224))\np = model.predict(img)\np.shape\n\nfigsize(20, 5)\n\nsubplot(1, 3, 1)\nimshow(img[0, :, :,])\nxlabel('Input')\n\nsubplot(1, 3, 2)\nimshow(p[0, :, :, 0])\nxlabel('Pr\u00e9diction')\n\nsubplot(1, 3, 3)\nimshow(y_test[num_obs][:, :, 0])\nxlabel('V\u00e9rit\u00e9')\n\n#m = tf.keras.metrics.MeanIoU(num_classes=2)\n#m(y_train[num_obs][:, :], p[0, :, :]).numpy()\n</code></pre>      <pre>\n<code>Populating the interactive namespace from numpy and matplotlib\n</code>\n</pre>     <pre>\n<code>Text(0.5, 0, 'V\u00e9rit\u00e9')</code>\n</pre>"},{"location":"deep_learning/module6/Module6_2/#meaniou-via-custom-training-loop","title":"MeanIoU via custom training loop","text":"<pre><code>epochs = 150\nloss_fn = tf.keras.losses.BinaryCrossentropy()\nmetric_fn = tf.keras.metrics.MeanIoU(num_classes=2)\noptimizer = tf.keras.optimizers.RMSprop(lr=1e-3)\n\n@tf.function\ndef train_step(x, y):\n  with tf.GradientTape() as tape:\n    #pr\u00e9diction sur le minibatch\n    y_pred = model(x, training=True)\n    #calcul de la fonction de perte moyenne sur le minibatch\n    loss_value = loss_fn(y, y_pred)\n\n  # calcul des gradients et retropropagation\n  grads = tape.gradient(loss_value, model.trainable_weights)\n  optimizer.apply_gradients(zip(grads, model.trainable_weights))\n\n  # mise \u00e0 jour des m\u00e9triques\n  metric_fn.update_state(y, y_pred)\n\n  return loss_value\n\n@tf.function\ndef test_step(x, y):\n  y_pred = model(x, training=False)\n  loss_value = loss_fn(y, y_pred)\n  metric_fn.update_state(y, y_pred)\n\n  return loss_value\n\nfor epoch in range(epochs):\n  losses = 0 \n  step_counter = 0\n  print(f\"\\nD\u00e9but de l'\u00e9poque {epoch+1},\")\n  start_time = time.time()\n\n  # it\u00e9ration sur les minibatchs du dataset\n  for step, (x_batch_train, y_batch_train) in enumerate(ds):\n    loss_value = train_step(x_batch_train, y_batch_train)\n    losses += loss_value\n    step_counter += 1\n\n    # Log tous les 10 batches\n    if step % 10 == 0:\n      print(f\"Loss sur le batch \u00e0 l'\u00e9tape {step} : {float(loss_value):.4f}\")\n\n  # Affichage des m\u00e9triques \u00e0 la fin de l'\u00e9poque\n  metric = metric_fn.result()\n  print()\n  print(f\"Loss : {losses/step_counter:.4f}\")\n  print(f\"M\u00e9trique pour l'\u00e9poque : {float(metric):.4f} \\n\")\n\n  # Reset de la m\u00e9trique \u00e0 la fin de chaque \u00e9poque\n  metric_fn.reset_states()\n\n  # validation loop \u00e0 la fin de chaque \u00e9poque\n  for x_batch_val, y_batch_val in ds_val:\n    val_loss = test_step(x_batch_val, y_batch_val)\n\n  val_metric = metric_fn.result()\n  metric_fn.reset_states()\n  print()\n  print(f\"Loss de validation : {float(val_loss):.4f}\")\n  print(f\"M\u00e9trique de validation : {float(val_metric):.4f}\")\n  print(f\"Dur\u00e9e de l'\u00e9poque: {time.time() - start_time:.2fs}\")\n</code></pre>      <pre>\n<code>\nD\u00e9but de l'\u00e9poque 1,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 7.0191\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0430\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0293\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0351\n\nLoss pour l'\u00e9poque 1: 0.2819\nM\u00e9trique pour l'\u00e9poque : 0.4788 \n\nM\u00e9trique de validation : 0.4605\nTime taken: 49.55s\n\nD\u00e9but de l'\u00e9poque 2,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0332\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0199\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0218\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0270\n\nLoss pour l'\u00e9poque 2: 0.0278\nM\u00e9trique pour l'\u00e9poque : 0.4580 \n\nM\u00e9trique de validation : 0.4872\nTime taken: 45.75s\n\nD\u00e9but de l'\u00e9poque 3,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0195\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0333\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0358\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0282\n\nLoss pour l'\u00e9poque 3: 0.0322\nM\u00e9trique pour l'\u00e9poque : 0.4840 \n\nM\u00e9trique de validation : 0.4672\nTime taken: 46.34s\n\nD\u00e9but de l'\u00e9poque 4,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0168\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0549\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0284\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0287\n\nLoss pour l'\u00e9poque 4: 0.0752\nM\u00e9trique pour l'\u00e9poque : 0.4583 \n\nM\u00e9trique de validation : 0.4425\nTime taken: 46.12s\n\nD\u00e9but de l'\u00e9poque 5,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0306\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0268\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0496\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0215\n\nLoss pour l'\u00e9poque 5: 0.0255\nM\u00e9trique pour l'\u00e9poque : 0.4507 \n\nM\u00e9trique de validation : 0.5737\nTime taken: 46.49s\n\nD\u00e9but de l'\u00e9poque 6,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0251\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0420\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0290\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0344\n\nLoss pour l'\u00e9poque 6: 0.0254\nM\u00e9trique pour l'\u00e9poque : 0.4841 \n\nM\u00e9trique de validation : 0.5058\nTime taken: 46.35s\n\nD\u00e9but de l'\u00e9poque 7,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0252\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0295\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0303\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0275\n\nLoss pour l'\u00e9poque 7: 0.0242\nM\u00e9trique pour l'\u00e9poque : 0.5879 \n\nM\u00e9trique de validation : 0.6794\nTime taken: 46.24s\n\nD\u00e9but de l'\u00e9poque 8,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0113\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0311\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0460\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0238\n\nLoss pour l'\u00e9poque 8: 0.0229\nM\u00e9trique pour l'\u00e9poque : 0.7405 \n\nM\u00e9trique de validation : 0.8725\nTime taken: 46.17s\n\nD\u00e9but de l'\u00e9poque 9,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0195\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0247\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0280\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0113\n\nLoss pour l'\u00e9poque 9: 0.0199\nM\u00e9trique pour l'\u00e9poque : 0.7708 \n\nM\u00e9trique de validation : 0.8318\nTime taken: 46.25s\n\nD\u00e9but de l'\u00e9poque 10,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0144\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0157\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0269\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0144\n\nLoss pour l'\u00e9poque 10: 0.0205\nM\u00e9trique pour l'\u00e9poque : 0.7492 \n\nM\u00e9trique de validation : 0.7882\nTime taken: 46.27s\n\nD\u00e9but de l'\u00e9poque 11,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0164\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0134\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0131\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0233\n\nLoss pour l'\u00e9poque 11: 0.0169\nM\u00e9trique pour l'\u00e9poque : 0.7501 \n\nM\u00e9trique de validation : 0.8674\nTime taken: 46.36s\n\nD\u00e9but de l'\u00e9poque 12,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0080\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0179\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0129\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0139\n\nLoss pour l'\u00e9poque 12: 0.0182\nM\u00e9trique pour l'\u00e9poque : 0.7268 \n\nM\u00e9trique de validation : 0.8340\nTime taken: 46.35s\n\nD\u00e9but de l'\u00e9poque 13,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0110\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0120\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0267\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0101\n\nLoss pour l'\u00e9poque 13: 0.0155\nM\u00e9trique pour l'\u00e9poque : 0.8617 \n\nM\u00e9trique de validation : 0.9058\nTime taken: 46.30s\n\nD\u00e9but de l'\u00e9poque 14,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0114\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0208\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0159\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0183\n\nLoss pour l'\u00e9poque 14: 0.0140\nM\u00e9trique pour l'\u00e9poque : 0.8874 \n\nM\u00e9trique de validation : 0.9075\nTime taken: 46.27s\n\nD\u00e9but de l'\u00e9poque 15,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0148\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0111\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0096\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0134\n\nLoss pour l'\u00e9poque 15: 0.0134\nM\u00e9trique pour l'\u00e9poque : 0.8942 \n\nM\u00e9trique de validation : 0.9439\nTime taken: 46.31s\n\nD\u00e9but de l'\u00e9poque 16,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0108\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0123\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0134\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0144\n\nLoss pour l'\u00e9poque 16: 0.0146\nM\u00e9trique pour l'\u00e9poque : 0.9080 \n\nM\u00e9trique de validation : 0.9345\nTime taken: 46.33s\n\nD\u00e9but de l'\u00e9poque 17,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0118\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0082\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0094\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0092\n\nLoss pour l'\u00e9poque 17: 0.0113\nM\u00e9trique pour l'\u00e9poque : 0.9381 \n\nM\u00e9trique de validation : 0.9214\nTime taken: 46.26s\n\nD\u00e9but de l'\u00e9poque 18,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0099\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0108\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0101\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0112\n\nLoss pour l'\u00e9poque 18: 0.0122\nM\u00e9trique pour l'\u00e9poque : 0.9276 \n\nM\u00e9trique de validation : 0.9503\nTime taken: 46.31s\n\nD\u00e9but de l'\u00e9poque 19,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0551\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0076\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0116\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0081\n\nLoss pour l'\u00e9poque 19: 0.0142\nM\u00e9trique pour l'\u00e9poque : 0.9052 \n\nM\u00e9trique de validation : 0.9200\nTime taken: 46.12s\n\nD\u00e9but de l'\u00e9poque 20,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0109\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0100\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0078\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0090\n\nLoss pour l'\u00e9poque 20: 0.0100\nM\u00e9trique pour l'\u00e9poque : 0.9258 \n\nM\u00e9trique de validation : 0.9278\nTime taken: 46.28s\n\nD\u00e9but de l'\u00e9poque 21,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0109\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0076\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0099\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0103\n\nLoss pour l'\u00e9poque 21: 0.0100\nM\u00e9trique pour l'\u00e9poque : 0.9288 \n\nM\u00e9trique de validation : 0.9279\nTime taken: 46.35s\n\nD\u00e9but de l'\u00e9poque 22,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0146\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0189\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0060\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0136\n\nLoss pour l'\u00e9poque 22: 0.0106\nM\u00e9trique pour l'\u00e9poque : 0.9230 \n\nM\u00e9trique de validation : 0.9297\nTime taken: 46.09s\n\nD\u00e9but de l'\u00e9poque 23,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0115\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0110\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0085\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0081\n\nLoss pour l'\u00e9poque 23: 0.0095\nM\u00e9trique pour l'\u00e9poque : 0.9433 \n\nM\u00e9trique de validation : 0.9500\nTime taken: 46.11s\n\nD\u00e9but de l'\u00e9poque 24,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0082\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0086\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0084\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0090\n\nLoss pour l'\u00e9poque 24: 0.0088\nM\u00e9trique pour l'\u00e9poque : 0.9513 \n\nM\u00e9trique de validation : 0.9456\nTime taken: 46.20s\n\nD\u00e9but de l'\u00e9poque 25,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0111\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0098\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0117\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0075\n\nLoss pour l'\u00e9poque 25: 0.0089\nM\u00e9trique pour l'\u00e9poque : 0.9488 \n\nM\u00e9trique de validation : 0.9462\nTime taken: 46.05s\n\nD\u00e9but de l'\u00e9poque 26,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0065\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0066\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0100\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0086\n\nLoss pour l'\u00e9poque 26: 0.0087\nM\u00e9trique pour l'\u00e9poque : 0.9487 \n\nM\u00e9trique de validation : 0.9410\nTime taken: 46.07s\n\nD\u00e9but de l'\u00e9poque 27,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0063\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0049\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0091\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0073\n\nLoss pour l'\u00e9poque 27: 0.0082\nM\u00e9trique pour l'\u00e9poque : 0.9476 \n\nM\u00e9trique de validation : 0.9424\nTime taken: 46.03s\n\nD\u00e9but de l'\u00e9poque 28,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0084\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0080\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0067\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0048\n\nLoss pour l'\u00e9poque 28: 0.0178\nM\u00e9trique pour l'\u00e9poque : 0.9385 \n\nM\u00e9trique de validation : 0.9521\nTime taken: 46.18s\n\nD\u00e9but de l'\u00e9poque 29,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0051\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0046\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0053\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0079\n\nLoss pour l'\u00e9poque 29: 0.0069\nM\u00e9trique pour l'\u00e9poque : 0.9534 \n\nM\u00e9trique de validation : 0.9517\nTime taken: 46.28s\n\nD\u00e9but de l'\u00e9poque 30,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0120\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0068\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0096\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0064\n\nLoss pour l'\u00e9poque 30: 0.0076\nM\u00e9trique pour l'\u00e9poque : 0.9505 \n\nM\u00e9trique de validation : 0.9457\nTime taken: 46.32s\n\nD\u00e9but de l'\u00e9poque 31,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0097\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0077\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0060\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0087\n\nLoss pour l'\u00e9poque 31: 0.0076\nM\u00e9trique pour l'\u00e9poque : 0.9501 \n\nM\u00e9trique de validation : 0.9449\nTime taken: 46.26s\n\nD\u00e9but de l'\u00e9poque 32,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0071\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0069\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0075\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0084\n\nLoss pour l'\u00e9poque 32: 0.0068\nM\u00e9trique pour l'\u00e9poque : 0.9477 \n\nM\u00e9trique de validation : 0.9411\nTime taken: 46.26s\n\nD\u00e9but de l'\u00e9poque 33,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0107\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0045\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0061\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0044\n\nLoss pour l'\u00e9poque 33: 0.0072\nM\u00e9trique pour l'\u00e9poque : 0.9497 \n\nM\u00e9trique de validation : 0.9480\nTime taken: 46.14s\n\nD\u00e9but de l'\u00e9poque 34,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0054\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0072\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0052\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0098\n\nLoss pour l'\u00e9poque 34: 0.0068\nM\u00e9trique pour l'\u00e9poque : 0.9507 \n\nM\u00e9trique de validation : 0.9493\nTime taken: 46.19s\n\nD\u00e9but de l'\u00e9poque 35,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0087\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0042\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0072\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0058\n\nLoss pour l'\u00e9poque 35: 0.0068\nM\u00e9trique pour l'\u00e9poque : 0.9478 \n\nM\u00e9trique de validation : 0.9457\nTime taken: 46.29s\n\nD\u00e9but de l'\u00e9poque 36,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0068\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0039\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0056\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0054\n\nLoss pour l'\u00e9poque 36: 0.0063\nM\u00e9trique pour l'\u00e9poque : 0.9500 \n\nM\u00e9trique de validation : 0.9484\nTime taken: 46.22s\n\nD\u00e9but de l'\u00e9poque 37,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0064\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0056\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0068\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0086\n\nLoss pour l'\u00e9poque 37: 0.0061\nM\u00e9trique pour l'\u00e9poque : 0.9526 \n\nM\u00e9trique de validation : 0.9474\nTime taken: 46.13s\n\nD\u00e9but de l'\u00e9poque 38,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0071\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0067\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0065\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0070\n\nLoss pour l'\u00e9poque 38: 0.0062\nM\u00e9trique pour l'\u00e9poque : 0.9523 \n\nM\u00e9trique de validation : 0.9510\nTime taken: 46.09s\n\nD\u00e9but de l'\u00e9poque 39,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0063\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0058\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0053\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0085\n\nLoss pour l'\u00e9poque 39: 0.0068\nM\u00e9trique pour l'\u00e9poque : 0.9374 \n\nM\u00e9trique de validation : 0.9503\nTime taken: 46.27s\n\nD\u00e9but de l'\u00e9poque 40,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0065\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0045\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0042\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0060\n\nLoss pour l'\u00e9poque 40: 0.0054\nM\u00e9trique pour l'\u00e9poque : 0.9499 \n\nM\u00e9trique de validation : 0.9488\nTime taken: 46.15s\n\nD\u00e9but de l'\u00e9poque 41,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0056\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0053\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0056\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0034\n\nLoss pour l'\u00e9poque 41: 0.0076\nM\u00e9trique pour l'\u00e9poque : 0.8844 \n\nM\u00e9trique de validation : 0.8885\nTime taken: 46.26s\n\nD\u00e9but de l'\u00e9poque 42,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0048\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0058\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0045\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0125\n\nLoss pour l'\u00e9poque 42: 0.0054\nM\u00e9trique pour l'\u00e9poque : 0.9294 \n\nM\u00e9trique de validation : 0.9410\nTime taken: 46.10s\n\nD\u00e9but de l'\u00e9poque 43,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0059\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0036\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0071\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0052\n\nLoss pour l'\u00e9poque 43: 0.0051\nM\u00e9trique pour l'\u00e9poque : 0.9509 \n\nM\u00e9trique de validation : 0.9512\nTime taken: 46.03s\n\nD\u00e9but de l'\u00e9poque 44,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0055\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0036\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0055\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0076\n\nLoss pour l'\u00e9poque 44: 0.0055\nM\u00e9trique pour l'\u00e9poque : 0.9509 \n\nM\u00e9trique de validation : 0.9496\nTime taken: 45.97s\n\nD\u00e9but de l'\u00e9poque 45,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0067\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0040\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0029\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0050\n\nLoss pour l'\u00e9poque 45: 0.0054\nM\u00e9trique pour l'\u00e9poque : 0.9517 \n\nM\u00e9trique de validation : 0.9495\nTime taken: 46.00s\n\nD\u00e9but de l'\u00e9poque 46,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0042\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0061\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0057\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0058\n\nLoss pour l'\u00e9poque 46: 0.0051\nM\u00e9trique pour l'\u00e9poque : 0.9527 \n\nM\u00e9trique de validation : 0.9487\nTime taken: 46.01s\n\nD\u00e9but de l'\u00e9poque 47,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0048\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0053\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0036\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0061\n\nLoss pour l'\u00e9poque 47: 0.0053\nM\u00e9trique pour l'\u00e9poque : 0.9519 \n\nM\u00e9trique de validation : 0.9515\nTime taken: 46.13s\n\nD\u00e9but de l'\u00e9poque 48,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0035\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0063\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0040\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0046\n\nLoss pour l'\u00e9poque 48: 0.0047\nM\u00e9trique pour l'\u00e9poque : 0.9540 \n\nM\u00e9trique de validation : 0.9524\nTime taken: 46.17s\n\nD\u00e9but de l'\u00e9poque 49,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0048\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0065\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0053\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0052\n\nLoss pour l'\u00e9poque 49: 0.0050\nM\u00e9trique pour l'\u00e9poque : 0.9473 \n\nM\u00e9trique de validation : 0.9491\nTime taken: 46.10s\n\nD\u00e9but de l'\u00e9poque 50,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0045\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0036\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0042\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0027\n\nLoss pour l'\u00e9poque 50: 0.0049\nM\u00e9trique pour l'\u00e9poque : 0.9534 \n\nM\u00e9trique de validation : 0.9522\nTime taken: 46.11s\n\nD\u00e9but de l'\u00e9poque 51,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0043\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0042\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0036\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0081\n\nLoss pour l'\u00e9poque 51: 0.0048\nM\u00e9trique pour l'\u00e9poque : 0.9526 \n\nM\u00e9trique de validation : 0.9485\nTime taken: 46.11s\n\nD\u00e9but de l'\u00e9poque 52,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0053\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0042\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0035\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0050\n\nLoss pour l'\u00e9poque 52: 0.0045\nM\u00e9trique pour l'\u00e9poque : 0.9511 \n\nM\u00e9trique de validation : 0.9524\nTime taken: 46.06s\n\nD\u00e9but de l'\u00e9poque 53,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0067\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0040\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0047\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0039\n\nLoss pour l'\u00e9poque 53: 0.0045\nM\u00e9trique pour l'\u00e9poque : 0.9523 \n\nM\u00e9trique de validation : 0.9413\nTime taken: 46.11s\n\nD\u00e9but de l'\u00e9poque 54,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0046\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0050\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0107\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0040\n\nLoss pour l'\u00e9poque 54: 0.0047\nM\u00e9trique pour l'\u00e9poque : 0.9511 \n\nM\u00e9trique de validation : 0.9506\nTime taken: 46.12s\n\nD\u00e9but de l'\u00e9poque 55,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0039\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0032\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0039\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0033\n\nLoss pour l'\u00e9poque 55: 0.0042\nM\u00e9trique pour l'\u00e9poque : 0.9539 \n\nM\u00e9trique de validation : 0.9516\nTime taken: 46.05s\n\nD\u00e9but de l'\u00e9poque 56,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0045\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0035\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0036\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0043\n\nLoss pour l'\u00e9poque 56: 0.0042\nM\u00e9trique pour l'\u00e9poque : 0.9537 \n\nM\u00e9trique de validation : 0.9508\nTime taken: 46.07s\n\nD\u00e9but de l'\u00e9poque 57,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0047\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0034\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0038\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0033\n\nLoss pour l'\u00e9poque 57: 0.0040\nM\u00e9trique pour l'\u00e9poque : 0.9537 \n\nM\u00e9trique de validation : 0.9504\nTime taken: 46.09s\n\nD\u00e9but de l'\u00e9poque 58,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0034\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0046\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0065\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0038\n\nLoss pour l'\u00e9poque 58: 0.0040\nM\u00e9trique pour l'\u00e9poque : 0.9513 \n\nM\u00e9trique de validation : 0.9523\nTime taken: 46.11s\n\nD\u00e9but de l'\u00e9poque 59,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0046\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0042\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0048\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0037\n\nLoss pour l'\u00e9poque 59: 0.0039\nM\u00e9trique pour l'\u00e9poque : 0.9529 \n\nM\u00e9trique de validation : 0.9524\nTime taken: 46.07s\n\nD\u00e9but de l'\u00e9poque 60,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0048\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0036\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0046\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0028\n\nLoss pour l'\u00e9poque 60: 0.0037\nM\u00e9trique pour l'\u00e9poque : 0.9549 \n\nM\u00e9trique de validation : 0.9544\nTime taken: 46.07s\n\nD\u00e9but de l'\u00e9poque 61,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0045\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0058\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0052\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0033\n\nLoss pour l'\u00e9poque 61: 0.0039\nM\u00e9trique pour l'\u00e9poque : 0.9547 \n\nM\u00e9trique de validation : 0.9514\nTime taken: 46.18s\n\nD\u00e9but de l'\u00e9poque 62,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0027\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0031\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0039\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0039\n\nLoss pour l'\u00e9poque 62: 0.0043\nM\u00e9trique pour l'\u00e9poque : 0.9541 \n\nM\u00e9trique de validation : 0.9502\nTime taken: 46.14s\n\nD\u00e9but de l'\u00e9poque 63,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0041\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0030\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0041\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0027\n\nLoss pour l'\u00e9poque 63: 0.0033\nM\u00e9trique pour l'\u00e9poque : 0.9554 \n\nM\u00e9trique de validation : 0.9536\nTime taken: 46.10s\n\nD\u00e9but de l'\u00e9poque 64,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0051\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0023\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0039\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0036\n\nLoss pour l'\u00e9poque 64: 0.0034\nM\u00e9trique pour l'\u00e9poque : 0.9563 \n\nM\u00e9trique de validation : 0.9531\nTime taken: 46.13s\n\nD\u00e9but de l'\u00e9poque 65,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0028\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0034\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0039\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0032\n\nLoss pour l'\u00e9poque 65: 0.0035\nM\u00e9trique pour l'\u00e9poque : 0.9557 \n\nM\u00e9trique de validation : 0.9521\nTime taken: 46.20s\n\nD\u00e9but de l'\u00e9poque 66,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0042\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0040\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0029\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0032\n\nLoss pour l'\u00e9poque 66: 0.0034\nM\u00e9trique pour l'\u00e9poque : 0.9547 \n\nM\u00e9trique de validation : 0.9533\nTime taken: 46.08s\n\nD\u00e9but de l'\u00e9poque 67,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0034\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0046\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0028\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0020\n\nLoss pour l'\u00e9poque 67: 0.0036\nM\u00e9trique pour l'\u00e9poque : 0.9544 \n\nM\u00e9trique de validation : 0.9523\nTime taken: 46.13s\n\nD\u00e9but de l'\u00e9poque 68,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0022\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0029\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0028\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0041\n\nLoss pour l'\u00e9poque 68: 0.0030\nM\u00e9trique pour l'\u00e9poque : 0.9548 \n\nM\u00e9trique de validation : 0.9529\nTime taken: 46.04s\n\nD\u00e9but de l'\u00e9poque 69,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0035\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0032\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0032\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0032\n\nLoss pour l'\u00e9poque 69: 0.0031\nM\u00e9trique pour l'\u00e9poque : 0.9551 \n\nM\u00e9trique de validation : 0.9534\nTime taken: 46.18s\n\nD\u00e9but de l'\u00e9poque 70,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0022\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0035\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0031\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0026\n\nLoss pour l'\u00e9poque 70: 0.0050\nM\u00e9trique pour l'\u00e9poque : 0.9235 \n\nM\u00e9trique de validation : 0.9368\nTime taken: 46.30s\n\nD\u00e9but de l'\u00e9poque 71,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0020\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0036\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0028\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0030\n\nLoss pour l'\u00e9poque 71: 0.0026\nM\u00e9trique pour l'\u00e9poque : 0.9457 \n\nM\u00e9trique de validation : 0.9512\nTime taken: 46.01s\n\nD\u00e9but de l'\u00e9poque 72,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0030\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0031\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0022\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0036\n\nLoss pour l'\u00e9poque 72: 0.0030\nM\u00e9trique pour l'\u00e9poque : 0.9542 \n\nM\u00e9trique de validation : 0.9524\nTime taken: 45.84s\n\nD\u00e9but de l'\u00e9poque 73,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0037\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0024\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0039\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0028\n\nLoss pour l'\u00e9poque 73: 0.0030\nM\u00e9trique pour l'\u00e9poque : 0.9553 \n\nM\u00e9trique de validation : 0.9538\nTime taken: 46.02s\n\nD\u00e9but de l'\u00e9poque 74,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0026\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0025\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0017\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0018\n\nLoss pour l'\u00e9poque 74: 0.0031\nM\u00e9trique pour l'\u00e9poque : 0.9552 \n\nM\u00e9trique de validation : 0.9518\nTime taken: 46.03s\n\nD\u00e9but de l'\u00e9poque 75,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0022\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0032\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0033\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0017\n\nLoss pour l'\u00e9poque 75: 0.0027\nM\u00e9trique pour l'\u00e9poque : 0.9549 \n\nM\u00e9trique de validation : 0.9528\nTime taken: 46.02s\n\nD\u00e9but de l'\u00e9poque 76,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0032\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0032\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0022\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0057\n\nLoss pour l'\u00e9poque 76: 0.0030\nM\u00e9trique pour l'\u00e9poque : 0.9559 \n\nM\u00e9trique de validation : 0.9519\nTime taken: 46.01s\n\nD\u00e9but de l'\u00e9poque 77,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0028\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0032\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0022\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0030\n\nLoss pour l'\u00e9poque 77: 0.0027\nM\u00e9trique pour l'\u00e9poque : 0.9549 \n\nM\u00e9trique de validation : 0.9536\nTime taken: 46.14s\n\nD\u00e9but de l'\u00e9poque 78,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0026\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0019\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0023\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0027\n\nLoss pour l'\u00e9poque 78: 0.0026\nM\u00e9trique pour l'\u00e9poque : 0.9566 \n\nM\u00e9trique de validation : 0.9530\nTime taken: 46.13s\n\nD\u00e9but de l'\u00e9poque 79,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0016\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0029\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0023\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0025\n\nLoss pour l'\u00e9poque 79: 0.0039\nM\u00e9trique pour l'\u00e9poque : 0.9526 \n\nM\u00e9trique de validation : 0.9535\nTime taken: 46.17s\n\nD\u00e9but de l'\u00e9poque 80,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0026\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0017\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0017\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0021\n\nLoss pour l'\u00e9poque 80: 0.0022\nM\u00e9trique pour l'\u00e9poque : 0.9591 \n\nM\u00e9trique de validation : 0.9545\nTime taken: 46.14s\n\nD\u00e9but de l'\u00e9poque 81,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0024\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0018\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0023\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0026\n\nLoss pour l'\u00e9poque 81: 0.0024\nM\u00e9trique pour l'\u00e9poque : 0.9581 \n\nM\u00e9trique de validation : 0.9535\nTime taken: 46.21s\n\nD\u00e9but de l'\u00e9poque 82,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0040\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0023\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0027\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0021\n\nLoss pour l'\u00e9poque 82: 0.0025\nM\u00e9trique pour l'\u00e9poque : 0.9576 \n\nM\u00e9trique de validation : 0.9541\nTime taken: 46.21s\n\nD\u00e9but de l'\u00e9poque 83,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0026\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0018\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0026\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0020\n\nLoss pour l'\u00e9poque 83: 0.0023\nM\u00e9trique pour l'\u00e9poque : 0.9574 \n\nM\u00e9trique de validation : 0.9547\nTime taken: 46.17s\n\nD\u00e9but de l'\u00e9poque 84,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0021\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0032\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0018\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0028\n\nLoss pour l'\u00e9poque 84: 0.0023\nM\u00e9trique pour l'\u00e9poque : 0.9581 \n\nM\u00e9trique de validation : 0.9552\nTime taken: 46.21s\n\nD\u00e9but de l'\u00e9poque 85,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0021\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0023\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0022\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0025\n\nLoss pour l'\u00e9poque 85: 0.0022\nM\u00e9trique pour l'\u00e9poque : 0.9586 \n\nM\u00e9trique de validation : 0.9547\nTime taken: 46.19s\n\nD\u00e9but de l'\u00e9poque 86,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0019\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0018\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0020\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0025\n\nLoss pour l'\u00e9poque 86: 0.0022\nM\u00e9trique pour l'\u00e9poque : 0.9587 \n\nM\u00e9trique de validation : 0.9560\nTime taken: 46.24s\n\nD\u00e9but de l'\u00e9poque 87,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0014\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0021\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0028\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0019\n\nLoss pour l'\u00e9poque 87: 0.0022\nM\u00e9trique pour l'\u00e9poque : 0.9586 \n\nM\u00e9trique de validation : 0.9559\nTime taken: 46.27s\n\nD\u00e9but de l'\u00e9poque 88,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0027\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0016\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0016\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0030\n\nLoss pour l'\u00e9poque 88: 0.0019\nM\u00e9trique pour l'\u00e9poque : 0.9599 \n\nM\u00e9trique de validation : 0.9553\nTime taken: 46.21s\n\nD\u00e9but de l'\u00e9poque 89,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0031\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0020\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0018\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0014\n\nLoss pour l'\u00e9poque 89: 0.0019\nM\u00e9trique pour l'\u00e9poque : 0.9592 \n\nM\u00e9trique de validation : 0.9551\nTime taken: 46.21s\n\nD\u00e9but de l'\u00e9poque 90,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0026\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0018\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0018\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0015\n\nLoss pour l'\u00e9poque 90: 0.0019\nM\u00e9trique pour l'\u00e9poque : 0.9599 \n\nM\u00e9trique de validation : 0.9551\nTime taken: 46.23s\n\nD\u00e9but de l'\u00e9poque 91,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0022\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0017\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0021\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0016\n\nLoss pour l'\u00e9poque 91: 0.0019\nM\u00e9trique pour l'\u00e9poque : 0.9584 \n\nM\u00e9trique de validation : 0.9564\nTime taken: 46.28s\n\nD\u00e9but de l'\u00e9poque 92,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0018\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0026\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0011\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0021\n\nLoss pour l'\u00e9poque 92: 0.0019\nM\u00e9trique pour l'\u00e9poque : 0.9587 \n\nM\u00e9trique de validation : 0.9553\nTime taken: 46.21s\n\nD\u00e9but de l'\u00e9poque 93,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0018\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0016\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0010\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0015\n\nLoss pour l'\u00e9poque 93: 0.0017\nM\u00e9trique pour l'\u00e9poque : 0.9593 \n\nM\u00e9trique de validation : 0.9563\nTime taken: 46.18s\n\nD\u00e9but de l'\u00e9poque 94,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0017\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0018\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0013\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0021\n\nLoss pour l'\u00e9poque 94: 0.0018\nM\u00e9trique pour l'\u00e9poque : 0.9591 \n\nM\u00e9trique de validation : 0.9542\nTime taken: 46.23s\n\nD\u00e9but de l'\u00e9poque 95,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0016\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0009\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0024\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0020\n\nLoss pour l'\u00e9poque 95: 0.0015\nM\u00e9trique pour l'\u00e9poque : 0.9602 \n\nM\u00e9trique de validation : 0.9557\nTime taken: 46.21s\n\nD\u00e9but de l'\u00e9poque 96,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0016\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0013\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0014\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0015\n\nLoss pour l'\u00e9poque 96: 0.0015\nM\u00e9trique pour l'\u00e9poque : 0.9610 \n\nM\u00e9trique de validation : 0.9549\nTime taken: 46.27s\n\nD\u00e9but de l'\u00e9poque 97,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0028\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0015\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0013\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0014\n\nLoss pour l'\u00e9poque 97: 0.0016\nM\u00e9trique pour l'\u00e9poque : 0.9594 \n\nM\u00e9trique de validation : 0.9559\nTime taken: 46.21s\n\nD\u00e9but de l'\u00e9poque 98,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0011\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0010\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0024\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0023\n\nLoss pour l'\u00e9poque 98: 0.0016\nM\u00e9trique pour l'\u00e9poque : 0.9603 \n\nM\u00e9trique de validation : 0.9549\nTime taken: 46.25s\n\nD\u00e9but de l'\u00e9poque 99,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0016\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0016\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0019\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0025\n\nLoss pour l'\u00e9poque 99: 0.0015\nM\u00e9trique pour l'\u00e9poque : 0.9592 \n\nM\u00e9trique de validation : 0.9556\nTime taken: 46.23s\n\nD\u00e9but de l'\u00e9poque 100,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0009\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0010\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0015\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0018\n\nLoss pour l'\u00e9poque 100: 0.0013\nM\u00e9trique pour l'\u00e9poque : 0.9610 \n\nM\u00e9trique de validation : 0.9559\nTime taken: 46.23s\n\nD\u00e9but de l'\u00e9poque 101,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0021\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0010\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0017\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0014\n\nLoss pour l'\u00e9poque 101: 0.0013\nM\u00e9trique pour l'\u00e9poque : 0.9609 \n\nM\u00e9trique de validation : 0.9554\nTime taken: 46.29s\n\nD\u00e9but de l'\u00e9poque 102,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0016\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0013\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0012\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0011\n\nLoss pour l'\u00e9poque 102: 0.0012\nM\u00e9trique pour l'\u00e9poque : 0.9615 \n\nM\u00e9trique de validation : 0.9551\nTime taken: 46.23s\n\nD\u00e9but de l'\u00e9poque 103,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0014\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0015\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0019\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0014\n\nLoss pour l'\u00e9poque 103: 0.0013\nM\u00e9trique pour l'\u00e9poque : 0.9605 \n\nM\u00e9trique de validation : 0.9562\nTime taken: 46.31s\n\nD\u00e9but de l'\u00e9poque 104,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0012\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0013\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0010\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0011\n\nLoss pour l'\u00e9poque 104: 0.0013\nM\u00e9trique pour l'\u00e9poque : 0.9609 \n\nM\u00e9trique de validation : 0.9556\nTime taken: 46.37s\n\nD\u00e9but de l'\u00e9poque 105,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0011\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0012\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0010\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0005\n\nLoss pour l'\u00e9poque 105: 0.0012\nM\u00e9trique pour l'\u00e9poque : 0.9613 \n\nM\u00e9trique de validation : 0.9555\nTime taken: 46.34s\n\nD\u00e9but de l'\u00e9poque 106,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0020\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0012\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0015\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0009\n\nLoss pour l'\u00e9poque 106: 0.0012\nM\u00e9trique pour l'\u00e9poque : 0.9616 \n\nM\u00e9trique de validation : 0.9568\nTime taken: 46.31s\n\nD\u00e9but de l'\u00e9poque 107,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0010\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0007\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0013\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0008\n\nLoss pour l'\u00e9poque 107: 0.0011\nM\u00e9trique pour l'\u00e9poque : 0.9611 \n\nM\u00e9trique de validation : 0.9565\nTime taken: 46.15s\n\nD\u00e9but de l'\u00e9poque 108,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0007\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0013\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0009\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0013\n\nLoss pour l'\u00e9poque 108: 0.0010\nM\u00e9trique pour l'\u00e9poque : 0.9623 \n\nM\u00e9trique de validation : 0.9566\nTime taken: 46.14s\n\nD\u00e9but de l'\u00e9poque 109,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0007\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0018\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0017\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0010\n\nLoss pour l'\u00e9poque 109: 0.0011\nM\u00e9trique pour l'\u00e9poque : 0.9616 \n\nM\u00e9trique de validation : 0.9562\nTime taken: 46.23s\n\nD\u00e9but de l'\u00e9poque 110,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0007\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0012\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0010\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0013\n\nLoss pour l'\u00e9poque 110: 0.0010\nM\u00e9trique pour l'\u00e9poque : 0.9624 \n\nM\u00e9trique de validation : 0.9564\nTime taken: 46.18s\n\nD\u00e9but de l'\u00e9poque 111,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0006\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0009\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0013\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0009\n\nLoss pour l'\u00e9poque 111: 0.0010\nM\u00e9trique pour l'\u00e9poque : 0.9619 \n\nM\u00e9trique de validation : 0.9582\nTime taken: 46.17s\n\nD\u00e9but de l'\u00e9poque 112,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0005\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0010\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0006\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0006\n\nLoss pour l'\u00e9poque 112: 0.0009\nM\u00e9trique pour l'\u00e9poque : 0.9623 \n\nM\u00e9trique de validation : 0.9560\nTime taken: 46.31s\n\nD\u00e9but de l'\u00e9poque 113,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0007\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0008\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0007\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0010\n\nLoss pour l'\u00e9poque 113: 0.0009\nM\u00e9trique pour l'\u00e9poque : 0.9630 \n\nM\u00e9trique de validation : 0.9580\nTime taken: 46.20s\n\nD\u00e9but de l'\u00e9poque 114,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0008\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0010\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0010\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0008\n\nLoss pour l'\u00e9poque 114: 0.0009\nM\u00e9trique pour l'\u00e9poque : 0.9634 \n\nM\u00e9trique de validation : 0.9585\nTime taken: 46.19s\n\nD\u00e9but de l'\u00e9poque 115,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0014\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0004\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0011\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0004\n\nLoss pour l'\u00e9poque 115: 0.0010\nM\u00e9trique pour l'\u00e9poque : 0.9632 \n\nM\u00e9trique de validation : 0.9567\nTime taken: 46.19s\n\nD\u00e9but de l'\u00e9poque 116,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0007\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0008\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0013\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0012\n\nLoss pour l'\u00e9poque 116: 0.0009\nM\u00e9trique pour l'\u00e9poque : 0.9633 \n\nM\u00e9trique de validation : 0.9556\nTime taken: 46.17s\n\nD\u00e9but de l'\u00e9poque 117,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0008\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0010\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0006\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0007\n\nLoss pour l'\u00e9poque 117: 0.0007\nM\u00e9trique pour l'\u00e9poque : 0.9646 \n\nM\u00e9trique de validation : 0.9589\nTime taken: 46.25s\n\nD\u00e9but de l'\u00e9poque 118,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0005\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0008\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0008\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0003\n\nLoss pour l'\u00e9poque 118: 0.0008\nM\u00e9trique pour l'\u00e9poque : 0.9639 \n\nM\u00e9trique de validation : 0.9583\nTime taken: 46.25s\n\nD\u00e9but de l'\u00e9poque 119,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0004\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0012\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0009\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0008\n\nLoss pour l'\u00e9poque 119: 0.0008\nM\u00e9trique pour l'\u00e9poque : 0.9649 \n\nM\u00e9trique de validation : 0.9571\nTime taken: 46.23s\n\nD\u00e9but de l'\u00e9poque 120,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0004\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0006\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0010\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0007\n\nLoss pour l'\u00e9poque 120: 0.0007\nM\u00e9trique pour l'\u00e9poque : 0.9648 \n\nM\u00e9trique de validation : 0.9561\nTime taken: 46.16s\n\nD\u00e9but de l'\u00e9poque 121,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0014\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0009\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0006\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0015\n\nLoss pour l'\u00e9poque 121: 0.0010\nM\u00e9trique pour l'\u00e9poque : 0.9620 \n\nM\u00e9trique de validation : 0.9566\nTime taken: 46.18s\n\nD\u00e9but de l'\u00e9poque 122,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0009\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0005\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0004\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0005\n\nLoss pour l'\u00e9poque 122: 0.0007\nM\u00e9trique pour l'\u00e9poque : 0.9660 \n\nM\u00e9trique de validation : 0.9571\nTime taken: 46.22s\n\nD\u00e9but de l'\u00e9poque 123,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0005\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0004\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0005\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0004\n\nLoss pour l'\u00e9poque 123: 0.0005\nM\u00e9trique pour l'\u00e9poque : 0.9680 \n\nM\u00e9trique de validation : 0.9569\nTime taken: 46.21s\n\nD\u00e9but de l'\u00e9poque 124,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0009\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0006\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0009\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0015\n\nLoss pour l'\u00e9poque 124: 0.0009\nM\u00e9trique pour l'\u00e9poque : 0.9661 \n\nM\u00e9trique de validation : 0.9553\nTime taken: 46.19s\n\nD\u00e9but de l'\u00e9poque 125,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0015\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0010\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0009\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0007\n\nLoss pour l'\u00e9poque 125: 0.0009\nM\u00e9trique pour l'\u00e9poque : 0.9630 \n\nM\u00e9trique de validation : 0.9578\nTime taken: 46.01s\n\nD\u00e9but de l'\u00e9poque 126,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0003\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0004\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0006\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0007\n\nLoss pour l'\u00e9poque 126: 0.0005\nM\u00e9trique pour l'\u00e9poque : 0.9692 \n\nM\u00e9trique de validation : 0.9574\nTime taken: 46.10s\n\nD\u00e9but de l'\u00e9poque 127,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0008\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0004\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0009\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0009\n\nLoss pour l'\u00e9poque 127: 0.0007\nM\u00e9trique pour l'\u00e9poque : 0.9662 \n\nM\u00e9trique de validation : 0.9575\nTime taken: 46.05s\n\nD\u00e9but de l'\u00e9poque 128,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0007\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0005\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0013\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0005\n\nLoss pour l'\u00e9poque 128: 0.0007\nM\u00e9trique pour l'\u00e9poque : 0.9657 \n\nM\u00e9trique de validation : 0.9574\nTime taken: 46.14s\n\nD\u00e9but de l'\u00e9poque 129,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0009\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0009\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0006\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0004\n\nLoss pour l'\u00e9poque 129: 0.0007\nM\u00e9trique pour l'\u00e9poque : 0.9647 \n\nM\u00e9trique de validation : 0.9574\nTime taken: 46.10s\n\nD\u00e9but de l'\u00e9poque 130,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0005\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0027\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0006\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0011\n\nLoss pour l'\u00e9poque 130: 0.0008\nM\u00e9trique pour l'\u00e9poque : 0.9643 \n\nM\u00e9trique de validation : 0.9570\nTime taken: 46.03s\n\nD\u00e9but de l'\u00e9poque 131,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0004\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0003\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0004\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0005\n\nLoss pour l'\u00e9poque 131: 0.0005\nM\u00e9trique pour l'\u00e9poque : 0.9671 \n\nM\u00e9trique de validation : 0.9597\nTime taken: 46.06s\n\nD\u00e9but de l'\u00e9poque 132,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0009\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0008\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0007\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0004\n\nLoss pour l'\u00e9poque 132: 0.0006\nM\u00e9trique pour l'\u00e9poque : 0.9653 \n\nM\u00e9trique de validation : 0.9569\nTime taken: 46.07s\n\nD\u00e9but de l'\u00e9poque 133,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0012\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0005\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0012\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0011\n\nLoss pour l'\u00e9poque 133: 0.0006\nM\u00e9trique pour l'\u00e9poque : 0.9652 \n\nM\u00e9trique de validation : 0.9574\nTime taken: 46.10s\n\nD\u00e9but de l'\u00e9poque 134,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0005\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0003\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0005\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0006\n\nLoss pour l'\u00e9poque 134: 0.0006\nM\u00e9trique pour l'\u00e9poque : 0.9647 \n\nM\u00e9trique de validation : 0.9576\nTime taken: 46.19s\n\nD\u00e9but de l'\u00e9poque 135,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0010\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0005\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0008\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0010\n\nLoss pour l'\u00e9poque 135: 0.0007\nM\u00e9trique pour l'\u00e9poque : 0.9637 \n\nM\u00e9trique de validation : 0.9569\nTime taken: 46.15s\n\nD\u00e9but de l'\u00e9poque 136,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0006\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0005\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0005\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0003\n\nLoss pour l'\u00e9poque 136: 0.0005\nM\u00e9trique pour l'\u00e9poque : 0.9649 \n\nM\u00e9trique de validation : 0.9562\nTime taken: 46.11s\n\nD\u00e9but de l'\u00e9poque 137,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0008\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0006\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0005\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0007\n\nLoss pour l'\u00e9poque 137: 0.0006\nM\u00e9trique pour l'\u00e9poque : 0.9643 \n\nM\u00e9trique de validation : 0.9575\nTime taken: 46.09s\n\nD\u00e9but de l'\u00e9poque 138,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0004\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0005\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0007\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0006\n\nLoss pour l'\u00e9poque 138: 0.0005\nM\u00e9trique pour l'\u00e9poque : 0.9660 \n\nM\u00e9trique de validation : 0.9565\nTime taken: 46.21s\n\nD\u00e9but de l'\u00e9poque 139,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0007\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0003\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0007\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0006\n\nLoss pour l'\u00e9poque 139: 0.0006\nM\u00e9trique pour l'\u00e9poque : 0.9646 \n\nM\u00e9trique de validation : 0.9571\nTime taken: 46.08s\n\nD\u00e9but de l'\u00e9poque 140,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0007\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0003\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0003\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0015\n\nLoss pour l'\u00e9poque 140: 0.0006\nM\u00e9trique pour l'\u00e9poque : 0.9662 \n\nM\u00e9trique de validation : 0.9570\nTime taken: 46.09s\n\nD\u00e9but de l'\u00e9poque 141,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0003\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0009\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0007\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0004\n\nLoss pour l'\u00e9poque 141: 0.0005\nM\u00e9trique pour l'\u00e9poque : 0.9648 \n\nM\u00e9trique de validation : 0.9575\nTime taken: 46.11s\n\nD\u00e9but de l'\u00e9poque 142,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0006\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0005\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0003\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0004\n\nLoss pour l'\u00e9poque 142: 0.0005\nM\u00e9trique pour l'\u00e9poque : 0.9675 \n\nM\u00e9trique de validation : 0.9570\nTime taken: 46.07s\n\nD\u00e9but de l'\u00e9poque 143,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0018\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0004\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0004\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0004\n\nLoss pour l'\u00e9poque 143: 0.0007\nM\u00e9trique pour l'\u00e9poque : 0.9632 \n\nM\u00e9trique de validation : 0.9589\nTime taken: 45.99s\n\nD\u00e9but de l'\u00e9poque 144,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0003\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0003\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0004\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0006\n\nLoss pour l'\u00e9poque 144: 0.0004\nM\u00e9trique pour l'\u00e9poque : 0.9668 \n\nM\u00e9trique de validation : 0.9579\nTime taken: 46.07s\n\nD\u00e9but de l'\u00e9poque 145,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0003\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0002\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0008\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0005\n\nLoss pour l'\u00e9poque 145: 0.0005\nM\u00e9trique pour l'\u00e9poque : 0.9655 \n\nM\u00e9trique de validation : 0.9570\nTime taken: 46.12s\n\nD\u00e9but de l'\u00e9poque 146,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0007\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0008\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0003\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0006\n\nLoss pour l'\u00e9poque 146: 0.0004\nM\u00e9trique pour l'\u00e9poque : 0.9680 \n\nM\u00e9trique de validation : 0.9590\nTime taken: 46.09s\n\nD\u00e9but de l'\u00e9poque 147,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0004\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0008\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0004\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0002\n\nLoss pour l'\u00e9poque 147: 0.0005\nM\u00e9trique pour l'\u00e9poque : 0.9688 \n\nM\u00e9trique de validation : 0.9589\nTime taken: 46.05s\n\nD\u00e9but de l'\u00e9poque 148,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0005\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0003\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0003\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0005\n\nLoss pour l'\u00e9poque 148: 0.0005\nM\u00e9trique pour l'\u00e9poque : 0.9672 \n\nM\u00e9trique de validation : 0.9590\nTime taken: 46.08s\n\nD\u00e9but de l'\u00e9poque 149,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0002\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0004\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0002\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0003\n\nLoss pour l'\u00e9poque 149: 0.0004\nM\u00e9trique pour l'\u00e9poque : 0.9696 \n\nM\u00e9trique de validation : 0.9574\nTime taken: 46.08s\n\nD\u00e9but de l'\u00e9poque 150,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0005\nLoss sur le batch \u00e0 l'\u00e9tape 10 : 0.0007\nLoss sur le batch \u00e0 l'\u00e9tape 20 : 0.0003\nLoss sur le batch \u00e0 l'\u00e9tape 30 : 0.0002\n\nLoss pour l'\u00e9poque 150: 0.0005\nM\u00e9trique pour l'\u00e9poque : 0.9667 \n\nM\u00e9trique de validation : 0.9586\nTime taken: 46.21s\n</code>\n</pre>        <pre><code>model.save('unet_nucleus_150_epochs.h5')\n</code></pre>"},{"location":"deep_learning/module6/Module6_2/#tiramisu","title":"Tiramisu","text":""},{"location":"deep_learning/module6/Module6_2/#idee_1","title":"Id\u00e9e","text":"<p>Recently, a new CNN architecture, Densely connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train.</p>       <p>In this paper, we extend DenseNets to deal with the problem of semantic segmentation.</p>"},{"location":"deep_learning/module6/Module6_2/#les-briques-de-bases_1","title":"Les briques de bases","text":"<ul> <li>we extend DenseNets to work as FCNs by adding an upsampling path to recover the full resoluution.</li> <li>we only upsample the feature maps created the preceding dense block.</li> <li>The higher resolution information is passed by means of a standard skip connection between the downsampling and the upsampling paths. The details of the proposed architecture are shown in Figure 1. </li> </ul>"},{"location":"deep_learning/module6/Module6_2/#les-layers-denses","title":"Les layers denses","text":"\\[x_{\\ell} := H([x_{\\ell-1}, x_{\\ell-2}, \\dots, x_{0}])\\] <ul> <li>\\(H\\) is defined as BN-ReLu-Conv(\\(3 \\times 3\\))-Dropout(\\(p=0.2\\)).</li> <li>The outpout dimension of each layer \\(\\ell\\) has \\(k\\) features, hereafter referred as to growth rate parameter.</li> </ul>      <pre><code>def bn_relu_conv(tensor, filters, kernel_size):\n  x = BatchNormalization()(tensor)\n  x = ReLU()(x)\n  x = Conv2D(filters,\n             kernel_size=kernel_size,\n             strides=1,\n             padding='same',\n             kernel_initializer='he_uniform'\n             )(x)\n  x = Dropout(0.2)(x)\n\n  return x\n</code></pre>      <ul> <li>Dense block layers are composed of BN, followed by a ReLU, a \\(3\\times3\\) same convolution (no resolution loss) and dropout with probability \\(p=0.2\\).</li> </ul>      <pre><code>def dense_layer(tensor, reps, filters):\n\n  skip_connection = []\n  #growth_rate = 0\n\n  for i in range(reps-1):\n    out_conv = bn_relu_conv(tensor, filters, 3)\n    #growth_rate += filters\n    tensor = Concatenate()([tensor, out_conv])\n    skip_connection.append(out_conv)\n\n\n  out_conv = bn_relu_conv(tensor, filters, 3)\n  skip_connection.append(out_conv)\n  #pour les avoir dans le bon ordre [x_{l-1}, x_{l-2}, ..., x_{0}]\n  skip_connection = skip_connection[::-1]\n  x = Concatenate()(skip_connection)\n\n  return x\n</code></pre>      <p>V\u00e9rifions \u00e0 quoi ressemble le bloc dense que nous avons \u00e9crit. On peut le visualiser avec l'option <code>plot_model</code> de <code>tf.keras</code>.</p>      <pre><code>IMG_SHAPE = 32, 32, 3\n\n\ninput = Input(IMG_SHAPE)\noutput = dense_layer(input, 4, 16)\n\nmodel = Model(input, output)\ntf.keras.utils.plot_model(model,rankdir='LR')\n</code></pre>               <p>V\u00e9rifions si l'on a le bon nombre de feature map \u00e0 la fin de chaque bloc dense, sur le chemin contractant. Les Conv(\\(1 \\times 1\\)) des blocs de transition down ne changeant pas le nombre de feature maps, on ne les rajoute pas ici.</p>      <pre><code>filters = 16\n#downsampling path\nreps = [4, 5, 7, 10, 12]\nf = 48\nf_size_d=[]\n\nfor k in reps:\n  c = np.random.random_sample(32*32*f).astype('float32')\n  c = tf.reshape(c,(-1,32,32, f))\n  out = dense_layer(c, k, filters)\n  out = Concatenate()([out,c])\n  f_size_d.append(out.shape[3])  \n  f = out.shape[3]\n\nprint(f_size_d)\n</code></pre>      <pre>\n<code>[112, 192, 304, 464, 656, 896, 1088]\n</code>\n</pre>"},{"location":"deep_learning/module6/Module6_2/#transition-down","title":"Transition down","text":"<ul> <li> <p>A transition down is introduced to reduce the spatial dimensionality of the feature maps. Such transformation is composed of a \\(1\\times1\\) convolution (which conserves the number of feature maps) followed by a \\(2 \\times 2\\) pooling operation.</p> </li> <li> <p>Transition down is composed of BN, followed by a ReLU, a \\(1 \\times 1\\) convolution, dropout with \\(p=0.2\\) and a non-overlapping max pooling of size \\(2 \\times 2\\).</p> </li> </ul>      <pre><code>def transition_down(tensor, filters):\n  x = bn_relu_conv(tensor, filters, 1)\n  x = MaxPool2D(pool_size=(2,2),\n                strides=(2,2),\n                padding='same')(x)\n\n  return x\n</code></pre>"},{"location":"deep_learning/module6/Module6_2/#transition-up","title":"Transition up","text":"<ul> <li>The transition up modules consist of a transpose convolution that upsamples the previous feature maps.</li> <li>Transition up is composed of a \\(3 \\times 3\\) transposed convolution with a stride of \\(2\\).</li> </ul>      <pre><code>def transition_up(tensor, filters):\n  x = Conv2DTranspose(filters=filters,\n                      kernel_size=3,\n                      strides=(2, 2),\n                      padding=\"same\",\n                      kernel_initializer='he_uniform')(tensor)\n\n  return x\n</code></pre>"},{"location":"deep_learning/module6/Module6_2/#mise-en-place-du-modele_1","title":"Mise en place du mod\u00e8le","text":"<ul> <li> <p>The upsampled feature maps are then concatenated to the ones coming from the skip connection to form the input of a new dense block.</p> </li> <li> <p>In the upsampling path [...] the input of a danse block is not concatenated with its output. Thus, the transposed convolution is applied only to the feature map obtained by the last dense blocks and not to all feature maps concatenated so far.</p> </li> <li> <p>The growth rate of the layer is set to \\(k=16\\).</p> </li> </ul>      <pre><code>def FCDensenet(filters, nb_layers, img_shape, num_classes, nb_blocks=5, nb_filters=48):\n\n  nb_filters = 48\n  upsampling_fm=[]\n\n  # input block\n  input = Input(shape=img_shape)\n  x = Conv2D(nb_filters,\n              kernel_size=3,\n              strides=1,\n              padding='same',\n              kernel_initializer='he_uniform'\n              )(input)\n\n  #downsampling blocks\n  for i in range(nb_blocks):\n    out_conv = dense_layer(x, nb_layers[i], filters)\n    out_conv = Concatenate()([out_conv, x])\n    nb_filters += filters*nb_layers[i]\n\n    upsampling_fm.append(out_conv)\n    upsampling = upsampling_fm[::-1]\n    x = transition_down(out_conv, nb_filters)\n\n  # Bottleneck\n  #print(f'bottleneck')\n  x = dense_layer(x, nb_layers[5], filters)\n  nb_filters = filters*nb_layers[5]\n  #print(f'x : {x.shape}')\n\n\n  #upsampling path\n  for i in range(nb_blocks):\n    #print(f'upsampling block {i}')\n    #print(f'there will be {nb_layers[6+i]} layers')\n\n    x = transition_up(x, nb_filters)\n    #print(f'x upsampled : {x.shape}')\n    #print(f'skip connection : {upsampling[i].shape}')\n    x = Concatenate()([x, upsampling[i]])\n    #print(f'x_before_dense : {x.shape}')\n\n    if nb_layers[6+i]==nb_layers[-1]:\n      #print('last layer')\n      out_conv = dense_layer(x, nb_layers[6+i], filters)\n      x = Concatenate()([out_conv, x])\n      #print(f'x_after_dense : {x.shape}')\n    else:\n      x = dense_layer(x, nb_layers[6+i], filters)\n      nb_filters = filters*nb_layers[6+i]\n\n  # output block\n  x = Conv2D(num_classes,\n            kernel_size=1,\n            strides=(1,1),\n            padding='same',\n            kernel_initializer='he_uniform')(x)\n  x = Activation('softmax')(x)\n\n\n  model = Model(input, x)\n\n  return model\n</code></pre>"},{"location":"deep_learning/module6/Module6_2/#fc-densenet103","title":"FC-Densenet103","text":"<pre><code>filters = 16\nnb_blocks = 5\nnb_layers = [4, 5, 7, 10, 12 ,15, 12, 10, 7, 5, 4]\nimg_shape = (224,224,3)\n\n# Free up RAM in case the model definition cells were run multiple times\nkeras.backend.clear_session()\nmodel = FCDensenet(filters, nb_layers, img_shape, num_classes=32)\n</code></pre>     <pre><code>model.summary()\n</code></pre>      <pre>\n<code>Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n__________________________________________________________________________________________________\nconv2d (Conv2D)                 (None, 224, 224, 48) 1344        input_1[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization (BatchNorma (None, 224, 224, 48) 192         conv2d[0][0]                     \n__________________________________________________________________________________________________\nre_lu (ReLU)                    (None, 224, 224, 48) 0           batch_normalization[0][0]        \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 224, 224, 16) 6928        re_lu[0][0]                      \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 224, 224, 16) 0           conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 224, 224, 64) 0           conv2d[0][0]                     \n                                                                 dropout[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_1 (BatchNor (None, 224, 224, 64) 256         concatenate[0][0]                \n__________________________________________________________________________________________________\nre_lu_1 (ReLU)                  (None, 224, 224, 64) 0           batch_normalization_1[0][0]      \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 224, 224, 16) 9232        re_lu_1[0][0]                    \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 224, 224, 16) 0           conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 224, 224, 80) 0           concatenate[0][0]                \n                                                                 dropout_1[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_2 (BatchNor (None, 224, 224, 80) 320         concatenate_1[0][0]              \n__________________________________________________________________________________________________\nre_lu_2 (ReLU)                  (None, 224, 224, 80) 0           batch_normalization_2[0][0]      \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 224, 224, 16) 11536       re_lu_2[0][0]                    \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 224, 224, 16) 0           conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 224, 224, 96) 0           concatenate_1[0][0]              \n                                                                 dropout_2[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_3 (BatchNor (None, 224, 224, 96) 384         concatenate_2[0][0]              \n__________________________________________________________________________________________________\nre_lu_3 (ReLU)                  (None, 224, 224, 96) 0           batch_normalization_3[0][0]      \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 224, 224, 16) 13840       re_lu_3[0][0]                    \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, 224, 224, 16) 0           conv2d_4[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_3 (Concatenate)     (None, 224, 224, 64) 0           dropout_3[0][0]                  \n                                                                 dropout_2[0][0]                  \n                                                                 dropout_1[0][0]                  \n                                                                 dropout[0][0]                    \n__________________________________________________________________________________________________\nconcatenate_4 (Concatenate)     (None, 224, 224, 112 0           concatenate_3[0][0]              \n                                                                 conv2d[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_4 (BatchNor (None, 224, 224, 112 448         concatenate_4[0][0]              \n__________________________________________________________________________________________________\nre_lu_4 (ReLU)                  (None, 224, 224, 112 0           batch_normalization_4[0][0]      \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 224, 224, 112 12656       re_lu_4[0][0]                    \n__________________________________________________________________________________________________\ndropout_4 (Dropout)             (None, 224, 224, 112 0           conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)    (None, 112, 112, 112 0           dropout_4[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_5 (BatchNor (None, 112, 112, 112 448         max_pooling2d[0][0]              \n__________________________________________________________________________________________________\nre_lu_5 (ReLU)                  (None, 112, 112, 112 0           batch_normalization_5[0][0]      \n__________________________________________________________________________________________________\nconv2d_6 (Conv2D)               (None, 112, 112, 16) 16144       re_lu_5[0][0]                    \n__________________________________________________________________________________________________\ndropout_5 (Dropout)             (None, 112, 112, 16) 0           conv2d_6[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_5 (Concatenate)     (None, 112, 112, 128 0           max_pooling2d[0][0]              \n                                                                 dropout_5[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_6 (BatchNor (None, 112, 112, 128 512         concatenate_5[0][0]              \n__________________________________________________________________________________________________\nre_lu_6 (ReLU)                  (None, 112, 112, 128 0           batch_normalization_6[0][0]      \n__________________________________________________________________________________________________\nconv2d_7 (Conv2D)               (None, 112, 112, 16) 18448       re_lu_6[0][0]                    \n__________________________________________________________________________________________________\ndropout_6 (Dropout)             (None, 112, 112, 16) 0           conv2d_7[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_6 (Concatenate)     (None, 112, 112, 144 0           concatenate_5[0][0]              \n                                                                 dropout_6[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_7 (BatchNor (None, 112, 112, 144 576         concatenate_6[0][0]              \n__________________________________________________________________________________________________\nre_lu_7 (ReLU)                  (None, 112, 112, 144 0           batch_normalization_7[0][0]      \n__________________________________________________________________________________________________\nconv2d_8 (Conv2D)               (None, 112, 112, 16) 20752       re_lu_7[0][0]                    \n__________________________________________________________________________________________________\ndropout_7 (Dropout)             (None, 112, 112, 16) 0           conv2d_8[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_7 (Concatenate)     (None, 112, 112, 160 0           concatenate_6[0][0]              \n                                                                 dropout_7[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_8 (BatchNor (None, 112, 112, 160 640         concatenate_7[0][0]              \n__________________________________________________________________________________________________\nre_lu_8 (ReLU)                  (None, 112, 112, 160 0           batch_normalization_8[0][0]      \n__________________________________________________________________________________________________\nconv2d_9 (Conv2D)               (None, 112, 112, 16) 23056       re_lu_8[0][0]                    \n__________________________________________________________________________________________________\ndropout_8 (Dropout)             (None, 112, 112, 16) 0           conv2d_9[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_8 (Concatenate)     (None, 112, 112, 176 0           concatenate_7[0][0]              \n                                                                 dropout_8[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_9 (BatchNor (None, 112, 112, 176 704         concatenate_8[0][0]              \n__________________________________________________________________________________________________\nre_lu_9 (ReLU)                  (None, 112, 112, 176 0           batch_normalization_9[0][0]      \n__________________________________________________________________________________________________\nconv2d_10 (Conv2D)              (None, 112, 112, 16) 25360       re_lu_9[0][0]                    \n__________________________________________________________________________________________________\ndropout_9 (Dropout)             (None, 112, 112, 16) 0           conv2d_10[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_9 (Concatenate)     (None, 112, 112, 80) 0           dropout_9[0][0]                  \n                                                                 dropout_8[0][0]                  \n                                                                 dropout_7[0][0]                  \n                                                                 dropout_6[0][0]                  \n                                                                 dropout_5[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_10 (Concatenate)    (None, 112, 112, 192 0           concatenate_9[0][0]              \n                                                                 max_pooling2d[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_10 (BatchNo (None, 112, 112, 192 768         concatenate_10[0][0]             \n__________________________________________________________________________________________________\nre_lu_10 (ReLU)                 (None, 112, 112, 192 0           batch_normalization_10[0][0]     \n__________________________________________________________________________________________________\nconv2d_11 (Conv2D)              (None, 112, 112, 192 37056       re_lu_10[0][0]                   \n__________________________________________________________________________________________________\ndropout_10 (Dropout)            (None, 112, 112, 192 0           conv2d_11[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 56, 56, 192)  0           dropout_10[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_11 (BatchNo (None, 56, 56, 192)  768         max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\nre_lu_11 (ReLU)                 (None, 56, 56, 192)  0           batch_normalization_11[0][0]     \n__________________________________________________________________________________________________\nconv2d_12 (Conv2D)              (None, 56, 56, 16)   27664       re_lu_11[0][0]                   \n__________________________________________________________________________________________________\ndropout_11 (Dropout)            (None, 56, 56, 16)   0           conv2d_12[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_11 (Concatenate)    (None, 56, 56, 208)  0           max_pooling2d_1[0][0]            \n                                                                 dropout_11[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_12 (BatchNo (None, 56, 56, 208)  832         concatenate_11[0][0]             \n__________________________________________________________________________________________________\nre_lu_12 (ReLU)                 (None, 56, 56, 208)  0           batch_normalization_12[0][0]     \n__________________________________________________________________________________________________\nconv2d_13 (Conv2D)              (None, 56, 56, 16)   29968       re_lu_12[0][0]                   \n__________________________________________________________________________________________________\ndropout_12 (Dropout)            (None, 56, 56, 16)   0           conv2d_13[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_12 (Concatenate)    (None, 56, 56, 224)  0           concatenate_11[0][0]             \n                                                                 dropout_12[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_13 (BatchNo (None, 56, 56, 224)  896         concatenate_12[0][0]             \n__________________________________________________________________________________________________\nre_lu_13 (ReLU)                 (None, 56, 56, 224)  0           batch_normalization_13[0][0]     \n__________________________________________________________________________________________________\nconv2d_14 (Conv2D)              (None, 56, 56, 16)   32272       re_lu_13[0][0]                   \n__________________________________________________________________________________________________\ndropout_13 (Dropout)            (None, 56, 56, 16)   0           conv2d_14[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_13 (Concatenate)    (None, 56, 56, 240)  0           concatenate_12[0][0]             \n                                                                 dropout_13[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_14 (BatchNo (None, 56, 56, 240)  960         concatenate_13[0][0]             \n__________________________________________________________________________________________________\nre_lu_14 (ReLU)                 (None, 56, 56, 240)  0           batch_normalization_14[0][0]     \n__________________________________________________________________________________________________\nconv2d_15 (Conv2D)              (None, 56, 56, 16)   34576       re_lu_14[0][0]                   \n__________________________________________________________________________________________________\ndropout_14 (Dropout)            (None, 56, 56, 16)   0           conv2d_15[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_14 (Concatenate)    (None, 56, 56, 256)  0           concatenate_13[0][0]             \n                                                                 dropout_14[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_15 (BatchNo (None, 56, 56, 256)  1024        concatenate_14[0][0]             \n__________________________________________________________________________________________________\nre_lu_15 (ReLU)                 (None, 56, 56, 256)  0           batch_normalization_15[0][0]     \n__________________________________________________________________________________________________\nconv2d_16 (Conv2D)              (None, 56, 56, 16)   36880       re_lu_15[0][0]                   \n__________________________________________________________________________________________________\ndropout_15 (Dropout)            (None, 56, 56, 16)   0           conv2d_16[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_15 (Concatenate)    (None, 56, 56, 272)  0           concatenate_14[0][0]             \n                                                                 dropout_15[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_16 (BatchNo (None, 56, 56, 272)  1088        concatenate_15[0][0]             \n__________________________________________________________________________________________________\nre_lu_16 (ReLU)                 (None, 56, 56, 272)  0           batch_normalization_16[0][0]     \n__________________________________________________________________________________________________\nconv2d_17 (Conv2D)              (None, 56, 56, 16)   39184       re_lu_16[0][0]                   \n__________________________________________________________________________________________________\ndropout_16 (Dropout)            (None, 56, 56, 16)   0           conv2d_17[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_16 (Concatenate)    (None, 56, 56, 288)  0           concatenate_15[0][0]             \n                                                                 dropout_16[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_17 (BatchNo (None, 56, 56, 288)  1152        concatenate_16[0][0]             \n__________________________________________________________________________________________________\nre_lu_17 (ReLU)                 (None, 56, 56, 288)  0           batch_normalization_17[0][0]     \n__________________________________________________________________________________________________\nconv2d_18 (Conv2D)              (None, 56, 56, 16)   41488       re_lu_17[0][0]                   \n__________________________________________________________________________________________________\ndropout_17 (Dropout)            (None, 56, 56, 16)   0           conv2d_18[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_17 (Concatenate)    (None, 56, 56, 112)  0           dropout_17[0][0]                 \n                                                                 dropout_16[0][0]                 \n                                                                 dropout_15[0][0]                 \n                                                                 dropout_14[0][0]                 \n                                                                 dropout_13[0][0]                 \n                                                                 dropout_12[0][0]                 \n                                                                 dropout_11[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_18 (Concatenate)    (None, 56, 56, 304)  0           concatenate_17[0][0]             \n                                                                 max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\nbatch_normalization_18 (BatchNo (None, 56, 56, 304)  1216        concatenate_18[0][0]             \n__________________________________________________________________________________________________\nre_lu_18 (ReLU)                 (None, 56, 56, 304)  0           batch_normalization_18[0][0]     \n__________________________________________________________________________________________________\nconv2d_19 (Conv2D)              (None, 56, 56, 304)  92720       re_lu_18[0][0]                   \n__________________________________________________________________________________________________\ndropout_18 (Dropout)            (None, 56, 56, 304)  0           conv2d_19[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)  (None, 28, 28, 304)  0           dropout_18[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_19 (BatchNo (None, 28, 28, 304)  1216        max_pooling2d_2[0][0]            \n__________________________________________________________________________________________________\nre_lu_19 (ReLU)                 (None, 28, 28, 304)  0           batch_normalization_19[0][0]     \n__________________________________________________________________________________________________\nconv2d_20 (Conv2D)              (None, 28, 28, 16)   43792       re_lu_19[0][0]                   \n__________________________________________________________________________________________________\ndropout_19 (Dropout)            (None, 28, 28, 16)   0           conv2d_20[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_19 (Concatenate)    (None, 28, 28, 320)  0           max_pooling2d_2[0][0]            \n                                                                 dropout_19[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_20 (BatchNo (None, 28, 28, 320)  1280        concatenate_19[0][0]             \n__________________________________________________________________________________________________\nre_lu_20 (ReLU)                 (None, 28, 28, 320)  0           batch_normalization_20[0][0]     \n__________________________________________________________________________________________________\nconv2d_21 (Conv2D)              (None, 28, 28, 16)   46096       re_lu_20[0][0]                   \n__________________________________________________________________________________________________\ndropout_20 (Dropout)            (None, 28, 28, 16)   0           conv2d_21[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_20 (Concatenate)    (None, 28, 28, 336)  0           concatenate_19[0][0]             \n                                                                 dropout_20[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_21 (BatchNo (None, 28, 28, 336)  1344        concatenate_20[0][0]             \n__________________________________________________________________________________________________\nre_lu_21 (ReLU)                 (None, 28, 28, 336)  0           batch_normalization_21[0][0]     \n__________________________________________________________________________________________________\nconv2d_22 (Conv2D)              (None, 28, 28, 16)   48400       re_lu_21[0][0]                   \n__________________________________________________________________________________________________\ndropout_21 (Dropout)            (None, 28, 28, 16)   0           conv2d_22[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_21 (Concatenate)    (None, 28, 28, 352)  0           concatenate_20[0][0]             \n                                                                 dropout_21[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_22 (BatchNo (None, 28, 28, 352)  1408        concatenate_21[0][0]             \n__________________________________________________________________________________________________\nre_lu_22 (ReLU)                 (None, 28, 28, 352)  0           batch_normalization_22[0][0]     \n__________________________________________________________________________________________________\nconv2d_23 (Conv2D)              (None, 28, 28, 16)   50704       re_lu_22[0][0]                   \n__________________________________________________________________________________________________\ndropout_22 (Dropout)            (None, 28, 28, 16)   0           conv2d_23[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_22 (Concatenate)    (None, 28, 28, 368)  0           concatenate_21[0][0]             \n                                                                 dropout_22[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_23 (BatchNo (None, 28, 28, 368)  1472        concatenate_22[0][0]             \n__________________________________________________________________________________________________\nre_lu_23 (ReLU)                 (None, 28, 28, 368)  0           batch_normalization_23[0][0]     \n__________________________________________________________________________________________________\nconv2d_24 (Conv2D)              (None, 28, 28, 16)   53008       re_lu_23[0][0]                   \n__________________________________________________________________________________________________\ndropout_23 (Dropout)            (None, 28, 28, 16)   0           conv2d_24[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_23 (Concatenate)    (None, 28, 28, 384)  0           concatenate_22[0][0]             \n                                                                 dropout_23[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_24 (BatchNo (None, 28, 28, 384)  1536        concatenate_23[0][0]             \n__________________________________________________________________________________________________\nre_lu_24 (ReLU)                 (None, 28, 28, 384)  0           batch_normalization_24[0][0]     \n__________________________________________________________________________________________________\nconv2d_25 (Conv2D)              (None, 28, 28, 16)   55312       re_lu_24[0][0]                   \n__________________________________________________________________________________________________\ndropout_24 (Dropout)            (None, 28, 28, 16)   0           conv2d_25[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_24 (Concatenate)    (None, 28, 28, 400)  0           concatenate_23[0][0]             \n                                                                 dropout_24[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_25 (BatchNo (None, 28, 28, 400)  1600        concatenate_24[0][0]             \n__________________________________________________________________________________________________\nre_lu_25 (ReLU)                 (None, 28, 28, 400)  0           batch_normalization_25[0][0]     \n__________________________________________________________________________________________________\nconv2d_26 (Conv2D)              (None, 28, 28, 16)   57616       re_lu_25[0][0]                   \n__________________________________________________________________________________________________\ndropout_25 (Dropout)            (None, 28, 28, 16)   0           conv2d_26[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_25 (Concatenate)    (None, 28, 28, 416)  0           concatenate_24[0][0]             \n                                                                 dropout_25[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_26 (BatchNo (None, 28, 28, 416)  1664        concatenate_25[0][0]             \n__________________________________________________________________________________________________\nre_lu_26 (ReLU)                 (None, 28, 28, 416)  0           batch_normalization_26[0][0]     \n__________________________________________________________________________________________________\nconv2d_27 (Conv2D)              (None, 28, 28, 16)   59920       re_lu_26[0][0]                   \n__________________________________________________________________________________________________\ndropout_26 (Dropout)            (None, 28, 28, 16)   0           conv2d_27[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_26 (Concatenate)    (None, 28, 28, 432)  0           concatenate_25[0][0]             \n                                                                 dropout_26[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_27 (BatchNo (None, 28, 28, 432)  1728        concatenate_26[0][0]             \n__________________________________________________________________________________________________\nre_lu_27 (ReLU)                 (None, 28, 28, 432)  0           batch_normalization_27[0][0]     \n__________________________________________________________________________________________________\nconv2d_28 (Conv2D)              (None, 28, 28, 16)   62224       re_lu_27[0][0]                   \n__________________________________________________________________________________________________\ndropout_27 (Dropout)            (None, 28, 28, 16)   0           conv2d_28[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_27 (Concatenate)    (None, 28, 28, 448)  0           concatenate_26[0][0]             \n                                                                 dropout_27[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_28 (BatchNo (None, 28, 28, 448)  1792        concatenate_27[0][0]             \n__________________________________________________________________________________________________\nre_lu_28 (ReLU)                 (None, 28, 28, 448)  0           batch_normalization_28[0][0]     \n__________________________________________________________________________________________________\nconv2d_29 (Conv2D)              (None, 28, 28, 16)   64528       re_lu_28[0][0]                   \n__________________________________________________________________________________________________\ndropout_28 (Dropout)            (None, 28, 28, 16)   0           conv2d_29[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_28 (Concatenate)    (None, 28, 28, 160)  0           dropout_28[0][0]                 \n                                                                 dropout_27[0][0]                 \n                                                                 dropout_26[0][0]                 \n                                                                 dropout_25[0][0]                 \n                                                                 dropout_24[0][0]                 \n                                                                 dropout_23[0][0]                 \n                                                                 dropout_22[0][0]                 \n                                                                 dropout_21[0][0]                 \n                                                                 dropout_20[0][0]                 \n                                                                 dropout_19[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_29 (Concatenate)    (None, 28, 28, 464)  0           concatenate_28[0][0]             \n                                                                 max_pooling2d_2[0][0]            \n__________________________________________________________________________________________________\nbatch_normalization_29 (BatchNo (None, 28, 28, 464)  1856        concatenate_29[0][0]             \n__________________________________________________________________________________________________\nre_lu_29 (ReLU)                 (None, 28, 28, 464)  0           batch_normalization_29[0][0]     \n__________________________________________________________________________________________________\nconv2d_30 (Conv2D)              (None, 28, 28, 464)  215760      re_lu_29[0][0]                   \n__________________________________________________________________________________________________\ndropout_29 (Dropout)            (None, 28, 28, 464)  0           conv2d_30[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 14, 14, 464)  0           dropout_29[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_30 (BatchNo (None, 14, 14, 464)  1856        max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nre_lu_30 (ReLU)                 (None, 14, 14, 464)  0           batch_normalization_30[0][0]     \n__________________________________________________________________________________________________\nconv2d_31 (Conv2D)              (None, 14, 14, 16)   66832       re_lu_30[0][0]                   \n__________________________________________________________________________________________________\ndropout_30 (Dropout)            (None, 14, 14, 16)   0           conv2d_31[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_30 (Concatenate)    (None, 14, 14, 480)  0           max_pooling2d_3[0][0]            \n                                                                 dropout_30[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_31 (BatchNo (None, 14, 14, 480)  1920        concatenate_30[0][0]             \n__________________________________________________________________________________________________\nre_lu_31 (ReLU)                 (None, 14, 14, 480)  0           batch_normalization_31[0][0]     \n__________________________________________________________________________________________________\nconv2d_32 (Conv2D)              (None, 14, 14, 16)   69136       re_lu_31[0][0]                   \n__________________________________________________________________________________________________\ndropout_31 (Dropout)            (None, 14, 14, 16)   0           conv2d_32[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_31 (Concatenate)    (None, 14, 14, 496)  0           concatenate_30[0][0]             \n                                                                 dropout_31[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_32 (BatchNo (None, 14, 14, 496)  1984        concatenate_31[0][0]             \n__________________________________________________________________________________________________\nre_lu_32 (ReLU)                 (None, 14, 14, 496)  0           batch_normalization_32[0][0]     \n__________________________________________________________________________________________________\nconv2d_33 (Conv2D)              (None, 14, 14, 16)   71440       re_lu_32[0][0]                   \n__________________________________________________________________________________________________\ndropout_32 (Dropout)            (None, 14, 14, 16)   0           conv2d_33[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_32 (Concatenate)    (None, 14, 14, 512)  0           concatenate_31[0][0]             \n                                                                 dropout_32[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_33 (BatchNo (None, 14, 14, 512)  2048        concatenate_32[0][0]             \n__________________________________________________________________________________________________\nre_lu_33 (ReLU)                 (None, 14, 14, 512)  0           batch_normalization_33[0][0]     \n__________________________________________________________________________________________________\nconv2d_34 (Conv2D)              (None, 14, 14, 16)   73744       re_lu_33[0][0]                   \n__________________________________________________________________________________________________\ndropout_33 (Dropout)            (None, 14, 14, 16)   0           conv2d_34[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_33 (Concatenate)    (None, 14, 14, 528)  0           concatenate_32[0][0]             \n                                                                 dropout_33[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_34 (BatchNo (None, 14, 14, 528)  2112        concatenate_33[0][0]             \n__________________________________________________________________________________________________\nre_lu_34 (ReLU)                 (None, 14, 14, 528)  0           batch_normalization_34[0][0]     \n__________________________________________________________________________________________________\nconv2d_35 (Conv2D)              (None, 14, 14, 16)   76048       re_lu_34[0][0]                   \n__________________________________________________________________________________________________\ndropout_34 (Dropout)            (None, 14, 14, 16)   0           conv2d_35[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_34 (Concatenate)    (None, 14, 14, 544)  0           concatenate_33[0][0]             \n                                                                 dropout_34[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_35 (BatchNo (None, 14, 14, 544)  2176        concatenate_34[0][0]             \n__________________________________________________________________________________________________\nre_lu_35 (ReLU)                 (None, 14, 14, 544)  0           batch_normalization_35[0][0]     \n__________________________________________________________________________________________________\nconv2d_36 (Conv2D)              (None, 14, 14, 16)   78352       re_lu_35[0][0]                   \n__________________________________________________________________________________________________\ndropout_35 (Dropout)            (None, 14, 14, 16)   0           conv2d_36[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_35 (Concatenate)    (None, 14, 14, 560)  0           concatenate_34[0][0]             \n                                                                 dropout_35[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_36 (BatchNo (None, 14, 14, 560)  2240        concatenate_35[0][0]             \n__________________________________________________________________________________________________\nre_lu_36 (ReLU)                 (None, 14, 14, 560)  0           batch_normalization_36[0][0]     \n__________________________________________________________________________________________________\nconv2d_37 (Conv2D)              (None, 14, 14, 16)   80656       re_lu_36[0][0]                   \n__________________________________________________________________________________________________\ndropout_36 (Dropout)            (None, 14, 14, 16)   0           conv2d_37[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_36 (Concatenate)    (None, 14, 14, 576)  0           concatenate_35[0][0]             \n                                                                 dropout_36[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_37 (BatchNo (None, 14, 14, 576)  2304        concatenate_36[0][0]             \n__________________________________________________________________________________________________\nre_lu_37 (ReLU)                 (None, 14, 14, 576)  0           batch_normalization_37[0][0]     \n__________________________________________________________________________________________________\nconv2d_38 (Conv2D)              (None, 14, 14, 16)   82960       re_lu_37[0][0]                   \n__________________________________________________________________________________________________\ndropout_37 (Dropout)            (None, 14, 14, 16)   0           conv2d_38[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_37 (Concatenate)    (None, 14, 14, 592)  0           concatenate_36[0][0]             \n                                                                 dropout_37[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_38 (BatchNo (None, 14, 14, 592)  2368        concatenate_37[0][0]             \n__________________________________________________________________________________________________\nre_lu_38 (ReLU)                 (None, 14, 14, 592)  0           batch_normalization_38[0][0]     \n__________________________________________________________________________________________________\nconv2d_39 (Conv2D)              (None, 14, 14, 16)   85264       re_lu_38[0][0]                   \n__________________________________________________________________________________________________\ndropout_38 (Dropout)            (None, 14, 14, 16)   0           conv2d_39[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_38 (Concatenate)    (None, 14, 14, 608)  0           concatenate_37[0][0]             \n                                                                 dropout_38[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_39 (BatchNo (None, 14, 14, 608)  2432        concatenate_38[0][0]             \n__________________________________________________________________________________________________\nre_lu_39 (ReLU)                 (None, 14, 14, 608)  0           batch_normalization_39[0][0]     \n__________________________________________________________________________________________________\nconv2d_40 (Conv2D)              (None, 14, 14, 16)   87568       re_lu_39[0][0]                   \n__________________________________________________________________________________________________\ndropout_39 (Dropout)            (None, 14, 14, 16)   0           conv2d_40[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_39 (Concatenate)    (None, 14, 14, 624)  0           concatenate_38[0][0]             \n                                                                 dropout_39[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_40 (BatchNo (None, 14, 14, 624)  2496        concatenate_39[0][0]             \n__________________________________________________________________________________________________\nre_lu_40 (ReLU)                 (None, 14, 14, 624)  0           batch_normalization_40[0][0]     \n__________________________________________________________________________________________________\nconv2d_41 (Conv2D)              (None, 14, 14, 16)   89872       re_lu_40[0][0]                   \n__________________________________________________________________________________________________\ndropout_40 (Dropout)            (None, 14, 14, 16)   0           conv2d_41[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_40 (Concatenate)    (None, 14, 14, 640)  0           concatenate_39[0][0]             \n                                                                 dropout_40[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_41 (BatchNo (None, 14, 14, 640)  2560        concatenate_40[0][0]             \n__________________________________________________________________________________________________\nre_lu_41 (ReLU)                 (None, 14, 14, 640)  0           batch_normalization_41[0][0]     \n__________________________________________________________________________________________________\nconv2d_42 (Conv2D)              (None, 14, 14, 16)   92176       re_lu_41[0][0]                   \n__________________________________________________________________________________________________\ndropout_41 (Dropout)            (None, 14, 14, 16)   0           conv2d_42[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_41 (Concatenate)    (None, 14, 14, 192)  0           dropout_41[0][0]                 \n                                                                 dropout_40[0][0]                 \n                                                                 dropout_39[0][0]                 \n                                                                 dropout_38[0][0]                 \n                                                                 dropout_37[0][0]                 \n                                                                 dropout_36[0][0]                 \n                                                                 dropout_35[0][0]                 \n                                                                 dropout_34[0][0]                 \n                                                                 dropout_33[0][0]                 \n                                                                 dropout_32[0][0]                 \n                                                                 dropout_31[0][0]                 \n                                                                 dropout_30[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_42 (Concatenate)    (None, 14, 14, 656)  0           concatenate_41[0][0]             \n                                                                 max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nbatch_normalization_42 (BatchNo (None, 14, 14, 656)  2624        concatenate_42[0][0]             \n__________________________________________________________________________________________________\nre_lu_42 (ReLU)                 (None, 14, 14, 656)  0           batch_normalization_42[0][0]     \n__________________________________________________________________________________________________\nconv2d_43 (Conv2D)              (None, 14, 14, 656)  430992      re_lu_42[0][0]                   \n__________________________________________________________________________________________________\ndropout_42 (Dropout)            (None, 14, 14, 656)  0           conv2d_43[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_4 (MaxPooling2D)  (None, 7, 7, 656)    0           dropout_42[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_43 (BatchNo (None, 7, 7, 656)    2624        max_pooling2d_4[0][0]            \n__________________________________________________________________________________________________\nre_lu_43 (ReLU)                 (None, 7, 7, 656)    0           batch_normalization_43[0][0]     \n__________________________________________________________________________________________________\nconv2d_44 (Conv2D)              (None, 7, 7, 16)     94480       re_lu_43[0][0]                   \n__________________________________________________________________________________________________\ndropout_43 (Dropout)            (None, 7, 7, 16)     0           conv2d_44[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_43 (Concatenate)    (None, 7, 7, 672)    0           max_pooling2d_4[0][0]            \n                                                                 dropout_43[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_44 (BatchNo (None, 7, 7, 672)    2688        concatenate_43[0][0]             \n__________________________________________________________________________________________________\nre_lu_44 (ReLU)                 (None, 7, 7, 672)    0           batch_normalization_44[0][0]     \n__________________________________________________________________________________________________\nconv2d_45 (Conv2D)              (None, 7, 7, 16)     96784       re_lu_44[0][0]                   \n__________________________________________________________________________________________________\ndropout_44 (Dropout)            (None, 7, 7, 16)     0           conv2d_45[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_44 (Concatenate)    (None, 7, 7, 688)    0           concatenate_43[0][0]             \n                                                                 dropout_44[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_45 (BatchNo (None, 7, 7, 688)    2752        concatenate_44[0][0]             \n__________________________________________________________________________________________________\nre_lu_45 (ReLU)                 (None, 7, 7, 688)    0           batch_normalization_45[0][0]     \n__________________________________________________________________________________________________\nconv2d_46 (Conv2D)              (None, 7, 7, 16)     99088       re_lu_45[0][0]                   \n__________________________________________________________________________________________________\ndropout_45 (Dropout)            (None, 7, 7, 16)     0           conv2d_46[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_45 (Concatenate)    (None, 7, 7, 704)    0           concatenate_44[0][0]             \n                                                                 dropout_45[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_46 (BatchNo (None, 7, 7, 704)    2816        concatenate_45[0][0]             \n__________________________________________________________________________________________________\nre_lu_46 (ReLU)                 (None, 7, 7, 704)    0           batch_normalization_46[0][0]     \n__________________________________________________________________________________________________\nconv2d_47 (Conv2D)              (None, 7, 7, 16)     101392      re_lu_46[0][0]                   \n__________________________________________________________________________________________________\ndropout_46 (Dropout)            (None, 7, 7, 16)     0           conv2d_47[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_46 (Concatenate)    (None, 7, 7, 720)    0           concatenate_45[0][0]             \n                                                                 dropout_46[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_47 (BatchNo (None, 7, 7, 720)    2880        concatenate_46[0][0]             \n__________________________________________________________________________________________________\nre_lu_47 (ReLU)                 (None, 7, 7, 720)    0           batch_normalization_47[0][0]     \n__________________________________________________________________________________________________\nconv2d_48 (Conv2D)              (None, 7, 7, 16)     103696      re_lu_47[0][0]                   \n__________________________________________________________________________________________________\ndropout_47 (Dropout)            (None, 7, 7, 16)     0           conv2d_48[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_47 (Concatenate)    (None, 7, 7, 736)    0           concatenate_46[0][0]             \n                                                                 dropout_47[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_48 (BatchNo (None, 7, 7, 736)    2944        concatenate_47[0][0]             \n__________________________________________________________________________________________________\nre_lu_48 (ReLU)                 (None, 7, 7, 736)    0           batch_normalization_48[0][0]     \n__________________________________________________________________________________________________\nconv2d_49 (Conv2D)              (None, 7, 7, 16)     106000      re_lu_48[0][0]                   \n__________________________________________________________________________________________________\ndropout_48 (Dropout)            (None, 7, 7, 16)     0           conv2d_49[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_48 (Concatenate)    (None, 7, 7, 752)    0           concatenate_47[0][0]             \n                                                                 dropout_48[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_49 (BatchNo (None, 7, 7, 752)    3008        concatenate_48[0][0]             \n__________________________________________________________________________________________________\nre_lu_49 (ReLU)                 (None, 7, 7, 752)    0           batch_normalization_49[0][0]     \n__________________________________________________________________________________________________\nconv2d_50 (Conv2D)              (None, 7, 7, 16)     108304      re_lu_49[0][0]                   \n__________________________________________________________________________________________________\ndropout_49 (Dropout)            (None, 7, 7, 16)     0           conv2d_50[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_49 (Concatenate)    (None, 7, 7, 768)    0           concatenate_48[0][0]             \n                                                                 dropout_49[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_50 (BatchNo (None, 7, 7, 768)    3072        concatenate_49[0][0]             \n__________________________________________________________________________________________________\nre_lu_50 (ReLU)                 (None, 7, 7, 768)    0           batch_normalization_50[0][0]     \n__________________________________________________________________________________________________\nconv2d_51 (Conv2D)              (None, 7, 7, 16)     110608      re_lu_50[0][0]                   \n__________________________________________________________________________________________________\ndropout_50 (Dropout)            (None, 7, 7, 16)     0           conv2d_51[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_50 (Concatenate)    (None, 7, 7, 784)    0           concatenate_49[0][0]             \n                                                                 dropout_50[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_51 (BatchNo (None, 7, 7, 784)    3136        concatenate_50[0][0]             \n__________________________________________________________________________________________________\nre_lu_51 (ReLU)                 (None, 7, 7, 784)    0           batch_normalization_51[0][0]     \n__________________________________________________________________________________________________\nconv2d_52 (Conv2D)              (None, 7, 7, 16)     112912      re_lu_51[0][0]                   \n__________________________________________________________________________________________________\ndropout_51 (Dropout)            (None, 7, 7, 16)     0           conv2d_52[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_51 (Concatenate)    (None, 7, 7, 800)    0           concatenate_50[0][0]             \n                                                                 dropout_51[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_52 (BatchNo (None, 7, 7, 800)    3200        concatenate_51[0][0]             \n__________________________________________________________________________________________________\nre_lu_52 (ReLU)                 (None, 7, 7, 800)    0           batch_normalization_52[0][0]     \n__________________________________________________________________________________________________\nconv2d_53 (Conv2D)              (None, 7, 7, 16)     115216      re_lu_52[0][0]                   \n__________________________________________________________________________________________________\ndropout_52 (Dropout)            (None, 7, 7, 16)     0           conv2d_53[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_52 (Concatenate)    (None, 7, 7, 816)    0           concatenate_51[0][0]             \n                                                                 dropout_52[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_53 (BatchNo (None, 7, 7, 816)    3264        concatenate_52[0][0]             \n__________________________________________________________________________________________________\nre_lu_53 (ReLU)                 (None, 7, 7, 816)    0           batch_normalization_53[0][0]     \n__________________________________________________________________________________________________\nconv2d_54 (Conv2D)              (None, 7, 7, 16)     117520      re_lu_53[0][0]                   \n__________________________________________________________________________________________________\ndropout_53 (Dropout)            (None, 7, 7, 16)     0           conv2d_54[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_53 (Concatenate)    (None, 7, 7, 832)    0           concatenate_52[0][0]             \n                                                                 dropout_53[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_54 (BatchNo (None, 7, 7, 832)    3328        concatenate_53[0][0]             \n__________________________________________________________________________________________________\nre_lu_54 (ReLU)                 (None, 7, 7, 832)    0           batch_normalization_54[0][0]     \n__________________________________________________________________________________________________\nconv2d_55 (Conv2D)              (None, 7, 7, 16)     119824      re_lu_54[0][0]                   \n__________________________________________________________________________________________________\ndropout_54 (Dropout)            (None, 7, 7, 16)     0           conv2d_55[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_54 (Concatenate)    (None, 7, 7, 848)    0           concatenate_53[0][0]             \n                                                                 dropout_54[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_55 (BatchNo (None, 7, 7, 848)    3392        concatenate_54[0][0]             \n__________________________________________________________________________________________________\nre_lu_55 (ReLU)                 (None, 7, 7, 848)    0           batch_normalization_55[0][0]     \n__________________________________________________________________________________________________\nconv2d_56 (Conv2D)              (None, 7, 7, 16)     122128      re_lu_55[0][0]                   \n__________________________________________________________________________________________________\ndropout_55 (Dropout)            (None, 7, 7, 16)     0           conv2d_56[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_55 (Concatenate)    (None, 7, 7, 864)    0           concatenate_54[0][0]             \n                                                                 dropout_55[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_56 (BatchNo (None, 7, 7, 864)    3456        concatenate_55[0][0]             \n__________________________________________________________________________________________________\nre_lu_56 (ReLU)                 (None, 7, 7, 864)    0           batch_normalization_56[0][0]     \n__________________________________________________________________________________________________\nconv2d_57 (Conv2D)              (None, 7, 7, 16)     124432      re_lu_56[0][0]                   \n__________________________________________________________________________________________________\ndropout_56 (Dropout)            (None, 7, 7, 16)     0           conv2d_57[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_56 (Concatenate)    (None, 7, 7, 880)    0           concatenate_55[0][0]             \n                                                                 dropout_56[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_57 (BatchNo (None, 7, 7, 880)    3520        concatenate_56[0][0]             \n__________________________________________________________________________________________________\nre_lu_57 (ReLU)                 (None, 7, 7, 880)    0           batch_normalization_57[0][0]     \n__________________________________________________________________________________________________\nconv2d_58 (Conv2D)              (None, 7, 7, 16)     126736      re_lu_57[0][0]                   \n__________________________________________________________________________________________________\ndropout_57 (Dropout)            (None, 7, 7, 16)     0           conv2d_58[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_57 (Concatenate)    (None, 7, 7, 240)    0           dropout_57[0][0]                 \n                                                                 dropout_56[0][0]                 \n                                                                 dropout_55[0][0]                 \n                                                                 dropout_54[0][0]                 \n                                                                 dropout_53[0][0]                 \n                                                                 dropout_52[0][0]                 \n                                                                 dropout_51[0][0]                 \n                                                                 dropout_50[0][0]                 \n                                                                 dropout_49[0][0]                 \n                                                                 dropout_48[0][0]                 \n                                                                 dropout_47[0][0]                 \n                                                                 dropout_46[0][0]                 \n                                                                 dropout_45[0][0]                 \n                                                                 dropout_44[0][0]                 \n                                                                 dropout_43[0][0]                 \n__________________________________________________________________________________________________\nconv2d_transpose (Conv2DTranspo (None, 14, 14, 240)  518640      concatenate_57[0][0]             \n__________________________________________________________________________________________________\nconcatenate_58 (Concatenate)    (None, 14, 14, 896)  0           conv2d_transpose[0][0]           \n                                                                 concatenate_42[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_58 (BatchNo (None, 14, 14, 896)  3584        concatenate_58[0][0]             \n__________________________________________________________________________________________________\nre_lu_58 (ReLU)                 (None, 14, 14, 896)  0           batch_normalization_58[0][0]     \n__________________________________________________________________________________________________\nconv2d_59 (Conv2D)              (None, 14, 14, 16)   129040      re_lu_58[0][0]                   \n__________________________________________________________________________________________________\ndropout_58 (Dropout)            (None, 14, 14, 16)   0           conv2d_59[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_59 (Concatenate)    (None, 14, 14, 912)  0           concatenate_58[0][0]             \n                                                                 dropout_58[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_59 (BatchNo (None, 14, 14, 912)  3648        concatenate_59[0][0]             \n__________________________________________________________________________________________________\nre_lu_59 (ReLU)                 (None, 14, 14, 912)  0           batch_normalization_59[0][0]     \n__________________________________________________________________________________________________\nconv2d_60 (Conv2D)              (None, 14, 14, 16)   131344      re_lu_59[0][0]                   \n__________________________________________________________________________________________________\ndropout_59 (Dropout)            (None, 14, 14, 16)   0           conv2d_60[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_60 (Concatenate)    (None, 14, 14, 928)  0           concatenate_59[0][0]             \n                                                                 dropout_59[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_60 (BatchNo (None, 14, 14, 928)  3712        concatenate_60[0][0]             \n__________________________________________________________________________________________________\nre_lu_60 (ReLU)                 (None, 14, 14, 928)  0           batch_normalization_60[0][0]     \n__________________________________________________________________________________________________\nconv2d_61 (Conv2D)              (None, 14, 14, 16)   133648      re_lu_60[0][0]                   \n__________________________________________________________________________________________________\ndropout_60 (Dropout)            (None, 14, 14, 16)   0           conv2d_61[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_61 (Concatenate)    (None, 14, 14, 944)  0           concatenate_60[0][0]             \n                                                                 dropout_60[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_61 (BatchNo (None, 14, 14, 944)  3776        concatenate_61[0][0]             \n__________________________________________________________________________________________________\nre_lu_61 (ReLU)                 (None, 14, 14, 944)  0           batch_normalization_61[0][0]     \n__________________________________________________________________________________________________\nconv2d_62 (Conv2D)              (None, 14, 14, 16)   135952      re_lu_61[0][0]                   \n__________________________________________________________________________________________________\ndropout_61 (Dropout)            (None, 14, 14, 16)   0           conv2d_62[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_62 (Concatenate)    (None, 14, 14, 960)  0           concatenate_61[0][0]             \n                                                                 dropout_61[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_62 (BatchNo (None, 14, 14, 960)  3840        concatenate_62[0][0]             \n__________________________________________________________________________________________________\nre_lu_62 (ReLU)                 (None, 14, 14, 960)  0           batch_normalization_62[0][0]     \n__________________________________________________________________________________________________\nconv2d_63 (Conv2D)              (None, 14, 14, 16)   138256      re_lu_62[0][0]                   \n__________________________________________________________________________________________________\ndropout_62 (Dropout)            (None, 14, 14, 16)   0           conv2d_63[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_63 (Concatenate)    (None, 14, 14, 976)  0           concatenate_62[0][0]             \n                                                                 dropout_62[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_63 (BatchNo (None, 14, 14, 976)  3904        concatenate_63[0][0]             \n__________________________________________________________________________________________________\nre_lu_63 (ReLU)                 (None, 14, 14, 976)  0           batch_normalization_63[0][0]     \n__________________________________________________________________________________________________\nconv2d_64 (Conv2D)              (None, 14, 14, 16)   140560      re_lu_63[0][0]                   \n__________________________________________________________________________________________________\ndropout_63 (Dropout)            (None, 14, 14, 16)   0           conv2d_64[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_64 (Concatenate)    (None, 14, 14, 992)  0           concatenate_63[0][0]             \n                                                                 dropout_63[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_64 (BatchNo (None, 14, 14, 992)  3968        concatenate_64[0][0]             \n__________________________________________________________________________________________________\nre_lu_64 (ReLU)                 (None, 14, 14, 992)  0           batch_normalization_64[0][0]     \n__________________________________________________________________________________________________\nconv2d_65 (Conv2D)              (None, 14, 14, 16)   142864      re_lu_64[0][0]                   \n__________________________________________________________________________________________________\ndropout_64 (Dropout)            (None, 14, 14, 16)   0           conv2d_65[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_65 (Concatenate)    (None, 14, 14, 1008) 0           concatenate_64[0][0]             \n                                                                 dropout_64[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_65 (BatchNo (None, 14, 14, 1008) 4032        concatenate_65[0][0]             \n__________________________________________________________________________________________________\nre_lu_65 (ReLU)                 (None, 14, 14, 1008) 0           batch_normalization_65[0][0]     \n__________________________________________________________________________________________________\nconv2d_66 (Conv2D)              (None, 14, 14, 16)   145168      re_lu_65[0][0]                   \n__________________________________________________________________________________________________\ndropout_65 (Dropout)            (None, 14, 14, 16)   0           conv2d_66[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_66 (Concatenate)    (None, 14, 14, 1024) 0           concatenate_65[0][0]             \n                                                                 dropout_65[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_66 (BatchNo (None, 14, 14, 1024) 4096        concatenate_66[0][0]             \n__________________________________________________________________________________________________\nre_lu_66 (ReLU)                 (None, 14, 14, 1024) 0           batch_normalization_66[0][0]     \n__________________________________________________________________________________________________\nconv2d_67 (Conv2D)              (None, 14, 14, 16)   147472      re_lu_66[0][0]                   \n__________________________________________________________________________________________________\ndropout_66 (Dropout)            (None, 14, 14, 16)   0           conv2d_67[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_67 (Concatenate)    (None, 14, 14, 1040) 0           concatenate_66[0][0]             \n                                                                 dropout_66[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_67 (BatchNo (None, 14, 14, 1040) 4160        concatenate_67[0][0]             \n__________________________________________________________________________________________________\nre_lu_67 (ReLU)                 (None, 14, 14, 1040) 0           batch_normalization_67[0][0]     \n__________________________________________________________________________________________________\nconv2d_68 (Conv2D)              (None, 14, 14, 16)   149776      re_lu_67[0][0]                   \n__________________________________________________________________________________________________\ndropout_67 (Dropout)            (None, 14, 14, 16)   0           conv2d_68[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_68 (Concatenate)    (None, 14, 14, 1056) 0           concatenate_67[0][0]             \n                                                                 dropout_67[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_68 (BatchNo (None, 14, 14, 1056) 4224        concatenate_68[0][0]             \n__________________________________________________________________________________________________\nre_lu_68 (ReLU)                 (None, 14, 14, 1056) 0           batch_normalization_68[0][0]     \n__________________________________________________________________________________________________\nconv2d_69 (Conv2D)              (None, 14, 14, 16)   152080      re_lu_68[0][0]                   \n__________________________________________________________________________________________________\ndropout_68 (Dropout)            (None, 14, 14, 16)   0           conv2d_69[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_69 (Concatenate)    (None, 14, 14, 1072) 0           concatenate_68[0][0]             \n                                                                 dropout_68[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_69 (BatchNo (None, 14, 14, 1072) 4288        concatenate_69[0][0]             \n__________________________________________________________________________________________________\nre_lu_69 (ReLU)                 (None, 14, 14, 1072) 0           batch_normalization_69[0][0]     \n__________________________________________________________________________________________________\nconv2d_70 (Conv2D)              (None, 14, 14, 16)   154384      re_lu_69[0][0]                   \n__________________________________________________________________________________________________\ndropout_69 (Dropout)            (None, 14, 14, 16)   0           conv2d_70[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_70 (Concatenate)    (None, 14, 14, 192)  0           dropout_69[0][0]                 \n                                                                 dropout_68[0][0]                 \n                                                                 dropout_67[0][0]                 \n                                                                 dropout_66[0][0]                 \n                                                                 dropout_65[0][0]                 \n                                                                 dropout_64[0][0]                 \n                                                                 dropout_63[0][0]                 \n                                                                 dropout_62[0][0]                 \n                                                                 dropout_61[0][0]                 \n                                                                 dropout_60[0][0]                 \n                                                                 dropout_59[0][0]                 \n                                                                 dropout_58[0][0]                 \n__________________________________________________________________________________________________\nconv2d_transpose_1 (Conv2DTrans (None, 28, 28, 192)  331968      concatenate_70[0][0]             \n__________________________________________________________________________________________________\nconcatenate_71 (Concatenate)    (None, 28, 28, 656)  0           conv2d_transpose_1[0][0]         \n                                                                 concatenate_29[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_70 (BatchNo (None, 28, 28, 656)  2624        concatenate_71[0][0]             \n__________________________________________________________________________________________________\nre_lu_70 (ReLU)                 (None, 28, 28, 656)  0           batch_normalization_70[0][0]     \n__________________________________________________________________________________________________\nconv2d_71 (Conv2D)              (None, 28, 28, 16)   94480       re_lu_70[0][0]                   \n__________________________________________________________________________________________________\ndropout_70 (Dropout)            (None, 28, 28, 16)   0           conv2d_71[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_72 (Concatenate)    (None, 28, 28, 672)  0           concatenate_71[0][0]             \n                                                                 dropout_70[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_71 (BatchNo (None, 28, 28, 672)  2688        concatenate_72[0][0]             \n__________________________________________________________________________________________________\nre_lu_71 (ReLU)                 (None, 28, 28, 672)  0           batch_normalization_71[0][0]     \n__________________________________________________________________________________________________\nconv2d_72 (Conv2D)              (None, 28, 28, 16)   96784       re_lu_71[0][0]                   \n__________________________________________________________________________________________________\ndropout_71 (Dropout)            (None, 28, 28, 16)   0           conv2d_72[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_73 (Concatenate)    (None, 28, 28, 688)  0           concatenate_72[0][0]             \n                                                                 dropout_71[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_72 (BatchNo (None, 28, 28, 688)  2752        concatenate_73[0][0]             \n__________________________________________________________________________________________________\nre_lu_72 (ReLU)                 (None, 28, 28, 688)  0           batch_normalization_72[0][0]     \n__________________________________________________________________________________________________\nconv2d_73 (Conv2D)              (None, 28, 28, 16)   99088       re_lu_72[0][0]                   \n__________________________________________________________________________________________________\ndropout_72 (Dropout)            (None, 28, 28, 16)   0           conv2d_73[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_74 (Concatenate)    (None, 28, 28, 704)  0           concatenate_73[0][0]             \n                                                                 dropout_72[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_73 (BatchNo (None, 28, 28, 704)  2816        concatenate_74[0][0]             \n__________________________________________________________________________________________________\nre_lu_73 (ReLU)                 (None, 28, 28, 704)  0           batch_normalization_73[0][0]     \n__________________________________________________________________________________________________\nconv2d_74 (Conv2D)              (None, 28, 28, 16)   101392      re_lu_73[0][0]                   \n__________________________________________________________________________________________________\ndropout_73 (Dropout)            (None, 28, 28, 16)   0           conv2d_74[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_75 (Concatenate)    (None, 28, 28, 720)  0           concatenate_74[0][0]             \n                                                                 dropout_73[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_74 (BatchNo (None, 28, 28, 720)  2880        concatenate_75[0][0]             \n__________________________________________________________________________________________________\nre_lu_74 (ReLU)                 (None, 28, 28, 720)  0           batch_normalization_74[0][0]     \n__________________________________________________________________________________________________\nconv2d_75 (Conv2D)              (None, 28, 28, 16)   103696      re_lu_74[0][0]                   \n__________________________________________________________________________________________________\ndropout_74 (Dropout)            (None, 28, 28, 16)   0           conv2d_75[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_76 (Concatenate)    (None, 28, 28, 736)  0           concatenate_75[0][0]             \n                                                                 dropout_74[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_75 (BatchNo (None, 28, 28, 736)  2944        concatenate_76[0][0]             \n__________________________________________________________________________________________________\nre_lu_75 (ReLU)                 (None, 28, 28, 736)  0           batch_normalization_75[0][0]     \n__________________________________________________________________________________________________\nconv2d_76 (Conv2D)              (None, 28, 28, 16)   106000      re_lu_75[0][0]                   \n__________________________________________________________________________________________________\ndropout_75 (Dropout)            (None, 28, 28, 16)   0           conv2d_76[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_77 (Concatenate)    (None, 28, 28, 752)  0           concatenate_76[0][0]             \n                                                                 dropout_75[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_76 (BatchNo (None, 28, 28, 752)  3008        concatenate_77[0][0]             \n__________________________________________________________________________________________________\nre_lu_76 (ReLU)                 (None, 28, 28, 752)  0           batch_normalization_76[0][0]     \n__________________________________________________________________________________________________\nconv2d_77 (Conv2D)              (None, 28, 28, 16)   108304      re_lu_76[0][0]                   \n__________________________________________________________________________________________________\ndropout_76 (Dropout)            (None, 28, 28, 16)   0           conv2d_77[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_78 (Concatenate)    (None, 28, 28, 768)  0           concatenate_77[0][0]             \n                                                                 dropout_76[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_77 (BatchNo (None, 28, 28, 768)  3072        concatenate_78[0][0]             \n__________________________________________________________________________________________________\nre_lu_77 (ReLU)                 (None, 28, 28, 768)  0           batch_normalization_77[0][0]     \n__________________________________________________________________________________________________\nconv2d_78 (Conv2D)              (None, 28, 28, 16)   110608      re_lu_77[0][0]                   \n__________________________________________________________________________________________________\ndropout_77 (Dropout)            (None, 28, 28, 16)   0           conv2d_78[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_79 (Concatenate)    (None, 28, 28, 784)  0           concatenate_78[0][0]             \n                                                                 dropout_77[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_78 (BatchNo (None, 28, 28, 784)  3136        concatenate_79[0][0]             \n__________________________________________________________________________________________________\nre_lu_78 (ReLU)                 (None, 28, 28, 784)  0           batch_normalization_78[0][0]     \n__________________________________________________________________________________________________\nconv2d_79 (Conv2D)              (None, 28, 28, 16)   112912      re_lu_78[0][0]                   \n__________________________________________________________________________________________________\ndropout_78 (Dropout)            (None, 28, 28, 16)   0           conv2d_79[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_80 (Concatenate)    (None, 28, 28, 800)  0           concatenate_79[0][0]             \n                                                                 dropout_78[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_79 (BatchNo (None, 28, 28, 800)  3200        concatenate_80[0][0]             \n__________________________________________________________________________________________________\nre_lu_79 (ReLU)                 (None, 28, 28, 800)  0           batch_normalization_79[0][0]     \n__________________________________________________________________________________________________\nconv2d_80 (Conv2D)              (None, 28, 28, 16)   115216      re_lu_79[0][0]                   \n__________________________________________________________________________________________________\ndropout_79 (Dropout)            (None, 28, 28, 16)   0           conv2d_80[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_81 (Concatenate)    (None, 28, 28, 160)  0           dropout_79[0][0]                 \n                                                                 dropout_78[0][0]                 \n                                                                 dropout_77[0][0]                 \n                                                                 dropout_76[0][0]                 \n                                                                 dropout_75[0][0]                 \n                                                                 dropout_74[0][0]                 \n                                                                 dropout_73[0][0]                 \n                                                                 dropout_72[0][0]                 \n                                                                 dropout_71[0][0]                 \n                                                                 dropout_70[0][0]                 \n__________________________________________________________________________________________________\nconv2d_transpose_2 (Conv2DTrans (None, 56, 56, 160)  230560      concatenate_81[0][0]             \n__________________________________________________________________________________________________\nconcatenate_82 (Concatenate)    (None, 56, 56, 464)  0           conv2d_transpose_2[0][0]         \n                                                                 concatenate_18[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_80 (BatchNo (None, 56, 56, 464)  1856        concatenate_82[0][0]             \n__________________________________________________________________________________________________\nre_lu_80 (ReLU)                 (None, 56, 56, 464)  0           batch_normalization_80[0][0]     \n__________________________________________________________________________________________________\nconv2d_81 (Conv2D)              (None, 56, 56, 16)   66832       re_lu_80[0][0]                   \n__________________________________________________________________________________________________\ndropout_80 (Dropout)            (None, 56, 56, 16)   0           conv2d_81[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_83 (Concatenate)    (None, 56, 56, 480)  0           concatenate_82[0][0]             \n                                                                 dropout_80[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_81 (BatchNo (None, 56, 56, 480)  1920        concatenate_83[0][0]             \n__________________________________________________________________________________________________\nre_lu_81 (ReLU)                 (None, 56, 56, 480)  0           batch_normalization_81[0][0]     \n__________________________________________________________________________________________________\nconv2d_82 (Conv2D)              (None, 56, 56, 16)   69136       re_lu_81[0][0]                   \n__________________________________________________________________________________________________\ndropout_81 (Dropout)            (None, 56, 56, 16)   0           conv2d_82[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_84 (Concatenate)    (None, 56, 56, 496)  0           concatenate_83[0][0]             \n                                                                 dropout_81[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_82 (BatchNo (None, 56, 56, 496)  1984        concatenate_84[0][0]             \n__________________________________________________________________________________________________\nre_lu_82 (ReLU)                 (None, 56, 56, 496)  0           batch_normalization_82[0][0]     \n__________________________________________________________________________________________________\nconv2d_83 (Conv2D)              (None, 56, 56, 16)   71440       re_lu_82[0][0]                   \n__________________________________________________________________________________________________\ndropout_82 (Dropout)            (None, 56, 56, 16)   0           conv2d_83[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_85 (Concatenate)    (None, 56, 56, 512)  0           concatenate_84[0][0]             \n                                                                 dropout_82[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_83 (BatchNo (None, 56, 56, 512)  2048        concatenate_85[0][0]             \n__________________________________________________________________________________________________\nre_lu_83 (ReLU)                 (None, 56, 56, 512)  0           batch_normalization_83[0][0]     \n__________________________________________________________________________________________________\nconv2d_84 (Conv2D)              (None, 56, 56, 16)   73744       re_lu_83[0][0]                   \n__________________________________________________________________________________________________\ndropout_83 (Dropout)            (None, 56, 56, 16)   0           conv2d_84[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_86 (Concatenate)    (None, 56, 56, 528)  0           concatenate_85[0][0]             \n                                                                 dropout_83[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_84 (BatchNo (None, 56, 56, 528)  2112        concatenate_86[0][0]             \n__________________________________________________________________________________________________\nre_lu_84 (ReLU)                 (None, 56, 56, 528)  0           batch_normalization_84[0][0]     \n__________________________________________________________________________________________________\nconv2d_85 (Conv2D)              (None, 56, 56, 16)   76048       re_lu_84[0][0]                   \n__________________________________________________________________________________________________\ndropout_84 (Dropout)            (None, 56, 56, 16)   0           conv2d_85[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_87 (Concatenate)    (None, 56, 56, 544)  0           concatenate_86[0][0]             \n                                                                 dropout_84[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_85 (BatchNo (None, 56, 56, 544)  2176        concatenate_87[0][0]             \n__________________________________________________________________________________________________\nre_lu_85 (ReLU)                 (None, 56, 56, 544)  0           batch_normalization_85[0][0]     \n__________________________________________________________________________________________________\nconv2d_86 (Conv2D)              (None, 56, 56, 16)   78352       re_lu_85[0][0]                   \n__________________________________________________________________________________________________\ndropout_85 (Dropout)            (None, 56, 56, 16)   0           conv2d_86[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_88 (Concatenate)    (None, 56, 56, 560)  0           concatenate_87[0][0]             \n                                                                 dropout_85[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_86 (BatchNo (None, 56, 56, 560)  2240        concatenate_88[0][0]             \n__________________________________________________________________________________________________\nre_lu_86 (ReLU)                 (None, 56, 56, 560)  0           batch_normalization_86[0][0]     \n__________________________________________________________________________________________________\nconv2d_87 (Conv2D)              (None, 56, 56, 16)   80656       re_lu_86[0][0]                   \n__________________________________________________________________________________________________\ndropout_86 (Dropout)            (None, 56, 56, 16)   0           conv2d_87[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_89 (Concatenate)    (None, 56, 56, 112)  0           dropout_86[0][0]                 \n                                                                 dropout_85[0][0]                 \n                                                                 dropout_84[0][0]                 \n                                                                 dropout_83[0][0]                 \n                                                                 dropout_82[0][0]                 \n                                                                 dropout_81[0][0]                 \n                                                                 dropout_80[0][0]                 \n__________________________________________________________________________________________________\nconv2d_transpose_3 (Conv2DTrans (None, 112, 112, 112 113008      concatenate_89[0][0]             \n__________________________________________________________________________________________________\nconcatenate_90 (Concatenate)    (None, 112, 112, 304 0           conv2d_transpose_3[0][0]         \n                                                                 concatenate_10[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_87 (BatchNo (None, 112, 112, 304 1216        concatenate_90[0][0]             \n__________________________________________________________________________________________________\nre_lu_87 (ReLU)                 (None, 112, 112, 304 0           batch_normalization_87[0][0]     \n__________________________________________________________________________________________________\nconv2d_88 (Conv2D)              (None, 112, 112, 16) 43792       re_lu_87[0][0]                   \n__________________________________________________________________________________________________\ndropout_87 (Dropout)            (None, 112, 112, 16) 0           conv2d_88[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_91 (Concatenate)    (None, 112, 112, 320 0           concatenate_90[0][0]             \n                                                                 dropout_87[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_88 (BatchNo (None, 112, 112, 320 1280        concatenate_91[0][0]             \n__________________________________________________________________________________________________\nre_lu_88 (ReLU)                 (None, 112, 112, 320 0           batch_normalization_88[0][0]     \n__________________________________________________________________________________________________\nconv2d_89 (Conv2D)              (None, 112, 112, 16) 46096       re_lu_88[0][0]                   \n__________________________________________________________________________________________________\ndropout_88 (Dropout)            (None, 112, 112, 16) 0           conv2d_89[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_92 (Concatenate)    (None, 112, 112, 336 0           concatenate_91[0][0]             \n                                                                 dropout_88[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_89 (BatchNo (None, 112, 112, 336 1344        concatenate_92[0][0]             \n__________________________________________________________________________________________________\nre_lu_89 (ReLU)                 (None, 112, 112, 336 0           batch_normalization_89[0][0]     \n__________________________________________________________________________________________________\nconv2d_90 (Conv2D)              (None, 112, 112, 16) 48400       re_lu_89[0][0]                   \n__________________________________________________________________________________________________\ndropout_89 (Dropout)            (None, 112, 112, 16) 0           conv2d_90[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_93 (Concatenate)    (None, 112, 112, 352 0           concatenate_92[0][0]             \n                                                                 dropout_89[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_90 (BatchNo (None, 112, 112, 352 1408        concatenate_93[0][0]             \n__________________________________________________________________________________________________\nre_lu_90 (ReLU)                 (None, 112, 112, 352 0           batch_normalization_90[0][0]     \n__________________________________________________________________________________________________\nconv2d_91 (Conv2D)              (None, 112, 112, 16) 50704       re_lu_90[0][0]                   \n__________________________________________________________________________________________________\ndropout_90 (Dropout)            (None, 112, 112, 16) 0           conv2d_91[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_94 (Concatenate)    (None, 112, 112, 368 0           concatenate_93[0][0]             \n                                                                 dropout_90[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_91 (BatchNo (None, 112, 112, 368 1472        concatenate_94[0][0]             \n__________________________________________________________________________________________________\nre_lu_91 (ReLU)                 (None, 112, 112, 368 0           batch_normalization_91[0][0]     \n__________________________________________________________________________________________________\nconv2d_92 (Conv2D)              (None, 112, 112, 16) 53008       re_lu_91[0][0]                   \n__________________________________________________________________________________________________\ndropout_91 (Dropout)            (None, 112, 112, 16) 0           conv2d_92[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_95 (Concatenate)    (None, 112, 112, 80) 0           dropout_91[0][0]                 \n                                                                 dropout_90[0][0]                 \n                                                                 dropout_89[0][0]                 \n                                                                 dropout_88[0][0]                 \n                                                                 dropout_87[0][0]                 \n__________________________________________________________________________________________________\nconv2d_transpose_4 (Conv2DTrans (None, 224, 224, 80) 57680       concatenate_95[0][0]             \n__________________________________________________________________________________________________\nconcatenate_96 (Concatenate)    (None, 224, 224, 192 0           conv2d_transpose_4[0][0]         \n                                                                 concatenate_4[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_92 (BatchNo (None, 224, 224, 192 768         concatenate_96[0][0]             \n__________________________________________________________________________________________________\nre_lu_92 (ReLU)                 (None, 224, 224, 192 0           batch_normalization_92[0][0]     \n__________________________________________________________________________________________________\nconv2d_93 (Conv2D)              (None, 224, 224, 16) 27664       re_lu_92[0][0]                   \n__________________________________________________________________________________________________\ndropout_92 (Dropout)            (None, 224, 224, 16) 0           conv2d_93[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_97 (Concatenate)    (None, 224, 224, 208 0           concatenate_96[0][0]             \n                                                                 dropout_92[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_93 (BatchNo (None, 224, 224, 208 832         concatenate_97[0][0]             \n__________________________________________________________________________________________________\nre_lu_93 (ReLU)                 (None, 224, 224, 208 0           batch_normalization_93[0][0]     \n__________________________________________________________________________________________________\nconv2d_94 (Conv2D)              (None, 224, 224, 16) 29968       re_lu_93[0][0]                   \n__________________________________________________________________________________________________\ndropout_93 (Dropout)            (None, 224, 224, 16) 0           conv2d_94[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_98 (Concatenate)    (None, 224, 224, 224 0           concatenate_97[0][0]             \n                                                                 dropout_93[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_94 (BatchNo (None, 224, 224, 224 896         concatenate_98[0][0]             \n__________________________________________________________________________________________________\nre_lu_94 (ReLU)                 (None, 224, 224, 224 0           batch_normalization_94[0][0]     \n__________________________________________________________________________________________________\nconv2d_95 (Conv2D)              (None, 224, 224, 16) 32272       re_lu_94[0][0]                   \n__________________________________________________________________________________________________\ndropout_94 (Dropout)            (None, 224, 224, 16) 0           conv2d_95[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_99 (Concatenate)    (None, 224, 224, 240 0           concatenate_98[0][0]             \n                                                                 dropout_94[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_95 (BatchNo (None, 224, 224, 240 960         concatenate_99[0][0]             \n__________________________________________________________________________________________________\nre_lu_95 (ReLU)                 (None, 224, 224, 240 0           batch_normalization_95[0][0]     \n__________________________________________________________________________________________________\nconv2d_96 (Conv2D)              (None, 224, 224, 16) 34576       re_lu_95[0][0]                   \n__________________________________________________________________________________________________\ndropout_95 (Dropout)            (None, 224, 224, 16) 0           conv2d_96[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_100 (Concatenate)   (None, 224, 224, 64) 0           dropout_95[0][0]                 \n                                                                 dropout_94[0][0]                 \n                                                                 dropout_93[0][0]                 \n                                                                 dropout_92[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_101 (Concatenate)   (None, 224, 224, 256 0           concatenate_100[0][0]            \n                                                                 concatenate_96[0][0]             \n__________________________________________________________________________________________________\nconv2d_97 (Conv2D)              (None, 224, 224, 32) 8224        concatenate_101[0][0]            \n__________________________________________________________________________________________________\nactivation (Activation)         (None, 224, 224, 32) 0           conv2d_97[0][0]                  \n==================================================================================================\nTotal params: 9,430,560\nTrainable params: 9,327,488\nNon-trainable params: 103,072\n__________________________________________________________________________________________________\n</code>\n</pre>        <pre><code>p = model.predict(x_train[:1])\np.shape\n</code></pre>      <pre>\n<code>(1, 224, 224, 32)</code>\n</pre>         <ul> <li>Finally the model is trained by minimizing the pixel-wise cross-entropy loss.</li> <li>We [..] train them with RMSprop with an initial learning rate of \\(1e-3\\).</li> </ul>      <pre><code>model.compile(loss='sparse_categorical_crossentropy', \n              optimizer=keras.optimizers.RMSprop(lr=1e-3),\n              metrics=[\"accuracy\"])\n</code></pre>     <pre><code>history = model.fit(ds,\n                    epochs=100,\n                    validation_data=ds_val)\n</code></pre>      <pre>\n<code>Epoch 1/100\n123/123 [==============================] - 48s 393ms/step - loss: 1.7000 - accuracy: 0.5569 - val_loss: 9.1128 - val_accuracy: 0.3007\nEpoch 2/100\n123/123 [==============================] - 43s 351ms/step - loss: 1.1311 - accuracy: 0.6471 - val_loss: 1.6813 - val_accuracy: 0.5465\nEpoch 3/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.8990 - accuracy: 0.7045 - val_loss: 1.8816 - val_accuracy: 0.4774\nEpoch 4/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.7812 - accuracy: 0.7444 - val_loss: 0.8675 - val_accuracy: 0.6934\nEpoch 5/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.7029 - accuracy: 0.7717 - val_loss: 1.2044 - val_accuracy: 0.6180\nEpoch 6/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.6366 - accuracy: 0.7987 - val_loss: 1.9921 - val_accuracy: 0.5372\nEpoch 7/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.5925 - accuracy: 0.8153 - val_loss: 0.7431 - val_accuracy: 0.7931\nEpoch 8/100\n123/123 [==============================] - 43s 350ms/step - loss: 0.5673 - accuracy: 0.8236 - val_loss: 0.8841 - val_accuracy: 0.7234\nEpoch 9/100\n123/123 [==============================] - 43s 350ms/step - loss: 0.5283 - accuracy: 0.8344 - val_loss: 0.4967 - val_accuracy: 0.8571\nEpoch 10/100\n123/123 [==============================] - 43s 350ms/step - loss: 0.5007 - accuracy: 0.8439 - val_loss: 1.2100 - val_accuracy: 0.6635\nEpoch 11/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.4731 - accuracy: 0.8528 - val_loss: 0.9340 - val_accuracy: 0.6997\nEpoch 12/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.4596 - accuracy: 0.8558 - val_loss: 0.6056 - val_accuracy: 0.7978\nEpoch 13/100\n123/123 [==============================] - 43s 350ms/step - loss: 0.4346 - accuracy: 0.8638 - val_loss: 0.5955 - val_accuracy: 0.8054\nEpoch 14/100\n123/123 [==============================] - 43s 350ms/step - loss: 0.4220 - accuracy: 0.8671 - val_loss: 0.9059 - val_accuracy: 0.7086\nEpoch 15/100\n123/123 [==============================] - 43s 350ms/step - loss: 0.4042 - accuracy: 0.8727 - val_loss: 0.6410 - val_accuracy: 0.7903\nEpoch 16/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.3944 - accuracy: 0.8750 - val_loss: 0.4750 - val_accuracy: 0.8408\nEpoch 17/100\n123/123 [==============================] - 43s 350ms/step - loss: 0.3799 - accuracy: 0.8799 - val_loss: 0.5152 - val_accuracy: 0.8351\nEpoch 18/100\n123/123 [==============================] - 43s 350ms/step - loss: 0.3625 - accuracy: 0.8835 - val_loss: 0.4473 - val_accuracy: 0.8606\nEpoch 19/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.3584 - accuracy: 0.8863 - val_loss: 0.7289 - val_accuracy: 0.7750\nEpoch 20/100\n123/123 [==============================] - 43s 350ms/step - loss: 0.3505 - accuracy: 0.8883 - val_loss: 0.5047 - val_accuracy: 0.8290\nEpoch 21/100\n123/123 [==============================] - 43s 350ms/step - loss: 0.3388 - accuracy: 0.8916 - val_loss: 0.5658 - val_accuracy: 0.8166\nEpoch 22/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.3284 - accuracy: 0.8945 - val_loss: 0.5581 - val_accuracy: 0.8154\nEpoch 23/100\n123/123 [==============================] - 43s 350ms/step - loss: 0.3181 - accuracy: 0.8972 - val_loss: 0.5958 - val_accuracy: 0.8203\nEpoch 24/100\n123/123 [==============================] - 43s 350ms/step - loss: 0.3262 - accuracy: 0.8970 - val_loss: 0.4110 - val_accuracy: 0.8761\nEpoch 25/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.3018 - accuracy: 0.9023 - val_loss: 0.5816 - val_accuracy: 0.8087\nEpoch 26/100\n123/123 [==============================] - 43s 350ms/step - loss: 0.3022 - accuracy: 0.9025 - val_loss: 0.3868 - val_accuracy: 0.8758\nEpoch 27/100\n123/123 [==============================] - 43s 350ms/step - loss: 0.2884 - accuracy: 0.9061 - val_loss: 0.3972 - val_accuracy: 0.8757\nEpoch 28/100\n123/123 [==============================] - 43s 350ms/step - loss: 0.2863 - accuracy: 0.9074 - val_loss: 0.9924 - val_accuracy: 0.7346\nEpoch 29/100\n123/123 [==============================] - 43s 350ms/step - loss: 0.2804 - accuracy: 0.9094 - val_loss: 0.7550 - val_accuracy: 0.8023\nEpoch 30/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.2726 - accuracy: 0.9112 - val_loss: 0.4430 - val_accuracy: 0.8712\nEpoch 31/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.2686 - accuracy: 0.9128 - val_loss: 0.4565 - val_accuracy: 0.8693\nEpoch 32/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.2639 - accuracy: 0.9144 - val_loss: 0.4576 - val_accuracy: 0.8583\nEpoch 33/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.2616 - accuracy: 0.9144 - val_loss: 0.4432 - val_accuracy: 0.8591\nEpoch 34/100\n123/123 [==============================] - 43s 352ms/step - loss: 0.2469 - accuracy: 0.9192 - val_loss: 0.6009 - val_accuracy: 0.8319\nEpoch 35/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.2495 - accuracy: 0.9191 - val_loss: 1.3375 - val_accuracy: 0.6777\nEpoch 36/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.2442 - accuracy: 0.9198 - val_loss: 0.4646 - val_accuracy: 0.8775\nEpoch 37/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.2401 - accuracy: 0.9211 - val_loss: 0.3355 - val_accuracy: 0.9030\nEpoch 38/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.2317 - accuracy: 0.9239 - val_loss: 0.4999 - val_accuracy: 0.8618\nEpoch 39/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.2368 - accuracy: 0.9230 - val_loss: 0.4376 - val_accuracy: 0.8730\nEpoch 40/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.2288 - accuracy: 0.9247 - val_loss: 0.4820 - val_accuracy: 0.8596\nEpoch 41/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.2210 - accuracy: 0.9266 - val_loss: 0.5659 - val_accuracy: 0.8137\nEpoch 42/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.2191 - accuracy: 0.9273 - val_loss: 0.3579 - val_accuracy: 0.8869\nEpoch 43/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.2143 - accuracy: 0.9292 - val_loss: 0.3729 - val_accuracy: 0.8863\nEpoch 44/100\n123/123 [==============================] - 43s 352ms/step - loss: 0.2168 - accuracy: 0.9286 - val_loss: 0.3855 - val_accuracy: 0.8894\nEpoch 45/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.2090 - accuracy: 0.9304 - val_loss: 0.3963 - val_accuracy: 0.8897\nEpoch 46/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.2058 - accuracy: 0.9315 - val_loss: 0.3581 - val_accuracy: 0.8983\nEpoch 47/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.2017 - accuracy: 0.9326 - val_loss: 0.4028 - val_accuracy: 0.8788\nEpoch 48/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.2027 - accuracy: 0.9326 - val_loss: 0.5269 - val_accuracy: 0.8561\nEpoch 49/100\n123/123 [==============================] - 43s 352ms/step - loss: 0.1976 - accuracy: 0.9337 - val_loss: 0.3576 - val_accuracy: 0.8962\nEpoch 50/100\n123/123 [==============================] - 43s 352ms/step - loss: 0.1984 - accuracy: 0.9341 - val_loss: 0.3405 - val_accuracy: 0.9113\nEpoch 51/100\n123/123 [==============================] - 43s 352ms/step - loss: 0.1917 - accuracy: 0.9356 - val_loss: 0.4795 - val_accuracy: 0.8662\nEpoch 52/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1909 - accuracy: 0.9358 - val_loss: 0.4778 - val_accuracy: 0.8651\nEpoch 53/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1850 - accuracy: 0.9379 - val_loss: 0.4367 - val_accuracy: 0.8779\nEpoch 54/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1844 - accuracy: 0.9379 - val_loss: 0.3973 - val_accuracy: 0.8913\nEpoch 55/100\n123/123 [==============================] - 43s 352ms/step - loss: 0.1805 - accuracy: 0.9390 - val_loss: 0.4477 - val_accuracy: 0.8826\nEpoch 56/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1800 - accuracy: 0.9392 - val_loss: 0.3471 - val_accuracy: 0.8987\nEpoch 57/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1792 - accuracy: 0.9398 - val_loss: 0.3472 - val_accuracy: 0.8999\nEpoch 58/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1748 - accuracy: 0.9408 - val_loss: 0.3264 - val_accuracy: 0.9033\nEpoch 59/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1774 - accuracy: 0.9405 - val_loss: 0.7149 - val_accuracy: 0.8260\nEpoch 60/100\n123/123 [==============================] - 43s 350ms/step - loss: 0.1714 - accuracy: 0.9418 - val_loss: 0.4711 - val_accuracy: 0.8601\nEpoch 61/100\n123/123 [==============================] - 43s 350ms/step - loss: 0.1716 - accuracy: 0.9416 - val_loss: 0.3424 - val_accuracy: 0.9060\nEpoch 62/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1676 - accuracy: 0.9429 - val_loss: 0.3823 - val_accuracy: 0.8977\nEpoch 63/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1644 - accuracy: 0.9440 - val_loss: 0.3168 - val_accuracy: 0.9119\nEpoch 64/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1652 - accuracy: 0.9437 - val_loss: 0.3276 - val_accuracy: 0.9053\nEpoch 65/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1613 - accuracy: 0.9449 - val_loss: 0.5732 - val_accuracy: 0.8595\nEpoch 66/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1604 - accuracy: 0.9450 - val_loss: 0.4227 - val_accuracy: 0.8792\nEpoch 67/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1604 - accuracy: 0.9450 - val_loss: 0.2825 - val_accuracy: 0.9182\nEpoch 68/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1567 - accuracy: 0.9463 - val_loss: 0.3430 - val_accuracy: 0.8953\nEpoch 69/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1567 - accuracy: 0.9462 - val_loss: 0.5003 - val_accuracy: 0.8352\nEpoch 70/100\n123/123 [==============================] - 43s 352ms/step - loss: 0.1532 - accuracy: 0.9472 - val_loss: 0.3316 - val_accuracy: 0.9040\nEpoch 71/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1511 - accuracy: 0.9479 - val_loss: 0.3720 - val_accuracy: 0.8978\nEpoch 72/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1515 - accuracy: 0.9477 - val_loss: 0.3675 - val_accuracy: 0.9019\nEpoch 73/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1474 - accuracy: 0.9492 - val_loss: 0.4759 - val_accuracy: 0.8688\nEpoch 74/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1475 - accuracy: 0.9489 - val_loss: 0.4692 - val_accuracy: 0.8696\nEpoch 75/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1467 - accuracy: 0.9492 - val_loss: 0.4158 - val_accuracy: 0.8747\nEpoch 76/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1465 - accuracy: 0.9492 - val_loss: 0.3227 - val_accuracy: 0.9141\nEpoch 77/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1435 - accuracy: 0.9500 - val_loss: 0.3192 - val_accuracy: 0.9146\nEpoch 78/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1420 - accuracy: 0.9506 - val_loss: 0.3807 - val_accuracy: 0.8848\nEpoch 79/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1412 - accuracy: 0.9509 - val_loss: 0.3369 - val_accuracy: 0.9101\nEpoch 80/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1392 - accuracy: 0.9516 - val_loss: 0.3258 - val_accuracy: 0.9090\nEpoch 81/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1372 - accuracy: 0.9523 - val_loss: 0.4064 - val_accuracy: 0.8784\nEpoch 82/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1380 - accuracy: 0.9519 - val_loss: 0.3446 - val_accuracy: 0.9037\nEpoch 83/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1365 - accuracy: 0.9525 - val_loss: 0.6431 - val_accuracy: 0.7953\nEpoch 84/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1359 - accuracy: 0.9529 - val_loss: 0.3985 - val_accuracy: 0.8960\nEpoch 85/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1347 - accuracy: 0.9528 - val_loss: 0.3419 - val_accuracy: 0.8967\nEpoch 86/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1331 - accuracy: 0.9533 - val_loss: 0.3882 - val_accuracy: 0.9004\nEpoch 87/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1324 - accuracy: 0.9535 - val_loss: 0.3790 - val_accuracy: 0.8895\nEpoch 88/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1310 - accuracy: 0.9539 - val_loss: 0.3125 - val_accuracy: 0.9154\nEpoch 89/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1300 - accuracy: 0.9544 - val_loss: 0.3441 - val_accuracy: 0.9096\nEpoch 90/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1284 - accuracy: 0.9548 - val_loss: 0.3716 - val_accuracy: 0.9063\nEpoch 91/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1278 - accuracy: 0.9550 - val_loss: 0.3738 - val_accuracy: 0.8969\nEpoch 92/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1267 - accuracy: 0.9554 - val_loss: 0.3466 - val_accuracy: 0.9083\nEpoch 93/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1256 - accuracy: 0.9556 - val_loss: 0.3364 - val_accuracy: 0.9146\nEpoch 94/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1242 - accuracy: 0.9561 - val_loss: 0.3062 - val_accuracy: 0.9176\nEpoch 95/100\n123/123 [==============================] - 43s 352ms/step - loss: 0.1231 - accuracy: 0.9565 - val_loss: 0.3467 - val_accuracy: 0.9145\nEpoch 96/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1228 - accuracy: 0.9565 - val_loss: 0.4429 - val_accuracy: 0.8918\nEpoch 97/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1217 - accuracy: 0.9569 - val_loss: 0.4052 - val_accuracy: 0.9002\nEpoch 98/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1217 - accuracy: 0.9569 - val_loss: 0.3283 - val_accuracy: 0.9131\nEpoch 99/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1196 - accuracy: 0.9575 - val_loss: 0.3455 - val_accuracy: 0.9088\nEpoch 100/100\n123/123 [==============================] - 43s 351ms/step - loss: 0.1192 - accuracy: 0.9576 - val_loss: 0.3164 - val_accuracy: 0.9156\n</code>\n</pre>        <pre><code>%pylab inline\nfigsize(10, 5)\n\nsubplot(1, 2, 1)\nplot(history.history['accuracy'], label='accuracy')\nplot(history.history['val_accuracy'], label='validation accuracy')\nlegend()\n\nsubplot(1, 2, 2)\nplot(history.history['loss'], label='loss')\nplot(history.history['val_loss'], label='validation loss')\n\nlegend()\nshow()\n</code></pre>      <pre>\n<code>Populating the interactive namespace from numpy and matplotlib\n</code>\n</pre>             <pre><code>model.save('fc_densenet_103_camvid_100epochs.h5')\n</code></pre>     <pre><code> model.evaluate(x_test, y_test)\n</code></pre>      <pre>\n<code>8/8 [==============================] - 6s 747ms/step - loss: 0.6612 - accuracy: 0.8556\n</code>\n</pre>     <pre>\n<code>[0.6612146496772766, 0.855552077293396]</code>\n</pre>         <p>La liste des classes et des couleurs attenantes est \u00e0 l'adresse http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/data/label_colors.txt</p>       <p>Interact labeller pour la segmentation</p> <p>http://mi.eng.cam.ac.uk/projects/cvgroup/software/index.html</p>       <p>\"The training images were hand-labeled with the assigned colors acting as indices into the list of object classes.\"</p>      <pre><code>colors = [(192, 192, 128),  \n(192, 192, 0),\n(192, 128, 192),\n(192, 128, 128),\n(192, 128, 64),\n(192, 64, 128), \n(192, 0, 192),   \n(192, 0, 128),\n(192, 0, 64),\n(128, 128, 192),\n(128, 128, 128),\n(128, 128, 64),\n(128, 128, 0),\n(128, 64, 128),\n(128, 64, 64),  \n(128, 0, 192),    \n(128, 0, 0),  \n(64, 192, 128), \n(64, 192, 0),\n(64, 128, 192),\n(64, 128, 64),  \n(64, 64, 128),    \n(64, 64, 0), \n(64, 0, 192),     \n(64, 0, 128),\n(64, 0, 64),\n(0, 128, 192),\n(0, 128, 64),\n(0, 64, 64),\n(0, 0, 192),\n(0, 0, 64),\n(0, 0, 0)]\n</code></pre>     <pre><code>class_weights = {0 : 9.60606490919,\n1 : 9.77766487673,\n2 : 10.0194993159,\n3 : 10.3749043447,\n4 : 10.4631076496,\n5 : 10.4920586873,\n6 : 10.4839211216,\n7 : 10.4356039685,\n8 : 10.4757462996,\n9 : 10.2342992955,\n10 : 4.3720296426,\n11 : 9.94147719336,\n12 : 5.43594061016,\n13 : 3.15728309302,\n14 : 10.1258833847,\n15 : 9.08285926082,\n16 : 3.47796865386,\n17 : 10.1577327692,\n18 : 9.32269173889,\n19 :    9.7926997517,\n20 :    10.4870856376,\n21 :    9.26646394904,\n22 :    9.88371158915,\n23 :    10.4613353846,\n24 :    7.95097080927,\n25 :    10.4920560223,\n26 :    9.99897358787,\n27 :    10.4371404226,\n28 :    10.1479245421,\n39 :    6.58893008318,\n30 :    10.4882678542,\n31 :    8.19641542163}\n</code></pre>     <pre><code>img = tf.reshape(x_train[75], (-1, 224, 224, 3))\np = model.predict(img)\np = np.argmax(p, axis=-1)\np.shape\n</code></pre>      <pre>\n<code>(1, 224, 224)</code>\n</pre>        <pre><code>from tensorflow.keras.models import load_model\n\nmodel = load_model('fc_densenet_103_camvid_100epochs.h5')\n</code></pre>"},{"location":"deep_learning/module6/Module6_2/#prediction_1","title":"Pr\u00e9diction","text":"<pre><code>num_obs=240\n\nimg = tf.reshape(x_train[num_obs], (-1, 224, 224, 3))\np = model.predict(img)\np = np.argmax(p, axis=-1)\np.shape\n\nfigsize(20, 5)\n\nsubplot(1, 3, 1)\nimshow(img[0, :, : ,:])\nxlabel('Input')\n\nsubplot(1, 3, 2)\nimshow(p[0, :, :])\nxlabel('Pr\u00e9diction')\n\nsubplot(1, 3, 3)\nimshow(y_train[num_obs][:, :])\nxlabel('V\u00e9rit\u00e9')\n\nm = tf.keras.metrics.MeanIoU(num_classes=32)\nm(y_train[num_obs][:, :], p[0, :, :]).numpy()\n</code></pre>      <pre>\n<code>0.7309558</code>\n</pre>"},{"location":"deep_learning/module6/Module6_2/#meaniou-via-custom-training-loop_1","title":"MeanIoU via custom training loop","text":"<pre><code>epochs = 150\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\nmetric_fn = tf.keras.metrics.MeanIoU(num_classes=32)\noptimizer = tf.keras.optimizers.RMSprop(lr=1e-3)\n\n@tf.function\ndef train_step(x, y):\n  with tf.GradientTape() as tape:\n    #pr\u00e9diction sur le minibatch\n    y_pred = model(x, training=True)\n    #calcul de la fonction de perte moyenne sur le minibatch\n    loss_value = loss_fn(y, y_pred)\n\n  # calcul des gradients et retropropagation\n  grads = tape.gradient(loss_value, model.trainable_weights)\n  optimizer.apply_gradients(zip(grads, model.trainable_weights))\n\n  # mise \u00e0 jour des m\u00e9triques\n  mask_pred = tf.argmax(y_pred, axis=-1)\n  metric_fn.update_state(y, mask_pred)\n\n  return loss_value\n\n@tf.function\ndef test_step(x, y):\n  y_pred = model(x, training=False)\n  mask_pred = tf.argmax(y_pred, axis=-1)\n  metric_fn.update_state(y, mask_pred)\n\nfor epoch in range(epochs):\n  losses = 0 \n  step_counter = 0\n  print(f\"\\nD\u00e9but de l'\u00e9poque {epoch+1},\")\n  start_time = time.time()\n\n  # it\u00e9ration sur les minibatchs du dataset\n  for step, (x_batch_train, y_batch_train) in enumerate(ds):\n    loss_value = train_step(x_batch_train, y_batch_train)\n    losses += loss_value\n    step_counter += 1\n\n    # Log tous les 100 batches\n    if step % 100 == 0:\n      print(f\"Loss sur le batch \u00e0 l'\u00e9tape {step} : {float(loss_value):.4f}\")\n\n  # Affichage des m\u00e9triques \u00e0 la fin de l'\u00e9poque\n  metric = metric_fn.result()\n  print(f\"Loss pour l'\u00e9poque : {losses/step_counter:.4f}\")\n  print(f\"M\u00e9trique pour l'\u00e9poque : {float(metric):.4f} \\n\")\n\n  # Reset de la m\u00e9trique \u00e0 la fin de chaque \u00e9poque\n  metric_fn.reset_states()\n\n  # validation loop \u00e0 la fin de chaque \u00e9poque\n  for x_batch_val, y_batch_val in val_dataset:\n    test_step(x_batch_val, y_batch_val)\n\n  val_metric = metric_fn.result()\n  metric_fn.reset_states()\n  print(\"Validation acc: %.4f\" % (float(val_metric),))\n  print(\"Time taken: %.2fs\" % (time.time() - start_time))\n</code></pre>"},{"location":"deep_learning/module6/Module6_2/#fc-densenet56","title":"FC-Densenet56","text":"<ul> <li>56 layers (FC-DenseNet56) with \\(4\\) layers par dense blocks and a growth rate of \\(12\\).</li> </ul>      <pre><code>#fc-densenet 56\nfilters = 12\nnb_blocks = 5\nnb_layers = [4, 4, 4, 4, 4 ,4, 4, 4, 4, 4, 4]\nimg_shape = (224,224,3)\n\nmodel2 = FCDensenet(filters, nb_layers, img_shape, num_classes=32)\nmodel2.summary()\n</code></pre>      <pre>\n<code>Model: \"model_3\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_4 (InputLayer)            [(None, 224, 224, 3) 0                                            \n__________________________________________________________________________________________________\nconv2d_200 (Conv2D)             (None, 224, 224, 48) 1344        input_4[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_194 (BatchN (None, 224, 224, 48) 192         conv2d_200[0][0]                 \n__________________________________________________________________________________________________\nre_lu_194 (ReLU)                (None, 224, 224, 48) 0           batch_normalization_194[0][0]    \n__________________________________________________________________________________________________\nconv2d_201 (Conv2D)             (None, 224, 224, 12) 5196        re_lu_194[0][0]                  \n__________________________________________________________________________________________________\ndropout_194 (Dropout)           (None, 224, 224, 12) 0           conv2d_201[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_220 (Concatenate)   (None, 224, 224, 60) 0           conv2d_200[0][0]                 \n                                                                 dropout_194[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_195 (BatchN (None, 224, 224, 60) 240         concatenate_220[0][0]            \n__________________________________________________________________________________________________\nre_lu_195 (ReLU)                (None, 224, 224, 60) 0           batch_normalization_195[0][0]    \n__________________________________________________________________________________________________\nconv2d_202 (Conv2D)             (None, 224, 224, 12) 6492        re_lu_195[0][0]                  \n__________________________________________________________________________________________________\ndropout_195 (Dropout)           (None, 224, 224, 12) 0           conv2d_202[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_221 (Concatenate)   (None, 224, 224, 72) 0           concatenate_220[0][0]            \n                                                                 dropout_195[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_196 (BatchN (None, 224, 224, 72) 288         concatenate_221[0][0]            \n__________________________________________________________________________________________________\nre_lu_196 (ReLU)                (None, 224, 224, 72) 0           batch_normalization_196[0][0]    \n__________________________________________________________________________________________________\nconv2d_203 (Conv2D)             (None, 224, 224, 12) 7788        re_lu_196[0][0]                  \n__________________________________________________________________________________________________\ndropout_196 (Dropout)           (None, 224, 224, 12) 0           conv2d_203[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_222 (Concatenate)   (None, 224, 224, 84) 0           concatenate_221[0][0]            \n                                                                 dropout_196[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_197 (BatchN (None, 224, 224, 84) 336         concatenate_222[0][0]            \n__________________________________________________________________________________________________\nre_lu_197 (ReLU)                (None, 224, 224, 84) 0           batch_normalization_197[0][0]    \n__________________________________________________________________________________________________\nconv2d_204 (Conv2D)             (None, 224, 224, 12) 9084        re_lu_197[0][0]                  \n__________________________________________________________________________________________________\ndropout_197 (Dropout)           (None, 224, 224, 12) 0           conv2d_204[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_223 (Concatenate)   (None, 224, 224, 48) 0           dropout_197[0][0]                \n                                                                 dropout_196[0][0]                \n                                                                 dropout_195[0][0]                \n                                                                 dropout_194[0][0]                \n__________________________________________________________________________________________________\nconcatenate_224 (Concatenate)   (None, 224, 224, 96) 0           concatenate_223[0][0]            \n                                                                 conv2d_200[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_198 (BatchN (None, 224, 224, 96) 384         concatenate_224[0][0]            \n__________________________________________________________________________________________________\nre_lu_198 (ReLU)                (None, 224, 224, 96) 0           batch_normalization_198[0][0]    \n__________________________________________________________________________________________________\nconv2d_205 (Conv2D)             (None, 224, 224, 96) 9312        re_lu_198[0][0]                  \n__________________________________________________________________________________________________\ndropout_198 (Dropout)           (None, 224, 224, 96) 0           conv2d_205[0][0]                 \n__________________________________________________________________________________________________\nmax_pooling2d_15 (MaxPooling2D) (None, 112, 112, 96) 0           dropout_198[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_199 (BatchN (None, 112, 112, 96) 384         max_pooling2d_15[0][0]           \n__________________________________________________________________________________________________\nre_lu_199 (ReLU)                (None, 112, 112, 96) 0           batch_normalization_199[0][0]    \n__________________________________________________________________________________________________\nconv2d_206 (Conv2D)             (None, 112, 112, 12) 10380       re_lu_199[0][0]                  \n__________________________________________________________________________________________________\ndropout_199 (Dropout)           (None, 112, 112, 12) 0           conv2d_206[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_225 (Concatenate)   (None, 112, 112, 108 0           max_pooling2d_15[0][0]           \n                                                                 dropout_199[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_200 (BatchN (None, 112, 112, 108 432         concatenate_225[0][0]            \n__________________________________________________________________________________________________\nre_lu_200 (ReLU)                (None, 112, 112, 108 0           batch_normalization_200[0][0]    \n__________________________________________________________________________________________________\nconv2d_207 (Conv2D)             (None, 112, 112, 12) 11676       re_lu_200[0][0]                  \n__________________________________________________________________________________________________\ndropout_200 (Dropout)           (None, 112, 112, 12) 0           conv2d_207[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_226 (Concatenate)   (None, 112, 112, 120 0           concatenate_225[0][0]            \n                                                                 dropout_200[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_201 (BatchN (None, 112, 112, 120 480         concatenate_226[0][0]            \n__________________________________________________________________________________________________\nre_lu_201 (ReLU)                (None, 112, 112, 120 0           batch_normalization_201[0][0]    \n__________________________________________________________________________________________________\nconv2d_208 (Conv2D)             (None, 112, 112, 12) 12972       re_lu_201[0][0]                  \n__________________________________________________________________________________________________\ndropout_201 (Dropout)           (None, 112, 112, 12) 0           conv2d_208[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_227 (Concatenate)   (None, 112, 112, 132 0           concatenate_226[0][0]            \n                                                                 dropout_201[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_202 (BatchN (None, 112, 112, 132 528         concatenate_227[0][0]            \n__________________________________________________________________________________________________\nre_lu_202 (ReLU)                (None, 112, 112, 132 0           batch_normalization_202[0][0]    \n__________________________________________________________________________________________________\nconv2d_209 (Conv2D)             (None, 112, 112, 12) 14268       re_lu_202[0][0]                  \n__________________________________________________________________________________________________\ndropout_202 (Dropout)           (None, 112, 112, 12) 0           conv2d_209[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_228 (Concatenate)   (None, 112, 112, 48) 0           dropout_202[0][0]                \n                                                                 dropout_201[0][0]                \n                                                                 dropout_200[0][0]                \n                                                                 dropout_199[0][0]                \n__________________________________________________________________________________________________\nconcatenate_229 (Concatenate)   (None, 112, 112, 144 0           concatenate_228[0][0]            \n                                                                 max_pooling2d_15[0][0]           \n__________________________________________________________________________________________________\nbatch_normalization_203 (BatchN (None, 112, 112, 144 576         concatenate_229[0][0]            \n__________________________________________________________________________________________________\nre_lu_203 (ReLU)                (None, 112, 112, 144 0           batch_normalization_203[0][0]    \n__________________________________________________________________________________________________\nconv2d_210 (Conv2D)             (None, 112, 112, 144 20880       re_lu_203[0][0]                  \n__________________________________________________________________________________________________\ndropout_203 (Dropout)           (None, 112, 112, 144 0           conv2d_210[0][0]                 \n__________________________________________________________________________________________________\nmax_pooling2d_16 (MaxPooling2D) (None, 56, 56, 144)  0           dropout_203[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_204 (BatchN (None, 56, 56, 144)  576         max_pooling2d_16[0][0]           \n__________________________________________________________________________________________________\nre_lu_204 (ReLU)                (None, 56, 56, 144)  0           batch_normalization_204[0][0]    \n__________________________________________________________________________________________________\nconv2d_211 (Conv2D)             (None, 56, 56, 12)   15564       re_lu_204[0][0]                  \n__________________________________________________________________________________________________\ndropout_204 (Dropout)           (None, 56, 56, 12)   0           conv2d_211[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_230 (Concatenate)   (None, 56, 56, 156)  0           max_pooling2d_16[0][0]           \n                                                                 dropout_204[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_205 (BatchN (None, 56, 56, 156)  624         concatenate_230[0][0]            \n__________________________________________________________________________________________________\nre_lu_205 (ReLU)                (None, 56, 56, 156)  0           batch_normalization_205[0][0]    \n__________________________________________________________________________________________________\nconv2d_212 (Conv2D)             (None, 56, 56, 12)   16860       re_lu_205[0][0]                  \n__________________________________________________________________________________________________\ndropout_205 (Dropout)           (None, 56, 56, 12)   0           conv2d_212[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_231 (Concatenate)   (None, 56, 56, 168)  0           concatenate_230[0][0]            \n                                                                 dropout_205[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_206 (BatchN (None, 56, 56, 168)  672         concatenate_231[0][0]            \n__________________________________________________________________________________________________\nre_lu_206 (ReLU)                (None, 56, 56, 168)  0           batch_normalization_206[0][0]    \n__________________________________________________________________________________________________\nconv2d_213 (Conv2D)             (None, 56, 56, 12)   18156       re_lu_206[0][0]                  \n__________________________________________________________________________________________________\ndropout_206 (Dropout)           (None, 56, 56, 12)   0           conv2d_213[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_232 (Concatenate)   (None, 56, 56, 180)  0           concatenate_231[0][0]            \n                                                                 dropout_206[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_207 (BatchN (None, 56, 56, 180)  720         concatenate_232[0][0]            \n__________________________________________________________________________________________________\nre_lu_207 (ReLU)                (None, 56, 56, 180)  0           batch_normalization_207[0][0]    \n__________________________________________________________________________________________________\nconv2d_214 (Conv2D)             (None, 56, 56, 12)   19452       re_lu_207[0][0]                  \n__________________________________________________________________________________________________\ndropout_207 (Dropout)           (None, 56, 56, 12)   0           conv2d_214[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_233 (Concatenate)   (None, 56, 56, 48)   0           dropout_207[0][0]                \n                                                                 dropout_206[0][0]                \n                                                                 dropout_205[0][0]                \n                                                                 dropout_204[0][0]                \n__________________________________________________________________________________________________\nconcatenate_234 (Concatenate)   (None, 56, 56, 192)  0           concatenate_233[0][0]            \n                                                                 max_pooling2d_16[0][0]           \n__________________________________________________________________________________________________\nbatch_normalization_208 (BatchN (None, 56, 56, 192)  768         concatenate_234[0][0]            \n__________________________________________________________________________________________________\nre_lu_208 (ReLU)                (None, 56, 56, 192)  0           batch_normalization_208[0][0]    \n__________________________________________________________________________________________________\nconv2d_215 (Conv2D)             (None, 56, 56, 192)  37056       re_lu_208[0][0]                  \n__________________________________________________________________________________________________\ndropout_208 (Dropout)           (None, 56, 56, 192)  0           conv2d_215[0][0]                 \n__________________________________________________________________________________________________\nmax_pooling2d_17 (MaxPooling2D) (None, 28, 28, 192)  0           dropout_208[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_209 (BatchN (None, 28, 28, 192)  768         max_pooling2d_17[0][0]           \n__________________________________________________________________________________________________\nre_lu_209 (ReLU)                (None, 28, 28, 192)  0           batch_normalization_209[0][0]    \n__________________________________________________________________________________________________\nconv2d_216 (Conv2D)             (None, 28, 28, 12)   20748       re_lu_209[0][0]                  \n__________________________________________________________________________________________________\ndropout_209 (Dropout)           (None, 28, 28, 12)   0           conv2d_216[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_235 (Concatenate)   (None, 28, 28, 204)  0           max_pooling2d_17[0][0]           \n                                                                 dropout_209[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_210 (BatchN (None, 28, 28, 204)  816         concatenate_235[0][0]            \n__________________________________________________________________________________________________\nre_lu_210 (ReLU)                (None, 28, 28, 204)  0           batch_normalization_210[0][0]    \n__________________________________________________________________________________________________\nconv2d_217 (Conv2D)             (None, 28, 28, 12)   22044       re_lu_210[0][0]                  \n__________________________________________________________________________________________________\ndropout_210 (Dropout)           (None, 28, 28, 12)   0           conv2d_217[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_236 (Concatenate)   (None, 28, 28, 216)  0           concatenate_235[0][0]            \n                                                                 dropout_210[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_211 (BatchN (None, 28, 28, 216)  864         concatenate_236[0][0]            \n__________________________________________________________________________________________________\nre_lu_211 (ReLU)                (None, 28, 28, 216)  0           batch_normalization_211[0][0]    \n__________________________________________________________________________________________________\nconv2d_218 (Conv2D)             (None, 28, 28, 12)   23340       re_lu_211[0][0]                  \n__________________________________________________________________________________________________\ndropout_211 (Dropout)           (None, 28, 28, 12)   0           conv2d_218[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_237 (Concatenate)   (None, 28, 28, 228)  0           concatenate_236[0][0]            \n                                                                 dropout_211[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_212 (BatchN (None, 28, 28, 228)  912         concatenate_237[0][0]            \n__________________________________________________________________________________________________\nre_lu_212 (ReLU)                (None, 28, 28, 228)  0           batch_normalization_212[0][0]    \n__________________________________________________________________________________________________\nconv2d_219 (Conv2D)             (None, 28, 28, 12)   24636       re_lu_212[0][0]                  \n__________________________________________________________________________________________________\ndropout_212 (Dropout)           (None, 28, 28, 12)   0           conv2d_219[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_238 (Concatenate)   (None, 28, 28, 48)   0           dropout_212[0][0]                \n                                                                 dropout_211[0][0]                \n                                                                 dropout_210[0][0]                \n                                                                 dropout_209[0][0]                \n__________________________________________________________________________________________________\nconcatenate_239 (Concatenate)   (None, 28, 28, 240)  0           concatenate_238[0][0]            \n                                                                 max_pooling2d_17[0][0]           \n__________________________________________________________________________________________________\nbatch_normalization_213 (BatchN (None, 28, 28, 240)  960         concatenate_239[0][0]            \n__________________________________________________________________________________________________\nre_lu_213 (ReLU)                (None, 28, 28, 240)  0           batch_normalization_213[0][0]    \n__________________________________________________________________________________________________\nconv2d_220 (Conv2D)             (None, 28, 28, 240)  57840       re_lu_213[0][0]                  \n__________________________________________________________________________________________________\ndropout_213 (Dropout)           (None, 28, 28, 240)  0           conv2d_220[0][0]                 \n__________________________________________________________________________________________________\nmax_pooling2d_18 (MaxPooling2D) (None, 14, 14, 240)  0           dropout_213[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_214 (BatchN (None, 14, 14, 240)  960         max_pooling2d_18[0][0]           \n__________________________________________________________________________________________________\nre_lu_214 (ReLU)                (None, 14, 14, 240)  0           batch_normalization_214[0][0]    \n__________________________________________________________________________________________________\nconv2d_221 (Conv2D)             (None, 14, 14, 12)   25932       re_lu_214[0][0]                  \n__________________________________________________________________________________________________\ndropout_214 (Dropout)           (None, 14, 14, 12)   0           conv2d_221[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_240 (Concatenate)   (None, 14, 14, 252)  0           max_pooling2d_18[0][0]           \n                                                                 dropout_214[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_215 (BatchN (None, 14, 14, 252)  1008        concatenate_240[0][0]            \n__________________________________________________________________________________________________\nre_lu_215 (ReLU)                (None, 14, 14, 252)  0           batch_normalization_215[0][0]    \n__________________________________________________________________________________________________\nconv2d_222 (Conv2D)             (None, 14, 14, 12)   27228       re_lu_215[0][0]                  \n__________________________________________________________________________________________________\ndropout_215 (Dropout)           (None, 14, 14, 12)   0           conv2d_222[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_241 (Concatenate)   (None, 14, 14, 264)  0           concatenate_240[0][0]            \n                                                                 dropout_215[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_216 (BatchN (None, 14, 14, 264)  1056        concatenate_241[0][0]            \n__________________________________________________________________________________________________\nre_lu_216 (ReLU)                (None, 14, 14, 264)  0           batch_normalization_216[0][0]    \n__________________________________________________________________________________________________\nconv2d_223 (Conv2D)             (None, 14, 14, 12)   28524       re_lu_216[0][0]                  \n__________________________________________________________________________________________________\ndropout_216 (Dropout)           (None, 14, 14, 12)   0           conv2d_223[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_242 (Concatenate)   (None, 14, 14, 276)  0           concatenate_241[0][0]            \n                                                                 dropout_216[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_217 (BatchN (None, 14, 14, 276)  1104        concatenate_242[0][0]            \n__________________________________________________________________________________________________\nre_lu_217 (ReLU)                (None, 14, 14, 276)  0           batch_normalization_217[0][0]    \n__________________________________________________________________________________________________\nconv2d_224 (Conv2D)             (None, 14, 14, 12)   29820       re_lu_217[0][0]                  \n__________________________________________________________________________________________________\ndropout_217 (Dropout)           (None, 14, 14, 12)   0           conv2d_224[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_243 (Concatenate)   (None, 14, 14, 48)   0           dropout_217[0][0]                \n                                                                 dropout_216[0][0]                \n                                                                 dropout_215[0][0]                \n                                                                 dropout_214[0][0]                \n__________________________________________________________________________________________________\nconcatenate_244 (Concatenate)   (None, 14, 14, 288)  0           concatenate_243[0][0]            \n                                                                 max_pooling2d_18[0][0]           \n__________________________________________________________________________________________________\nbatch_normalization_218 (BatchN (None, 14, 14, 288)  1152        concatenate_244[0][0]            \n__________________________________________________________________________________________________\nre_lu_218 (ReLU)                (None, 14, 14, 288)  0           batch_normalization_218[0][0]    \n__________________________________________________________________________________________________\nconv2d_225 (Conv2D)             (None, 14, 14, 288)  83232       re_lu_218[0][0]                  \n__________________________________________________________________________________________________\ndropout_218 (Dropout)           (None, 14, 14, 288)  0           conv2d_225[0][0]                 \n__________________________________________________________________________________________________\nmax_pooling2d_19 (MaxPooling2D) (None, 7, 7, 288)    0           dropout_218[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_219 (BatchN (None, 7, 7, 288)    1152        max_pooling2d_19[0][0]           \n__________________________________________________________________________________________________\nre_lu_219 (ReLU)                (None, 7, 7, 288)    0           batch_normalization_219[0][0]    \n__________________________________________________________________________________________________\nconv2d_226 (Conv2D)             (None, 7, 7, 12)     31116       re_lu_219[0][0]                  \n__________________________________________________________________________________________________\ndropout_219 (Dropout)           (None, 7, 7, 12)     0           conv2d_226[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_245 (Concatenate)   (None, 7, 7, 300)    0           max_pooling2d_19[0][0]           \n                                                                 dropout_219[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_220 (BatchN (None, 7, 7, 300)    1200        concatenate_245[0][0]            \n__________________________________________________________________________________________________\nre_lu_220 (ReLU)                (None, 7, 7, 300)    0           batch_normalization_220[0][0]    \n__________________________________________________________________________________________________\nconv2d_227 (Conv2D)             (None, 7, 7, 12)     32412       re_lu_220[0][0]                  \n__________________________________________________________________________________________________\ndropout_220 (Dropout)           (None, 7, 7, 12)     0           conv2d_227[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_246 (Concatenate)   (None, 7, 7, 312)    0           concatenate_245[0][0]            \n                                                                 dropout_220[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_221 (BatchN (None, 7, 7, 312)    1248        concatenate_246[0][0]            \n__________________________________________________________________________________________________\nre_lu_221 (ReLU)                (None, 7, 7, 312)    0           batch_normalization_221[0][0]    \n__________________________________________________________________________________________________\nconv2d_228 (Conv2D)             (None, 7, 7, 12)     33708       re_lu_221[0][0]                  \n__________________________________________________________________________________________________\ndropout_221 (Dropout)           (None, 7, 7, 12)     0           conv2d_228[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_247 (Concatenate)   (None, 7, 7, 324)    0           concatenate_246[0][0]            \n                                                                 dropout_221[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_222 (BatchN (None, 7, 7, 324)    1296        concatenate_247[0][0]            \n__________________________________________________________________________________________________\nre_lu_222 (ReLU)                (None, 7, 7, 324)    0           batch_normalization_222[0][0]    \n__________________________________________________________________________________________________\nconv2d_229 (Conv2D)             (None, 7, 7, 12)     35004       re_lu_222[0][0]                  \n__________________________________________________________________________________________________\ndropout_222 (Dropout)           (None, 7, 7, 12)     0           conv2d_229[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_248 (Concatenate)   (None, 7, 7, 48)     0           dropout_222[0][0]                \n                                                                 dropout_221[0][0]                \n                                                                 dropout_220[0][0]                \n                                                                 dropout_219[0][0]                \n__________________________________________________________________________________________________\nconv2d_transpose_15 (Conv2DTran (None, 14, 14, 48)   20784       concatenate_248[0][0]            \n__________________________________________________________________________________________________\nconcatenate_249 (Concatenate)   (None, 14, 14, 336)  0           conv2d_transpose_15[0][0]        \n                                                                 concatenate_244[0][0]            \n__________________________________________________________________________________________________\nbatch_normalization_223 (BatchN (None, 14, 14, 336)  1344        concatenate_249[0][0]            \n__________________________________________________________________________________________________\nre_lu_223 (ReLU)                (None, 14, 14, 336)  0           batch_normalization_223[0][0]    \n__________________________________________________________________________________________________\nconv2d_230 (Conv2D)             (None, 14, 14, 12)   36300       re_lu_223[0][0]                  \n__________________________________________________________________________________________________\ndropout_223 (Dropout)           (None, 14, 14, 12)   0           conv2d_230[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_250 (Concatenate)   (None, 14, 14, 348)  0           concatenate_249[0][0]            \n                                                                 dropout_223[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_224 (BatchN (None, 14, 14, 348)  1392        concatenate_250[0][0]            \n__________________________________________________________________________________________________\nre_lu_224 (ReLU)                (None, 14, 14, 348)  0           batch_normalization_224[0][0]    \n__________________________________________________________________________________________________\nconv2d_231 (Conv2D)             (None, 14, 14, 12)   37596       re_lu_224[0][0]                  \n__________________________________________________________________________________________________\ndropout_224 (Dropout)           (None, 14, 14, 12)   0           conv2d_231[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_251 (Concatenate)   (None, 14, 14, 360)  0           concatenate_250[0][0]            \n                                                                 dropout_224[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_225 (BatchN (None, 14, 14, 360)  1440        concatenate_251[0][0]            \n__________________________________________________________________________________________________\nre_lu_225 (ReLU)                (None, 14, 14, 360)  0           batch_normalization_225[0][0]    \n__________________________________________________________________________________________________\nconv2d_232 (Conv2D)             (None, 14, 14, 12)   38892       re_lu_225[0][0]                  \n__________________________________________________________________________________________________\ndropout_225 (Dropout)           (None, 14, 14, 12)   0           conv2d_232[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_252 (Concatenate)   (None, 14, 14, 372)  0           concatenate_251[0][0]            \n                                                                 dropout_225[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_226 (BatchN (None, 14, 14, 372)  1488        concatenate_252[0][0]            \n__________________________________________________________________________________________________\nre_lu_226 (ReLU)                (None, 14, 14, 372)  0           batch_normalization_226[0][0]    \n__________________________________________________________________________________________________\nconv2d_233 (Conv2D)             (None, 14, 14, 12)   40188       re_lu_226[0][0]                  \n__________________________________________________________________________________________________\ndropout_226 (Dropout)           (None, 14, 14, 12)   0           conv2d_233[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_253 (Concatenate)   (None, 14, 14, 48)   0           dropout_226[0][0]                \n                                                                 dropout_225[0][0]                \n                                                                 dropout_224[0][0]                \n                                                                 dropout_223[0][0]                \n__________________________________________________________________________________________________\nconcatenate_254 (Concatenate)   (None, 14, 14, 384)  0           concatenate_253[0][0]            \n                                                                 concatenate_249[0][0]            \n__________________________________________________________________________________________________\nconv2d_transpose_16 (Conv2DTran (None, 28, 28, 48)   165936      concatenate_254[0][0]            \n__________________________________________________________________________________________________\nconcatenate_255 (Concatenate)   (None, 28, 28, 288)  0           conv2d_transpose_16[0][0]        \n                                                                 concatenate_239[0][0]            \n__________________________________________________________________________________________________\nbatch_normalization_227 (BatchN (None, 28, 28, 288)  1152        concatenate_255[0][0]            \n__________________________________________________________________________________________________\nre_lu_227 (ReLU)                (None, 28, 28, 288)  0           batch_normalization_227[0][0]    \n__________________________________________________________________________________________________\nconv2d_234 (Conv2D)             (None, 28, 28, 12)   31116       re_lu_227[0][0]                  \n__________________________________________________________________________________________________\ndropout_227 (Dropout)           (None, 28, 28, 12)   0           conv2d_234[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_256 (Concatenate)   (None, 28, 28, 300)  0           concatenate_255[0][0]            \n                                                                 dropout_227[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_228 (BatchN (None, 28, 28, 300)  1200        concatenate_256[0][0]            \n__________________________________________________________________________________________________\nre_lu_228 (ReLU)                (None, 28, 28, 300)  0           batch_normalization_228[0][0]    \n__________________________________________________________________________________________________\nconv2d_235 (Conv2D)             (None, 28, 28, 12)   32412       re_lu_228[0][0]                  \n__________________________________________________________________________________________________\ndropout_228 (Dropout)           (None, 28, 28, 12)   0           conv2d_235[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_257 (Concatenate)   (None, 28, 28, 312)  0           concatenate_256[0][0]            \n                                                                 dropout_228[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_229 (BatchN (None, 28, 28, 312)  1248        concatenate_257[0][0]            \n__________________________________________________________________________________________________\nre_lu_229 (ReLU)                (None, 28, 28, 312)  0           batch_normalization_229[0][0]    \n__________________________________________________________________________________________________\nconv2d_236 (Conv2D)             (None, 28, 28, 12)   33708       re_lu_229[0][0]                  \n__________________________________________________________________________________________________\ndropout_229 (Dropout)           (None, 28, 28, 12)   0           conv2d_236[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_258 (Concatenate)   (None, 28, 28, 324)  0           concatenate_257[0][0]            \n                                                                 dropout_229[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_230 (BatchN (None, 28, 28, 324)  1296        concatenate_258[0][0]            \n__________________________________________________________________________________________________\nre_lu_230 (ReLU)                (None, 28, 28, 324)  0           batch_normalization_230[0][0]    \n__________________________________________________________________________________________________\nconv2d_237 (Conv2D)             (None, 28, 28, 12)   35004       re_lu_230[0][0]                  \n__________________________________________________________________________________________________\ndropout_230 (Dropout)           (None, 28, 28, 12)   0           conv2d_237[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_259 (Concatenate)   (None, 28, 28, 48)   0           dropout_230[0][0]                \n                                                                 dropout_229[0][0]                \n                                                                 dropout_228[0][0]                \n                                                                 dropout_227[0][0]                \n__________________________________________________________________________________________________\nconcatenate_260 (Concatenate)   (None, 28, 28, 336)  0           concatenate_259[0][0]            \n                                                                 concatenate_255[0][0]            \n__________________________________________________________________________________________________\nconv2d_transpose_17 (Conv2DTran (None, 56, 56, 48)   145200      concatenate_260[0][0]            \n__________________________________________________________________________________________________\nconcatenate_261 (Concatenate)   (None, 56, 56, 240)  0           conv2d_transpose_17[0][0]        \n                                                                 concatenate_234[0][0]            \n__________________________________________________________________________________________________\nbatch_normalization_231 (BatchN (None, 56, 56, 240)  960         concatenate_261[0][0]            \n__________________________________________________________________________________________________\nre_lu_231 (ReLU)                (None, 56, 56, 240)  0           batch_normalization_231[0][0]    \n__________________________________________________________________________________________________\nconv2d_238 (Conv2D)             (None, 56, 56, 12)   25932       re_lu_231[0][0]                  \n__________________________________________________________________________________________________\ndropout_231 (Dropout)           (None, 56, 56, 12)   0           conv2d_238[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_262 (Concatenate)   (None, 56, 56, 252)  0           concatenate_261[0][0]            \n                                                                 dropout_231[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_232 (BatchN (None, 56, 56, 252)  1008        concatenate_262[0][0]            \n__________________________________________________________________________________________________\nre_lu_232 (ReLU)                (None, 56, 56, 252)  0           batch_normalization_232[0][0]    \n__________________________________________________________________________________________________\nconv2d_239 (Conv2D)             (None, 56, 56, 12)   27228       re_lu_232[0][0]                  \n__________________________________________________________________________________________________\ndropout_232 (Dropout)           (None, 56, 56, 12)   0           conv2d_239[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_263 (Concatenate)   (None, 56, 56, 264)  0           concatenate_262[0][0]            \n                                                                 dropout_232[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_233 (BatchN (None, 56, 56, 264)  1056        concatenate_263[0][0]            \n__________________________________________________________________________________________________\nre_lu_233 (ReLU)                (None, 56, 56, 264)  0           batch_normalization_233[0][0]    \n__________________________________________________________________________________________________\nconv2d_240 (Conv2D)             (None, 56, 56, 12)   28524       re_lu_233[0][0]                  \n__________________________________________________________________________________________________\ndropout_233 (Dropout)           (None, 56, 56, 12)   0           conv2d_240[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_264 (Concatenate)   (None, 56, 56, 276)  0           concatenate_263[0][0]            \n                                                                 dropout_233[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_234 (BatchN (None, 56, 56, 276)  1104        concatenate_264[0][0]            \n__________________________________________________________________________________________________\nre_lu_234 (ReLU)                (None, 56, 56, 276)  0           batch_normalization_234[0][0]    \n__________________________________________________________________________________________________\nconv2d_241 (Conv2D)             (None, 56, 56, 12)   29820       re_lu_234[0][0]                  \n__________________________________________________________________________________________________\ndropout_234 (Dropout)           (None, 56, 56, 12)   0           conv2d_241[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_265 (Concatenate)   (None, 56, 56, 48)   0           dropout_234[0][0]                \n                                                                 dropout_233[0][0]                \n                                                                 dropout_232[0][0]                \n                                                                 dropout_231[0][0]                \n__________________________________________________________________________________________________\nconcatenate_266 (Concatenate)   (None, 56, 56, 288)  0           concatenate_265[0][0]            \n                                                                 concatenate_261[0][0]            \n__________________________________________________________________________________________________\nconv2d_transpose_18 (Conv2DTran (None, 112, 112, 48) 124464      concatenate_266[0][0]            \n__________________________________________________________________________________________________\nconcatenate_267 (Concatenate)   (None, 112, 112, 192 0           conv2d_transpose_18[0][0]        \n                                                                 concatenate_229[0][0]            \n__________________________________________________________________________________________________\nbatch_normalization_235 (BatchN (None, 112, 112, 192 768         concatenate_267[0][0]            \n__________________________________________________________________________________________________\nre_lu_235 (ReLU)                (None, 112, 112, 192 0           batch_normalization_235[0][0]    \n__________________________________________________________________________________________________\nconv2d_242 (Conv2D)             (None, 112, 112, 12) 20748       re_lu_235[0][0]                  \n__________________________________________________________________________________________________\ndropout_235 (Dropout)           (None, 112, 112, 12) 0           conv2d_242[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_268 (Concatenate)   (None, 112, 112, 204 0           concatenate_267[0][0]            \n                                                                 dropout_235[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_236 (BatchN (None, 112, 112, 204 816         concatenate_268[0][0]            \n__________________________________________________________________________________________________\nre_lu_236 (ReLU)                (None, 112, 112, 204 0           batch_normalization_236[0][0]    \n__________________________________________________________________________________________________\nconv2d_243 (Conv2D)             (None, 112, 112, 12) 22044       re_lu_236[0][0]                  \n__________________________________________________________________________________________________\ndropout_236 (Dropout)           (None, 112, 112, 12) 0           conv2d_243[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_269 (Concatenate)   (None, 112, 112, 216 0           concatenate_268[0][0]            \n                                                                 dropout_236[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_237 (BatchN (None, 112, 112, 216 864         concatenate_269[0][0]            \n__________________________________________________________________________________________________\nre_lu_237 (ReLU)                (None, 112, 112, 216 0           batch_normalization_237[0][0]    \n__________________________________________________________________________________________________\nconv2d_244 (Conv2D)             (None, 112, 112, 12) 23340       re_lu_237[0][0]                  \n__________________________________________________________________________________________________\ndropout_237 (Dropout)           (None, 112, 112, 12) 0           conv2d_244[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_270 (Concatenate)   (None, 112, 112, 228 0           concatenate_269[0][0]            \n                                                                 dropout_237[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_238 (BatchN (None, 112, 112, 228 912         concatenate_270[0][0]            \n__________________________________________________________________________________________________\nre_lu_238 (ReLU)                (None, 112, 112, 228 0           batch_normalization_238[0][0]    \n__________________________________________________________________________________________________\nconv2d_245 (Conv2D)             (None, 112, 112, 12) 24636       re_lu_238[0][0]                  \n__________________________________________________________________________________________________\ndropout_238 (Dropout)           (None, 112, 112, 12) 0           conv2d_245[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_271 (Concatenate)   (None, 112, 112, 48) 0           dropout_238[0][0]                \n                                                                 dropout_237[0][0]                \n                                                                 dropout_236[0][0]                \n                                                                 dropout_235[0][0]                \n__________________________________________________________________________________________________\nconcatenate_272 (Concatenate)   (None, 112, 112, 240 0           concatenate_271[0][0]            \n                                                                 concatenate_267[0][0]            \n__________________________________________________________________________________________________\nconv2d_transpose_19 (Conv2DTran (None, 224, 224, 48) 103728      concatenate_272[0][0]            \n__________________________________________________________________________________________________\nconcatenate_273 (Concatenate)   (None, 224, 224, 144 0           conv2d_transpose_19[0][0]        \n                                                                 concatenate_224[0][0]            \n__________________________________________________________________________________________________\nbatch_normalization_239 (BatchN (None, 224, 224, 144 576         concatenate_273[0][0]            \n__________________________________________________________________________________________________\nre_lu_239 (ReLU)                (None, 224, 224, 144 0           batch_normalization_239[0][0]    \n__________________________________________________________________________________________________\nconv2d_246 (Conv2D)             (None, 224, 224, 12) 15564       re_lu_239[0][0]                  \n__________________________________________________________________________________________________\ndropout_239 (Dropout)           (None, 224, 224, 12) 0           conv2d_246[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_274 (Concatenate)   (None, 224, 224, 156 0           concatenate_273[0][0]            \n                                                                 dropout_239[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_240 (BatchN (None, 224, 224, 156 624         concatenate_274[0][0]            \n__________________________________________________________________________________________________\nre_lu_240 (ReLU)                (None, 224, 224, 156 0           batch_normalization_240[0][0]    \n__________________________________________________________________________________________________\nconv2d_247 (Conv2D)             (None, 224, 224, 12) 16860       re_lu_240[0][0]                  \n__________________________________________________________________________________________________\ndropout_240 (Dropout)           (None, 224, 224, 12) 0           conv2d_247[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_275 (Concatenate)   (None, 224, 224, 168 0           concatenate_274[0][0]            \n                                                                 dropout_240[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_241 (BatchN (None, 224, 224, 168 672         concatenate_275[0][0]            \n__________________________________________________________________________________________________\nre_lu_241 (ReLU)                (None, 224, 224, 168 0           batch_normalization_241[0][0]    \n__________________________________________________________________________________________________\nconv2d_248 (Conv2D)             (None, 224, 224, 12) 18156       re_lu_241[0][0]                  \n__________________________________________________________________________________________________\ndropout_241 (Dropout)           (None, 224, 224, 12) 0           conv2d_248[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_276 (Concatenate)   (None, 224, 224, 180 0           concatenate_275[0][0]            \n                                                                 dropout_241[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_242 (BatchN (None, 224, 224, 180 720         concatenate_276[0][0]            \n__________________________________________________________________________________________________\nre_lu_242 (ReLU)                (None, 224, 224, 180 0           batch_normalization_242[0][0]    \n__________________________________________________________________________________________________\nconv2d_249 (Conv2D)             (None, 224, 224, 12) 19452       re_lu_242[0][0]                  \n__________________________________________________________________________________________________\ndropout_242 (Dropout)           (None, 224, 224, 12) 0           conv2d_249[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_277 (Concatenate)   (None, 224, 224, 48) 0           dropout_242[0][0]                \n                                                                 dropout_241[0][0]                \n                                                                 dropout_240[0][0]                \n                                                                 dropout_239[0][0]                \n__________________________________________________________________________________________________\nconcatenate_278 (Concatenate)   (None, 224, 224, 192 0           concatenate_277[0][0]            \n                                                                 concatenate_273[0][0]            \n__________________________________________________________________________________________________\nconv2d_250 (Conv2D)             (None, 224, 224, 32) 6176        concatenate_278[0][0]            \n__________________________________________________________________________________________________\nactivation_3 (Activation)       (None, 224, 224, 32) 0           conv2d_250[0][0]                 \n==================================================================================================\nTotal params: 1,858,208\nTrainable params: 1,837,040\nNon-trainable params: 21,168\n__________________________________________________________________________________________________\n</code>\n</pre>        <pre><code>epochs = 150\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\nmetric_fn = tf.keras.metrics.MeanIoU(num_classes=32)\noptimizer = tf.keras.optimizers.RMSprop(lr=1e-3)\n\n@tf.function\ndef train_step(x, y):\n  with tf.GradientTape() as tape:\n    #pr\u00e9diction sur le minibatch\n    y_pred = model2(x_batch_train, training=True)\n    #calcul de la fonction de perte moyenne sur le minibatch\n    loss_value = loss_fn(y_batch_train, y_pred)\n\n  # calcul des gradients et retropropagation\n  grads = tape.gradient(loss_value, model2.trainable_weights)\n  optimizer.apply_gradients(zip(grads, model2.trainable_weights))\n\n  # mise \u00e0 jour des m\u00e9triques\n  mask_pred = tf.argmax(y_pred, axis=-1)\n  metric_fn.update_state(y_batch_train, mask_pred)\n\n  return loss_value\n\n\nfor epoch in range(epochs):\n  losses = 0 \n  step_counter = 0\n  print(f\"\\nD\u00e9but de l'\u00e9poque {epoch+1},\")\n  start_time = time.time()\n\n  # it\u00e9ration sur les minibatchs du dataset\n  for step, (x_batch_train, y_batch_train) in enumerate(ds):\n    loss_value = train_step(x_batch_train, y_batch_train)\n    losses += loss_value\n    step_counter += 1\n\n    # Log tous les 100 batches\n    if step % 100 == 0:\n      print(f\"Loss sur le batch \u00e0 l'\u00e9tape {step} : {float(loss_value):.4f}\")\n\n  # Affichage des m\u00e9triques \u00e0 la fin de l'\u00e9poque\n  metric = metric_fn.result()\n  print(f\"Loss pour l'\u00e9poque : {losses/step_counter:.4f}\")\n  print(f\"M\u00e9trique pour l'\u00e9poque : {float(metric):.4f} \\n\")\n\n  # Reset de la m\u00e9trique \u00e0 la fin de chaque \u00e9poque\n  metric_fn.reset_states()\n</code></pre>      <pre>\n<code>\nD\u00e9but de l'\u00e9poque 1,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 6.2236\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.1210\nLoss pour l'\u00e9poque : 0.5069 \n\nM\u00e9trique pour l'\u00e9poque : 0.5838 \n\n\nD\u00e9but de l'\u00e9poque 2,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 1.7736\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0305\nLoss pour l'\u00e9poque : 0.0815 \n\nM\u00e9trique pour l'\u00e9poque : 0.9157 \n\n\nD\u00e9but de l'\u00e9poque 3,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 1.1833\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0114\nLoss pour l'\u00e9poque : 0.0406 \n\nM\u00e9trique pour l'\u00e9poque : 0.9635 \n\n\nD\u00e9but de l'\u00e9poque 4,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.7455\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0044\nLoss pour l'\u00e9poque : 0.0229 \n\nM\u00e9trique pour l'\u00e9poque : 0.9824 \n\n\nD\u00e9but de l'\u00e9poque 5,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.6260\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0023\nLoss pour l'\u00e9poque : 0.0155 \n\nM\u00e9trique pour l'\u00e9poque : 0.9891 \n\n\nD\u00e9but de l'\u00e9poque 6,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.2041\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0014\nLoss pour l'\u00e9poque : 0.0118 \n\nM\u00e9trique pour l'\u00e9poque : 0.9902 \n\n\nD\u00e9but de l'\u00e9poque 7,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.1994\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0017\nLoss pour l'\u00e9poque : 0.0094 \n\nM\u00e9trique pour l'\u00e9poque : 0.9931 \n\n\nD\u00e9but de l'\u00e9poque 8,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0930\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0009\nLoss pour l'\u00e9poque : 0.0091 \n\nM\u00e9trique pour l'\u00e9poque : 0.9930 \n\n\nD\u00e9but de l'\u00e9poque 9,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.1203\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0011\nLoss pour l'\u00e9poque : 0.0061 \n\nM\u00e9trique pour l'\u00e9poque : 0.9958 \n\n\nD\u00e9but de l'\u00e9poque 10,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.1503\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0007\nLoss pour l'\u00e9poque : 0.0073 \n\nM\u00e9trique pour l'\u00e9poque : 0.9943 \n\n\nD\u00e9but de l'\u00e9poque 11,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.1016\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0009\nLoss pour l'\u00e9poque : 0.0060 \n\nM\u00e9trique pour l'\u00e9poque : 0.9956 \n\n\nD\u00e9but de l'\u00e9poque 12,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.1979\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0006\nLoss pour l'\u00e9poque : 0.0061 \n\nM\u00e9trique pour l'\u00e9poque : 0.9884 \n\n\nD\u00e9but de l'\u00e9poque 13,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0657\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0006\nLoss pour l'\u00e9poque : 0.0046 \n\nM\u00e9trique pour l'\u00e9poque : 0.9961 \n\n\nD\u00e9but de l'\u00e9poque 14,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0976\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0006\nLoss pour l'\u00e9poque : 0.0049 \n\nM\u00e9trique pour l'\u00e9poque : 0.9968 \n\n\nD\u00e9but de l'\u00e9poque 15,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.1594\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0005\nLoss pour l'\u00e9poque : 0.0048 \n\nM\u00e9trique pour l'\u00e9poque : 0.9955 \n\n\nD\u00e9but de l'\u00e9poque 16,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0422\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0009\nLoss pour l'\u00e9poque : 0.0034 \n\nM\u00e9trique pour l'\u00e9poque : 0.9972 \n\n\nD\u00e9but de l'\u00e9poque 17,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0528\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0006\nLoss pour l'\u00e9poque : 0.0036 \n\nM\u00e9trique pour l'\u00e9poque : 0.9956 \n\n\nD\u00e9but de l'\u00e9poque 18,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.1152\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0005\nLoss pour l'\u00e9poque : 0.0053 \n\nM\u00e9trique pour l'\u00e9poque : 0.9973 \n\n\nD\u00e9but de l'\u00e9poque 19,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.1081\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0004\nLoss pour l'\u00e9poque : 0.0041 \n\nM\u00e9trique pour l'\u00e9poque : 0.9979 \n\n\nD\u00e9but de l'\u00e9poque 20,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0213\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0057\nLoss pour l'\u00e9poque : 0.0022 \n\nM\u00e9trique pour l'\u00e9poque : 0.9985 \n\n\nD\u00e9but de l'\u00e9poque 21,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0283\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0006\nLoss pour l'\u00e9poque : 0.0031 \n\nM\u00e9trique pour l'\u00e9poque : 0.9975 \n\n\nD\u00e9but de l'\u00e9poque 22,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0597\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0004\nLoss pour l'\u00e9poque : 0.0031 \n\nM\u00e9trique pour l'\u00e9poque : 0.9978 \n\n\nD\u00e9but de l'\u00e9poque 23,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0511\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0005\nLoss pour l'\u00e9poque : 0.0028 \n\nM\u00e9trique pour l'\u00e9poque : 0.9986 \n\n\nD\u00e9but de l'\u00e9poque 24,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0251\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0032 \n\nM\u00e9trique pour l'\u00e9poque : 0.9975 \n\n\nD\u00e9but de l'\u00e9poque 25,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0162\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0020 \n\nM\u00e9trique pour l'\u00e9poque : 0.9990 \n\n\nD\u00e9but de l'\u00e9poque 26,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0684\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0026 \n\nM\u00e9trique pour l'\u00e9poque : 0.9982 \n\n\nD\u00e9but de l'\u00e9poque 27,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0100\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0138\nLoss pour l'\u00e9poque : 0.0019 \n\nM\u00e9trique pour l'\u00e9poque : 0.9987 \n\n\nD\u00e9but de l'\u00e9poque 28,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0796\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0007\nLoss pour l'\u00e9poque : 0.0024 \n\nM\u00e9trique pour l'\u00e9poque : 0.9961 \n\n\nD\u00e9but de l'\u00e9poque 29,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0195\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0025 \n\nM\u00e9trique pour l'\u00e9poque : 0.9979 \n\n\nD\u00e9but de l'\u00e9poque 30,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0373\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0004\nLoss pour l'\u00e9poque : 0.0020 \n\nM\u00e9trique pour l'\u00e9poque : 0.9988 \n\n\nD\u00e9but de l'\u00e9poque 31,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0257\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0017 \n\nM\u00e9trique pour l'\u00e9poque : 0.9990 \n\n\nD\u00e9but de l'\u00e9poque 32,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0571\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0075\nLoss pour l'\u00e9poque : 0.0020 \n\nM\u00e9trique pour l'\u00e9poque : 0.9985 \n\n\nD\u00e9but de l'\u00e9poque 33,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0167\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0028 \n\nM\u00e9trique pour l'\u00e9poque : 0.9985 \n\n\nD\u00e9but de l'\u00e9poque 34,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.1389\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0024 \n\nM\u00e9trique pour l'\u00e9poque : 0.9991 \n\n\nD\u00e9but de l'\u00e9poque 35,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0218\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0004\nLoss pour l'\u00e9poque : 0.0021 \n\nM\u00e9trique pour l'\u00e9poque : 0.9985 \n\n\nD\u00e9but de l'\u00e9poque 36,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0266\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0019\nLoss pour l'\u00e9poque : 0.0018 \n\nM\u00e9trique pour l'\u00e9poque : 0.9990 \n\n\nD\u00e9but de l'\u00e9poque 37,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0375\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0017 \n\nM\u00e9trique pour l'\u00e9poque : 0.9989 \n\n\nD\u00e9but de l'\u00e9poque 38,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0757\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0017 \n\nM\u00e9trique pour l'\u00e9poque : 0.9984 \n\n\nD\u00e9but de l'\u00e9poque 39,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0158\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0012 \n\nM\u00e9trique pour l'\u00e9poque : 0.9988 \n\n\nD\u00e9but de l'\u00e9poque 40,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0213\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0134\nLoss pour l'\u00e9poque : 0.0016 \n\nM\u00e9trique pour l'\u00e9poque : 0.9989 \n\n\nD\u00e9but de l'\u00e9poque 41,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0277\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0013 \n\nM\u00e9trique pour l'\u00e9poque : 0.9985 \n\n\nD\u00e9but de l'\u00e9poque 42,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0056\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0014 \n\nM\u00e9trique pour l'\u00e9poque : 0.9219 \n\n\nD\u00e9but de l'\u00e9poque 43,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0042\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0004\nLoss pour l'\u00e9poque : 0.0014 \n\nM\u00e9trique pour l'\u00e9poque : 0.9992 \n\n\nD\u00e9but de l'\u00e9poque 44,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0116\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0005\nLoss pour l'\u00e9poque : 0.0014 \n\nM\u00e9trique pour l'\u00e9poque : 0.9993 \n\n\nD\u00e9but de l'\u00e9poque 45,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0064\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0005\nLoss pour l'\u00e9poque : 0.0011 \n\nM\u00e9trique pour l'\u00e9poque : 0.9994 \n\n\nD\u00e9but de l'\u00e9poque 46,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0045\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0016 \n\nM\u00e9trique pour l'\u00e9poque : 0.9990 \n\n\nD\u00e9but de l'\u00e9poque 47,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0124\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0013 \n\nM\u00e9trique pour l'\u00e9poque : 0.9993 \n\n\nD\u00e9but de l'\u00e9poque 48,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0082\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0094\nLoss pour l'\u00e9poque : 0.0010 \n\nM\u00e9trique pour l'\u00e9poque : 0.9994 \n\n\nD\u00e9but de l'\u00e9poque 49,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0224\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0013 \n\nM\u00e9trique pour l'\u00e9poque : 0.9992 \n\n\nD\u00e9but de l'\u00e9poque 50,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0030\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0013 \n\nM\u00e9trique pour l'\u00e9poque : 0.9990 \n\n\nD\u00e9but de l'\u00e9poque 51,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0092\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0009 \n\nM\u00e9trique pour l'\u00e9poque : 0.9993 \n\n\nD\u00e9but de l'\u00e9poque 52,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0067\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0002\nLoss pour l'\u00e9poque : 0.0012 \n\nM\u00e9trique pour l'\u00e9poque : 0.9992 \n\n\nD\u00e9but de l'\u00e9poque 53,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0082\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0009 \n\nM\u00e9trique pour l'\u00e9poque : 0.9995 \n\n\nD\u00e9but de l'\u00e9poque 54,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0072\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0002\nLoss pour l'\u00e9poque : 0.0012 \n\nM\u00e9trique pour l'\u00e9poque : 0.9993 \n\n\nD\u00e9but de l'\u00e9poque 55,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0171\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0010 \n\nM\u00e9trique pour l'\u00e9poque : 0.9993 \n\n\nD\u00e9but de l'\u00e9poque 56,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0078\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0008 \n\nM\u00e9trique pour l'\u00e9poque : 0.9994 \n\n\nD\u00e9but de l'\u00e9poque 57,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0219\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0010 \n\nM\u00e9trique pour l'\u00e9poque : 0.9995 \n\n\nD\u00e9but de l'\u00e9poque 58,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0051\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0004\nLoss pour l'\u00e9poque : 0.0011 \n\nM\u00e9trique pour l'\u00e9poque : 0.9996 \n\n\nD\u00e9but de l'\u00e9poque 59,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0263\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0011 \n\nM\u00e9trique pour l'\u00e9poque : 0.9996 \n\n\nD\u00e9but de l'\u00e9poque 60,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0068\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0009 \n\nM\u00e9trique pour l'\u00e9poque : 0.9994 \n\n\nD\u00e9but de l'\u00e9poque 61,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0217\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0010 \n\nM\u00e9trique pour l'\u00e9poque : 0.9995 \n\n\nD\u00e9but de l'\u00e9poque 62,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0091\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0009 \n\nM\u00e9trique pour l'\u00e9poque : 0.9996 \n\n\nD\u00e9but de l'\u00e9poque 63,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0074\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0004\nLoss pour l'\u00e9poque : 0.0050 \n\nM\u00e9trique pour l'\u00e9poque : 0.9992 \n\n\nD\u00e9but de l'\u00e9poque 64,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0949\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0002\nLoss pour l'\u00e9poque : 0.0017 \n\nM\u00e9trique pour l'\u00e9poque : 0.9993 \n\n\nD\u00e9but de l'\u00e9poque 65,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0065\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0010 \n\nM\u00e9trique pour l'\u00e9poque : 0.9995 \n\n\nD\u00e9but de l'\u00e9poque 66,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0035\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0002\nLoss pour l'\u00e9poque : 0.0008 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 67,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0160\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0006\nLoss pour l'\u00e9poque : 0.0012 \n\nM\u00e9trique pour l'\u00e9poque : 0.9993 \n\n\nD\u00e9but de l'\u00e9poque 68,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0043\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0009 \n\nM\u00e9trique pour l'\u00e9poque : 0.9227 \n\n\nD\u00e9but de l'\u00e9poque 69,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0646\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0015 \n\nM\u00e9trique pour l'\u00e9poque : 0.9993 \n\n\nD\u00e9but de l'\u00e9poque 70,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0109\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0004\nLoss pour l'\u00e9poque : 0.0009 \n\nM\u00e9trique pour l'\u00e9poque : 0.9995 \n\n\nD\u00e9but de l'\u00e9poque 71,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0045\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0009 \n\nM\u00e9trique pour l'\u00e9poque : 0.9995 \n\n\nD\u00e9but de l'\u00e9poque 72,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0168\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0009 \n\nM\u00e9trique pour l'\u00e9poque : 0.9994 \n\n\nD\u00e9but de l'\u00e9poque 73,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0061\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0009 \n\nM\u00e9trique pour l'\u00e9poque : 0.9994 \n\n\nD\u00e9but de l'\u00e9poque 74,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0022\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0004\nLoss pour l'\u00e9poque : 0.0008 \n\nM\u00e9trique pour l'\u00e9poque : 0.9994 \n\n\nD\u00e9but de l'\u00e9poque 75,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0069\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0004\nLoss pour l'\u00e9poque : 0.0009 \n\nM\u00e9trique pour l'\u00e9poque : 0.9994 \n\n\nD\u00e9but de l'\u00e9poque 76,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0036\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0009 \n\nM\u00e9trique pour l'\u00e9poque : 0.9996 \n\n\nD\u00e9but de l'\u00e9poque 77,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0034\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 78,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0041\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0006\nLoss pour l'\u00e9poque : 0.0008 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 79,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0151\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0004\nLoss pour l'\u00e9poque : 0.0008 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 80,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0035\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 81,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0078\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0009 \n\nM\u00e9trique pour l'\u00e9poque : 0.9995 \n\n\nD\u00e9but de l'\u00e9poque 82,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0095\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0007\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 83,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0126\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0009 \n\nM\u00e9trique pour l'\u00e9poque : 0.9995 \n\n\nD\u00e9but de l'\u00e9poque 84,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0056\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0008 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 85,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0044\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0002\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 86,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0024\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0004\nLoss pour l'\u00e9poque : 0.0009 \n\nM\u00e9trique pour l'\u00e9poque : 0.9996 \n\n\nD\u00e9but de l'\u00e9poque 87,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0030\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0002\nLoss pour l'\u00e9poque : 0.0008 \n\nM\u00e9trique pour l'\u00e9poque : 0.9995 \n\n\nD\u00e9but de l'\u00e9poque 88,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0122\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9991 \n\n\nD\u00e9but de l'\u00e9poque 89,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0050\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9995 \n\n\nD\u00e9but de l'\u00e9poque 90,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0070\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9996 \n\n\nD\u00e9but de l'\u00e9poque 91,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0019\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0002\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 92,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0036\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0002\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 93,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0029\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0008 \n\nM\u00e9trique pour l'\u00e9poque : 0.9996 \n\n\nD\u00e9but de l'\u00e9poque 94,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0011\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0004\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 95,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0031\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 96,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0053\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0002\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 97,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0035\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0008 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 98,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0019\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0014\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9996 \n\n\nD\u00e9but de l'\u00e9poque 99,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0007\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0022\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 100,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0050\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0004\nLoss pour l'\u00e9poque : 0.0008 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 101,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0174\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0002\nLoss pour l'\u00e9poque : 0.0008 \n\nM\u00e9trique pour l'\u00e9poque : 0.9996 \n\n\nD\u00e9but de l'\u00e9poque 102,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0062\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0002\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 103,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0063\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0002\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 104,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0033\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 105,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0021\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0002\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 106,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0020\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9228 \n\n\nD\u00e9but de l'\u00e9poque 107,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0183\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0009 \n\nM\u00e9trique pour l'\u00e9poque : 0.9228 \n\n\nD\u00e9but de l'\u00e9poque 108,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0107\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 109,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0095\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0002\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 110,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0060\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 111,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0027\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0002\nLoss pour l'\u00e9poque : 0.0008 \n\nM\u00e9trique pour l'\u00e9poque : 0.9996 \n\n\nD\u00e9but de l'\u00e9poque 112,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0009\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0002\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 113,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0013\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0005\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 114,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0127\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0004\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 115,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0026\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0004\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 116,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0056\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0008 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 117,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0015\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 118,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0031\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0002\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9996 \n\n\nD\u00e9but de l'\u00e9poque 119,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0009\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0005\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 120,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0040\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0004\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 121,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0010\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 122,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0024\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 123,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0022\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0022\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 124,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0029\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 125,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0025\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0002\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 126,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0023\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 127,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0020\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 128,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0029\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 129,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0038\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0002\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 130,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0073\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 131,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0038\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0007\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 132,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0047\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 133,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0077\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 134,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0046\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 135,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0026\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0002\nLoss pour l'\u00e9poque : 0.0008 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 136,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0080\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 137,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0022\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0004\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 138,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0038\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 139,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0018\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0004\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 140,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0013\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0004\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 141,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0076\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 142,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0032\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0005\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 143,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0022\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0002\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 144,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0011\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0002\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 145,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0019\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0002\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9229 \n\n\nD\u00e9but de l'\u00e9poque 146,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0042\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 147,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0009\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 148,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0023\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0006\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n\nD\u00e9but de l'\u00e9poque 149,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0088\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0007 \n\nM\u00e9trique pour l'\u00e9poque : 0.9997 \n\n\nD\u00e9but de l'\u00e9poque 150,\nLoss sur le batch \u00e0 l'\u00e9tape 0 : 0.0022\nLoss sur le batch \u00e0 l'\u00e9tape 100 : 0.0003\nLoss pour l'\u00e9poque : 0.0006 \n\nM\u00e9trique pour l'\u00e9poque : 0.9998 \n\n</code>\n</pre>        <pre><code>num_obs=150\n\nimg = tf.reshape(x_train[num_obs], (-1, 224, 224, 3))\np = model2.predict(img)\np = np.argmax(p, axis=-1)\np.shape\n\nfigsize(20, 5)\n\nsubplot(1, 3, 1)\nimshow(img[0, :, : ,:])\nxlabel('Input')\n\nsubplot(1, 3, 2)\nimshow(p[0, :, :])\nxlabel('Pr\u00e9diction')\n\nsubplot(1, 3, 3)\nimshow(y_train[num_obs][:, :])\nxlabel('V\u00e9rit\u00e9')\n\nm = tf.keras.metrics.MeanIoU(num_classes=32)\nm(y_train[num_obs][:, :], p[0, :, :]).numpy()\n</code></pre>      <pre>\n<code>0.42460242</code>\n</pre>"},{"location":"deep_learning/module6/Module6_2/#fc-densenet67","title":"FC-Densenet67","text":"<ul> <li>67 layers (FC-DenseNet67) with \\(5\\) layers per dense block and a growth rate of \\(16\\).</li> </ul>      <pre><code>#fc-densenet 67\nfilters = 16\nnb_blocks = 5\nnb_layers = [5, 5, 5, 5, 5 ,5, 5, 5, 5, 5, 5]\n# Free up RAM in case the model definition cells were run multiple times\nkeras.backend.clear_session()\nmodel = FCDensenet(filters, nb_layers, img_shape, num_classes=32)\nmodel.summary()\n</code></pre>      <pre>\n<code>Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n__________________________________________________________________________________________________\nconv2d (Conv2D)                 (None, 224, 224, 48) 1344        input_1[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization (BatchNorma (None, 224, 224, 48) 192         conv2d[0][0]                     \n__________________________________________________________________________________________________\nre_lu (ReLU)                    (None, 224, 224, 48) 0           batch_normalization[0][0]        \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 224, 224, 16) 6928        re_lu[0][0]                      \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 224, 224, 16) 0           conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 224, 224, 64) 0           conv2d[0][0]                     \n                                                                 dropout[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_1 (BatchNor (None, 224, 224, 64) 256         concatenate[0][0]                \n__________________________________________________________________________________________________\nre_lu_1 (ReLU)                  (None, 224, 224, 64) 0           batch_normalization_1[0][0]      \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 224, 224, 16) 9232        re_lu_1[0][0]                    \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 224, 224, 16) 0           conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 224, 224, 80) 0           concatenate[0][0]                \n                                                                 dropout_1[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_2 (BatchNor (None, 224, 224, 80) 320         concatenate_1[0][0]              \n__________________________________________________________________________________________________\nre_lu_2 (ReLU)                  (None, 224, 224, 80) 0           batch_normalization_2[0][0]      \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 224, 224, 16) 11536       re_lu_2[0][0]                    \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 224, 224, 16) 0           conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 224, 224, 96) 0           concatenate_1[0][0]              \n                                                                 dropout_2[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_3 (BatchNor (None, 224, 224, 96) 384         concatenate_2[0][0]              \n__________________________________________________________________________________________________\nre_lu_3 (ReLU)                  (None, 224, 224, 96) 0           batch_normalization_3[0][0]      \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 224, 224, 16) 13840       re_lu_3[0][0]                    \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, 224, 224, 16) 0           conv2d_4[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_3 (Concatenate)     (None, 224, 224, 112 0           concatenate_2[0][0]              \n                                                                 dropout_3[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_4 (BatchNor (None, 224, 224, 112 448         concatenate_3[0][0]              \n__________________________________________________________________________________________________\nre_lu_4 (ReLU)                  (None, 224, 224, 112 0           batch_normalization_4[0][0]      \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 224, 224, 16) 16144       re_lu_4[0][0]                    \n__________________________________________________________________________________________________\ndropout_4 (Dropout)             (None, 224, 224, 16) 0           conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_4 (Concatenate)     (None, 224, 224, 80) 0           dropout_4[0][0]                  \n                                                                 dropout_3[0][0]                  \n                                                                 dropout_2[0][0]                  \n                                                                 dropout_1[0][0]                  \n                                                                 dropout[0][0]                    \n__________________________________________________________________________________________________\nconcatenate_5 (Concatenate)     (None, 224, 224, 128 0           concatenate_4[0][0]              \n                                                                 conv2d[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_5 (BatchNor (None, 224, 224, 128 512         concatenate_5[0][0]              \n__________________________________________________________________________________________________\nre_lu_5 (ReLU)                  (None, 224, 224, 128 0           batch_normalization_5[0][0]      \n__________________________________________________________________________________________________\nconv2d_6 (Conv2D)               (None, 224, 224, 128 16512       re_lu_5[0][0]                    \n__________________________________________________________________________________________________\ndropout_5 (Dropout)             (None, 224, 224, 128 0           conv2d_6[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)    (None, 112, 112, 128 0           dropout_5[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_6 (BatchNor (None, 112, 112, 128 512         max_pooling2d[0][0]              \n__________________________________________________________________________________________________\nre_lu_6 (ReLU)                  (None, 112, 112, 128 0           batch_normalization_6[0][0]      \n__________________________________________________________________________________________________\nconv2d_7 (Conv2D)               (None, 112, 112, 16) 18448       re_lu_6[0][0]                    \n__________________________________________________________________________________________________\ndropout_6 (Dropout)             (None, 112, 112, 16) 0           conv2d_7[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_6 (Concatenate)     (None, 112, 112, 144 0           max_pooling2d[0][0]              \n                                                                 dropout_6[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_7 (BatchNor (None, 112, 112, 144 576         concatenate_6[0][0]              \n__________________________________________________________________________________________________\nre_lu_7 (ReLU)                  (None, 112, 112, 144 0           batch_normalization_7[0][0]      \n__________________________________________________________________________________________________\nconv2d_8 (Conv2D)               (None, 112, 112, 16) 20752       re_lu_7[0][0]                    \n__________________________________________________________________________________________________\ndropout_7 (Dropout)             (None, 112, 112, 16) 0           conv2d_8[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_7 (Concatenate)     (None, 112, 112, 160 0           concatenate_6[0][0]              \n                                                                 dropout_7[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_8 (BatchNor (None, 112, 112, 160 640         concatenate_7[0][0]              \n__________________________________________________________________________________________________\nre_lu_8 (ReLU)                  (None, 112, 112, 160 0           batch_normalization_8[0][0]      \n__________________________________________________________________________________________________\nconv2d_9 (Conv2D)               (None, 112, 112, 16) 23056       re_lu_8[0][0]                    \n__________________________________________________________________________________________________\ndropout_8 (Dropout)             (None, 112, 112, 16) 0           conv2d_9[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_8 (Concatenate)     (None, 112, 112, 176 0           concatenate_7[0][0]              \n                                                                 dropout_8[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_9 (BatchNor (None, 112, 112, 176 704         concatenate_8[0][0]              \n__________________________________________________________________________________________________\nre_lu_9 (ReLU)                  (None, 112, 112, 176 0           batch_normalization_9[0][0]      \n__________________________________________________________________________________________________\nconv2d_10 (Conv2D)              (None, 112, 112, 16) 25360       re_lu_9[0][0]                    \n__________________________________________________________________________________________________\ndropout_9 (Dropout)             (None, 112, 112, 16) 0           conv2d_10[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_9 (Concatenate)     (None, 112, 112, 192 0           concatenate_8[0][0]              \n                                                                 dropout_9[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_10 (BatchNo (None, 112, 112, 192 768         concatenate_9[0][0]              \n__________________________________________________________________________________________________\nre_lu_10 (ReLU)                 (None, 112, 112, 192 0           batch_normalization_10[0][0]     \n__________________________________________________________________________________________________\nconv2d_11 (Conv2D)              (None, 112, 112, 16) 27664       re_lu_10[0][0]                   \n__________________________________________________________________________________________________\ndropout_10 (Dropout)            (None, 112, 112, 16) 0           conv2d_11[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_10 (Concatenate)    (None, 112, 112, 80) 0           dropout_10[0][0]                 \n                                                                 dropout_9[0][0]                  \n                                                                 dropout_8[0][0]                  \n                                                                 dropout_7[0][0]                  \n                                                                 dropout_6[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_11 (Concatenate)    (None, 112, 112, 208 0           concatenate_10[0][0]             \n                                                                 max_pooling2d[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_11 (BatchNo (None, 112, 112, 208 832         concatenate_11[0][0]             \n__________________________________________________________________________________________________\nre_lu_11 (ReLU)                 (None, 112, 112, 208 0           batch_normalization_11[0][0]     \n__________________________________________________________________________________________________\nconv2d_12 (Conv2D)              (None, 112, 112, 208 43472       re_lu_11[0][0]                   \n__________________________________________________________________________________________________\ndropout_11 (Dropout)            (None, 112, 112, 208 0           conv2d_12[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 56, 56, 208)  0           dropout_11[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_12 (BatchNo (None, 56, 56, 208)  832         max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\nre_lu_12 (ReLU)                 (None, 56, 56, 208)  0           batch_normalization_12[0][0]     \n__________________________________________________________________________________________________\nconv2d_13 (Conv2D)              (None, 56, 56, 16)   29968       re_lu_12[0][0]                   \n__________________________________________________________________________________________________\ndropout_12 (Dropout)            (None, 56, 56, 16)   0           conv2d_13[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_12 (Concatenate)    (None, 56, 56, 224)  0           max_pooling2d_1[0][0]            \n                                                                 dropout_12[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_13 (BatchNo (None, 56, 56, 224)  896         concatenate_12[0][0]             \n__________________________________________________________________________________________________\nre_lu_13 (ReLU)                 (None, 56, 56, 224)  0           batch_normalization_13[0][0]     \n__________________________________________________________________________________________________\nconv2d_14 (Conv2D)              (None, 56, 56, 16)   32272       re_lu_13[0][0]                   \n__________________________________________________________________________________________________\ndropout_13 (Dropout)            (None, 56, 56, 16)   0           conv2d_14[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_13 (Concatenate)    (None, 56, 56, 240)  0           concatenate_12[0][0]             \n                                                                 dropout_13[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_14 (BatchNo (None, 56, 56, 240)  960         concatenate_13[0][0]             \n__________________________________________________________________________________________________\nre_lu_14 (ReLU)                 (None, 56, 56, 240)  0           batch_normalization_14[0][0]     \n__________________________________________________________________________________________________\nconv2d_15 (Conv2D)              (None, 56, 56, 16)   34576       re_lu_14[0][0]                   \n__________________________________________________________________________________________________\ndropout_14 (Dropout)            (None, 56, 56, 16)   0           conv2d_15[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_14 (Concatenate)    (None, 56, 56, 256)  0           concatenate_13[0][0]             \n                                                                 dropout_14[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_15 (BatchNo (None, 56, 56, 256)  1024        concatenate_14[0][0]             \n__________________________________________________________________________________________________\nre_lu_15 (ReLU)                 (None, 56, 56, 256)  0           batch_normalization_15[0][0]     \n__________________________________________________________________________________________________\nconv2d_16 (Conv2D)              (None, 56, 56, 16)   36880       re_lu_15[0][0]                   \n__________________________________________________________________________________________________\ndropout_15 (Dropout)            (None, 56, 56, 16)   0           conv2d_16[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_15 (Concatenate)    (None, 56, 56, 272)  0           concatenate_14[0][0]             \n                                                                 dropout_15[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_16 (BatchNo (None, 56, 56, 272)  1088        concatenate_15[0][0]             \n__________________________________________________________________________________________________\nre_lu_16 (ReLU)                 (None, 56, 56, 272)  0           batch_normalization_16[0][0]     \n__________________________________________________________________________________________________\nconv2d_17 (Conv2D)              (None, 56, 56, 16)   39184       re_lu_16[0][0]                   \n__________________________________________________________________________________________________\ndropout_16 (Dropout)            (None, 56, 56, 16)   0           conv2d_17[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_16 (Concatenate)    (None, 56, 56, 80)   0           dropout_16[0][0]                 \n                                                                 dropout_15[0][0]                 \n                                                                 dropout_14[0][0]                 \n                                                                 dropout_13[0][0]                 \n                                                                 dropout_12[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_17 (Concatenate)    (None, 56, 56, 288)  0           concatenate_16[0][0]             \n                                                                 max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\nbatch_normalization_17 (BatchNo (None, 56, 56, 288)  1152        concatenate_17[0][0]             \n__________________________________________________________________________________________________\nre_lu_17 (ReLU)                 (None, 56, 56, 288)  0           batch_normalization_17[0][0]     \n__________________________________________________________________________________________________\nconv2d_18 (Conv2D)              (None, 56, 56, 288)  83232       re_lu_17[0][0]                   \n__________________________________________________________________________________________________\ndropout_17 (Dropout)            (None, 56, 56, 288)  0           conv2d_18[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)  (None, 28, 28, 288)  0           dropout_17[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_18 (BatchNo (None, 28, 28, 288)  1152        max_pooling2d_2[0][0]            \n__________________________________________________________________________________________________\nre_lu_18 (ReLU)                 (None, 28, 28, 288)  0           batch_normalization_18[0][0]     \n__________________________________________________________________________________________________\nconv2d_19 (Conv2D)              (None, 28, 28, 16)   41488       re_lu_18[0][0]                   \n__________________________________________________________________________________________________\ndropout_18 (Dropout)            (None, 28, 28, 16)   0           conv2d_19[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_18 (Concatenate)    (None, 28, 28, 304)  0           max_pooling2d_2[0][0]            \n                                                                 dropout_18[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_19 (BatchNo (None, 28, 28, 304)  1216        concatenate_18[0][0]             \n__________________________________________________________________________________________________\nre_lu_19 (ReLU)                 (None, 28, 28, 304)  0           batch_normalization_19[0][0]     \n__________________________________________________________________________________________________\nconv2d_20 (Conv2D)              (None, 28, 28, 16)   43792       re_lu_19[0][0]                   \n__________________________________________________________________________________________________\ndropout_19 (Dropout)            (None, 28, 28, 16)   0           conv2d_20[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_19 (Concatenate)    (None, 28, 28, 320)  0           concatenate_18[0][0]             \n                                                                 dropout_19[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_20 (BatchNo (None, 28, 28, 320)  1280        concatenate_19[0][0]             \n__________________________________________________________________________________________________\nre_lu_20 (ReLU)                 (None, 28, 28, 320)  0           batch_normalization_20[0][0]     \n__________________________________________________________________________________________________\nconv2d_21 (Conv2D)              (None, 28, 28, 16)   46096       re_lu_20[0][0]                   \n__________________________________________________________________________________________________\ndropout_20 (Dropout)            (None, 28, 28, 16)   0           conv2d_21[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_20 (Concatenate)    (None, 28, 28, 336)  0           concatenate_19[0][0]             \n                                                                 dropout_20[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_21 (BatchNo (None, 28, 28, 336)  1344        concatenate_20[0][0]             \n__________________________________________________________________________________________________\nre_lu_21 (ReLU)                 (None, 28, 28, 336)  0           batch_normalization_21[0][0]     \n__________________________________________________________________________________________________\nconv2d_22 (Conv2D)              (None, 28, 28, 16)   48400       re_lu_21[0][0]                   \n__________________________________________________________________________________________________\ndropout_21 (Dropout)            (None, 28, 28, 16)   0           conv2d_22[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_21 (Concatenate)    (None, 28, 28, 352)  0           concatenate_20[0][0]             \n                                                                 dropout_21[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_22 (BatchNo (None, 28, 28, 352)  1408        concatenate_21[0][0]             \n__________________________________________________________________________________________________\nre_lu_22 (ReLU)                 (None, 28, 28, 352)  0           batch_normalization_22[0][0]     \n__________________________________________________________________________________________________\nconv2d_23 (Conv2D)              (None, 28, 28, 16)   50704       re_lu_22[0][0]                   \n__________________________________________________________________________________________________\ndropout_22 (Dropout)            (None, 28, 28, 16)   0           conv2d_23[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_22 (Concatenate)    (None, 28, 28, 80)   0           dropout_22[0][0]                 \n                                                                 dropout_21[0][0]                 \n                                                                 dropout_20[0][0]                 \n                                                                 dropout_19[0][0]                 \n                                                                 dropout_18[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_23 (Concatenate)    (None, 28, 28, 368)  0           concatenate_22[0][0]             \n                                                                 max_pooling2d_2[0][0]            \n__________________________________________________________________________________________________\nbatch_normalization_23 (BatchNo (None, 28, 28, 368)  1472        concatenate_23[0][0]             \n__________________________________________________________________________________________________\nre_lu_23 (ReLU)                 (None, 28, 28, 368)  0           batch_normalization_23[0][0]     \n__________________________________________________________________________________________________\nconv2d_24 (Conv2D)              (None, 28, 28, 368)  135792      re_lu_23[0][0]                   \n__________________________________________________________________________________________________\ndropout_23 (Dropout)            (None, 28, 28, 368)  0           conv2d_24[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 14, 14, 368)  0           dropout_23[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_24 (BatchNo (None, 14, 14, 368)  1472        max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nre_lu_24 (ReLU)                 (None, 14, 14, 368)  0           batch_normalization_24[0][0]     \n__________________________________________________________________________________________________\nconv2d_25 (Conv2D)              (None, 14, 14, 16)   53008       re_lu_24[0][0]                   \n__________________________________________________________________________________________________\ndropout_24 (Dropout)            (None, 14, 14, 16)   0           conv2d_25[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_24 (Concatenate)    (None, 14, 14, 384)  0           max_pooling2d_3[0][0]            \n                                                                 dropout_24[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_25 (BatchNo (None, 14, 14, 384)  1536        concatenate_24[0][0]             \n__________________________________________________________________________________________________\nre_lu_25 (ReLU)                 (None, 14, 14, 384)  0           batch_normalization_25[0][0]     \n__________________________________________________________________________________________________\nconv2d_26 (Conv2D)              (None, 14, 14, 16)   55312       re_lu_25[0][0]                   \n__________________________________________________________________________________________________\ndropout_25 (Dropout)            (None, 14, 14, 16)   0           conv2d_26[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_25 (Concatenate)    (None, 14, 14, 400)  0           concatenate_24[0][0]             \n                                                                 dropout_25[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_26 (BatchNo (None, 14, 14, 400)  1600        concatenate_25[0][0]             \n__________________________________________________________________________________________________\nre_lu_26 (ReLU)                 (None, 14, 14, 400)  0           batch_normalization_26[0][0]     \n__________________________________________________________________________________________________\nconv2d_27 (Conv2D)              (None, 14, 14, 16)   57616       re_lu_26[0][0]                   \n__________________________________________________________________________________________________\ndropout_26 (Dropout)            (None, 14, 14, 16)   0           conv2d_27[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_26 (Concatenate)    (None, 14, 14, 416)  0           concatenate_25[0][0]             \n                                                                 dropout_26[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_27 (BatchNo (None, 14, 14, 416)  1664        concatenate_26[0][0]             \n__________________________________________________________________________________________________\nre_lu_27 (ReLU)                 (None, 14, 14, 416)  0           batch_normalization_27[0][0]     \n__________________________________________________________________________________________________\nconv2d_28 (Conv2D)              (None, 14, 14, 16)   59920       re_lu_27[0][0]                   \n__________________________________________________________________________________________________\ndropout_27 (Dropout)            (None, 14, 14, 16)   0           conv2d_28[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_27 (Concatenate)    (None, 14, 14, 432)  0           concatenate_26[0][0]             \n                                                                 dropout_27[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_28 (BatchNo (None, 14, 14, 432)  1728        concatenate_27[0][0]             \n__________________________________________________________________________________________________\nre_lu_28 (ReLU)                 (None, 14, 14, 432)  0           batch_normalization_28[0][0]     \n__________________________________________________________________________________________________\nconv2d_29 (Conv2D)              (None, 14, 14, 16)   62224       re_lu_28[0][0]                   \n__________________________________________________________________________________________________\ndropout_28 (Dropout)            (None, 14, 14, 16)   0           conv2d_29[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_28 (Concatenate)    (None, 14, 14, 80)   0           dropout_28[0][0]                 \n                                                                 dropout_27[0][0]                 \n                                                                 dropout_26[0][0]                 \n                                                                 dropout_25[0][0]                 \n                                                                 dropout_24[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_29 (Concatenate)    (None, 14, 14, 448)  0           concatenate_28[0][0]             \n                                                                 max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nbatch_normalization_29 (BatchNo (None, 14, 14, 448)  1792        concatenate_29[0][0]             \n__________________________________________________________________________________________________\nre_lu_29 (ReLU)                 (None, 14, 14, 448)  0           batch_normalization_29[0][0]     \n__________________________________________________________________________________________________\nconv2d_30 (Conv2D)              (None, 14, 14, 448)  201152      re_lu_29[0][0]                   \n__________________________________________________________________________________________________\ndropout_29 (Dropout)            (None, 14, 14, 448)  0           conv2d_30[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_4 (MaxPooling2D)  (None, 7, 7, 448)    0           dropout_29[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_30 (BatchNo (None, 7, 7, 448)    1792        max_pooling2d_4[0][0]            \n__________________________________________________________________________________________________\nre_lu_30 (ReLU)                 (None, 7, 7, 448)    0           batch_normalization_30[0][0]     \n__________________________________________________________________________________________________\nconv2d_31 (Conv2D)              (None, 7, 7, 16)     64528       re_lu_30[0][0]                   \n__________________________________________________________________________________________________\ndropout_30 (Dropout)            (None, 7, 7, 16)     0           conv2d_31[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_30 (Concatenate)    (None, 7, 7, 464)    0           max_pooling2d_4[0][0]            \n                                                                 dropout_30[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_31 (BatchNo (None, 7, 7, 464)    1856        concatenate_30[0][0]             \n__________________________________________________________________________________________________\nre_lu_31 (ReLU)                 (None, 7, 7, 464)    0           batch_normalization_31[0][0]     \n__________________________________________________________________________________________________\nconv2d_32 (Conv2D)              (None, 7, 7, 16)     66832       re_lu_31[0][0]                   \n__________________________________________________________________________________________________\ndropout_31 (Dropout)            (None, 7, 7, 16)     0           conv2d_32[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_31 (Concatenate)    (None, 7, 7, 480)    0           concatenate_30[0][0]             \n                                                                 dropout_31[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_32 (BatchNo (None, 7, 7, 480)    1920        concatenate_31[0][0]             \n__________________________________________________________________________________________________\nre_lu_32 (ReLU)                 (None, 7, 7, 480)    0           batch_normalization_32[0][0]     \n__________________________________________________________________________________________________\nconv2d_33 (Conv2D)              (None, 7, 7, 16)     69136       re_lu_32[0][0]                   \n__________________________________________________________________________________________________\ndropout_32 (Dropout)            (None, 7, 7, 16)     0           conv2d_33[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_32 (Concatenate)    (None, 7, 7, 496)    0           concatenate_31[0][0]             \n                                                                 dropout_32[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_33 (BatchNo (None, 7, 7, 496)    1984        concatenate_32[0][0]             \n__________________________________________________________________________________________________\nre_lu_33 (ReLU)                 (None, 7, 7, 496)    0           batch_normalization_33[0][0]     \n__________________________________________________________________________________________________\nconv2d_34 (Conv2D)              (None, 7, 7, 16)     71440       re_lu_33[0][0]                   \n__________________________________________________________________________________________________\ndropout_33 (Dropout)            (None, 7, 7, 16)     0           conv2d_34[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_33 (Concatenate)    (None, 7, 7, 512)    0           concatenate_32[0][0]             \n                                                                 dropout_33[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_34 (BatchNo (None, 7, 7, 512)    2048        concatenate_33[0][0]             \n__________________________________________________________________________________________________\nre_lu_34 (ReLU)                 (None, 7, 7, 512)    0           batch_normalization_34[0][0]     \n__________________________________________________________________________________________________\nconv2d_35 (Conv2D)              (None, 7, 7, 16)     73744       re_lu_34[0][0]                   \n__________________________________________________________________________________________________\ndropout_34 (Dropout)            (None, 7, 7, 16)     0           conv2d_35[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_34 (Concatenate)    (None, 7, 7, 80)     0           dropout_34[0][0]                 \n                                                                 dropout_33[0][0]                 \n                                                                 dropout_32[0][0]                 \n                                                                 dropout_31[0][0]                 \n                                                                 dropout_30[0][0]                 \n__________________________________________________________________________________________________\nconv2d_transpose (Conv2DTranspo (None, 14, 14, 80)   57680       concatenate_34[0][0]             \n__________________________________________________________________________________________________\nconcatenate_35 (Concatenate)    (None, 14, 14, 528)  0           conv2d_transpose[0][0]           \n                                                                 concatenate_29[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_35 (BatchNo (None, 14, 14, 528)  2112        concatenate_35[0][0]             \n__________________________________________________________________________________________________\nre_lu_35 (ReLU)                 (None, 14, 14, 528)  0           batch_normalization_35[0][0]     \n__________________________________________________________________________________________________\nconv2d_36 (Conv2D)              (None, 14, 14, 16)   76048       re_lu_35[0][0]                   \n__________________________________________________________________________________________________\ndropout_35 (Dropout)            (None, 14, 14, 16)   0           conv2d_36[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_36 (Concatenate)    (None, 14, 14, 544)  0           concatenate_35[0][0]             \n                                                                 dropout_35[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_36 (BatchNo (None, 14, 14, 544)  2176        concatenate_36[0][0]             \n__________________________________________________________________________________________________\nre_lu_36 (ReLU)                 (None, 14, 14, 544)  0           batch_normalization_36[0][0]     \n__________________________________________________________________________________________________\nconv2d_37 (Conv2D)              (None, 14, 14, 16)   78352       re_lu_36[0][0]                   \n__________________________________________________________________________________________________\ndropout_36 (Dropout)            (None, 14, 14, 16)   0           conv2d_37[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_37 (Concatenate)    (None, 14, 14, 560)  0           concatenate_36[0][0]             \n                                                                 dropout_36[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_37 (BatchNo (None, 14, 14, 560)  2240        concatenate_37[0][0]             \n__________________________________________________________________________________________________\nre_lu_37 (ReLU)                 (None, 14, 14, 560)  0           batch_normalization_37[0][0]     \n__________________________________________________________________________________________________\nconv2d_38 (Conv2D)              (None, 14, 14, 16)   80656       re_lu_37[0][0]                   \n__________________________________________________________________________________________________\ndropout_37 (Dropout)            (None, 14, 14, 16)   0           conv2d_38[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_38 (Concatenate)    (None, 14, 14, 576)  0           concatenate_37[0][0]             \n                                                                 dropout_37[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_38 (BatchNo (None, 14, 14, 576)  2304        concatenate_38[0][0]             \n__________________________________________________________________________________________________\nre_lu_38 (ReLU)                 (None, 14, 14, 576)  0           batch_normalization_38[0][0]     \n__________________________________________________________________________________________________\nconv2d_39 (Conv2D)              (None, 14, 14, 16)   82960       re_lu_38[0][0]                   \n__________________________________________________________________________________________________\ndropout_38 (Dropout)            (None, 14, 14, 16)   0           conv2d_39[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_39 (Concatenate)    (None, 14, 14, 592)  0           concatenate_38[0][0]             \n                                                                 dropout_38[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_39 (BatchNo (None, 14, 14, 592)  2368        concatenate_39[0][0]             \n__________________________________________________________________________________________________\nre_lu_39 (ReLU)                 (None, 14, 14, 592)  0           batch_normalization_39[0][0]     \n__________________________________________________________________________________________________\nconv2d_40 (Conv2D)              (None, 14, 14, 16)   85264       re_lu_39[0][0]                   \n__________________________________________________________________________________________________\ndropout_39 (Dropout)            (None, 14, 14, 16)   0           conv2d_40[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_40 (Concatenate)    (None, 14, 14, 80)   0           dropout_39[0][0]                 \n                                                                 dropout_38[0][0]                 \n                                                                 dropout_37[0][0]                 \n                                                                 dropout_36[0][0]                 \n                                                                 dropout_35[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_41 (Concatenate)    (None, 14, 14, 608)  0           concatenate_40[0][0]             \n                                                                 concatenate_35[0][0]             \n__________________________________________________________________________________________________\nconv2d_transpose_1 (Conv2DTrans (None, 28, 28, 80)   437840      concatenate_41[0][0]             \n__________________________________________________________________________________________________\nconcatenate_42 (Concatenate)    (None, 28, 28, 448)  0           conv2d_transpose_1[0][0]         \n                                                                 concatenate_23[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_40 (BatchNo (None, 28, 28, 448)  1792        concatenate_42[0][0]             \n__________________________________________________________________________________________________\nre_lu_40 (ReLU)                 (None, 28, 28, 448)  0           batch_normalization_40[0][0]     \n__________________________________________________________________________________________________\nconv2d_41 (Conv2D)              (None, 28, 28, 16)   64528       re_lu_40[0][0]                   \n__________________________________________________________________________________________________\ndropout_40 (Dropout)            (None, 28, 28, 16)   0           conv2d_41[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_43 (Concatenate)    (None, 28, 28, 464)  0           concatenate_42[0][0]             \n                                                                 dropout_40[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_41 (BatchNo (None, 28, 28, 464)  1856        concatenate_43[0][0]             \n__________________________________________________________________________________________________\nre_lu_41 (ReLU)                 (None, 28, 28, 464)  0           batch_normalization_41[0][0]     \n__________________________________________________________________________________________________\nconv2d_42 (Conv2D)              (None, 28, 28, 16)   66832       re_lu_41[0][0]                   \n__________________________________________________________________________________________________\ndropout_41 (Dropout)            (None, 28, 28, 16)   0           conv2d_42[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_44 (Concatenate)    (None, 28, 28, 480)  0           concatenate_43[0][0]             \n                                                                 dropout_41[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_42 (BatchNo (None, 28, 28, 480)  1920        concatenate_44[0][0]             \n__________________________________________________________________________________________________\nre_lu_42 (ReLU)                 (None, 28, 28, 480)  0           batch_normalization_42[0][0]     \n__________________________________________________________________________________________________\nconv2d_43 (Conv2D)              (None, 28, 28, 16)   69136       re_lu_42[0][0]                   \n__________________________________________________________________________________________________\ndropout_42 (Dropout)            (None, 28, 28, 16)   0           conv2d_43[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_45 (Concatenate)    (None, 28, 28, 496)  0           concatenate_44[0][0]             \n                                                                 dropout_42[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_43 (BatchNo (None, 28, 28, 496)  1984        concatenate_45[0][0]             \n__________________________________________________________________________________________________\nre_lu_43 (ReLU)                 (None, 28, 28, 496)  0           batch_normalization_43[0][0]     \n__________________________________________________________________________________________________\nconv2d_44 (Conv2D)              (None, 28, 28, 16)   71440       re_lu_43[0][0]                   \n__________________________________________________________________________________________________\ndropout_43 (Dropout)            (None, 28, 28, 16)   0           conv2d_44[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_46 (Concatenate)    (None, 28, 28, 512)  0           concatenate_45[0][0]             \n                                                                 dropout_43[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_44 (BatchNo (None, 28, 28, 512)  2048        concatenate_46[0][0]             \n__________________________________________________________________________________________________\nre_lu_44 (ReLU)                 (None, 28, 28, 512)  0           batch_normalization_44[0][0]     \n__________________________________________________________________________________________________\nconv2d_45 (Conv2D)              (None, 28, 28, 16)   73744       re_lu_44[0][0]                   \n__________________________________________________________________________________________________\ndropout_44 (Dropout)            (None, 28, 28, 16)   0           conv2d_45[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_47 (Concatenate)    (None, 28, 28, 80)   0           dropout_44[0][0]                 \n                                                                 dropout_43[0][0]                 \n                                                                 dropout_42[0][0]                 \n                                                                 dropout_41[0][0]                 \n                                                                 dropout_40[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_48 (Concatenate)    (None, 28, 28, 528)  0           concatenate_47[0][0]             \n                                                                 concatenate_42[0][0]             \n__________________________________________________________________________________________________\nconv2d_transpose_2 (Conv2DTrans (None, 56, 56, 80)   380240      concatenate_48[0][0]             \n__________________________________________________________________________________________________\nconcatenate_49 (Concatenate)    (None, 56, 56, 368)  0           conv2d_transpose_2[0][0]         \n                                                                 concatenate_17[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_45 (BatchNo (None, 56, 56, 368)  1472        concatenate_49[0][0]             \n__________________________________________________________________________________________________\nre_lu_45 (ReLU)                 (None, 56, 56, 368)  0           batch_normalization_45[0][0]     \n__________________________________________________________________________________________________\nconv2d_46 (Conv2D)              (None, 56, 56, 16)   53008       re_lu_45[0][0]                   \n__________________________________________________________________________________________________\ndropout_45 (Dropout)            (None, 56, 56, 16)   0           conv2d_46[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_50 (Concatenate)    (None, 56, 56, 384)  0           concatenate_49[0][0]             \n                                                                 dropout_45[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_46 (BatchNo (None, 56, 56, 384)  1536        concatenate_50[0][0]             \n__________________________________________________________________________________________________\nre_lu_46 (ReLU)                 (None, 56, 56, 384)  0           batch_normalization_46[0][0]     \n__________________________________________________________________________________________________\nconv2d_47 (Conv2D)              (None, 56, 56, 16)   55312       re_lu_46[0][0]                   \n__________________________________________________________________________________________________\ndropout_46 (Dropout)            (None, 56, 56, 16)   0           conv2d_47[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_51 (Concatenate)    (None, 56, 56, 400)  0           concatenate_50[0][0]             \n                                                                 dropout_46[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_47 (BatchNo (None, 56, 56, 400)  1600        concatenate_51[0][0]             \n__________________________________________________________________________________________________\nre_lu_47 (ReLU)                 (None, 56, 56, 400)  0           batch_normalization_47[0][0]     \n__________________________________________________________________________________________________\nconv2d_48 (Conv2D)              (None, 56, 56, 16)   57616       re_lu_47[0][0]                   \n__________________________________________________________________________________________________\ndropout_47 (Dropout)            (None, 56, 56, 16)   0           conv2d_48[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_52 (Concatenate)    (None, 56, 56, 416)  0           concatenate_51[0][0]             \n                                                                 dropout_47[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_48 (BatchNo (None, 56, 56, 416)  1664        concatenate_52[0][0]             \n__________________________________________________________________________________________________\nre_lu_48 (ReLU)                 (None, 56, 56, 416)  0           batch_normalization_48[0][0]     \n__________________________________________________________________________________________________\nconv2d_49 (Conv2D)              (None, 56, 56, 16)   59920       re_lu_48[0][0]                   \n__________________________________________________________________________________________________\ndropout_48 (Dropout)            (None, 56, 56, 16)   0           conv2d_49[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_53 (Concatenate)    (None, 56, 56, 432)  0           concatenate_52[0][0]             \n                                                                 dropout_48[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_49 (BatchNo (None, 56, 56, 432)  1728        concatenate_53[0][0]             \n__________________________________________________________________________________________________\nre_lu_49 (ReLU)                 (None, 56, 56, 432)  0           batch_normalization_49[0][0]     \n__________________________________________________________________________________________________\nconv2d_50 (Conv2D)              (None, 56, 56, 16)   62224       re_lu_49[0][0]                   \n__________________________________________________________________________________________________\ndropout_49 (Dropout)            (None, 56, 56, 16)   0           conv2d_50[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_54 (Concatenate)    (None, 56, 56, 80)   0           dropout_49[0][0]                 \n                                                                 dropout_48[0][0]                 \n                                                                 dropout_47[0][0]                 \n                                                                 dropout_46[0][0]                 \n                                                                 dropout_45[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_55 (Concatenate)    (None, 56, 56, 448)  0           concatenate_54[0][0]             \n                                                                 concatenate_49[0][0]             \n__________________________________________________________________________________________________\nconv2d_transpose_3 (Conv2DTrans (None, 112, 112, 80) 322640      concatenate_55[0][0]             \n__________________________________________________________________________________________________\nconcatenate_56 (Concatenate)    (None, 112, 112, 288 0           conv2d_transpose_3[0][0]         \n                                                                 concatenate_11[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_50 (BatchNo (None, 112, 112, 288 1152        concatenate_56[0][0]             \n__________________________________________________________________________________________________\nre_lu_50 (ReLU)                 (None, 112, 112, 288 0           batch_normalization_50[0][0]     \n__________________________________________________________________________________________________\nconv2d_51 (Conv2D)              (None, 112, 112, 16) 41488       re_lu_50[0][0]                   \n__________________________________________________________________________________________________\ndropout_50 (Dropout)            (None, 112, 112, 16) 0           conv2d_51[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_57 (Concatenate)    (None, 112, 112, 304 0           concatenate_56[0][0]             \n                                                                 dropout_50[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_51 (BatchNo (None, 112, 112, 304 1216        concatenate_57[0][0]             \n__________________________________________________________________________________________________\nre_lu_51 (ReLU)                 (None, 112, 112, 304 0           batch_normalization_51[0][0]     \n__________________________________________________________________________________________________\nconv2d_52 (Conv2D)              (None, 112, 112, 16) 43792       re_lu_51[0][0]                   \n__________________________________________________________________________________________________\ndropout_51 (Dropout)            (None, 112, 112, 16) 0           conv2d_52[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_58 (Concatenate)    (None, 112, 112, 320 0           concatenate_57[0][0]             \n                                                                 dropout_51[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_52 (BatchNo (None, 112, 112, 320 1280        concatenate_58[0][0]             \n__________________________________________________________________________________________________\nre_lu_52 (ReLU)                 (None, 112, 112, 320 0           batch_normalization_52[0][0]     \n__________________________________________________________________________________________________\nconv2d_53 (Conv2D)              (None, 112, 112, 16) 46096       re_lu_52[0][0]                   \n__________________________________________________________________________________________________\ndropout_52 (Dropout)            (None, 112, 112, 16) 0           conv2d_53[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_59 (Concatenate)    (None, 112, 112, 336 0           concatenate_58[0][0]             \n                                                                 dropout_52[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_53 (BatchNo (None, 112, 112, 336 1344        concatenate_59[0][0]             \n__________________________________________________________________________________________________\nre_lu_53 (ReLU)                 (None, 112, 112, 336 0           batch_normalization_53[0][0]     \n__________________________________________________________________________________________________\nconv2d_54 (Conv2D)              (None, 112, 112, 16) 48400       re_lu_53[0][0]                   \n__________________________________________________________________________________________________\ndropout_53 (Dropout)            (None, 112, 112, 16) 0           conv2d_54[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_60 (Concatenate)    (None, 112, 112, 352 0           concatenate_59[0][0]             \n                                                                 dropout_53[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_54 (BatchNo (None, 112, 112, 352 1408        concatenate_60[0][0]             \n__________________________________________________________________________________________________\nre_lu_54 (ReLU)                 (None, 112, 112, 352 0           batch_normalization_54[0][0]     \n__________________________________________________________________________________________________\nconv2d_55 (Conv2D)              (None, 112, 112, 16) 50704       re_lu_54[0][0]                   \n__________________________________________________________________________________________________\ndropout_54 (Dropout)            (None, 112, 112, 16) 0           conv2d_55[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_61 (Concatenate)    (None, 112, 112, 80) 0           dropout_54[0][0]                 \n                                                                 dropout_53[0][0]                 \n                                                                 dropout_52[0][0]                 \n                                                                 dropout_51[0][0]                 \n                                                                 dropout_50[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_62 (Concatenate)    (None, 112, 112, 368 0           concatenate_61[0][0]             \n                                                                 concatenate_56[0][0]             \n__________________________________________________________________________________________________\nconv2d_transpose_4 (Conv2DTrans (None, 224, 224, 80) 265040      concatenate_62[0][0]             \n__________________________________________________________________________________________________\nconcatenate_63 (Concatenate)    (None, 224, 224, 208 0           conv2d_transpose_4[0][0]         \n                                                                 concatenate_5[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_55 (BatchNo (None, 224, 224, 208 832         concatenate_63[0][0]             \n__________________________________________________________________________________________________\nre_lu_55 (ReLU)                 (None, 224, 224, 208 0           batch_normalization_55[0][0]     \n__________________________________________________________________________________________________\nconv2d_56 (Conv2D)              (None, 224, 224, 16) 29968       re_lu_55[0][0]                   \n__________________________________________________________________________________________________\ndropout_55 (Dropout)            (None, 224, 224, 16) 0           conv2d_56[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_64 (Concatenate)    (None, 224, 224, 224 0           concatenate_63[0][0]             \n                                                                 dropout_55[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_56 (BatchNo (None, 224, 224, 224 896         concatenate_64[0][0]             \n__________________________________________________________________________________________________\nre_lu_56 (ReLU)                 (None, 224, 224, 224 0           batch_normalization_56[0][0]     \n__________________________________________________________________________________________________\nconv2d_57 (Conv2D)              (None, 224, 224, 16) 32272       re_lu_56[0][0]                   \n__________________________________________________________________________________________________\ndropout_56 (Dropout)            (None, 224, 224, 16) 0           conv2d_57[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_65 (Concatenate)    (None, 224, 224, 240 0           concatenate_64[0][0]             \n                                                                 dropout_56[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_57 (BatchNo (None, 224, 224, 240 960         concatenate_65[0][0]             \n__________________________________________________________________________________________________\nre_lu_57 (ReLU)                 (None, 224, 224, 240 0           batch_normalization_57[0][0]     \n__________________________________________________________________________________________________\nconv2d_58 (Conv2D)              (None, 224, 224, 16) 34576       re_lu_57[0][0]                   \n__________________________________________________________________________________________________\ndropout_57 (Dropout)            (None, 224, 224, 16) 0           conv2d_58[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_66 (Concatenate)    (None, 224, 224, 256 0           concatenate_65[0][0]             \n                                                                 dropout_57[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_58 (BatchNo (None, 224, 224, 256 1024        concatenate_66[0][0]             \n__________________________________________________________________________________________________\nre_lu_58 (ReLU)                 (None, 224, 224, 256 0           batch_normalization_58[0][0]     \n__________________________________________________________________________________________________\nconv2d_59 (Conv2D)              (None, 224, 224, 16) 36880       re_lu_58[0][0]                   \n__________________________________________________________________________________________________\ndropout_58 (Dropout)            (None, 224, 224, 16) 0           conv2d_59[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_67 (Concatenate)    (None, 224, 224, 272 0           concatenate_66[0][0]             \n                                                                 dropout_58[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_59 (BatchNo (None, 224, 224, 272 1088        concatenate_67[0][0]             \n__________________________________________________________________________________________________\nre_lu_59 (ReLU)                 (None, 224, 224, 272 0           batch_normalization_59[0][0]     \n__________________________________________________________________________________________________\nconv2d_60 (Conv2D)              (None, 224, 224, 16) 39184       re_lu_59[0][0]                   \n__________________________________________________________________________________________________\ndropout_59 (Dropout)            (None, 224, 224, 16) 0           conv2d_60[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_68 (Concatenate)    (None, 224, 224, 80) 0           dropout_59[0][0]                 \n                                                                 dropout_58[0][0]                 \n                                                                 dropout_57[0][0]                 \n                                                                 dropout_56[0][0]                 \n                                                                 dropout_55[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_69 (Concatenate)    (None, 224, 224, 288 0           concatenate_68[0][0]             \n                                                                 concatenate_63[0][0]             \n__________________________________________________________________________________________________\nconv2d_61 (Conv2D)              (None, 224, 224, 32) 9248        concatenate_69[0][0]             \n__________________________________________________________________________________________________\nactivation (Activation)         (None, 224, 224, 32) 0           conv2d_61[0][0]                  \n==================================================================================================\nTotal params: 4,684,032\nTrainable params: 4,644,352\nNon-trainable params: 39,680\n__________________________________________________________________________________________________\n</code>\n</pre>        <pre><code>model.compile(loss='sparse_categorical_crossentropy', \n              optimizer=keras.optimizers.RMSprop(lr=1e-3),\n              metrics=[\"accuracy\"])\n</code></pre>     <pre><code>model.fit(ds,\n          epochs=2)\n</code></pre>"},{"location":"deep_learning/module6/module6/","title":"Module 6 : La segmentation d'images","text":"<p>La segmentation est une composante essentielle de nombreux syst\u00e8mes de compr\u00e9hension visuelle.  Elle consiste \u00e0 partitionner les images (ou les trames vid\u00e9o) en plusieurs segments ou objets. La segmentation joue un r\u00f4le central dans un large \u00e9ventail d'applications, notamment l'analyse d'images m\u00e9dicales (par exemple, l'extraction des limites d'une tumeur et la mesure des volumes de tissus), les v\u00e9hicules autonomes (par exemple, la d\u00e9tection des surfaces navigables et des pi\u00e9tons), la vid\u00e9osurveillance et la r\u00e9alit\u00e9 augment\u00e9e pour n'en citer que quelques-unes.</p> <p>La segmentation des images peut \u00eatre formul\u00e9e comme un probl\u00e8me de classification des pixels avec des \u00e9tiquettes s\u00e9mantiques (segmentation s\u00e9mantique) ou de partitionnement des objets individuels (segmentation d'instance). La segmentation s\u00e9mantique effectue un \u00e9tiquetage au niveau du pixel avec un ensemble de cat\u00e9gories d'objets (homme, voiture, arbre, ciel) pour tous les pixels de l'image, donc c'est g\u00e9n\u00e9ralement une entreprise plus difficile que la classification d'image, qui pr\u00e9voit un seul label pour l'image enti\u00e8re. La segmentation d'instance \u00e9tend encore la port\u00e9e de la segmentation s\u00e9mantique en d\u00e9tectant et en d\u00e9limitant chaque objet d'int\u00e9r\u00eat dans l'image (par exemple, le partitionnement de personnes individuelles).</p> <p>Le label \\(y\\) est alors un masque ou chaque pour chaque pixel de l'observation on associe une classe.</p>  <p>(a) : l'observation \\(x\\), (b) : le label/masque \\(y\\), (c) : le masque pr\u00e9dit \\(\\hat{y}\\). U-Net: Convolutional Networks for Biomedical Image Segmentation</p>"},{"location":"deep_learning/module6/module6/#les-nouvelles-operations-associees","title":"Les nouvelles op\u00e9rations associ\u00e9es","text":""},{"location":"deep_learning/module6/module6/#la-convolution-transposee","title":"La convolution transpos\u00e9e","text":"<p>La convolution transpos\u00e9e a pour but de restaurer les dimensions d'une feature map obtenue apr\u00e8s une convolution. On ne r\u00e9cup\u00e8re pas l'information d'origine, uniquement les dimensions.</p> <p>Les convolutions transpos\u00e9es sont utilis\u00e9es dans le cadre de la segmentation d'image pour augmenter la taille des features maps. Dans le cadre classique d'un CNN, l'op\u00e9ration de MaxPooling ou AvgPooling est l\u00e0 pour r\u00e9duire la taille des features maps, mais l'op\u00e9ration inverse dite de UpSampling n'est pas directement utils\u00e9e. On lui pr\u00e9f\u00e8re l'op\u00e9ration de convolution transpos\u00e9e qui elle poss\u00e8de des param\u00e8tres apprenables.</p>"},{"location":"deep_learning/module6/module6/#lentropie-croisee-par-pixels","title":"L'entropie crois\u00e9e par pixels","text":"<p>La segmentation \u00e9tant d\u00e9finie comme un probl\u00e8me de classification particuliers la fonction de perte utilis\u00e9e est l'entropie crois\u00e9e. La diff\u00e9rence avec une classification usuelle est que cette fois ci l'entropie crois\u00e9e se fait au niveau du pixel m\u00eame, puisque le label est directement donn\u00e9 au niveau du pixel.</p>  <p>Chaque pixel du masque \\(y\\) poss\u00e8de une classe sp\u00e9cifique, la sortie d'un mod\u00e8le de segmentation est alors une couche de convolution ayant autant de filtres qu'il y a de classes dans \\(y\\). Ici par exemple nous avons 5 classes, donc 5 feature maps en sortie.</p> <p>On obtient alors le masque de pr\u00e9diction en appliquant la fonction argmax par rapport aux feature maps.</p>"},{"location":"deep_learning/module6/module6/#les-metriques-de-segmentation","title":"Les m\u00e9triques de segmentation","text":""},{"location":"deep_learning/module6/module6/#dice","title":"Dice","text":"<p>le coefficient Dice, est essentiellement une mesure du chevauchement entre deux masques. Cette mesure varie de 0 \u00e0 1, un coefficient Dice de 1 indiquant un chevauchement parfait et complet. Le coefficient Dice a \u00e9t\u00e9 d\u00e9velopp\u00e9 \u00e0 l'origine pour des donn\u00e9es binaires, et peut \u00eatre calcul\u00e9 comme :</p> \\[     \\text{Dice}(y, \\hat{y}) = \\frac{{2\\left| {y \\cap \\hat{y}} \\right|}}{{\\left| y \\right| + \\left| \\hat{y} \\right|}} \\]"},{"location":"deep_learning/module6/module6/#intersection-over-union","title":"Intersection over Union","text":"<p>La mesure \"Intersection over Union\" (IoU), \u00e9galement appel\u00e9e indice Jaccard, est essentiellement une m\u00e9thode permettant de quantifier le pourcentage de chevauchement entre le masque cible et nos r\u00e9sultats de pr\u00e9diction.</p> \\[     \\text{IoU}(y, \\hat{y}) = \\frac{{\\left| {y \\cap \\hat{y}} \\right|}}{{\\left| y \\cup \\hat{y} \\right|}} \\] <p>La m\u00e9trique IoU mesure simplement le nombre de pixels communs entre le masque cible et le masque de pr\u00e9diction, divis\u00e9 par le nombre total de pixels pr\u00e9sents sur les deux masques.</p> <p>Pour une segmentation binaire (deux classes) ou multi-classes, la valeur moyenne de l'IoU de l'image est calcul\u00e9e en prenant la valeur de l'IoU de chaque classe et en en faisant la moyenne.</p>"},{"location":"deep_learning/module6/module6/#les-relations-entre-ces-deux-metriques","title":"Les relations entre ces deux m\u00e9triques","text":"<p>Si l'on se concentre sur le cas d'une seule classe \u00e0 chaque fois, on peut r\u00e9exprimer ces m\u00e9triques en terme de matrice de confusion.</p>     \\(\\hat{y}=1\\) \\(\\hat{y}=0\\)     \\(y=1\\) TP FN   \\(y=0\\) FP TN    <p>On obtient alors les formules suivantes.</p> \\[     \\mathrm{Dice}(y, \\hat{y}) = \\frac{2TP}{2TP+FP+FN} \\] \\[     \\mathrm{IoU}(y, \\hat{y}) = \\frac{TP}{TP+FP+FN} \\] <p>Ainsi, ces deux m\u00e9triques v\u00e9rifient l'in\u00e9galit\u00e9 suivante.</p> \\[     \\frac{\\text{Dice}}{2} \\leq \\text{IoU} \\leq \\text{Dice} \\] <p>On a l'\u00e9galit\u00e9 dans deux cas : si le masque \\(y\\) et la pr\u00e9diction \\(\\hat{y}\\) on une parfaite concordance, alors \\(\\text{Dice}=\\text{IoU}=1\\), et si le masque \\(y\\) et la pr\u00e9diction \\(\\hat{y}\\) sont compl\u00e8tement disjoints, on a alors \\(\\text{Dice}=\\text{IoU}=0\\).</p> <p>On a en fait une relation plus profonde entre ces deux m\u00e9triques. Pour un masque \\(y\\) fix\u00e9, ces 2 m\u00e9triques sont toujours positivement corr\u00e9l\u00e9es. En d'autres termes, si le classifieur A est meilleur que le classifieur B pour \\(y\\) par rapport \u00e0 l'une des deux m\u00e9triques, alors il est aussi meilleur pour \\(y\\) pour l'autre m\u00e9trique.</p> <p>Doit-on pour autant en conclure que ces 2 m\u00e9triques sont \u00e9quivalentes ? Pas n\u00e9cessairemnet.</p> <p>La question est de savoir comment se comporte ces m\u00e9triques lorsque l'on prend en compte la moyenne sur un ensemble d'observations (typiquement un minibatch). La diff\u00e9rence \u00e9merge lorsque l'on essaye de quantifier \u00e0 quel point le classifieur B est moins bon que le classifieur A pour tout observation donn\u00e9e.</p> <p>En g\u00e9n\u00e9ral, \\(\\text{IoU}\\) p\u00e9nalise plus les observations mal classifi\u00e9es que \\(\\text{Dice}\\).</p>  <p>Remarque</p> <p>Si une image ne comporte qu'un seul pixel d'une certaine classe, que le classifieur le d\u00e9tecte ainsi qu'un autre pixel, alors</p> \\[     \\mathrm{Dice} = \\frac{2}{3} \\] \\[     \\mathrm{IoU} = \\frac{1}{2} \\] <p>Ainsi, m\u00eame si la pr\u00e9cision n'est que de \\(50 \\%\\) le score \\(\\text{Dice}\\) est plus \u00e9lev\u00e9.</p>  <p>Sur le long terme, les deux m\u00e9triques peuvent se r\u00e9sumer sous la forme suivante. La m\u00e9trique \\(\\text{Dice}\\) donne la performance moyenne d'un classifieur, alors que la m\u00e9trique \\(\\text{IoU}\\) donne la performance dans le pire des cas.</p> <p>Cependant, les deux m\u00e9triques surestiment l'importance des r\u00e9gions avec peu ou pas de pixels d'une classe d\u00e9tectable. On verra dans la partie TP, il est relativement facile d'avoir une m\u00e9trique \\(\\text{IoU} \\geq 0,9\\) alors que le mod\u00e8le n'a toujours pas appris les r\u00e9gions du masques \\(y\\).</p>"},{"location":"deep_learning/module6/module6/#les-architectures-considerees","title":"Les architectures consid\u00e9r\u00e9es","text":""},{"location":"deep_learning/module6/module6/#unet","title":"Unet","text":"<p>U-Net: Convolutional Networks for Biomedical Image Segmentation</p>"},{"location":"deep_learning/module6/module6/#tiramisu","title":"Tiramisu","text":""},{"location":"deep_learning/module7/Module7_2/","title":"Pratique","text":"(function() {   function addWidgetsRenderer() {     var requireJsScript = document.createElement('script');     requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js';      var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]');     var jupyterWidgetsScript = document.createElement('script');     var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js';     var widgetState;      // Fallback for older version:     try {       widgetState = mimeElement &amp;&amp; JSON.parse(mimeElement.innerHTML);        if (widgetState &amp;&amp; (widgetState.version_major &lt; 2 || !widgetState.version_major)) {         widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js';       }     } catch(e) {}      jupyterWidgetsScript.src = widgetRendererSrc;      document.body.appendChild(requireJsScript);     document.body.appendChild(jupyterWidgetsScript);   }    document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }());"},{"location":"deep_learning/module7/Module7_2/#tp-module-7-les-modeles-generateurs","title":"TP Module 7 : Les mod\u00e8les g\u00e9n\u00e9rateurs","text":"<pre><code>import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import ReLU\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import Conv2DTranspose\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Add\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import UpSampling2D\n\n\nprint(tf.__version__)\nprint(keras.__version__)\n\nimport random\nimport os\nimport numpy as np\n\n# freeze de l'al\u00e9atoire, pour avoir des exp\u00e9riences reproductibles.\nRANDOM_SEED = 42\n\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\ntf.random.set_seed(RANDOM_SEED)\n</code></pre>      <pre>\n<code>2.2.0\n2.3.0-tf\n</code>\n</pre>        <pre><code>!nvidia-smi\n</code></pre>      <pre>\n<code>Thu May 28 12:31:23 2020       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   36C    P8     7W /  75W |      0MiB /  7611MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code>\n</pre>"},{"location":"deep_learning/module7/Module7_2/#import-data","title":"Import Data","text":"<pre><code>(x_train, _), (x_test, _) = keras.datasets.cifar10.load_data()\ncifar = np.concatenate([x_train, x_test], axis=0)\ncifar = (cifar.astype(\"float32\")-127.5)/ 127.5\ncifar.shape\n</code></pre>      <pre>\n<code>(60000, 32, 32, 3)</code>\n</pre>        <pre><code>ds = tf.data.Dataset.from_tensor_slices(cifar)\nds = ds.shuffle(buffer_size=60000)\nds = ds.batch(256)\nds = ds.prefetch(1)\n</code></pre>     <pre><code>for item in ds.take(1):\n  print(item.shape)\n</code></pre>      <pre>\n<code>(256, 32, 32, 3)\n</code>\n</pre>        <pre><code>!mkdir data_faces &amp;&amp; wget https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip \n</code></pre>      <pre>\n<code>--2020-05-27 18:11:41--  https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip\nResolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 52.219.116.41\nConnecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|52.219.116.41|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1443490838 (1.3G) [application/zip]\nSaving to: \u2018celeba.zip\u2019\n\nceleba.zip          100%[===================&gt;]   1.34G  16.5MB/s    in 86s     \n\n2020-05-27 18:13:08 (16.0 MB/s) - \u2018celeba.zip\u2019 saved [1443490838/1443490838]\n\n</code>\n</pre>        <pre><code>import zipfile\n\nwith zipfile.ZipFile(\"celeba.zip\",\"r\") as zip_ref:\n  zip_ref.extractall(\"data_faces/\")\nprint('Done')\n</code></pre>     <pre><code>root = 'data_faces/img_align_celeba'\nimg_list = os.listdir(root)\nprint(len(img_list))\nimg_list.sort()\nrandom.shuffle(img_list)\nimg_list[0]\n</code></pre>      <pre>\n<code>202599\n</code>\n</pre>     <pre>\n<code>'020294.jpg'</code>\n</pre>        <pre><code>def parse_image(filename):\n    image = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(image)\n    #This will convert to float values in [0, 1]\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, [64, 64])\n    return image\n</code></pre>     <pre><code>import pathlib\nds = tf.data.Dataset.list_files(str(pathlib.Path(root)/'*.*'), seed=RANDOM_SEED)\nds = ds.map(parse_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\nds = ds.batch(256)\nds = ds.prefetch(1)\n</code></pre>     <pre><code>for item in ds.take(1):\n  print(item.shape)\n</code></pre>      <pre>\n<code>(256, 64, 64, 3)\n</code>\n</pre>"},{"location":"deep_learning/module7/Module7_2/#vae","title":"VAE","text":""},{"location":"deep_learning/module7/Module7_2/#blocs-de-base","title":"Blocs de base","text":"<pre><code>def conv_bn_relu(tensor, filters, kernel_size, strides):\n\n  x = Conv2D(filters,\n             kernel_size,\n             strides,\n             padding='same',\n             kernel_initializer='he_normal')(tensor)\n  x = BatchNormalization()(x)\n  x = ReLU()(x)\n\n  return x\n\ndef conv_bn_lrelu(tensor, filters, kernel_size, strides):\n\n  x = Conv2D(filters,\n             kernel_size,\n             strides,\n             padding='same',\n             kernel_initializer='he_normal')(tensor)\n  x = BatchNormalization()(x)\n  x = LeakyReLU()(x)\n\n  return x\n\ndef convT_bn_relu(tensor, filters, kernel_size, strides):\n\n  x = Conv2DTranspose(filters,\n                      kernel_size,\n                      strides,\n                      padding='same',\n                      kernel_initializer='he_normal')(tensor)\n  x = BatchNormalization()(x)\n  x = ReLU()(x)\n\n  return x\n</code></pre>"},{"location":"deep_learning/module7/Module7_2/#encodeur","title":"Encodeur","text":"<pre><code>class Sampling(tf.keras.layers.Layer):\n    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding.\"\"\"\n\n    def call(self, inputs):\n        z_mean, z_log_var = inputs\n        batch = tf.shape(z_mean)[0]\n        dim = tf.shape(z_mean)[1]\n        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n</code></pre>      \\[\\begin{aligned} \\mathbf{z} &amp;\\sim q_\\phi(\\mathbf{z}\\vert\\mathbf{x}^{(i)}) = \\mathcal{N}(\\mathbf{z}; \\boldsymbol{\\mu}^{(i)}, \\boldsymbol{\\sigma}^{2(i)}\\boldsymbol{I}) &amp; \\\\ \\mathbf{z} &amp;= \\boldsymbol{\\mu} + \\boldsymbol{\\sigma} \\odot \\boldsymbol{\\epsilon} \\text{, where } \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\boldsymbol{I}) &amp; \\scriptstyle{\\text{ Astuce de Reparam\u00e9trisation.}} \\end{aligned}\\]      <pre><code>latent_dim = 32\ndim = 64\nencoder_inputs = Input(shape=(64, 64, 3))\n# partie conv2d pour extraire les features, le strides de 2 permet de r\u00e9duire la taille des features maps sans faire appel au pooling, qui n'est pas\n#disponible dans la partie decodeur.\n\n\nx = conv_bn_relu(encoder_inputs, dim, 4, 2)\nx = conv_bn_relu(x, dim * 2, 4, 2)\nx = conv_bn_relu(x, dim * 4, 4, 2)\nx = conv_bn_relu(x, dim * 8, 4, 2)\n\n#sortie de la partie convolutive vers l'espace latent\nx = Flatten()(x)\nx = Dense(256, activation=\"relu\")(x)\n\n#calu de mu et log_var\nz_mean = Dense(latent_dim, name=\"z_mean\")(x)\nz_log_var = Dense(latent_dim, name=\"z_log_var\")(x)\n\n#ehcnatillonage de z dans l'espace latent\nz = Sampling()([z_mean, z_log_var])\n\nencoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\nencoder.summary()\n</code></pre>      <pre>\n<code>Model: \"encoder\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n__________________________________________________________________________________________________\nconv2d (Conv2D)                 (None, 32, 32, 64)   3136        input_1[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization (BatchNorma (None, 32, 32, 64)   256         conv2d[0][0]                     \n__________________________________________________________________________________________________\nre_lu (ReLU)                    (None, 32, 32, 64)   0           batch_normalization[0][0]        \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 16, 16, 128)  131200      re_lu[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_1 (BatchNor (None, 16, 16, 128)  512         conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nre_lu_1 (ReLU)                  (None, 16, 16, 128)  0           batch_normalization_1[0][0]      \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 8, 8, 256)    524544      re_lu_1[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_2 (BatchNor (None, 8, 8, 256)    1024        conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nre_lu_2 (ReLU)                  (None, 8, 8, 256)    0           batch_normalization_2[0][0]      \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 4, 4, 512)    2097664     re_lu_2[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_3 (BatchNor (None, 4, 4, 512)    2048        conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nre_lu_3 (ReLU)                  (None, 4, 4, 512)    0           batch_normalization_3[0][0]      \n__________________________________________________________________________________________________\nflatten (Flatten)               (None, 8192)         0           re_lu_3[0][0]                    \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 256)          2097408     flatten[0][0]                    \n__________________________________________________________________________________________________\nz_mean (Dense)                  (None, 32)           8224        dense[0][0]                      \n__________________________________________________________________________________________________\nz_log_var (Dense)               (None, 32)           8224        dense[0][0]                      \n__________________________________________________________________________________________________\nsampling (Sampling)             (None, 32)           0           z_mean[0][0]                     \n                                                                 z_log_var[0][0]                  \n==================================================================================================\nTotal params: 4,874,240\nTrainable params: 4,872,320\nNon-trainable params: 1,920\n__________________________________________________________________________________________________\n</code>\n</pre>"},{"location":"deep_learning/module7/Module7_2/#decodeur","title":"D\u00e9codeur","text":"<pre><code>latent_inputs = keras.Input(shape=(latent_dim,))\n\n\nx = Dense(4 * 4 * 512, activation=\"relu\")(latent_inputs)\nx = Reshape((4, 4, 512))(x)\n\nx = convT_bn_relu(x, dim*8, 4, 2)\nx = convT_bn_relu(x, dim*4, 4, 2)\nx = convT_bn_relu(x, dim*2, 4, 2)\nx = convT_bn_relu(x, dim*1, 4, 2)\n\n# l'activation sigmoide est ici pour avoir des pixels \u00e0 valeur dans l'intervalle [0,1], comme une image RGB normalis\u00e9e\ndecoder_outputs = Conv2DTranspose(3, 3, activation=\"sigmoid\", padding=\"same\")(x)\n\ndecoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\ndecoder.summary()\n</code></pre>      <pre>\n<code>Model: \"decoder\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 32)]              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 8192)              270336    \n_________________________________________________________________\nreshape (Reshape)            (None, 4, 4, 512)         0         \n_________________________________________________________________\nconv2d_transpose (Conv2DTran (None, 8, 8, 512)         4194816   \n_________________________________________________________________\nbatch_normalization_4 (Batch (None, 8, 8, 512)         2048      \n_________________________________________________________________\nre_lu_4 (ReLU)               (None, 8, 8, 512)         0         \n_________________________________________________________________\nconv2d_transpose_1 (Conv2DTr (None, 16, 16, 256)       2097408   \n_________________________________________________________________\nbatch_normalization_5 (Batch (None, 16, 16, 256)       1024      \n_________________________________________________________________\nre_lu_5 (ReLU)               (None, 16, 16, 256)       0         \n_________________________________________________________________\nconv2d_transpose_2 (Conv2DTr (None, 32, 32, 128)       524416    \n_________________________________________________________________\nbatch_normalization_6 (Batch (None, 32, 32, 128)       512       \n_________________________________________________________________\nre_lu_6 (ReLU)               (None, 32, 32, 128)       0         \n_________________________________________________________________\nconv2d_transpose_3 (Conv2DTr (None, 64, 64, 64)        131136    \n_________________________________________________________________\nbatch_normalization_7 (Batch (None, 64, 64, 64)        256       \n_________________________________________________________________\nre_lu_7 (ReLU)               (None, 64, 64, 64)        0         \n_________________________________________________________________\nconv2d_transpose_4 (Conv2DTr (None, 64, 64, 3)         1731      \n=================================================================\nTotal params: 7,223,683\nTrainable params: 7,221,763\nNon-trainable params: 1,920\n_________________________________________________________________\n</code>\n</pre>"},{"location":"deep_learning/module7/Module7_2/#custom-class","title":"Custom Class","text":"<pre><code>class VAE(keras.Model):\n    def __init__(self, encoder, decoder, beta, **kwargs):\n        super(VAE, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.beta = beta\n\n    def train_step(self, data):\n        #print(type(data))\n        #data = data[0]\n        #print(data.numpy().shape)\n        with tf.GradientTape() as tape:\n            z_mean, z_log_var, z = encoder(data)\n            reconstruction = decoder(z)\n\n            reconstruction_loss = tf.reduce_mean(\n                keras.losses.binary_crossentropy(data, reconstruction)\n            )\n            reconstruction_loss *= 32*32\n\n            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n            kl_loss = tf.reduce_mean(kl_loss)\n            kl_loss *= -0.5\n\n            beta_kl_loss = self.beta*kl_loss\n\n            total_loss = reconstruction_loss + beta_kl_loss\n\n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        return {\n            \"loss\" : total_loss,\n            \"reconstruction_loss\" : reconstruction_loss,\n            \"kl_loss\" : kl_loss,\n            \"beta_kl\" : beta_kl_loss,\n        }\n\n    def call(self, input):\n      x = self.encoder(input)\n      x = self.decoder(x)\n\n      return x\n</code></pre>      <p><pre><code>def train_step(self, data):\n        data = data[0]\n        with tf.GradientTape() as tape:\n            z_mean, z_log_var, z = encoder(data)\n            reconstruction = decoder(z)\n</code></pre> <pre><code>            reconstruction_loss = tf.reduce_mean(\n                keras.losses.binary_crossentropy(data, reconstruction)\n            )\n            reconstruction_loss *= 32 * 32\n</code></pre> \\(\\(- \\mathbb{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z}\\vert\\mathbf{x})} \\log p_\\theta(\\mathbf{x}\\vert\\mathbf{z})\\)\\)</p> <p><pre><code>            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n            kl_loss = tf.reduce_mean(kl_loss)\n            kl_loss *= -0.5\n</code></pre> \\(\\(D_\\text{KL}(q_\\phi(\\mathbf{z}\\vert\\mathbf{x})\\|p_\\theta(\\mathbf{z}))\\)\\) <pre><code>            beta_kl_loss = beta*kl_loss\n</code></pre> \\(\\(\\beta D_\\text{KL}(q_\\phi(\\mathbf{z}\\vert\\mathbf{x})\\|p_\\theta(\\mathbf{z}))\\)\\) <pre><code>            total_loss = reconstruction_loss + beta_kl_loss\n</code></pre> \\(\\(L_\\text{BETAVAE}(\\phi, \\beta) = - \\mathbb{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z}\\vert\\mathbf{x})} \\log p_\\theta(\\mathbf{x}\\vert\\mathbf{z}) + \\beta D_\\text{KL}(q_\\phi(\\mathbf{z}\\vert\\mathbf{x})\\|p_\\theta(\\mathbf{z}))\\)\\)</p>      <pre><code>vae = VAE(encoder, decoder, 4)\nvae.compile(optimizer=keras.optimizers.Adam(0.0001))\n</code></pre>     <pre><code>vae.fit(ds,\n        epochs=20)\n</code></pre>      <pre>\n<code>Epoch 1/20\n792/792 [==============================] - 384s 485ms/step - loss: 574.6848 - reconstruction_loss: 568.6266 - kl_loss: 1.5145 - beta_kl: 6.0582\nEpoch 2/20\n792/792 [==============================] - 402s 507ms/step - loss: 541.8736 - reconstruction_loss: 534.0956 - kl_loss: 1.9445 - beta_kl: 7.7780\nEpoch 3/20\n792/792 [==============================] - 402s 508ms/step - loss: 537.4665 - reconstruction_loss: 529.5588 - kl_loss: 1.9769 - beta_kl: 7.9077\nEpoch 4/20\n792/792 [==============================] - 401s 506ms/step - loss: 535.1494 - reconstruction_loss: 527.2075 - kl_loss: 1.9855 - beta_kl: 7.9419\nEpoch 5/20\n792/792 [==============================] - 401s 507ms/step - loss: 533.7810 - reconstruction_loss: 525.8899 - kl_loss: 1.9728 - beta_kl: 7.8911\nEpoch 6/20\n792/792 [==============================] - 401s 507ms/step - loss: 532.8291 - reconstruction_loss: 525.0067 - kl_loss: 1.9556 - beta_kl: 7.8224\nEpoch 7/20\n792/792 [==============================] - 401s 507ms/step - loss: 532.0754 - reconstruction_loss: 524.2798 - kl_loss: 1.9489 - beta_kl: 7.7957\nEpoch 8/20\n792/792 [==============================] - 402s 507ms/step - loss: 531.3555 - reconstruction_loss: 523.5367 - kl_loss: 1.9547 - beta_kl: 7.8189\nEpoch 9/20\n792/792 [==============================] - 401s 506ms/step - loss: 530.7033 - reconstruction_loss: 522.8648 - kl_loss: 1.9596 - beta_kl: 7.8385\nEpoch 10/20\n792/792 [==============================] - 401s 507ms/step - loss: 530.1680 - reconstruction_loss: 522.3529 - kl_loss: 1.9538 - beta_kl: 7.8151\nEpoch 11/20\n792/792 [==============================] - 402s 507ms/step - loss: 529.7750 - reconstruction_loss: 521.9918 - kl_loss: 1.9458 - beta_kl: 7.7832\nEpoch 12/20\n792/792 [==============================] - 401s 506ms/step - loss: 529.4123 - reconstruction_loss: 521.6518 - kl_loss: 1.9401 - beta_kl: 7.7606\nEpoch 13/20\n792/792 [==============================] - 401s 506ms/step - loss: 529.1230 - reconstruction_loss: 521.3838 - kl_loss: 1.9348 - beta_kl: 7.7392\nEpoch 14/20\n792/792 [==============================] - 401s 506ms/step - loss: 528.8762 - reconstruction_loss: 521.1514 - kl_loss: 1.9312 - beta_kl: 7.7248\nEpoch 15/20\n792/792 [==============================] - 401s 506ms/step - loss: 528.6445 - reconstruction_loss: 520.9294 - kl_loss: 1.9288 - beta_kl: 7.7151\nEpoch 16/20\n792/792 [==============================] - 401s 506ms/step - loss: 528.4214 - reconstruction_loss: 520.7149 - kl_loss: 1.9266 - beta_kl: 7.7065\nEpoch 17/20\n792/792 [==============================] - 401s 506ms/step - loss: 528.2218 - reconstruction_loss: 520.5209 - kl_loss: 1.9252 - beta_kl: 7.7010\nEpoch 18/20\n792/792 [==============================] - 401s 506ms/step - loss: 528.0136 - reconstruction_loss: 520.3204 - kl_loss: 1.9233 - beta_kl: 7.6931\nEpoch 19/20\n792/792 [==============================] - 401s 506ms/step - loss: 527.8560 - reconstruction_loss: 520.1635 - kl_loss: 1.9231 - beta_kl: 7.6925\nEpoch 20/20\n792/792 [==============================] - 400s 506ms/step - loss: 527.6743 - reconstruction_loss: 519.9862 - kl_loss: 1.9220 - beta_kl: 7.6881\n</code>\n</pre>     <pre>\n<code>&lt;tensorflow.python.keras.callbacks.History at 0x7ff760107be0&gt;</code>\n</pre>        <pre><code># display images generated from randomly sampled latent vector\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nn = 15\nimg_size = 64\nfigure = np.zeros((img_size * n, img_size * n, 3))\n\nfor i in range(n):\n    for j in range(n):\n        z_sample = np.array([np.random.normal(0, 1 ,size=latent_dim)])\n        x_decoded = decoder.predict(z_sample)\n        img = x_decoded[0].reshape(img_size, img_size, 3)\n        figure[i * img_size: (i + 1) * img_size,j * img_size: (j + 1) * img_size] = img\n\n\nplt.figure(figsize=(20, 20))\nplt.imshow(figure, cmap='Greys_r')\nplt.show()\n</code></pre>              <pre><code>encoder.save('beta_encoder_celebA_20epochs.h5')\ndecoder.save('beta_decoder_celebA_20epochs.h5')\n</code></pre>     <pre><code>img = cifar[4500, :, :, :]\nplt.imshow(img)\nplt.show()\n</code></pre>              <pre><code>img = tf.reshape(img, (-1,32,32,3))\nimg_encode = encoder.predict(img)\nimg_decode = decoder.predict(img_encode)\n</code></pre>     <pre><code>img_decode = tf.reshape(img_decode, (-1,32,32,3))\nimg_dec = img_decode[0, :, :, :]\nplt.imshow(img_dec)\nplt.show()\n</code></pre>              <pre><code>z_sample = np.array([np.random.normal(0, 1 ,size=latent_dim)])\nimg_decode = decoder.predict(z_sample)\nimg_decode = tf.reshape(img_decode, (-1,64,64,3))\nimg_dec = img_decode[0, :, :, :]\nplt.imshow(img_dec)\nplt.show()\n</code></pre>"},{"location":"deep_learning/module7/Module7_2/#gan","title":"GAN","text":"<pre><code>IMG_SHAPE = (32, 32, 3)\nnoise_dim = 128\n</code></pre>"},{"location":"deep_learning/module7/Module7_2/#blocs-de-base_1","title":"Blocs de base","text":"<pre><code>def bn_relu_conv(tensor, filters, kernel_size, strides, use_bn = True):\n\n  if use_bn:\n    x = BatchNormalization()(tensor)\n    x = ReLU()(x)\n  else:\n    x = ReLU()(tensor)\n\n  x = Conv2D(filters=filters,\n             kernel_size=kernel_size,\n             strides=strides,\n             padding='same',\n             kernel_initializer='he_uniform',\n             use_bias=False)(x)\n\n  return x\n\ndef conv_block(tensor, filters, use_bn, sampling = None):\n\n  x = bn_relu_conv(tensor,\n                   filters,\n                   kernel_size = 3,\n                   strides = 1,\n                   use_bn = use_bn)\n\n  if sampling == 'up':\n    x = UpSampling2D()(x)\n    x = bn_relu_conv(x,\n                     filters,\n                     kernel_size = 3,\n                     strides = 1,\n                     use_bn = use_bn)\n\n    shortcut = UpSampling2D()(tensor)\n    shortcut = Conv2D(filters = filters,\n                      kernel_size = 1,\n                      strides = 1,\n                      kernel_initializer='he_uniform')(shortcut)\n    x = Add()([x, shortcut])\n\n  elif sampling == 'down':\n    x = bn_relu_conv(x,\n                     filters,\n                     kernel_size = 3,\n                     strides = 1,\n                     use_bn = use_bn)\n    x = AveragePooling2D()(x)\n\n    shortcut = Conv2D(filters = filters,\n                      kernel_size = 1,\n                      strides = 1,\n                      kernel_initializer='he_uniform')(tensor)\n    shortcut = AveragePooling2D()(shortcut)\n\n    x = Add()([x, shortcut])\n\n  elif sampling == None:\n    x = bn_relu_conv(x,\n                     filters,\n                     kernel_size = 3,\n                     strides = 1,\n                     use_bn = use_bn)\n\n    x = Add()([x, tensor])\n  else:\n    raise Exception('invalid sampling value')\n\n  return x\n</code></pre>"},{"location":"deep_learning/module7/Module7_2/#critique","title":"Critique","text":"<pre><code>def get_discriminator_model():\n    img_input = Input(shape=IMG_SHAPE)\n\n    x = conv_block(img_input,\n                   filters = 128,\n                   use_bn = False,\n                   sampling = 'down')\n    x = conv_block(x,\n                   filters = 128,\n                   use_bn = False,\n                   sampling = 'down')\n    x = conv_block(x,\n                   filters = 128,\n                   use_bn = False,\n                   sampling = None)\n    x = conv_block(x,\n                   filters = 128,\n                   use_bn = False,\n                   sampling = None)\n\n    x = ReLU()(x)\n    x = AveragePooling2D()(x)\n    x = Flatten()(x)\n    x = Dense(1)(x)\n\n    d_model = keras.models.Model(img_input, x, name=\"discriminator\")\n    return d_model\n\n\nd_model = get_discriminator_model()\nd_model.summary()\n</code></pre>      <pre>\n<code>Model: \"discriminator\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_25 (InputLayer)           [(None, 32, 32, 3)]  0                                            \n__________________________________________________________________________________________________\nre_lu_131 (ReLU)                (None, 32, 32, 3)    0           input_25[0][0]                   \n__________________________________________________________________________________________________\nconv2d_145 (Conv2D)             (None, 32, 32, 128)  3456        re_lu_131[0][0]                  \n__________________________________________________________________________________________________\nre_lu_132 (ReLU)                (None, 32, 32, 128)  0           conv2d_145[0][0]                 \n__________________________________________________________________________________________________\nconv2d_146 (Conv2D)             (None, 32, 32, 128)  147456      re_lu_132[0][0]                  \n__________________________________________________________________________________________________\nconv2d_147 (Conv2D)             (None, 32, 32, 128)  512         input_25[0][0]                   \n__________________________________________________________________________________________________\naverage_pooling2d_52 (AveragePo (None, 16, 16, 128)  0           conv2d_146[0][0]                 \n__________________________________________________________________________________________________\naverage_pooling2d_53 (AveragePo (None, 16, 16, 128)  0           conv2d_147[0][0]                 \n__________________________________________________________________________________________________\nadd_46 (Add)                    (None, 16, 16, 128)  0           average_pooling2d_52[0][0]       \n                                                                 average_pooling2d_53[0][0]       \n__________________________________________________________________________________________________\nre_lu_133 (ReLU)                (None, 16, 16, 128)  0           add_46[0][0]                     \n__________________________________________________________________________________________________\nconv2d_148 (Conv2D)             (None, 16, 16, 128)  147456      re_lu_133[0][0]                  \n__________________________________________________________________________________________________\nre_lu_134 (ReLU)                (None, 16, 16, 128)  0           conv2d_148[0][0]                 \n__________________________________________________________________________________________________\nconv2d_149 (Conv2D)             (None, 16, 16, 128)  147456      re_lu_134[0][0]                  \n__________________________________________________________________________________________________\nconv2d_150 (Conv2D)             (None, 16, 16, 128)  16512       add_46[0][0]                     \n__________________________________________________________________________________________________\naverage_pooling2d_54 (AveragePo (None, 8, 8, 128)    0           conv2d_149[0][0]                 \n__________________________________________________________________________________________________\naverage_pooling2d_55 (AveragePo (None, 8, 8, 128)    0           conv2d_150[0][0]                 \n__________________________________________________________________________________________________\nadd_47 (Add)                    (None, 8, 8, 128)    0           average_pooling2d_54[0][0]       \n                                                                 average_pooling2d_55[0][0]       \n__________________________________________________________________________________________________\nre_lu_135 (ReLU)                (None, 8, 8, 128)    0           add_47[0][0]                     \n__________________________________________________________________________________________________\nconv2d_151 (Conv2D)             (None, 8, 8, 128)    147456      re_lu_135[0][0]                  \n__________________________________________________________________________________________________\nre_lu_136 (ReLU)                (None, 8, 8, 128)    0           conv2d_151[0][0]                 \n__________________________________________________________________________________________________\nconv2d_152 (Conv2D)             (None, 8, 8, 128)    147456      re_lu_136[0][0]                  \n__________________________________________________________________________________________________\nadd_48 (Add)                    (None, 8, 8, 128)    0           conv2d_152[0][0]                 \n                                                                 add_47[0][0]                     \n__________________________________________________________________________________________________\nre_lu_137 (ReLU)                (None, 8, 8, 128)    0           add_48[0][0]                     \n__________________________________________________________________________________________________\nconv2d_153 (Conv2D)             (None, 8, 8, 128)    147456      re_lu_137[0][0]                  \n__________________________________________________________________________________________________\nre_lu_138 (ReLU)                (None, 8, 8, 128)    0           conv2d_153[0][0]                 \n__________________________________________________________________________________________________\nconv2d_154 (Conv2D)             (None, 8, 8, 128)    147456      re_lu_138[0][0]                  \n__________________________________________________________________________________________________\nadd_49 (Add)                    (None, 8, 8, 128)    0           conv2d_154[0][0]                 \n                                                                 add_48[0][0]                     \n__________________________________________________________________________________________________\nre_lu_139 (ReLU)                (None, 8, 8, 128)    0           add_49[0][0]                     \n__________________________________________________________________________________________________\naverage_pooling2d_56 (AveragePo (None, 4, 4, 128)    0           re_lu_139[0][0]                  \n__________________________________________________________________________________________________\nflatten_3 (Flatten)             (None, 2048)         0           average_pooling2d_56[0][0]       \n__________________________________________________________________________________________________\ndense_7 (Dense)                 (None, 1)            2049        flatten_3[0][0]                  \n==================================================================================================\nTotal params: 1,054,721\nTrainable params: 1,054,721\nNon-trainable params: 0\n__________________________________________________________________________________________________\n</code>\n</pre>"},{"location":"deep_learning/module7/Module7_2/#generateur","title":"G\u00e9n\u00e9rateur","text":"<pre><code>def get_generator_model():\n    noise = Input(shape=(noise_dim,))\n\n    x = Dense(4*4*128, use_bias=False)(noise)\n\n    x = Reshape((4, 4, 128))(x)\n\n    x = conv_block(x,\n                   filters = 128,\n                   use_bn = True,\n                   sampling = 'up')\n    x = conv_block(x,\n                   filters = 128,\n                   use_bn = True,\n                   sampling = 'up')\n    x = conv_block(x,\n                   filters = 128,\n                   use_bn = True,\n                   sampling = 'up')\n\n    x = Conv2D(filters = 3,\n               kernel_size = 3,\n               padding='same',\n               kernel_initializer='he_uniform')(x)\n\n    x = Activation('tanh')(x)\n\n    g_model = keras.models.Model(noise, x, name=\"generator\")\n    return g_model\n\n\ng_model = get_generator_model()\ng_model.summary()\n</code></pre>      <pre>\n<code>Model: \"generator\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_29 (InputLayer)           [(None, 128)]        0                                            \n__________________________________________________________________________________________________\ndense_11 (Dense)                (None, 2048)         262144      input_29[0][0]                   \n__________________________________________________________________________________________________\nreshape_6 (Reshape)             (None, 4, 4, 128)    0           dense_11[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_51 (BatchNo (None, 4, 4, 128)    512         reshape_6[0][0]                  \n__________________________________________________________________________________________________\nre_lu_154 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_51[0][0]     \n__________________________________________________________________________________________________\nconv2d_177 (Conv2D)             (None, 4, 4, 128)    147456      re_lu_154[0][0]                  \n__________________________________________________________________________________________________\nup_sampling2d_18 (UpSampling2D) (None, 8, 8, 128)    0           conv2d_177[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_52 (BatchNo (None, 8, 8, 128)    512         up_sampling2d_18[0][0]           \n__________________________________________________________________________________________________\nre_lu_155 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_52[0][0]     \n__________________________________________________________________________________________________\nup_sampling2d_19 (UpSampling2D) (None, 8, 8, 128)    0           reshape_6[0][0]                  \n__________________________________________________________________________________________________\nconv2d_178 (Conv2D)             (None, 8, 8, 128)    147456      re_lu_155[0][0]                  \n__________________________________________________________________________________________________\nconv2d_179 (Conv2D)             (None, 8, 8, 128)    16512       up_sampling2d_19[0][0]           \n__________________________________________________________________________________________________\nadd_57 (Add)                    (None, 8, 8, 128)    0           conv2d_178[0][0]                 \n                                                                 conv2d_179[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_53 (BatchNo (None, 8, 8, 128)    512         add_57[0][0]                     \n__________________________________________________________________________________________________\nre_lu_156 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_53[0][0]     \n__________________________________________________________________________________________________\nconv2d_180 (Conv2D)             (None, 8, 8, 128)    147456      re_lu_156[0][0]                  \n__________________________________________________________________________________________________\nup_sampling2d_20 (UpSampling2D) (None, 16, 16, 128)  0           conv2d_180[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_54 (BatchNo (None, 16, 16, 128)  512         up_sampling2d_20[0][0]           \n__________________________________________________________________________________________________\nre_lu_157 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_54[0][0]     \n__________________________________________________________________________________________________\nup_sampling2d_21 (UpSampling2D) (None, 16, 16, 128)  0           add_57[0][0]                     \n__________________________________________________________________________________________________\nconv2d_181 (Conv2D)             (None, 16, 16, 128)  147456      re_lu_157[0][0]                  \n__________________________________________________________________________________________________\nconv2d_182 (Conv2D)             (None, 16, 16, 128)  16512       up_sampling2d_21[0][0]           \n__________________________________________________________________________________________________\nadd_58 (Add)                    (None, 16, 16, 128)  0           conv2d_181[0][0]                 \n                                                                 conv2d_182[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_55 (BatchNo (None, 16, 16, 128)  512         add_58[0][0]                     \n__________________________________________________________________________________________________\nre_lu_158 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_55[0][0]     \n__________________________________________________________________________________________________\nconv2d_183 (Conv2D)             (None, 16, 16, 128)  147456      re_lu_158[0][0]                  \n__________________________________________________________________________________________________\nup_sampling2d_22 (UpSampling2D) (None, 32, 32, 128)  0           conv2d_183[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_56 (BatchNo (None, 32, 32, 128)  512         up_sampling2d_22[0][0]           \n__________________________________________________________________________________________________\nre_lu_159 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_56[0][0]     \n__________________________________________________________________________________________________\nup_sampling2d_23 (UpSampling2D) (None, 32, 32, 128)  0           add_58[0][0]                     \n__________________________________________________________________________________________________\nconv2d_184 (Conv2D)             (None, 32, 32, 128)  147456      re_lu_159[0][0]                  \n__________________________________________________________________________________________________\nconv2d_185 (Conv2D)             (None, 32, 32, 128)  16512       up_sampling2d_23[0][0]           \n__________________________________________________________________________________________________\nadd_59 (Add)                    (None, 32, 32, 128)  0           conv2d_184[0][0]                 \n                                                                 conv2d_185[0][0]                 \n__________________________________________________________________________________________________\nconv2d_186 (Conv2D)             (None, 32, 32, 3)    3459        add_59[0][0]                     \n__________________________________________________________________________________________________\nactivation (Activation)         (None, 32, 32, 3)    0           conv2d_186[0][0]                 \n==================================================================================================\nTotal params: 1,202,947\nTrainable params: 1,201,411\nNon-trainable params: 1,536\n__________________________________________________________________________________________________\n</code>\n</pre>"},{"location":"deep_learning/module7/Module7_2/#custom-class_1","title":"Custom Class","text":"<pre><code>class WGAN(keras.Model):\n    def __init__(\n        self,\n        discriminator,\n        generator,\n        latent_dim,\n        discriminator_extra_steps=3,\n        gp_weight=10.0,\n    ):\n        super(WGAN, self).__init__()\n        self.discriminator = discriminator\n        self.generator = generator\n        self.latent_dim = latent_dim\n        self.d_steps = discriminator_extra_steps\n        self.gp_weight = gp_weight\n\n    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n        super(WGAN, self).compile()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.d_loss_fn = d_loss_fn\n        self.g_loss_fn = g_loss_fn\n\n    def gradient_penalty(self, batch_size, real_images, fake_images):\n        \"\"\" Calculates the gradient penalty.\n\n        This loss is calculated on an interpolated image\n        and added to the discriminator loss.\n        \"\"\"\n        # get the interplated image\n        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n        diff = fake_images - real_images\n        interpolated = real_images + alpha * diff\n\n        with tf.GradientTape() as gp_tape:\n            gp_tape.watch(interpolated)\n            pred = self.discriminator(interpolated, training=True)\n\n        grads = gp_tape.gradient(pred, [interpolated])[0]\n        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n        gp = tf.reduce_mean((norm - 1.0) ** 2)\n        return gp\n\n    def train_step(self, real_images):\n        if isinstance(real_images, tuple):\n            real_images = real_images[0]\n\n        # Get the batch size\n        batch_size = tf.shape(real_images)[0]\n\n        for i in range(self.d_steps):\n            # Get the latent vector\n            random_latent_vectors = tf.random.normal(\n                shape=(batch_size, self.latent_dim)\n            )\n            with tf.GradientTape() as tape:\n                fake_images = self.generator(random_latent_vectors, training=True)\n                fake_logits = self.discriminator(fake_images, training=True)\n                real_logits = self.discriminator(real_images, training=True)\n\n                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n                gp = self.gradient_penalty(batch_size, real_images, fake_images)\n                d_loss = d_cost + gp * self.gp_weight\n\n            # Update the weights\n            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n            self.d_optimizer.apply_gradients(\n                zip(d_gradient, self.discriminator.trainable_variables)\n            )\n\n        # Train the generator now.\n        # Get the latent vector\n        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n\n        with tf.GradientTape() as tape:\n            generated_images = self.generator(random_latent_vectors, training=True)\n            gen_img_logits = self.discriminator(generated_images, training=True)\n            g_loss = self.g_loss_fn(gen_img_logits)\n\n        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n        # Update the weights\n        self.g_optimizer.apply_gradients(\n            zip(gen_gradient, self.generator.trainable_variables)\n        )\n        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n</code></pre>"},{"location":"deep_learning/module7/Module7_2/#details","title":"D\u00e9tails","text":""},{"location":"deep_learning/module7/Module7_2/#init","title":"Init","text":"<p>Ici, rien de particuliers, on d\u00e9finit les variables de notre classe.</p> <pre><code>class WGAN(keras.Model):\n    def __init__(\n        self,\n        discriminator,\n        generator,\n        latent_dim,\n        discriminator_extra_steps=3,\n        gp_weight=10.0,\n    ):\n        super(WGAN, self).__init__()\n        self.discriminator = discriminator\n        self.generator = generator\n        self.latent_dim = latent_dim\n        self.d_steps = discriminator_extra_steps\n        self.gp_weight = gp_weight\n</code></pre>"},{"location":"deep_learning/module7/Module7_2/#compile","title":"Compile","text":"<p>Comme l'on a deux r\u00e9seaux qui s'entra\u00eenent de fa\u00e7on s\u00e9par\u00e9e, on a deux optimiseurs et deux fonctions de pertes. On doit donc modifier la m\u00e9thode <code>.compile()</code>. <pre><code>    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n        super(WGAN, self).compile()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.d_loss_fn = d_loss_fn\n        self.g_loss_fn = g_loss_fn\n</code></pre></p>"},{"location":"deep_learning/module7/Module7_2/#gradient-penalty","title":"Gradient penalty","text":"<p>La p\u00e9nalit\u00e9 du gradient est un des points cl\u00e9 faisant que les GAN de Wasserstein s'entra\u00eenent de fa\u00e7on correcte sans mode collapse.</p>       <p><pre><code>    def gradient_penalty(self, batch_size, real_images, fake_images):\n</code></pre> La p\u00e9nalit\u00e9 du gradient se calcule sur une observation interpol\u00e9e, ie une observation que l'on obtient en faisant une combinaison lin\u00e9aire de \\(x\\) et \\(\\tilde{x}\\).</p> \\[\\hat{x} := \\tilde{x} + \\varepsilon(x - \\tilde{x})\\] <p>Avec \\(\\tilde{x} = G(z)\\), \\(x\\) une observation du minibatch,  et \\(\\varepsilon \\sim \\mathcal{N}(0, 1)\\).</p> <p><pre><code>        # interpolation de l'image\n        epsilon = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n        diff = fake_images - real_images\n        interpolated = real_images + epsilon * diff\n</code></pre> Le but de la fonction <code>gradient_penalty</code> est alors de calculer la moyenne suivante.</p> \\[\\mathbb{E}_{\\hat{x} \\sim \\mathbb{P}_{\\hat{x}}}[(||\\nabla_{\\hat{x}}D(\\hat{x})||_{2}-1)^{2}]\\] <p>Rappelons tout d'abord que nous travaillons sur des minibatchs, ie calculer \\(\\mathbb{E}_{\\hat{x} \\sim \\mathbb{P}_{\\hat{x}}}(-)\\) revient donc \u00e0 calculer l'estimation non biais\u00e9e correspondante sur le minibatch \\(M\\) en cours, c'est \u00e0 dire calculer la moyenne suivante.</p> \\[\\frac{1}{N}\\sum_{x \\in M} (||\\nabla_{\\hat{x}}D(\\hat{x})||_{2}-1)^{2}\\] <p>Donc, pour chaque observation \\(x\\) du minibatch \\(M\\) :   1. on l'interpole en \\(\\hat{x}\\),   2. on calcule le gradient \\(\\nabla_{\\hat{x}}D(\\hat{x})\\) par rapport \u00e0 \\(\\hat{x}\\),   3. On calcule la norme \\(L_{2}\\), \\(||\\nabla_{\\hat{x}}D(\\hat{x})||_{2}\\),   4. On fait la somme \\(\\sum_{x \\in M} (||\\nabla_{\\hat{x}}D(\\hat{x})||_{2}-1)^{2}\\),   5. On divise.</p> <p>Rappellons que pour un tenseur \\(T= (t_{i,j,k})_{i,j,k}\\) sur 3 axes (ici une image), sa norme \\(L_{2}\\) est d\u00e9finie de fa\u00e7on usuelle par la formule suivante. </p> \\[ ||T||_{2} := \\sqrt{\\sum_{i,j,k} t_{i,j,k}^{2} }\\] <p>Pour calculer un gradient on utilise <code>with tf.GradientTape() as gp_tape:</code> et on lui dit quel variable surveiller avec <code>gp_tape.watch(interpolated)</code>.</p> <pre><code>        with tf.GradientTape() as gp_tape:\n            gp_tape.watch(interpolated)\n            # 1. Get the discriminator output for this interpolated image.\n            pred = self.discriminator(interpolated, training=True)\n\n        # 2. Calcul du gradient par rapport \u00e0 l'image interpol\u00e9e.\n        grads = gp_tape.gradient(pred, [interpolated])[0]\n        # Calcul de la norme du gradient\n        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n        gp = tf.reduce_mean((norm - 1.0) ** 2)\n        return gp\n</code></pre> <p>Fonction enti\u00e8re :</p> <pre><code>    def gradient_penalty(self, batch_size, real_images, fake_images):\n        # interpolation de l'image\n        epsilon = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n        diff = fake_images - real_images\n        interpolated = real_images + epsilon * diff\n\n        with tf.GradientTape() as gp_tape:\n            gp_tape.watch(interpolated)\n            # 1. Get the discriminator output for this interpolated image.\n            pred = self.discriminator(interpolated, training=True)\n\n        # 2. Calcul du gradient par rapport \u00e0 l'image interpol\u00e9e.\n        grads = gp_tape.gradient(pred, [interpolated])[0]\n        # Calcul de la norme du gradient\n        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n        gp = tf.reduce_mean((norm - 1.0) ** 2)\n        return gp\n</code></pre>       <p>Pour l'instant, cette fonction n'est pas utilis\u00e9e, elle est utilis\u00e9e dans la section suivante.</p>"},{"location":"deep_learning/module7/Module7_2/#train-step","title":"Train step","text":"<p>Dans l'\u00e9tape d'entra\u00eenement, on fait les choses suivantes.</p> <p>Pour chaque minibatch, </p> <ol> <li>On entra\u00eene le g\u00e9n\u00e9rateur et on calcule la perte associ\u00e9e.</li> <li>On entra\u00eene le critique et on calcule la perte associ\u00e9e.</li> <li>On calcule la p\u00e9nalit\u00e9 du gradient.</li> <li>On multiplie cette p\u00e9nalit\u00e9 par une contante.</li> <li>On ajoute cette p\u00e9nalit\u00e9 \u00e0 la perte du critique.</li> <li>On retourne ces m\u00e9triques dans un dictionnaire.</li> </ol>       <p>Rappelons que les pertes pour le critique \\(D\\), \\(\\mathcal{L}_{D}^{WGAN}\\), et le g\u00e9n\u00e9rateur \\(G\\), \\(\\mathcal{L}_{G}^{WGAN}\\), on les formules suivantes.</p> \\[\\begin{aligned} \\mathcal{L}_{D}^{WGAN} :=  &amp; \\, \\mathbb{E}_{z  \\sim p(z)}[D(G(z))] - \\mathbb{E}_{x  \\sim \\mathbb{P}_{data}}[D(x)] \\\\ \\mathcal{L}_{G}^{WGAN} := &amp; \\, -\\mathbb{E}_{z  \\sim p(z)}[D(G(z))] \\end{aligned}\\]       <p>Un fois la p\u00e9nalit\u00e9 de gradient ajout\u00e9e, on obtient les formules suivantes.</p> \\[\\begin{aligned} \\mathcal{L}_{D}^{WGAN\\_GP} :=  &amp; \\, \\mathcal{L}_{D}^{WGAN} + \\lambda \\mathbb{E}_{\\hat{x} \\sim \\mathbb{P}_{\\hat{x}}}[(||\\nabla_{\\hat{x}}D(\\hat{x})||_{2}-1)^{2}] \\\\ \\mathcal{L}_{G}^{WGAN\\_GP} := &amp; \\, \\mathcal{L}_{G}^{WGAN} = - \\mathbb{E}_{z  \\sim p(z)}[D(G(z))] \\end{aligned}\\] <p>Avec \\(\\hat{x} := \\tilde{x} + \\varepsilon(x - \\tilde{x})\\), avec \\(\\tilde{x} = G(z)\\), et \\(\\varepsilon \\sim \\mathcal{N}(0, 1)\\). </p>       <p>D\u00e9taillons \u00e9tapes par \u00e9tapes.</p>       <pre><code>    def train_step(self, real_images):\n        if isinstance(real_images, tuple):\n            real_images = real_images[0]\n\n        # Taille du batch\n        batch_size = tf.shape(real_images)[0]\n</code></pre>       <p>On entra\u00eene d'abord le critique, l'article d'origine sugg\u00e8re d'entra\u00eener le critique plus longtemps que le g\u00e9n\u00e9rateur. Ici, on l'entra\u00eenera 3 \u00e9tapes pour une \u00e9tape de g\u00e9n\u00e9rateur.</p> <p><pre><code>        for i in range(self.d_steps):\n            random_latent_vectors = tf.random.normal(\n                shape=(batch_size, self.latent_dim)\n            )\n</code></pre> On prend un vecteur latent \\(z \\sim p(z)\\) de dimension (batch_size, latent_dim)</p> <p><pre><code>            with tf.GradientTape() as tape:\n                fake_images = self.generator(random_latent_vectors, training=True)\n</code></pre> On g\u00e9n\u00e8re de fausses images \u00e0 partir du g\u00e9n\u00e9rateur, \\(\\hat{x} = G(z)\\). <pre><code>                fake_logits = self.discriminator(fake_images, training=True)\n</code></pre> Le critique donne un score aux fausses images, \\(D(G(z))\\). <pre><code>                real_logits = self.discriminator(real_images, training=True)\n</code></pre> Le critique donne un score aux vraies images, \\(D(x)\\). <pre><code>                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n</code></pre> On calcule la perte du critique entre les fausses et les vraies images, \\(\\mathcal{L}_{D}^{WGAN}\\). <pre><code>                gp = self.gradient_penalty(batch_size, real_images, fake_images)\n</code></pre> Calcul de la p\u00e9nalit\u00e9 du gradient \\(\\mathbb{E}_{\\hat{x} \\sim \\mathbb{P}_{\\hat{x}}}[(||\\nabla_{\\hat{x}}D(\\hat{x})||_{2}-1)^{2}]\\). <pre><code>                d_loss = d_cost + gp * self.gp_weight\n</code></pre> Ajout de la p\u00e9nalit\u00e9 du gradient \u00e0 la perte du critique, \\(\\mathcal{L}_{D}^{WGAN\\_GP}\\). <pre><code>            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n            self.d_optimizer.apply_gradients(\n                zip(d_gradient, self.discriminator.trainable_variables)\n            )\n</code></pre> Calcul du gradient pour la perte du critique et mise a jour des poids du critique, \\(\\vartheta \\leftarrow \\vartheta -\\eta\\nabla_{\\vartheta}\\mathcal{L}_{D}^{WGAN\\_GP}\\).</p> <p>On passe alors maintenant \u00e0 l'entra\u00eenement du g\u00e9n\u00e9rateur, qui se d\u00e9roule de la m\u00eame fa\u00e7on.</p> <pre><code>        # on prend un vecteur latent\n        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n        with tf.GradientTape() as tape:\n            # On g\u00e9n\u00e8re de fausses images\n            generated_images = self.generator(random_latent_vectors, training=True)\n            # On obtient le score du critique sur les fausses images\n            gen_img_logits = self.discriminator(generated_images, training=True)\n            # On calcule la perte du g\u00e9n\u00e9rateur\n            g_loss = self.g_loss_fn(gen_img_logits)\n\n        # On prend le gradient de la perte du g\u00e9n\u00e9rateur\n        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n\n        # Mise \u00e0 jour des poids du g\u00e9n\u00e9rateur\n        self.g_optimizer.apply_gradients(\n            zip(gen_gradient, self.generator.trainable_variables)\n        )\n</code></pre> <p>On renvoit les nouvelles m\u00e9triques sous la forme d'un dictionaire. <pre><code>        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n</code></pre></p>       <p>On doit maintenant d\u00e9finir les fonctions de pertes que l'on va calculer. On rappelle que l'on a les formules suivantes.</p> \\[\\begin{aligned} \\mathcal{L}_{D}^{WGAN} :=  &amp; \\, \\mathbb{E}_{z  \\sim p(z)}[D(G(z))] - \\mathbb{E}_{x  \\sim \\mathbb{P}_{data}}[D(x)] \\\\ \\mathcal{L}_{G}^{WGAN} := &amp; \\, -\\mathbb{E}_{z  \\sim p(z)}[D(G(z))] \\end{aligned}\\] <p>\\(D(G(z))\\) \u00e9tant d\u00e9fini comme <code>fake_images</code> et \\(D(x)\\) comme <code>real_images</code> plus haut dans la fonction <code>train_step</code>, il reste juste \u00e0 prendre la moyenne. La p\u00e9nalit\u00e9 du gradient \u00e9tant directement r\u00e9jout\u00e9e dans <code>train_step</code>.</p> <pre><code>def discriminator_loss(real_img, fake_img):\n    real_loss = tf.reduce_mean(real_img)\n    fake_loss = tf.reduce_mean(fake_img)\n    return fake_loss - real_loss\n\n\ndef generator_loss(fake_img):\n    return -tf.reduce_mean(fake_img)\n</code></pre>      <pre><code>def discriminator_loss(real_img, fake_img):\n    real_loss = tf.reduce_mean(real_img)\n    fake_loss = tf.reduce_mean(fake_img)\n    return fake_loss - real_loss\n\n\ndef generator_loss(fake_img):\n    return -tf.reduce_mean(fake_img)\n</code></pre>     <pre><code># Optimizer for both the networks\n# learning_rate=0.0002, beta_1=0.5 are recommened\ngenerator_optimizer = keras.optimizers.Adam(learning_rate=0.0002,\n                                            beta_1=0.5,\n                                            beta_2=0.9)\n\ndiscriminator_optimizer = keras.optimizers.Adam(learning_rate=0.0002,\n                                                beta_1=0.5,\n                                                beta_2=0.9)\n\n# Get the wgan model\nwgan = WGAN(\n    discriminator=d_model,\n    generator=g_model,\n    latent_dim=noise_dim,\n    discriminator_extra_steps=3,\n)\n\n# Compile\nwgan.compile(\n    d_optimizer=discriminator_optimizer,\n    g_optimizer=generator_optimizer,\n    g_loss_fn=generator_loss,\n    d_loss_fn=discriminator_loss,\n)\n\ncifar10 = keras.datasets.cifar10\n(train_images, _), (test_images, _) = cifar10.load_data()\nprint(f\"Number of examples: {len(train_images)}\")\nprint(f\"Shape of the images in the dataset: {train_images.shape[1:]}\")\n\n# we will reshape each sample to (28, 28, 1) and normalize the pixel values in [-1, 1].\ntrain_images = train_images.reshape(train_images.shape[0], *IMG_SHAPE).astype(\"float32\")\ntrain_images = (train_images - 127.5) / 127.5\n</code></pre>      <pre>\n<code>Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170500096/170498071 [==============================] - 6s 0us/step\nNumber of examples: 50000\nShape of the images in the dataset: (32, 32, 3)\n</code>\n</pre>        <pre><code># Epochs to train\nepochs = 10\n\n# Start training\nwgan.fit(train_images,\n         batch_size=64,\n         epochs=epochs)\n</code></pre>      <pre>\n<code>Epoch 1/10\n782/782 [==============================] - 447s 572ms/step - d_loss: -1.3801 - g_loss: 3.5764\nEpoch 2/10\n782/782 [==============================] - 448s 573ms/step - d_loss: -1.3828 - g_loss: 10.7120\nEpoch 3/10\n782/782 [==============================] - 448s 573ms/step - d_loss: -1.4531 - g_loss: 7.7392\nEpoch 4/10\n782/782 [==============================] - 448s 573ms/step - d_loss: -1.4142 - g_loss: 12.2459\nEpoch 5/10\n782/782 [==============================] - 448s 573ms/step - d_loss: -1.3634 - g_loss: 9.3231\nEpoch 6/10\n782/782 [==============================] - 449s 574ms/step - d_loss: -1.3804 - g_loss: 6.6700\nEpoch 7/10\n782/782 [==============================] - 449s 574ms/step - d_loss: -1.3052 - g_loss: 6.9166\nEpoch 8/10\n782/782 [==============================] - 449s 574ms/step - d_loss: -1.3354 - g_loss: 7.5646\nEpoch 9/10\n782/782 [==============================] - 449s 574ms/step - d_loss: -1.3280 - g_loss: 7.2135\nEpoch 10/10\n782/782 [==============================] - 449s 574ms/step - d_loss: -1.2898 - g_loss: 7.1648\n</code>\n</pre>     <pre>\n<code>&lt;tensorflow.python.keras.callbacks.History at 0x7f6508277f60&gt;</code>\n</pre>        <pre><code>g_model.save('gen_cifar_30epochs.h5')\nd_model.save('critic_cifar_30epochs.h5')\n</code></pre>     <pre><code># display images generated from randomly sampled latent vector\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nn = 15\nimg_size = 32\nfigure = np.zeros((img_size * n, img_size * n, 3))\n\nfor i in range(n):\n    for j in range(n):\n        z_sample = np.array([np.random.normal(0, 1 ,size=noise_dim)])\n        x_decoded = g_model.predict(z_sample)\n        img = x_decoded[0].reshape(img_size, img_size, 3)\n        img = (127.5*img +127.5)/255\n        figure[i * img_size: (i + 1) * img_size,j * img_size: (j + 1) * img_size] = img\n\n\nplt.figure(figsize=(20, 20))\nplt.imshow(figure, cmap='Greys_r')\nplt.show()\n</code></pre>              <pre><code>z_sample = np.array([np.random.normal(0, 1 ,size=noise_dim)])\nimg_decode = g_model.predict(z_sample)\nimg_decode = tf.reshape(img_decode, (-1,32,32,3))\nimg_decode = (127.5*img_decode +127.5)/255\nimg_dec = img_decode[0, :, :, :]\nplt.imshow(img_dec)\nplt.show()\n</code></pre>"},{"location":"deep_learning/module9/Module9_TFTRT/","title":"Pratique, TFTRT","text":"(function() {   function addWidgetsRenderer() {     var requireJsScript = document.createElement('script');     requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js';      var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]');     var jupyterWidgetsScript = document.createElement('script');     var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js';     var widgetState;      // Fallback for older version:     try {       widgetState = mimeElement &amp;&amp; JSON.parse(mimeElement.innerHTML);        if (widgetState &amp;&amp; (widgetState.version_major &lt; 2 || !widgetState.version_major)) {         widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js';       }     } catch(e) {}      jupyterWidgetsScript.src = widgetRendererSrc;      document.body.appendChild(requireJsScript);     document.body.appendChild(jupyterWidgetsScript);   }    document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }());"},{"location":"deep_learning/module9/Module9_TFTRT/#tp-module-9-optimisation-des-modeles","title":"TP Module 9 : Optimisation des mod\u00e8les","text":"<p>But de l'optimisation des mod\u00e8les :</p> <ul> <li>R\u00e9duire la taille du mod\u00e8le.</li> <li>Acc\u00e9l\u00e9rer le temps d'inf\u00e9rence.</li> <li>R\u00e9duire la consommation \u00e9nerg\u00e9tique du mod\u00e8le.</li> </ul> <p>Quelles sont les diff\u00e9rentes fa\u00e7on d'optimiser une mod\u00e8le ?</p> <ul> <li>R\u00e9duire sa pr\u00e9cision num\u00e9rique.</li> <li>Les param\u00e8tres et fonctions d'activations d'un mod\u00e8le sont le plus souvent repr\u00e9sent\u00e9s en <code>float32</code>. Quantification</li> <li>Toutes les op\u00e9rations du graphe d'un mod\u00e8le sont elles n\u00e9c\u00e9ssaires durant l'inf\u00e9rence ? Fusion des couches</li> <li>Tous les param\u00e8tres contribuent ils \u00e0 la performance du mod\u00e8le ? Pruning</li> <li>Am\u00e9liorer les allers-retours entre GPU et CPU.</li> </ul>       <ul> <li>Ref</li> <li>https://colab.research.google.com/github/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/A_tale_of_quantization.ipynb#scrollTo=HHZ-SsZU5VAW</li> <li>https://colab.research.google.com/github/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/Model_Pruning_in_Deep_Learning_with_tfmot.ipynb#scrollTo=cEDnyetVLTcM</li> <li>https://colab.research.google.com/github/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/Custom_Image_Classification_EdgeTPU.ipynb#scrollTo=57GUpImAP7KC</li> </ul>       <ul> <li> <p>TF Lite</p> </li> <li> <p>https://www.tensorflow.org/lite/performance/post_training_quantization</p> </li> <li> <p>https://www.tensorflow.org/lite/performance/post_training_quant</p> </li> <li> <p>https://www.tensorflow.org/lite/performance/post_training_quantization#dynamic_range_quantization</p> </li> <li> <p>https://www.tensorflow.org/lite/performance/best_practices#profile_your_application_with_platform_specific_tools</p> </li> <li> <p>https://www.tensorflow.org/lite/performance/post_training_integer_quant</p> </li> <li> <p>Model Optimization/Pruning</p> </li> <li> <p>https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras#overview</p> </li> <li> <p>https://www.tensorflow.org/model_optimization/api_docs/python/tfmot</p> </li> <li> <p>https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/ConstantSparsity</p> </li> </ul> <p>-https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/PruningSchedule</p> <ul> <li>https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide</li> </ul> <p>-https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide#prune_whole_model_sequential_and_functional</p> <ul> <li> <p>https://www.tensorflow.org/model_optimization/guide/quantization/training</p> </li> <li> <p>https://tensorflow.google.cn/model_optimization/guide/pruning/pruning_with_keras</p> </li> <li> <p>Save/Load</p> </li> <li> <p>https://www.tensorflow.org/tutorials/keras/save_and_load</p> </li> </ul>"},{"location":"deep_learning/module9/Module9_TFTRT/#import-libs","title":"Import libs","text":"<pre><code>import tensorflow as tf\nfrom tensorflow import keras\n\nprint(f'tf : {tf.__version__}')\nprint(f'keras : {keras.__version__}')\n\nimport pandas as pd\nimport numpy as np\nimport random\nimport os\nimport datetime\n\n# freeze de l'al\u00e9atoire, pour avoir des exp\u00e9riences reproductibles.\nRANDOM_SEED = 42\n\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\ntf.random.set_seed(RANDOM_SEED)\n</code></pre>      <pre>\n<code>tf : 2.1.0\nkeras : 2.2.4-tf\n</code>\n</pre>        <pre><code>from tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.models import Model\n\nfrom sklearn.model_selection import train_test_split\n\nfrom imutils import paths\nimport matplotlib.pyplot as plt\nimport shutil\nimport time\nimport cv2\n</code></pre>     <pre><code>!nvidia-smi\n</code></pre>      <pre>\n<code>Tue Sep  1 09:25:15 2020       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   39C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n|                               |                      |                 ERR! |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code>\n</pre>        <pre><code>comp = pd.DataFrame()\ncomp['model'] = []\ncomp['pr\u00e9cision'] = []\n</code></pre>"},{"location":"deep_learning/module9/Module9_TFTRT/#import-du-dataset","title":"Import du dataset","text":"<pre><code># Gather Flowers-17 dataset\ndata_root = tf.keras.utils.get_file(\n  \"/content/flower_photos\", \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\",\n   untar=True)\n</code></pre>     <pre><code># Untar the dataset\n!tar -xvf flower_photos.tar.gz\n</code></pre>      <pre>\n<code>flower_photos/\nflower_photos/roses/\nflower_photos/roses/14810868100_87eb739f26_m.jpg\nflower_photos/roses/1446090416_f0cad5fde4.jpg\nflower_photos/roses/15319767030_e6c5602a77_m.jpg\nflower_photos/roses/15032112248_30c5284e54_n.jpg\nflower_photos/roses/7211616670_2d49ecb3a5_m.jpg\nflower_photos/roses/15674450867_0ced942941_n.jpg\nflower_photos/roses/17158274118_00ec99a23c.jpg\nflower_photos/roses/14019883858_e5d2a0ec10_n.jpg\nflower_photos/roses/8035908422_87220425d2_n.jpg\nflower_photos/roses/14747962886_2bff6bb323_m.jpg\nflower_photos/roses/4356781875_92c5cd93c0.jpg\nflower_photos/roses/8524505546_b242bd4928_n.jpg\nflower_photos/roses/9406573080_60eab9278e_n.jpg\nflower_photos/roses/6039330368_c30ed224c4_m.jpg\nflower_photos/roses/14414100710_753a36fce9.jpg\nflower_photos/roses/3292434691_392071d702_n.jpg\nflower_photos/roses/5273722065_c85d8543c2_m.jpg\nflower_photos/roses/2215318403_06eb99176a.jpg\nflower_photos/roses/172311368_49412f881b.jpg\nflower_photos/roses/15255964454_0a64eb67fa.jpg\nflower_photos/roses/4504220673_af754fcb40_n.jpg\nflower_photos/roses/505517255_cfbb6f6394.jpg\nflower_photos/roses/6347846935_51e3dc2481_n.jpg\nflower_photos/roses/2273917656_6d6c038283.jpg\nflower_photos/roses/8775267816_726ddc6d92_n.jpg\nflower_photos/roses/5206847130_ee4bf0e4de_n.jpg\nflower_photos/roses/5777669976_a205f61e5b.jpg\nflower_photos/roses/14001990976_bd2da42dbc.jpg\nflower_photos/roses/4713531680_1110a2fa07_n.jpg\nflower_photos/roses/14154164774_3b39d36778.jpg\nflower_photos/roses/494803274_f84f21d53a.jpg\nflower_photos/roses/5570018782_c56bee942f.jpg\nflower_photos/roses/9404876600_04f6d37685.jpg\nflower_photos/roses/5578760521_e54aca6bed_n.jpg\nflower_photos/roses/15060816740_68e1b2c31b.jpg\nflower_photos/roses/15174615529_144ae28bdb_n.jpg\nflower_photos/roses/15312360171_57bde98799_n.jpg\nflower_photos/roses/4648680921_80dfc4f12a.jpg\nflower_photos/roses/3026375835_a20ecdd140_m.jpg\nflower_photos/roses/4396642388_3081a38875_n.jpg\nflower_photos/roses/3451177763_729a4d54af_n.jpg\nflower_photos/roses/4702438868_278b9cf41c_n.jpg\nflower_photos/roses/12572786553_634868f7f2_n.jpg\nflower_photos/roses/1756973583_4aac7df00d_m.jpg\nflower_photos/roses/12243069253_e512464095_n.jpg\nflower_photos/roses/7251352826_69b62cba2c_m.jpg\nflower_photos/roses/8671682526_7058143c99.jpg\nflower_photos/roses/23891005905_17ce9e6936.jpg\nflower_photos/roses/2408236801_f43c6bcff2.jpg\nflower_photos/roses/3909587261_f8cd3e7fe7.jpg\nflower_photos/roses/15901230359_1819e96b89_n.jpg\nflower_photos/roses/9309388105_12c0b8dd54_m.jpg\nflower_photos/roses/8388497874_1fe750cc95_m.jpg\nflower_photos/roses/6969041818_a505baa68e_m.jpg\nflower_photos/roses/6690926183_afedba9f15_n.jpg\nflower_photos/roses/3873271620_1d9d314f01_n.jpg\nflower_photos/roses/2677417735_a697052d2d_n.jpg\nflower_photos/roses/1775233884_12ff5a124f.jpg\nflower_photos/roses/18563353954_b761d97155_m.jpg\nflower_photos/roses/15094168139_8f636ffa1d_n.jpg\nflower_photos/roses/6255593451_b8a3aa8f7a_m.jpg\nflower_photos/roses/2960709681_e95940c0f0_n.jpg\nflower_photos/roses/2888138918_402096c7fb.jpg\nflower_photos/roses/12243068283_ee4c2683e2_n.jpg\nflower_photos/roses/5332550500_ab341aefd8.jpg\nflower_photos/roses/8692040971_826614516f_n.jpg\nflower_photos/roses/5148639829_781eb7d346.jpg\nflower_photos/roses/15277801151_5ed88f40f0_n.jpg\nflower_photos/roses/8462246855_1bdfee7478.jpg\nflower_photos/roses/5979193298_639e877248.jpg\nflower_photos/roses/8337607102_d9e0fa887e.jpg\nflower_photos/roses/19271410704_932d1f2c97_n.jpg\nflower_photos/roses/22506717337_0fd63e53e9.jpg\nflower_photos/roses/8035910225_125beceb98_n.jpg\nflower_photos/roses/16903172207_2cd7aca66a.jpg\nflower_photos/roses/898102603_2d5152f09a.jpg\nflower_photos/roses/8742493689_fb852f0228_n.jpg\nflower_photos/roses/160954292_6c2b4fda65_n.jpg\nflower_photos/roses/16552686350_db8db55cd2.jpg\nflower_photos/roses/118974357_0faa23cce9_n.jpg\nflower_photos/roses/17051448596_69348f7fce_m.jpg\nflower_photos/roses/17077876795_6dd1b03f54_m.jpg\nflower_photos/roses/15333843782_060cef3030.jpg\nflower_photos/roses/3465443774_6b0c75a3b1_n.jpg\nflower_photos/roses/563847503_89e9756c80.jpg\nflower_photos/roses/459042023_6273adc312_n.jpg\nflower_photos/roses/4414135084_1ac7e6cd54.jpg\nflower_photos/roses/15999816377_4b95e0b538_n.jpg\nflower_photos/roses/7683456068_02644b8382_m.jpg\nflower_photos/roses/3705716290_cb7d803130_n.jpg\nflower_photos/roses/99383371_37a5ac12a3_n.jpg\nflower_photos/roses/3065719996_c16ecd5551.jpg\nflower_photos/roses/20409866779_ac473f55e0_m.jpg\nflower_photos/roses/3664842094_5fd60ee26b.jpg\nflower_photos/roses/14683774134_6367640585.jpg\nflower_photos/roses/13929462317_96342a9a44.jpg\nflower_photos/roses/14880561916_79aeb812fd_n.jpg\nflower_photos/roses/4231745228_ece86330d9.jpg\nflower_photos/roses/10503217854_e66a804309.jpg\nflower_photos/roses/6879112993_5a29208438_n.jpg\nflower_photos/roses/12338444334_72fcc2fc58_m.jpg\nflower_photos/roses/6158504080_b844a9ae05.jpg\nflower_photos/roses/6209630964_e8de48fe04_m.jpg\nflower_photos/roses/23232710191_cc57620cd5.jpg\nflower_photos/roses/1392579828_ab5a139052.jpg\nflower_photos/roses/3872230296_6c477309f3_n.jpg\nflower_photos/roses/4654893119_45d232016b.jpg\nflower_photos/roses/5892908233_6756199a43.jpg\nflower_photos/roses/3141434519_aaa64c4f65_n.jpg\nflower_photos/roses/4998708839_c53ee536a8_n.jpg\nflower_photos/roses/6653567281_768a1fd160.jpg\nflower_photos/roses/8181940917_1ac63937d5_n.jpg\nflower_photos/roses/873660804_37f5c6a46e_n.jpg\nflower_photos/roses/5529341024_0c35f2657d.jpg\nflower_photos/roses/3292654244_4a220ab96f_m.jpg\nflower_photos/roses/9298314004_c1a8146521.jpg\nflower_photos/roses/12407768513_3440238148_n.jpg\nflower_photos/roses/15498482197_8878cdfb07_n.jpg\nflower_photos/roses/15566697073_9a214b700e_n.jpg\nflower_photos/roses/6676529655_9672b6f955_m.jpg\nflower_photos/roses/319298955_0c72bd36bf.jpg\nflower_photos/roses/9216324117_5fa1e2bc25_n.jpg\nflower_photos/roses/15184419268_7230e9728e.jpg\nflower_photos/roses/6363951285_a802238d4e.jpg\nflower_photos/roses/1446097778_97149b8362.jpg\nflower_photos/roses/515121050_dcb99890be.jpg\nflower_photos/roses/8516036987_8a06dfe1b5_n.jpg\nflower_photos/roses/2550860627_998a4fc4c1.jpg\nflower_photos/roses/909277823_e6fb8cb5c8_n.jpg\nflower_photos/roses/4267024012_295e7141a3_n.jpg\nflower_photos/roses/5231103167_a03280e9f6_n.jpg\nflower_photos/roses/20596941736_f2c5f496cf.jpg\nflower_photos/roses/16078501836_3ac067e18a.jpg\nflower_photos/roses/3560426426_1c66cb8330.jpg\nflower_photos/roses/3268459296_a7346c6b2c.jpg\nflower_photos/roses/388405293_4db1d71f21_n.jpg\nflower_photos/roses/3422228549_f147d6e642.jpg\nflower_photos/roses/2265390547_2409007cef_n.jpg\nflower_photos/roses/3742168238_d961937e68_n.jpg\nflower_photos/roses/6867597533_d65d1c39fb_n.jpg\nflower_photos/roses/19440805164_920b28da61_n.jpg\nflower_photos/roses/16152205512_9d6cb80fb6.jpg\nflower_photos/roses/527513005_41497ca4dc.jpg\nflower_photos/roses/5292988046_a10f4b0365_n.jpg\nflower_photos/roses/16525204061_9b47be3726_m.jpg\nflower_photos/roses/7376473742_532364cee5_n.jpg\nflower_photos/roses/10090824183_d02c613f10_m.jpg\nflower_photos/roses/3412874275_ca78ee024d_m.jpg\nflower_photos/roses/2364976562_a184463083_m.jpg\nflower_photos/roses/4292443009_3a2831b0b9_m.jpg\nflower_photos/roses/14993880427_95d0f27257.jpg\nflower_photos/roses/5717319579_190e85c7d1_m.jpg\nflower_photos/roses/5578766623_542c91dfaa_n.jpg\nflower_photos/roses/5835539224_75967fc400_m.jpg\nflower_photos/roses/13231224664_4af5293a37.jpg\nflower_photos/roses/15509799653_0562d4a4fa.jpg\nflower_photos/roses/15222804561_0fde5eb4ae_n.jpg\nflower_photos/roses/5159317458_bbb22e2f65_n.jpg\nflower_photos/roses/2568105249_15720d081f_n.jpg\nflower_photos/roses/4918137796_21f0922b0c_n.jpg\nflower_photos/roses/2951375433_ae2726d9d2_m.jpg\nflower_photos/roses/3829990289_c0c3821e4d_m.jpg\nflower_photos/roses/10894627425_ec76bbc757_n.jpg\nflower_photos/roses/8742493617_c2a9bf854f_m.jpg\nflower_photos/roses/8692051081_dffa8709e7_m.jpg\nflower_photos/roses/18990187093_09f2bff8fc_m.jpg\nflower_photos/roses/8063462557_e0a8bd6c64_n.jpg\nflower_photos/roses/16476788181_0e2ffc719a.jpg\nflower_photos/roses/2949945463_366bc63079_n.jpg\nflower_photos/roses/15951588433_c0713cbfc6_m.jpg\nflower_photos/roses/6036837996_7fbdcdb3c5_n.jpg\nflower_photos/roses/5721768347_2ec4d2247b_n.jpg\nflower_photos/roses/18741313803_1bbf842fc6_n.jpg\nflower_photos/roses/2713683760_d98cd2a05b_m.jpg\nflower_photos/roses/15820572326_be2ea4a55c_n.jpg\nflower_photos/roses/4713533500_fcc295de70_n.jpg\nflower_photos/roses/16051111039_0f0626a241_n.jpg\nflower_photos/roses/3556123230_936bf084a5_n.jpg\nflower_photos/roses/17700322054_1c4fdaa034_m.jpg\nflower_photos/roses/4754734410_94d98463a5.jpg\nflower_photos/roses/21347496068_f4d3339607.jpg\nflower_photos/roses/18599603859_f2ec616ddf_n.jpg\nflower_photos/roses/921984328_a60076f070_m.jpg\nflower_photos/roses/8125886145_ae99f91fd0.jpg\nflower_photos/roses/5172171681_5934378f08.jpg\nflower_photos/roses/5083072098_81587295d5.jpg\nflower_photos/roses/4881402397_1c664af2f7_n.jpg\nflower_photos/roses/14221192676_eb8c89a7d6_n.jpg\nflower_photos/roses/5840476802_dfa40deb1f_m.jpg\nflower_photos/roses/5050969148_a0090f762a.jpg\nflower_photos/roses/4724951744_61877ec101_n.jpg\nflower_photos/roses/5628552852_60bbe8d9b0_n.jpg\nflower_photos/roses/1667199972_7ba7d999c1_m.jpg\nflower_photos/roses/2535495431_e6f950443c.jpg\nflower_photos/roses/15424480096_45bb574b33.jpg\nflower_photos/roses/6783408274_974796e92f.jpg\nflower_photos/roses/3253243865_435c1f2c2b_m.jpg\nflower_photos/roses/466486216_ab13b55763.jpg\nflower_photos/roses/9167147034_0a66ee3616_n.jpg\nflower_photos/roses/5897035797_e67bf68124_n.jpg\nflower_photos/roses/3751835302_d5a03f55e8_n.jpg\nflower_photos/roses/15537825851_a80b6321d7_n.jpg\nflower_photos/roses/2980099495_cf272e90ca_m.jpg\nflower_photos/roses/9433167170_fa056d3175.jpg\nflower_photos/roses/15761264350_4caaf080f6_m.jpg\nflower_photos/roses/1801614110_bb9fa46830.jpg\nflower_photos/roses/8926641787_d2515dfe8f_m.jpg\nflower_photos/roses/5398569540_7d134c42cb_n.jpg\nflower_photos/roses/12240577184_b0de0e53ea_n.jpg\nflower_photos/roses/272481307_1eb47ba3e0_n.jpg\nflower_photos/roses/3667366832_7a8017c528_n.jpg\nflower_photos/roses/5402157745_a384f0583d_n.jpg\nflower_photos/roses/4093390305_4010c736c9.jpg\nflower_photos/roses/19566556880_476c66c5ee_n.jpg\nflower_photos/roses/14176042519_5792b37555.jpg\nflower_photos/roses/16772483324_09f24813a1_n.jpg\nflower_photos/roses/3109712111_75cea2dee6.jpg\nflower_photos/roses/3072908271_08764c732a_m.jpg\nflower_photos/roses/16449467833_d82aac5749_m.jpg\nflower_photos/roses/15104537437_f6730b38c3_n.jpg\nflower_photos/roses/15802657001_40fe77c030_m.jpg\nflower_photos/roses/3231873181_faf2da6382.jpg\nflower_photos/roses/102501987_3cdb8e5394_n.jpg\nflower_photos/roses/6687138903_ff6ae12758_n.jpg\nflower_photos/roses/4413509121_a62879598a.jpg\nflower_photos/roses/15977362155_461030c196_m.jpg\nflower_photos/roses/15236835789_6009b8f33d.jpg\nflower_photos/roses/6125332325_b768e08bd9_n.jpg\nflower_photos/roses/8442304572_2fdc9c7547_n.jpg\nflower_photos/roses/4575042086_7674b76297_n.jpg\nflower_photos/roses/20825078671_90b0389c70_m.jpg\nflower_photos/roses/15821959372_518b9dcf57_n.jpg\nflower_photos/roses/2053476785_c162a3e358.jpg\nflower_photos/roses/16209331331_343c899d38.jpg\nflower_photos/roses/12323085443_8ac0cdb713_n.jpg\nflower_photos/roses/3742155164_14b557a51c_n.jpg\nflower_photos/roses/13342823005_16d3df58df_n.jpg\nflower_photos/roses/13279526615_a3b0059bec.jpg\nflower_photos/roses/4325834819_ab56661dcc_m.jpg\nflower_photos/roses/3654988152_b11178bbcb.jpg\nflower_photos/roses/7551637034_55ae047756_n.jpg\nflower_photos/roses/8960904651_9a0b727258.jpg\nflower_photos/roses/16545641666_2781e542a0_m.jpg\nflower_photos/roses/22325299158_6e32e599f8_m.jpg\nflower_photos/roses/4723876257_d87b781986.jpg\nflower_photos/roses/4951581805_b049304f1b_n.jpg\nflower_photos/roses/537207677_f96a0507bb.jpg\nflower_photos/roses/5961803532_9368212949_m.jpg\nflower_photos/roses/22679076_bdb4c24401_m.jpg\nflower_photos/roses/8747396730_966149e6fe_n.jpg\nflower_photos/roses/4694341873_65fe187a4e_n.jpg\nflower_photos/roses/17040847367_b54d05bf52.jpg\nflower_photos/roses/3921794817_276eb4386b.jpg\nflower_photos/roses/1666341535_99c6f7509f_n.jpg\nflower_photos/roses/15738649506_2b4c2fd933_m.jpg\nflower_photos/roses/18220342690_f1c20134bd.jpg\nflower_photos/roses/5497730366_44d758d8f5.jpg\nflower_photos/roses/3102535578_ec8c12a7b6_m.jpg\nflower_photos/roses/4279989256_9a48c0d194_n.jpg\nflower_photos/roses/15750320284_22ef21c682.jpg\nflower_photos/roses/2122401867_cd86c5f114_n.jpg\nflower_photos/roses/3315973481_850d2253e9_n.jpg\nflower_photos/roses/3624546109_8eb98f0cdb.jpg\nflower_photos/roses/1540738662_7b4152e344_m.jpg\nflower_photos/roses/16018886851_c32746cb72.jpg\nflower_photos/roses/15681454551_b6f73ce443_n.jpg\nflower_photos/roses/410425647_4586667858.jpg\nflower_photos/roses/9355706808_a9a723a8e8_n.jpg\nflower_photos/roses/17554868955_35f48516cd_m.jpg\nflower_photos/roses/17449165090_dfb27af360_n.jpg\nflower_photos/roses/5419629292_2f06e4b295.jpg\nflower_photos/roses/229488796_21ac6ee16d_n.jpg\nflower_photos/roses/3500121696_5b6a69effb_n.jpg\nflower_photos/roses/5398974188_799753449c.jpg\nflower_photos/roses/8987479080_32ab912d10_n.jpg\nflower_photos/roses/8394286483_69fe04cc7f.jpg\nflower_photos/roses/6347847065_83cf87333b_n.jpg\nflower_photos/roses/3655527028_0fab2b547d_n.jpg\nflower_photos/roses/110472418_87b6a3aa98_m.jpg\nflower_photos/roses/3948347096_42261f047a_m.jpg\nflower_photos/roses/2065522422_cfdd80044a_n.jpg\nflower_photos/roses/159079265_d77a9ac920_n.jpg\nflower_photos/roses/16643944275_3cd4cd966c.jpg\nflower_photos/roses/7304710956_015b41f802_m.jpg\nflower_photos/roses/6409000675_6eb6806e59.jpg\nflower_photos/roses/21522100663_455b77a90c_n.jpg\nflower_photos/roses/5181899042_0a6ffe0c8a_n.jpg\nflower_photos/roses/2535466143_5823e48b63.jpg\nflower_photos/roses/2414954629_3708a1a04d.jpg\nflower_photos/roses/685724528_6cd5cbe203.jpg\nflower_photos/roses/3713368809_eba7fa2fbf_m.jpg\nflower_photos/roses/16961613890_695b36aab2_m.jpg\nflower_photos/roses/4279990882_031482f8b6_n.jpg\nflower_photos/roses/3236806990_a90c7bb520_m.jpg\nflower_photos/roses/4735314389_94fe1b2a9f_n.jpg\nflower_photos/roses/19988406792_68201f76e3_n.jpg\nflower_photos/roses/12434194695_a7c4e73c6b_n.jpg\nflower_photos/roses/16339359979_6d742660b8_n.jpg\nflower_photos/roses/3145692843_d46ba4703c.jpg\nflower_photos/roses/15697872479_ed48e9dd73_n.jpg\nflower_photos/roses/7285188160_49d84b95a3_m.jpg\nflower_photos/roses/4804011140_7defedf4b7_m.jpg\nflower_photos/roses/512694812_48ba9c0b49_n.jpg\nflower_photos/roses/2197754124_5c8a146761_n.jpg\nflower_photos/roses/14408977935_a397e796b8_m.jpg\nflower_photos/roses/9614492283_66020fb4eb_n.jpg\nflower_photos/roses/5249439791_196b4e7fc7.jpg\nflower_photos/roses/7186509956_c37c02fb43_n.jpg\nflower_photos/roses/7316409504_7cf3707f8a_m.jpg\nflower_photos/roses/5086249859_d066b37b8a_m.jpg\nflower_photos/roses/18376177250_86060cbdc9.jpg\nflower_photos/roses/15699509054_d3e125286f_n.jpg\nflower_photos/roses/4608559939_3487bf3b62_n.jpg\nflower_photos/roses/18389368680_91c24a2087_z.jpg\nflower_photos/roses/15061894841_e5aca59ecd_n.jpg\nflower_photos/roses/2976723295_b16ab04231.jpg\nflower_photos/roses/3903276582_fe05bf84c7_n.jpg\nflower_photos/roses/16670921315_0fc48d7ab2_n.jpg\nflower_photos/roses/410421672_563550467c.jpg\nflower_photos/roses/3634244527_e72c47842c_n.jpg\nflower_photos/roses/488849503_63a290a8c2_m.jpg\nflower_photos/roses/218630974_5646dafc63_m.jpg\nflower_photos/roses/4243078361_7b92a932cd_n.jpg\nflower_photos/roses/2788276815_8f730bd942.jpg\nflower_photos/roses/2448812029_047d981092_m.jpg\nflower_photos/roses/1402130395_0b89d76029.jpg\nflower_photos/roses/15274443248_76b9f3eb24.jpg\nflower_photos/roses/24781114_bc83aa811e_n.jpg\nflower_photos/roses/4625089819_55c45a189c.jpg\nflower_photos/roses/14312910041_b747240d56_n.jpg\nflower_photos/roses/15602874619_03fd934bed.jpg\nflower_photos/roses/14982802401_a3dfb22afb.jpg\nflower_photos/roses/14510185271_b5d75dd98e_n.jpg\nflower_photos/roses/9216321995_83df405ea9.jpg\nflower_photos/roses/12406229175_82e2ac649c_n.jpg\nflower_photos/roses/14107161906_5737e0e4ec.jpg\nflower_photos/roses/7409458444_0bfc9a0682_n.jpg\nflower_photos/roses/21346056089_e6f8074e5f_m.jpg\nflower_photos/roses/8642943283_47e44d049d_m.jpg\nflower_photos/roses/19153732586_9de58c8f53_n.jpg\nflower_photos/roses/9320934277_4fb95aef5d_n.jpg\nflower_photos/roses/2265579414_2e00a8f265_n.jpg\nflower_photos/roses/8667746487_781af9e615_n.jpg\nflower_photos/roses/2093263381_afd51358a3.jpg\nflower_photos/roses/9337528427_3d09b7012b.jpg\nflower_photos/roses/2471103806_87ba53d997_n.jpg\nflower_photos/roses/14687731322_5613f76353.jpg\nflower_photos/roses/5731750490_ba3325b7ee_n.jpg\nflower_photos/roses/14597445311_8acb60247e.jpg\nflower_photos/roses/1645761726_2b1be95472.jpg\nflower_photos/roses/16149016979_23ef42b642_m.jpg\nflower_photos/roses/298670754_f25edda891.jpg\nflower_photos/roses/14381787252_e8e12e277a_n.jpg\nflower_photos/roses/3621011057_0d03bd171b_n.jpg\nflower_photos/roses/16157873719_bf0bdf8558_n.jpg\nflower_photos/roses/2536282942_b5ca27577e.jpg\nflower_photos/roses/18760363474_a707331322_n.jpg\nflower_photos/roses/22982871191_ec61e36939_n.jpg\nflower_photos/roses/17702388233_f29dc14834_m.jpg\nflower_photos/roses/3697780051_83e50a6dd1_m.jpg\nflower_photos/roses/5061135742_2870a7b691_n.jpg\nflower_photos/roses/12450781274_eb78723921.jpg\nflower_photos/roses/6053143173_991c011b23.jpg\nflower_photos/roses/5060536705_b370a5c543_n.jpg\nflower_photos/roses/18584002386_cec0df537d_n.jpg\nflower_photos/roses/14943194730_f48b4d4547_n.jpg\nflower_photos/roses/8241471746_5d81fdd3c0_n.jpg\nflower_photos/roses/12240303_80d87f77a3_n.jpg\nflower_photos/roses/3278709893_ba4956a572_n.jpg\nflower_photos/roses/4061451210_1650590c6a.jpg\nflower_photos/roses/3971662839_5cb2963b20_n.jpg\nflower_photos/roses/8096324039_4db2555490.jpg\nflower_photos/roses/14414123198_24606fb32d.jpg\nflower_photos/roses/5492988531_574cdc2bf0_n.jpg\nflower_photos/roses/3115889021_053f3b8e5a.jpg\nflower_photos/roses/15949087094_a8f565295c_m.jpg\nflower_photos/roses/9160289562_ab2718d19b.jpg\nflower_photos/roses/17302463621_d82be11f01_n.jpg\nflower_photos/roses/16100313047_c2e23cbb3d_n.jpg\nflower_photos/roses/14145188939_b4de638bd3_n.jpg\nflower_photos/roses/3526860692_4c551191b1_m.jpg\nflower_photos/roses/15186434972_e353da940a.jpg\nflower_photos/roses/4558025386_2c47314528.jpg\nflower_photos/roses/9164900485_605aa12da8.jpg\nflower_photos/roses/6069602140_866eecf7c2_m.jpg\nflower_photos/roses/4248222578_b4d5868b32.jpg\nflower_photos/roses/2258973326_03c0145f15_n.jpg\nflower_photos/roses/8060338380_eb6c806624_n.jpg\nflower_photos/roses/4360743371_6238b36d8c_m.jpg\nflower_photos/roses/12202373204_34fb07205b.jpg\nflower_photos/roses/7455236056_b6d71a8dab.jpg\nflower_photos/roses/2409069862_b128ee2a71.jpg\nflower_photos/roses/3871586333_5a708d5cf4_n.jpg\nflower_photos/roses/4242976586_607a8f9843_n.jpg\nflower_photos/roses/1757822526_fe30b9b3ca_m.jpg\nflower_photos/roses/269037241_07fceff56a_m.jpg\nflower_photos/roses/5212877807_a3ddf06a7c_n.jpg\nflower_photos/roses/568715474_bdb64ccc32.jpg\nflower_photos/roses/8622493424_877ae35ed7.jpg\nflower_photos/roses/5799616059_0ffda02e54.jpg\nflower_photos/roses/17062080069_36ac7907d2_n.jpg\nflower_photos/roses/16484100863_979beacb08.jpg\nflower_photos/roses/3753920123_c7ebc18ee3.jpg\nflower_photos/roses/353897245_5453f35a8e.jpg\nflower_photos/roses/15859434664_67bf3ef29f.jpg\nflower_photos/roses/323872063_7264e7e018_m.jpg\nflower_photos/roses/1813435848_7852708394_n.jpg\nflower_photos/roses/1949195327_75f76c12b1.jpg\nflower_photos/roses/9369421752_db1ab2a6a4_m.jpg\nflower_photos/roses/2892056920_918c52889b_m.jpg\nflower_photos/roses/4065283966_9504b98269.jpg\nflower_photos/roses/2059172936_032ffc12aa.jpg\nflower_photos/roses/3661675690_ed2d05fa5f_n.jpg\nflower_photos/roses/5156037859_1673720a11_m.jpg\nflower_photos/roses/326541992_d542103ca8_n.jpg\nflower_photos/roses/14172324538_2147808483_n.jpg\nflower_photos/roses/7302931078_30054c1970_n.jpg\nflower_photos/roses/16666836810_216f50e9c3_m.jpg\nflower_photos/roses/7345657862_689366e79a.jpg\nflower_photos/roses/3663244576_97f595cf4a.jpg\nflower_photos/roses/2682566502_967e7eaa2a.jpg\nflower_photos/roses/5863698305_04a4277401_n.jpg\nflower_photos/roses/17990320484_93bba345d2_m.jpg\nflower_photos/roses/19823402005_2db025dd66_m.jpg\nflower_photos/roses/7376471712_e1be793f94.jpg\nflower_photos/roses/6655078437_759fd626fd_n.jpg\nflower_photos/roses/4860145119_b1c3cbaa4e_n.jpg\nflower_photos/roses/5736328472_8f25e6f6e7.jpg\nflower_photos/roses/4921988677_e2eb0c9a24_m.jpg\nflower_photos/roses/4553266758_09d4dbdac9_n.jpg\nflower_photos/roses/12395698413_c0388278f7.jpg\nflower_photos/roses/16691277899_9433f39155_n.jpg\nflower_photos/roses/15822837396_96b392fda8_m.jpg\nflower_photos/roses/145862135_ab710de93c_n.jpg\nflower_photos/roses/5960270643_1b8a94822e_m.jpg\nflower_photos/roses/4363734507_5cc4ed6e01.jpg\nflower_photos/roses/2225411981_6638c3e988.jpg\nflower_photos/roses/7147367479_f7a6ef0798.jpg\nflower_photos/roses/3997609936_8db20b7141_n.jpg\nflower_photos/roses/16229215579_e7dd808e9c.jpg\nflower_photos/roses/6108118824_5b0231a56d.jpg\nflower_photos/roses/15922772266_1167a06620.jpg\nflower_photos/roses/8723767157_c45bfd3ab6.jpg\nflower_photos/roses/16152175716_55d6968e08_n.jpg\nflower_photos/roses/3276552939_8c31b22d3e.jpg\nflower_photos/roses/4797595918_79887b1229_n.jpg\nflower_photos/roses/14267691818_301aceda07.jpg\nflower_photos/roses/2609353769_dc3654f12f.jpg\nflower_photos/roses/8524505682_bda885af3a_n.jpg\nflower_photos/roses/11233672494_d8bf0a3dbf_n.jpg\nflower_photos/roses/6732261031_861a1026fa_n.jpg\nflower_photos/roses/7420699022_60fa574524_m.jpg\nflower_photos/roses/2777518561_105abc8cfc_n.jpg\nflower_photos/roses/2423565102_2f1a00bb1b_n.jpg\nflower_photos/roses/9216323421_f737c1d50e.jpg\nflower_photos/roses/7525783408_0999483bf4_m.jpg\nflower_photos/roses/5223191368_01aedb6547_n.jpg\nflower_photos/roses/6936225976_a91b60d8c2_m.jpg\nflower_photos/roses/22093190909_77223e6f53_n.jpg\nflower_photos/roses/394990940_7af082cf8d_n.jpg\nflower_photos/roses/3179751458_9646d839f6_n.jpg\nflower_photos/roses/5602220566_5cdde8fa6c_n.jpg\nflower_photos/roses/16424992340_c1d9eb72b4.jpg\nflower_photos/roses/2325232198_751645d0bb_n.jpg\nflower_photos/roses/6473543547_4fefdbd5dc.jpg\nflower_photos/roses/6231418894_7946a7712b_n.jpg\nflower_photos/roses/509239741_28e2cfe492_m.jpg\nflower_photos/roses/15965652160_de91389965_m.jpg\nflower_photos/roses/3203779656_3580151ea4_m.jpg\nflower_photos/roses/2491600761_7e9d6776e8_m.jpg\nflower_photos/roses/3074406590_91c697c805_n.jpg\nflower_photos/roses/15011625580_7974c44bce.jpg\nflower_photos/roses/14414117598_cf70df30de.jpg\nflower_photos/roses/22385375599_1faf334f5d_n.jpg\nflower_photos/roses/5360769702_ec28c53b9e_n.jpg\nflower_photos/roses/3494252600_29f26e3ff0_n.jpg\nflower_photos/roses/5487945052_bcb8e9fc8b_m.jpg\nflower_photos/roses/13979889721_42a59ca9fa_m.jpg\nflower_photos/roses/5073473370_bdbb5a99fc.jpg\nflower_photos/roses/9300754115_dd79670066_n.jpg\nflower_photos/roses/2607130050_9c34310004.jpg\nflower_photos/roses/5180896559_b8cfefc21e.jpg\nflower_photos/roses/3265902330_d8b1e44545.jpg\nflower_photos/roses/8036594516_69a7da5f73_m.jpg\nflower_photos/roses/15255964274_cf2ecdf702.jpg\nflower_photos/roses/165985535_7178ce6350.jpg\nflower_photos/roses/15419696882_9394168a10_n.jpg\nflower_photos/roses/5526964611_76ef13025c_n.jpg\nflower_photos/roses/4609168052_3d4e1d3804_n.jpg\nflower_photos/roses/4731069260_b270f47803_n.jpg\nflower_photos/roses/3554620445_082dd0bec4_n.jpg\nflower_photos/roses/4765063233_f64440c20b.jpg\nflower_photos/roses/4764674741_82b8f93359_n.jpg\nflower_photos/roses/1788484468_f73afa6c32_n.jpg\nflower_photos/roses/4644336779_acd973528c.jpg\nflower_photos/roses/123128873_546b8b7355_n.jpg\nflower_photos/roses/5234278003_d827fcd73b_m.jpg\nflower_photos/roses/2141413229_3f0425f972_n.jpg\nflower_photos/roses/8524505868_236f4c94b5.jpg\nflower_photos/roses/6016195304_75306bb79a.jpg\nflower_photos/roses/20622485918_90fc000c86_n.jpg\nflower_photos/roses/9338237628_4d2547608c.jpg\nflower_photos/roses/9702378513_229a96b754_m.jpg\nflower_photos/roses/6111589202_8b9555364c_m.jpg\nflower_photos/roses/1469726748_f359f4a8c5.jpg\nflower_photos/roses/5182167964_9d1a0be0b8_n.jpg\nflower_photos/roses/7419966772_d6c1c22a81.jpg\nflower_photos/roses/8590442797_07fa2141c0_n.jpg\nflower_photos/roses/3475572132_01ae28e834_n.jpg\nflower_photos/roses/1793211631_68c31a74dc.jpg\nflower_photos/roses/3550491463_3eb092054c_m.jpg\nflower_photos/roses/7820626638_3e2d712303.jpg\nflower_photos/roses/15190665092_5c1c37a066_m.jpg\nflower_photos/roses/17953368844_be3d18cf30_m.jpg\nflower_photos/roses/4684127262_6c3346188d.jpg\nflower_photos/roses/3208417632_19138d8e35_n.jpg\nflower_photos/roses/8853083579_dd1dfa3188.jpg\nflower_photos/roses/12406418663_af20dc225f_n.jpg\nflower_photos/roses/19919867648_043cf02fc3.jpg\nflower_photos/roses/16334786972_1b3e71cab8_m.jpg\nflower_photos/roses/15172358234_28706749a5.jpg\nflower_photos/roses/6163179241_f093f45d95_n.jpg\nflower_photos/roses/11102341464_508d558dfc_n.jpg\nflower_photos/roses/6241886381_cc722785af.jpg\nflower_photos/roses/2863863372_605e29c03e_m.jpg\nflower_photos/roses/6363976189_e7155e5f9c.jpg\nflower_photos/roses/14166797345_d2ab9da518.jpg\nflower_photos/roses/6309548569_932fee8313_m.jpg\nflower_photos/roses/537625768_791e973b40.jpg\nflower_photos/roses/12238827553_cf427bfd51_n.jpg\nflower_photos/roses/17305246720_1866d6303b.jpg\nflower_photos/roses/15202632426_d88efb321a_n.jpg\nflower_photos/roses/5212885371_fe27c406a2_n.jpg\nflower_photos/roses/4809566219_88f9a1aea3.jpg\nflower_photos/roses/3576488381_611f3446e0_n.jpg\nflower_photos/roses/8050213579_48e1e7109f.jpg\nflower_photos/roses/14494590921_3bb1dc7b88_n.jpg\nflower_photos/roses/7865295712_bcc94d120c.jpg\nflower_photos/roses/8949720453_66e8304c30.jpg\nflower_photos/roses/5537794501_a0767743fd_n.jpg\nflower_photos/roses/3450344423_63ba3190e3.jpg\nflower_photos/roses/1831404161_d2df86fd70.jpg\nflower_photos/roses/6864417932_36fa4ceecf_n.jpg\nflower_photos/roses/15712574834_2f121c7cf9_m.jpg\nflower_photos/roses/12165480946_c4a3fe182d_n.jpg\nflower_photos/roses/6570546331_ffb9dab0bf_n.jpg\nflower_photos/roses/4504731519_9a260b6607_n.jpg\nflower_photos/roses/2347579838_dd6d2aaefc_n.jpg\nflower_photos/roses/2332478138_28f1d586e4_n.jpg\nflower_photos/roses/5089363428_2c5a1272ea.jpg\nflower_photos/roses/2300959680_8d22fa5ee2.jpg\nflower_photos/roses/14573732424_1bb91e2e42_n.jpg\nflower_photos/roses/3052753519_d087aaeacb_n.jpg\nflower_photos/roses/5990626258_697f007308_n.jpg\nflower_photos/roses/850416050_31b3ff7086.jpg\nflower_photos/roses/475947979_554062a608_m.jpg\nflower_photos/roses/3171577977_8608282f04_m.jpg\nflower_photos/roses/12045735155_42547ce4e9_n.jpg\nflower_photos/roses/4910094611_8c7170fc95_n.jpg\nflower_photos/roses/295257304_de893fc94d.jpg\nflower_photos/roses/8523394349_61b31fdd8f_m.jpg\nflower_photos/roses/2392457180_f02dab5c65.jpg\nflower_photos/roses/5193918046_d44e4fcd75_m.jpg\nflower_photos/roses/8032328803_30afac8b07_m.jpg\nflower_photos/roses/174109630_3c544b8a2f.jpg\nflower_photos/roses/12562723334_a2e0a9e3c8_n.jpg\nflower_photos/roses/4900231976_f8ced2b42a_n.jpg\nflower_photos/roses/3045046293_57f6d52065_m.jpg\nflower_photos/roses/475936554_a2b38aaa8e.jpg\nflower_photos/roses/2973256732_1926295f35.jpg\nflower_photos/roses/13264214185_d6aa79b3bd.jpg\nflower_photos/roses/1461381091_aaaa663bbe_n.jpg\nflower_photos/roses/17165596357_392a12391f.jpg\nflower_photos/roses/7187035716_5d0fb95c31_n.jpg\nflower_photos/roses/9633056561_6f1b7e8faf_m.jpg\nflower_photos/roses/16374919860_4e445de29f_n.jpg\nflower_photos/roses/756943228_e15a7b2318.jpg\nflower_photos/roses/2535466393_6556afeb2f_m.jpg\nflower_photos/roses/5088766459_f81f50e57d_n.jpg\nflower_photos/roses/534228982_4afbcece9b_m.jpg\nflower_photos/roses/8644003462_2272de26eb.jpg\nflower_photos/roses/4609166128_b7ed49b40b_m.jpg\nflower_photos/roses/13235124703_a7e1266e44.jpg\nflower_photos/roses/3104672186_5f75647448_n.jpg\nflower_photos/roses/12240165555_98625b1e88_n.jpg\nflower_photos/roses/3264570182_c7ded528ba_m.jpg\nflower_photos/roses/6347846687_3f0a7c3176.jpg\nflower_photos/roses/3630246240_4fee9a33db.jpg\nflower_photos/roses/9423755543_edb35141a3_n.jpg\nflower_photos/roses/7502389724_85b4a6c855_n.jpg\nflower_photos/roses/2756028421_b3d5eea526_n.jpg\nflower_photos/roses/8209458141_38f38be65c_m.jpg\nflower_photos/roses/3103591125_99107c8bbe_n.jpg\nflower_photos/roses/2501297526_cbd66a3f7e_m.jpg\nflower_photos/roses/2735666555_01d53e74fe.jpg\nflower_photos/roses/8562144481_1d629848ff.jpg\nflower_photos/roses/21413573151_e681c6a97a.jpg\nflower_photos/roses/2183357362_4b4da4b6b5.jpg\nflower_photos/roses/9159362388_c6f4cf3812_n.jpg\nflower_photos/roses/8667101118_87ea757b15.jpg\nflower_photos/roses/16316557109_7fc55c1cbc_m.jpg\nflower_photos/roses/5249566718_6109630c83_m.jpg\nflower_photos/roses/5333437251_ce0aa6925d_n.jpg\nflower_photos/roses/2675221506_5286c0595f.jpg\nflower_photos/roses/1485142251_ca89254442.jpg\nflower_photos/roses/483444865_65962cea07_m.jpg\nflower_photos/roses/4588034197_e300b0872a_n.jpg\nflower_photos/roses/8502529435_c6e40d0df4.jpg\nflower_photos/roses/5060519573_c628547e20_n.jpg\nflower_photos/roses/1445228333_59a07e0801.jpg\nflower_photos/roses/8674140377_ae7b0be523.jpg\nflower_photos/roses/4675532860_890504a4a3_m.jpg\nflower_photos/roses/2627513944_b1361e60ec_m.jpg\nflower_photos/roses/4495885281_fe2a3b671d.jpg\nflower_photos/roses/1562198683_8cd8cb5876_n.jpg\nflower_photos/roses/17090993740_fcc8b60b81.jpg\nflower_photos/roses/9609569441_eeb8566e94.jpg\nflower_photos/roses/512578026_f6e6f2ad26.jpg\nflower_photos/roses/11944957684_2cc806276e.jpg\nflower_photos/roses/3407482427_49d5c75291_m.jpg\nflower_photos/roses/9353111163_7a89b2df35_n.jpg\nflower_photos/roses/9458445402_79e4dfa89c.jpg\nflower_photos/roses/4256169180_55df2048a0.jpg\nflower_photos/roses/11694025703_9a906fedc1_n.jpg\nflower_photos/roses/6803363808_9f9ce98186_m.jpg\nflower_photos/roses/8437935944_aab997560a_n.jpg\nflower_photos/roses/16258946661_f9739cdc0a.jpg\nflower_photos/roses/4979895172_ca06eba616.jpg\nflower_photos/roses/9164924345_6b63637acf.jpg\nflower_photos/roses/14970973709_968910640e_n.jpg\nflower_photos/roses/8983268106_dc913d17d8_m.jpg\nflower_photos/roses/4505921907_21c8002fde.jpg\nflower_photos/roses/3415176946_248afe9f32.jpg\nflower_photos/roses/4503599544_3822e7d1be.jpg\nflower_photos/roses/180613732_3a7aba0b80_n.jpg\nflower_photos/roses/6280787884_141cd7b382_n.jpg\nflower_photos/roses/2331651885_619653a5d3.jpg\nflower_photos/roses/16155980245_6ab8d7b888.jpg\nflower_photos/roses/921138131_9e1393eb2b_m.jpg\nflower_photos/roses/6105809987_8f3d7a8d67_n.jpg\nflower_photos/roses/6950609394_c53b8c6ac0_m.jpg\nflower_photos/sunflowers/\nflower_photos/sunflowers/15118243470_7e0a7f159c_n.jpg\nflower_photos/sunflowers/22183529245_ce13557515_m.jpg\nflower_photos/sunflowers/20753711039_0b11d24b50_n.jpg\nflower_photos/sunflowers/5007598545_90e08e81c1_n.jpg\nflower_photos/sunflowers/215798352_184d8040d1.jpg\nflower_photos/sunflowers/175638423_058c07afb9.jpg\nflower_photos/sunflowers/6239758929_50e5e5a476_m.jpg\nflower_photos/sunflowers/15240466871_ec45b65554_m.jpg\nflower_photos/sunflowers/8266310743_02095e782d_m.jpg\nflower_photos/sunflowers/5994586159_1dd99d66b4_n.jpg\nflower_photos/sunflowers/21728822928_9f6817325a_n.jpg\nflower_photos/sunflowers/3514340206_efb8198a80_n.jpg\nflower_photos/sunflowers/10386540106_1431e73086_m.jpg\nflower_photos/sunflowers/22419079265_8902cddb7d_n.jpg\nflower_photos/sunflowers/19710076021_f5bb162540.jpg\nflower_photos/sunflowers/4805544785_a63241f6d0_n.jpg\nflower_photos/sunflowers/23286304156_3635f7de05.jpg\nflower_photos/sunflowers/15054864058_2edca122a9_n.jpg\nflower_photos/sunflowers/5955475577_3d923874d9_n.jpg\nflower_photos/sunflowers/5970300143_36b42437de_n.jpg\nflower_photos/sunflowers/14460075029_5cd715bb72_m.jpg\nflower_photos/sunflowers/9231555352_d2dd8f8e68_m.jpg\nflower_photos/sunflowers/3846907701_e13b66aa87.jpg\nflower_photos/sunflowers/4626721387_88f89d5cc9_n.jpg\nflower_photos/sunflowers/5437996076_cf7e2ac32e_n.jpg\nflower_photos/sunflowers/9904127656_f76a5a4811_m.jpg\nflower_photos/sunflowers/244074259_47ce6d3ef9.jpg\nflower_photos/sunflowers/14741907467_fab96f3b2b_n.jpg\nflower_photos/sunflowers/3883895985_bd20198371.jpg\nflower_photos/sunflowers/20344282483_05abb0b837.jpg\nflower_photos/sunflowers/19697910486_0086d893a2.jpg\nflower_photos/sunflowers/15054753070_4f6ae0e763_m.jpg\nflower_photos/sunflowers/7935826214_9b57628203_m.jpg\nflower_photos/sunflowers/5967283168_90dd4daf28_n.jpg\nflower_photos/sunflowers/40411100_7fbe10ec0f_n.jpg\nflower_photos/sunflowers/9481563239_01b585b41d_n.jpg\nflower_photos/sunflowers/1596293240_2d5b53495a_m.jpg\nflower_photos/sunflowers/15054750690_198b6ab0f2_n.jpg\nflower_photos/sunflowers/22183521655_56221bf2a4_n.jpg\nflower_photos/sunflowers/9610373158_5250bce6ac_n.jpg\nflower_photos/sunflowers/2960610406_b61930727f_n.jpg\nflower_photos/sunflowers/6122711533_2c219f0392_n.jpg\nflower_photos/sunflowers/20078409301_aa8061bd0b_n.jpg\nflower_photos/sunflowers/5957007921_62333981d2_n.jpg\nflower_photos/sunflowers/19504937128_a4ae90fcbd_m.jpg\nflower_photos/sunflowers/3922005347_7b6fb82fcd.jpg\nflower_photos/sunflowers/9511172241_8aee411e2e.jpg\nflower_photos/sunflowers/8563099326_8be9177101.jpg\nflower_photos/sunflowers/4942258704_c4146b710a_n.jpg\nflower_photos/sunflowers/20022771089_3cc7e5086d_m.jpg\nflower_photos/sunflowers/9538283930_0faea083bb_n.jpg\nflower_photos/sunflowers/4625255191_26e17a28c9_n.jpg\nflower_photos/sunflowers/14678298676_6db8831ee6_m.jpg\nflower_photos/sunflowers/17148843706_df148301ac_n.jpg\nflower_photos/sunflowers/4933822272_79af205b94.jpg\nflower_photos/sunflowers/6140808687_88df0fd733.jpg\nflower_photos/sunflowers/6953297_8576bf4ea3.jpg\nflower_photos/sunflowers/22416421196_caf131c9fa_m.jpg\nflower_photos/sunflowers/6166888942_7058198713_m.jpg\nflower_photos/sunflowers/9651392844_77f90589ba_n.jpg\nflower_photos/sunflowers/19784656639_cd7f0a4a26_m.jpg\nflower_photos/sunflowers/4932735362_6e1017140f.jpg\nflower_photos/sunflowers/6140661443_bb48344226.jpg\nflower_photos/sunflowers/5231868667_f0baa71feb_n.jpg\nflower_photos/sunflowers/14144522269_bc20029375_m.jpg\nflower_photos/sunflowers/2927020075_54c9186797_n.jpg\nflower_photos/sunflowers/151898652_b5f1c70b98_n.jpg\nflower_photos/sunflowers/2706736074_b0fba20b3e.jpg\nflower_photos/sunflowers/8292914969_4a76608250_m.jpg\nflower_photos/sunflowers/20965412955_2c640b13bd.jpg\nflower_photos/sunflowers/4933229479_c1708bd503.jpg\nflower_photos/sunflowers/5492906452_80943bfd04.jpg\nflower_photos/sunflowers/15123604714_dd034a4a3b_n.jpg\nflower_photos/sunflowers/2619000556_6634478e64_n.jpg\nflower_photos/sunflowers/14925397351_c7f209d804_n.jpg\nflower_photos/sunflowers/6145005439_ef6e07f9c6_n.jpg\nflower_photos/sunflowers/857698097_8068a2c135_n.jpg\nflower_photos/sunflowers/7176729812_7c053921fb_m.jpg\nflower_photos/sunflowers/20344366953_44fb51051b.jpg\nflower_photos/sunflowers/6080086410_17a02dcfb8.jpg\nflower_photos/sunflowers/20777375650_ef854bf645.jpg\nflower_photos/sunflowers/23204123212_ef32fbafbe.jpg\nflower_photos/sunflowers/7581713708_8eae6794f2.jpg\nflower_photos/sunflowers/6198569425_e953b9e6cc_m.jpg\nflower_photos/sunflowers/2979297519_17a08b37f6_m.jpg\nflower_photos/sunflowers/7820398908_4316bbba45.jpg\nflower_photos/sunflowers/3766264038_ea701c7131_n.jpg\nflower_photos/sunflowers/235651658_a7b3e7cbdd.jpg\nflower_photos/sunflowers/4847062576_bae870479c_n.jpg\nflower_photos/sunflowers/7530313068_ddd2dc1f44_m.jpg\nflower_photos/sunflowers/19442589512_e733cfea0f.jpg\nflower_photos/sunflowers/4933229561_881d4673e7_m.jpg\nflower_photos/sunflowers/2950505226_529e013bf7_m.jpg\nflower_photos/sunflowers/14121915990_4b76718077_m.jpg\nflower_photos/sunflowers/15839183375_49bf4f75e8_m.jpg\nflower_photos/sunflowers/2729206569_9dd2b5a3ed.jpg\nflower_photos/sunflowers/10386503264_e05387e1f7_m.jpg\nflower_photos/sunflowers/19359539074_d7e32e6616_n.jpg\nflower_photos/sunflowers/4019748730_ee09b39a43.jpg\nflower_photos/sunflowers/1297092593_e573c0a3d6.jpg\nflower_photos/sunflowers/19453165201_2aa747e0bf.jpg\nflower_photos/sunflowers/4933824012_8cbfe606f6.jpg\nflower_photos/sunflowers/2894191705_a1d2d80c80.jpg\nflower_photos/sunflowers/14969295739_c132a08663_n.jpg\nflower_photos/sunflowers/5979111199_495884b578_n.jpg\nflower_photos/sunflowers/20183071136_c297e74fcc_m.jpg\nflower_photos/sunflowers/1044296388_912143e1d4.jpg\nflower_photos/sunflowers/4235259239_21f2eb4f2e.jpg\nflower_photos/sunflowers/9483429732_5ae73eb672_n.jpg\nflower_photos/sunflowers/274846229_990e976683_n.jpg\nflower_photos/sunflowers/14889779907_3d401bbac7_m.jpg\nflower_photos/sunflowers/6061177447_d8ce96aee0.jpg\nflower_photos/sunflowers/5020805135_1219d7523d.jpg\nflower_photos/sunflowers/1008566138_6927679c8a.jpg\nflower_photos/sunflowers/4869189730_f47c124cda_n.jpg\nflower_photos/sunflowers/6053739964_a1d9ab3ed1_n.jpg\nflower_photos/sunflowers/3681233294_4f06cd8903.jpg\nflower_photos/sunflowers/7791014076_07a897cb85_n.jpg\nflower_photos/sunflowers/4746638094_f5336788a0_n.jpg\nflower_photos/sunflowers/20410533613_56da1cce7c.jpg\nflower_photos/sunflowers/4080112931_cb20b3d51a_n.jpg\nflower_photos/sunflowers/5004121118_e9393e60d0_n.jpg\nflower_photos/sunflowers/9564240106_0577e919da_n.jpg\nflower_photos/sunflowers/2723995667_31f32294b4.jpg\nflower_photos/sunflowers/15191613243_82ee8e0fe8.jpg\nflower_photos/sunflowers/2720698862_486d3ec079_m.jpg\nflower_photos/sunflowers/4414081772_8a0e8a1327.jpg\nflower_photos/sunflowers/4868595281_1e58083785.jpg\nflower_photos/sunflowers/22255608949_172d7c8d22_m.jpg\nflower_photos/sunflowers/5995136822_8e1eed76f5_n.jpg\nflower_photos/sunflowers/7176736574_14446539cb_n.jpg\nflower_photos/sunflowers/8368015811_2893411cf7_n.jpg\nflower_photos/sunflowers/15122112402_cafa41934f.jpg\nflower_photos/sunflowers/210076535_80951bc5d5.jpg\nflower_photos/sunflowers/5970868068_fe1c8b282e_n.jpg\nflower_photos/sunflowers/16143151468_4f3c033e33.jpg\nflower_photos/sunflowers/6606806621_5267acdd38.jpg\nflower_photos/sunflowers/5223643767_d8beb7e410.jpg\nflower_photos/sunflowers/184682320_73ccf74710.jpg\nflower_photos/sunflowers/3897174387_07aac6bf5f_n.jpg\nflower_photos/sunflowers/200557977_bf24d9550b.jpg\nflower_photos/sunflowers/6199086734_b7ddc65816_m.jpg\nflower_photos/sunflowers/15069459615_7e0fd61914_n.jpg\nflower_photos/sunflowers/8174970894_7f9a26be7e.jpg\nflower_photos/sunflowers/3154932290_4bf43bd34f_n.jpg\nflower_photos/sunflowers/23356825566_f5885875f2.jpg\nflower_photos/sunflowers/13354458753_7b586f7c95_n.jpg\nflower_photos/sunflowers/15043962658_dcf9dff5e9_n.jpg\nflower_photos/sunflowers/3950020811_dab89bebc0_n.jpg\nflower_photos/sunflowers/18250039435_7654bc11be_n.jpg\nflower_photos/sunflowers/16975010069_7afd290657_m.jpg\nflower_photos/sunflowers/14741866338_bdc8bfc8d5_n.jpg\nflower_photos/sunflowers/5139969631_743880e01d_n.jpg\nflower_photos/sunflowers/16967372357_15b1b9a812_n.jpg\nflower_photos/sunflowers/4878447831_e904c60cf8_n.jpg\nflower_photos/sunflowers/3594967811_697184b026_n.jpg\nflower_photos/sunflowers/4933229357_1c5cc03f65_m.jpg\nflower_photos/sunflowers/7820305664_82148f3bfb_n.jpg\nflower_photos/sunflowers/12471290635_1f9e3aae16_n.jpg\nflower_photos/sunflowers/2575272111_f04d79b9af_n.jpg\nflower_photos/sunflowers/8174935013_b16626b49b.jpg\nflower_photos/sunflowers/13096076565_72c2c60875_n.jpg\nflower_photos/sunflowers/15460162172_014bcce403.jpg\nflower_photos/sunflowers/15054865217_e398d0dc9f_n.jpg\nflower_photos/sunflowers/15472217046_2699b25584.jpg\nflower_photos/sunflowers/40410814_fba3837226_n.jpg\nflower_photos/sunflowers/15066430311_fb57fa92b0_m.jpg\nflower_photos/sunflowers/145303599_2627e23815_n.jpg\nflower_photos/sunflowers/9681915384_b3b646dc92_m.jpg\nflower_photos/sunflowers/5970869550_d7d9fabebd_n.jpg\nflower_photos/sunflowers/3568114325_d6b1363497.jpg\nflower_photos/sunflowers/7270375648_79f0caef42_n.jpg\nflower_photos/sunflowers/8174941335_56389b53e9_n.jpg\nflower_photos/sunflowers/58636535_bc53ef0a21_m.jpg\nflower_photos/sunflowers/40410963_3ac280f23a_n.jpg\nflower_photos/sunflowers/12471441503_d188b5f31a_m.jpg\nflower_photos/sunflowers/15054866658_c1a6223403_m.jpg\nflower_photos/sunflowers/1485456230_58d8e45e88.jpg\nflower_photos/sunflowers/15054865768_2cc87ac9d4_m.jpg\nflower_photos/sunflowers/5139977423_d413b23fde_m.jpg\nflower_photos/sunflowers/14646282112_447cc7d1f9.jpg\nflower_photos/sunflowers/3731075939_6c92d7fe68_m.jpg\nflower_photos/sunflowers/7603036176_9e8967cd21.jpg\nflower_photos/sunflowers/8265023280_713f2c69d0_m.jpg\nflower_photos/sunflowers/14889392928_9742aed45b_m.jpg\nflower_photos/sunflowers/19710925313_31682fa22b_m.jpg\nflower_photos/sunflowers/9381481549_5a5d503e42_n.jpg\nflower_photos/sunflowers/200011914_93f57ed68b.jpg\nflower_photos/sunflowers/3596902268_049e33a2cb_n.jpg\nflower_photos/sunflowers/3665455426_9cd1c3af4a_n.jpg\nflower_photos/sunflowers/11881770944_22b4f2f8f6_n.jpg\nflower_photos/sunflowers/5973935729_2868f2db1f_n.jpg\nflower_photos/sunflowers/3575811488_a31714472a.jpg\nflower_photos/sunflowers/1244774242_25a20d99a9.jpg\nflower_photos/sunflowers/4895720722_8247f2015b_n.jpg\nflower_photos/sunflowers/200557979_a16112aac1_n.jpg\nflower_photos/sunflowers/15241052342_466b38b68d.jpg\nflower_photos/sunflowers/5979111555_61b400c070_n.jpg\nflower_photos/sunflowers/15054866898_60ee50ec6b_n.jpg\nflower_photos/sunflowers/5923649444_a823e534e9.jpg\nflower_photos/sunflowers/8543642705_b841b0e5f6.jpg\nflower_photos/sunflowers/9213511121_836a458021_m.jpg\nflower_photos/sunflowers/6482016439_b0d06dac04.jpg\nflower_photos/sunflowers/4746648726_e37a2de16d_n.jpg\nflower_photos/sunflowers/12471791574_bb1be83df4.jpg\nflower_photos/sunflowers/9497774935_a7daec5433_n.jpg\nflower_photos/sunflowers/15026703621_e15e9d55f0_n.jpg\nflower_photos/sunflowers/23645265812_24352ff6bf.jpg\nflower_photos/sunflowers/4816636411_0135bfe2c9_n.jpg\nflower_photos/sunflowers/3146795631_d062f233c1.jpg\nflower_photos/sunflowers/14814264272_4b39a102f9_n.jpg\nflower_photos/sunflowers/22405882322_d4561f8469_n.jpg\nflower_photos/sunflowers/4186808407_06688641e2_n.jpg\nflower_photos/sunflowers/15495579081_661cb260d1_n.jpg\nflower_photos/sunflowers/9558632814_e78a780f4f.jpg\nflower_photos/sunflowers/6116210027_61923f4b64.jpg\nflower_photos/sunflowers/21134000558_d7d6c9b1fe_n.jpg\nflower_photos/sunflowers/6125761554_4e72819ce4_m.jpg\nflower_photos/sunflowers/201809908_0ef84bb351.jpg\nflower_photos/sunflowers/5330608174_b49f7a4c48_m.jpg\nflower_photos/sunflowers/5896354497_6a19162741.jpg\nflower_photos/sunflowers/14348961225_09bd803317_n.jpg\nflower_photos/sunflowers/9482209981_bf7bf6022b_m.jpg\nflower_photos/sunflowers/4895718876_0246882882_n.jpg\nflower_photos/sunflowers/4755705724_976621a1e7.jpg\nflower_photos/sunflowers/877083343_e3338c4125.jpg\nflower_photos/sunflowers/5994572653_ea98afa3af_n.jpg\nflower_photos/sunflowers/200557981_f800fa1af9.jpg\nflower_photos/sunflowers/2940221732_3507f3e927_n.jpg\nflower_photos/sunflowers/678714585_addc9aaaef.jpg\nflower_photos/sunflowers/2823659190_afdabee45c.jpg\nflower_photos/sunflowers/4872892690_52dc25b0b4.jpg\nflower_photos/sunflowers/3001531316_efae24d37d_n.jpg\nflower_photos/sunflowers/4110787181_f73f12d107_m.jpg\nflower_photos/sunflowers/4872284527_ff52128b97.jpg\nflower_photos/sunflowers/4940287066_385afd9c18_m.jpg\nflower_photos/sunflowers/5020805619_6c710793f7.jpg\nflower_photos/sunflowers/9056495873_66e351b17c_n.jpg\nflower_photos/sunflowers/6866250080_ae80df0cd5_m.jpg\nflower_photos/sunflowers/2697194548_ec8f8de97c_n.jpg\nflower_photos/sunflowers/1379256773_bb2eb0d95b_n.jpg\nflower_photos/sunflowers/18097401209_910a46fae1_n.jpg\nflower_photos/sunflowers/2767688889_b176b0c3fb.jpg\nflower_photos/sunflowers/4933230161_12f3ee7587.jpg\nflower_photos/sunflowers/6606743797_c90c669757.jpg\nflower_photos/sunflowers/3749090865_b90f28a585_n.jpg\nflower_photos/sunflowers/40410686_272bc66faf_m.jpg\nflower_photos/sunflowers/14741813110_94964c39e2_n.jpg\nflower_photos/sunflowers/4813483281_f3707a71e7_n.jpg\nflower_photos/sunflowers/23247483352_0defc7a6dc_n.jpg\nflower_photos/sunflowers/4932143849_018486cbf7.jpg\nflower_photos/sunflowers/8014735546_3db46bb1fe_n.jpg\nflower_photos/sunflowers/15745084272_36402f5ee6_n.jpg\nflower_photos/sunflowers/10386702973_e74a34c806_n.jpg\nflower_photos/sunflowers/15030133005_9728102622_z.jpg\nflower_photos/sunflowers/20621698991_dcb323911d.jpg\nflower_photos/sunflowers/9240129413_f240ce7866_n.jpg\nflower_photos/sunflowers/6050020905_881295ac72_n.jpg\nflower_photos/sunflowers/4933228903_9ae82d0b9d.jpg\nflower_photos/sunflowers/6140693467_211a135b6d.jpg\nflower_photos/sunflowers/3838274225_36010c6254_n.jpg\nflower_photos/sunflowers/5923085891_27617463fe.jpg\nflower_photos/sunflowers/5043409856_395300dbe5_m.jpg\nflower_photos/sunflowers/1240625276_fb3bd0c7b1.jpg\nflower_photos/sunflowers/1314584013_fe935fdeb1_n.jpg\nflower_photos/sunflowers/14698136411_23bdcff7bf_n.jpg\nflower_photos/sunflowers/6198569587_23c3693328_m.jpg\nflower_photos/sunflowers/14460081668_eda8795693_m.jpg\nflower_photos/sunflowers/9484354480_07ff2ef0a6.jpg\nflower_photos/sunflowers/5028817729_f04d32bac8_n.jpg\nflower_photos/sunflowers/9302733302_2cb92cf275.jpg\nflower_photos/sunflowers/14881304632_54a9dfb8be.jpg\nflower_photos/sunflowers/9111896677_ff0b6fa6f6_n.jpg\nflower_photos/sunflowers/20905163782_312e2c3bda_n.jpg\nflower_photos/sunflowers/9461693602_710f20904f.jpg\nflower_photos/sunflowers/8705462313_4458d64cd4.jpg\nflower_photos/sunflowers/5998488415_a6bacd9f83.jpg\nflower_photos/sunflowers/12471443383_b71e7a7480_m.jpg\nflower_photos/sunflowers/3749091071_c146b33c74_n.jpg\nflower_photos/sunflowers/4489516263_e49fe82637_n.jpg\nflower_photos/sunflowers/287233531_74d4605814_m.jpg\nflower_photos/sunflowers/26254755_1bfc494ef1_n.jpg\nflower_photos/sunflowers/3832945398_96509d192b.jpg\nflower_photos/sunflowers/14928117202_139d2142cc_n.jpg\nflower_photos/sunflowers/9445830851_e9a126fd1d_n.jpg\nflower_photos/sunflowers/7176723954_e41618edc1_n.jpg\nflower_photos/sunflowers/193874852_fb633d8d00_n.jpg\nflower_photos/sunflowers/7721658400_0dec46d225.jpg\nflower_photos/sunflowers/4933822052_2dfef02517.jpg\nflower_photos/sunflowers/5526324308_b333da0e57_n.jpg\nflower_photos/sunflowers/9655029591_7a77f87500.jpg\nflower_photos/sunflowers/21821266773_7113d34c35_m.jpg\nflower_photos/sunflowers/9359374034_21fb12d613_n.jpg\nflower_photos/sunflowers/147804446_ef9244c8ce_m.jpg\nflower_photos/sunflowers/4821232343_7e0bcfbfdf_n.jpg\nflower_photos/sunflowers/7857605684_fc86440c23.jpg\nflower_photos/sunflowers/4673984698_6ec14d5b79.jpg\nflower_photos/sunflowers/1419608016_707b887337_n.jpg\nflower_photos/sunflowers/16656015339_2ccb7cd18d.jpg\nflower_photos/sunflowers/8929213942_5544191250_n.jpg\nflower_photos/sunflowers/265422922_bbbde781d2_m.jpg\nflower_photos/sunflowers/193878348_43571127b9_n.jpg\nflower_photos/sunflowers/4980406384_791774d953.jpg\nflower_photos/sunflowers/4042816698_578a1d599e.jpg\nflower_photos/sunflowers/5967284308_85714d8cf7_m.jpg\nflower_photos/sunflowers/253586685_ee5b5f5232.jpg\nflower_photos/sunflowers/14397276020_49f9423614.jpg\nflower_photos/sunflowers/35477171_13cb52115c_n.jpg\nflower_photos/sunflowers/15243175532_ac28c48e14_m.jpg\nflower_photos/sunflowers/20704967595_a9c9b8d431.jpg\nflower_photos/sunflowers/5979668702_fdaec9e164_n.jpg\nflower_photos/sunflowers/9610373748_b9cb67bd55.jpg\nflower_photos/sunflowers/2598973480_07de93e91d_n.jpg\nflower_photos/sunflowers/4414084638_03d2db38ae.jpg\nflower_photos/sunflowers/3846717708_ea11383ed8.jpg\nflower_photos/sunflowers/4895719476_bd3b6bd6fd_n.jpg\nflower_photos/sunflowers/1267876087_a1b3c63dc9.jpg\nflower_photos/sunflowers/15108515192_f686dce398_n.jpg\nflower_photos/sunflowers/22478719251_276cb094f9_n.jpg\nflower_photos/sunflowers/969913643_9d5cd2fe45_m.jpg\nflower_photos/sunflowers/14741813010_5d44e33088_n.jpg\nflower_photos/sunflowers/3893436870_034b79d118_n.jpg\nflower_photos/sunflowers/4932144003_cbffc89bf0.jpg\nflower_photos/sunflowers/14925398441_107f3e0304_n.jpg\nflower_photos/sunflowers/9558630626_52a1b7d702_m.jpg\nflower_photos/sunflowers/8929288228_6795bcb1fe.jpg\nflower_photos/sunflowers/9384867134_83af458a19_n.jpg\nflower_photos/sunflowers/4989952542_35f2cdd5e2_n.jpg\nflower_photos/sunflowers/2733109082_1351f6738a_n.jpg\nflower_photos/sunflowers/20171662239_f69b6c12bd_n.jpg\nflower_photos/sunflowers/3154932076_eff5c38231_n.jpg\nflower_photos/sunflowers/9410186154_465642ed35.jpg\nflower_photos/sunflowers/4914793782_d0ea760791.jpg\nflower_photos/sunflowers/2588453601_66f2a03cca_n.jpg\nflower_photos/sunflowers/184682506_8a9b8c662d.jpg\nflower_photos/sunflowers/3798841385_38142ea3c6_n.jpg\nflower_photos/sunflowers/3951246342_930138610b_n.jpg\nflower_photos/sunflowers/5032376020_2ed312306c.jpg\nflower_photos/sunflowers/1043442695_4556c4c13d_n.jpg\nflower_photos/sunflowers/22686342422_c0b9e2f38e.jpg\nflower_photos/sunflowers/4818994715_9d90527d18_n.jpg\nflower_photos/sunflowers/19519101829_46af0b4547_m.jpg\nflower_photos/sunflowers/4848279231_c4960e28b2_n.jpg\nflower_photos/sunflowers/1240624822_4111dde542.jpg\nflower_photos/sunflowers/5015462205_440898fe41_n.jpg\nflower_photos/sunflowers/184682652_c927a49226_m.jpg\nflower_photos/sunflowers/200288046_0032f322ff_n.jpg\nflower_photos/sunflowers/4160805260_cf758daeae_n.jpg\nflower_photos/sunflowers/2443095419_17b920d155_m.jpg\nflower_photos/sunflowers/1217254584_4b3028b93d.jpg\nflower_photos/sunflowers/8081530919_c882d46bb0_n.jpg\nflower_photos/sunflowers/5339004958_a0a6f385fd_m.jpg\nflower_photos/sunflowers/14925397761_46ecfa24e0.jpg\nflower_photos/sunflowers/2759796022_55bd47bfa2_n.jpg\nflower_photos/sunflowers/7820523050_76c8caa025.jpg\nflower_photos/sunflowers/44079668_34dfee3da1_n.jpg\nflower_photos/sunflowers/5139971615_434ff8ed8b_n.jpg\nflower_photos/sunflowers/5994569021_749d5e2da3_n.jpg\nflower_photos/sunflowers/3062794421_295f8c2c4e.jpg\nflower_photos/sunflowers/6606815161_3c4372760f.jpg\nflower_photos/sunflowers/7586498522_4dcab1c8d2_m.jpg\nflower_photos/sunflowers/15081164641_45a7b92b3a_m.jpg\nflower_photos/sunflowers/8929274876_17efc1774a_n.jpg\nflower_photos/sunflowers/22992257000_76dbc599e7_m.jpg\nflower_photos/sunflowers/1240626292_52cd5d7fb1_m.jpg\nflower_photos/sunflowers/4933229197_ff75a40d55.jpg\nflower_photos/sunflowers/1715303025_e7065327e2.jpg\nflower_photos/sunflowers/4895721242_89014e723c_n.jpg\nflower_photos/sunflowers/3784815653_5df39aa9c2_m.jpg\nflower_photos/sunflowers/10386522775_4f8c616999_m.jpg\nflower_photos/sunflowers/2816503473_580306e772.jpg\nflower_photos/sunflowers/14646280372_dd50be16e4_n.jpg\nflower_photos/sunflowers/5042785753_392cc4e74d_n.jpg\nflower_photos/sunflowers/2678588376_6ca64a4a54_n.jpg\nflower_photos/sunflowers/5979669004_d9736206c9_n.jpg\nflower_photos/sunflowers/4877195645_791c3a83b9_m.jpg\nflower_photos/sunflowers/9431890901_cd11bda584_n.jpg\nflower_photos/sunflowers/14646281372_5f13794b47.jpg\nflower_photos/sunflowers/2996573407_5e473b9359.jpg\nflower_photos/sunflowers/4831577091_f56157a5d5_n.jpg\nflower_photos/sunflowers/19508264965_d1dfb565ea_n.jpg\nflower_photos/sunflowers/8174972548_0051c2d431.jpg\nflower_photos/sunflowers/5738580862_e128192f75.jpg\nflower_photos/sunflowers/22405887122_75eda1872f_m.jpg\nflower_photos/sunflowers/8480886751_71d88bfdc0_n.jpg\nflower_photos/sunflowers/4414083164_3f285f8ac5.jpg\nflower_photos/sunflowers/200557983_10a88672fc.jpg\nflower_photos/sunflowers/19595718862_c68896370c_m.jpg\nflower_photos/sunflowers/4932735566_2327bf319a.jpg\nflower_photos/sunflowers/5970301989_fe3a68aac8_m.jpg\nflower_photos/sunflowers/5018120483_cc0421b176_m.jpg\nflower_photos/sunflowers/4932736136_0115955987.jpg\nflower_photos/sunflowers/8202034834_ee0ee91e04_n.jpg\nflower_photos/sunflowers/6606741847_f0198d83ff.jpg\nflower_photos/sunflowers/15493195788_60530f2398_m.jpg\nflower_photos/sunflowers/15054751430_5af76f6096_n.jpg\nflower_photos/sunflowers/16832961488_5f7e70eb5e_n.jpg\nflower_photos/sunflowers/1788133737_b1133d1aa7.jpg\nflower_photos/sunflowers/9825716455_f12bcc8d4e_n.jpg\nflower_photos/sunflowers/4746668678_0e2693b1b9_n.jpg\nflower_photos/sunflowers/7654774598_6b715a8d3e.jpg\nflower_photos/sunflowers/3815322974_52c12dbde3.jpg\nflower_photos/sunflowers/5955501969_e42f038a6f_n.jpg\nflower_photos/sunflowers/15207507116_8b7f894508_m.jpg\nflower_photos/sunflowers/19349582128_68a662075e_n.jpg\nflower_photos/sunflowers/14921668662_3ffc5b9db3_n.jpg\nflower_photos/sunflowers/5917253022_4e3142d48b_n.jpg\nflower_photos/sunflowers/4977385375_e271e282f9.jpg\nflower_photos/sunflowers/417251603_69f0ee57a9_m.jpg\nflower_photos/sunflowers/27466794_57e4fe5656.jpg\nflower_photos/sunflowers/9206376642_8348ba5c7a.jpg\nflower_photos/sunflowers/2949654221_909b0c86a1_n.jpg\nflower_photos/sunflowers/4895721788_f10208ab77_n.jpg\nflower_photos/sunflowers/9699724719_a8439cc0fd_n.jpg\nflower_photos/sunflowers/4933230547_394f618009_m.jpg\nflower_photos/sunflowers/9783416751_b2a03920f7_n.jpg\nflower_photos/sunflowers/4271193206_666ef60aa0_m.jpg\nflower_photos/sunflowers/10386525695_2c38fea555_n.jpg\nflower_photos/sunflowers/9240005603_6a9b71dcea_n.jpg\nflower_photos/sunflowers/4893660821_eb7f02bef3_n.jpg\nflower_photos/sunflowers/14901528533_ac1ce09063.jpg\nflower_photos/sunflowers/4933230991_d50c0f7c66.jpg\nflower_photos/sunflowers/5966729883_67f4fede93.jpg\nflower_photos/sunflowers/18766965343_9f42d4bedc_m.jpg\nflower_photos/sunflowers/21796333524_38fc8e0ab5_n.jpg\nflower_photos/sunflowers/4890268276_563f40a193.jpg\nflower_photos/sunflowers/184682920_97ae41ce60_m.jpg\nflower_photos/sunflowers/6606746467_a668c8d417.jpg\nflower_photos/sunflowers/6482016425_d8fab362f6.jpg\nflower_photos/sunflowers/164670455_29d8e02bbd_n.jpg\nflower_photos/sunflowers/7369484298_332f69bd88_n.jpg\nflower_photos/sunflowers/9558628596_722c29ec60_m.jpg\nflower_photos/sunflowers/4933821940_38064522a8.jpg\nflower_photos/sunflowers/310380634_60e6c79989.jpg\nflower_photos/sunflowers/4664767140_fe01231322_n.jpg\nflower_photos/sunflowers/164672339_f2b5b164f6.jpg\nflower_photos/sunflowers/9535500195_543d0b729b.jpg\nflower_photos/sunflowers/4341530649_c17bbc5d01.jpg\nflower_photos/sunflowers/15266715291_dfa3f1d49f_n.jpg\nflower_photos/sunflowers/15973657966_d6f6005539_n.jpg\nflower_photos/sunflowers/2816256710_a2d3616fae.jpg\nflower_photos/sunflowers/21374127408_5ffbe87bb2.jpg\nflower_photos/sunflowers/7804213238_1d92ae5edb_m.jpg\nflower_photos/sunflowers/197011740_21825de2bf.jpg\nflower_photos/sunflowers/17433282043_441b0a07f4_n.jpg\nflower_photos/sunflowers/10862313945_e8ed9202d9_m.jpg\nflower_photos/sunflowers/4933823300_39fd4420b6.jpg\nflower_photos/sunflowers/2442985637_8748180f69.jpg\nflower_photos/sunflowers/3683873444_be4a609c46.jpg\nflower_photos/sunflowers/6140892289_92805cc590.jpg\nflower_photos/sunflowers/13095941995_9a66faa713_n.jpg\nflower_photos/sunflowers/9491955955_d0b2c83834.jpg\nflower_photos/sunflowers/8234846550_fdaf326dbe.jpg\nflower_photos/sunflowers/8038712786_5bdeed3c7f_m.jpg\nflower_photos/sunflowers/6104713425_8a3277e34a.jpg\nflower_photos/sunflowers/7012364067_5ffc7654c9_m.jpg\nflower_photos/sunflowers/7176729016_d73ff2211e.jpg\nflower_photos/sunflowers/4528959364_fa544b0f4e_m.jpg\nflower_photos/sunflowers/6250692311_cb60c85ee9_n.jpg\nflower_photos/sunflowers/3848405800_8eea982c40.jpg\nflower_photos/sunflowers/5069564563_ae03792c3c_m.jpg\nflower_photos/sunflowers/15054864508_0334b892be_m.jpg\nflower_photos/sunflowers/3334350831_f8755a2095_n.jpg\nflower_photos/sunflowers/2803725948_5fd1f2fc99_n.jpg\nflower_photos/sunflowers/8174935717_d19367d502.jpg\nflower_photos/sunflowers/7012366081_019c8a17a4_m.jpg\nflower_photos/sunflowers/21995435890_e5672244a4_m.jpg\nflower_photos/sunflowers/2425164088_4a5d2cdf21_n.jpg\nflower_photos/sunflowers/1484598527_579a272f53.jpg\nflower_photos/sunflowers/6627521877_6e43fb3c49_m.jpg\nflower_photos/sunflowers/5043409092_5b12cc985a_m.jpg\nflower_photos/sunflowers/8014734302_65c6e83bb4_m.jpg\nflower_photos/sunflowers/18972803569_1a0634f398_m.jpg\nflower_photos/sunflowers/3568925290_faf7aec3a0.jpg\nflower_photos/sunflowers/274848710_5185cf33b1_n.jpg\nflower_photos/sunflowers/6133988570_9dc778e622_m.jpg\nflower_photos/sunflowers/5927432662_3ffd2461c2_n.jpg\nflower_photos/sunflowers/10386540696_0a95ee53a8_n.jpg\nflower_photos/sunflowers/20871601265_daa4be4291_n.jpg\nflower_photos/sunflowers/4933230395_7930697335_m.jpg\nflower_photos/sunflowers/14932787983_d6e05f2434_m.jpg\nflower_photos/sunflowers/9309473873_9d62b9082e.jpg\nflower_photos/sunflowers/5556633113_0a04f5ed8a_n.jpg\nflower_photos/sunflowers/5037790727_57c527494f.jpg\nflower_photos/sunflowers/3840761441_7c648abf4d_n.jpg\nflower_photos/sunflowers/4398771472_44f2a0c162_n.jpg\nflower_photos/sunflowers/6042014768_b57f0bfc79_n.jpg\nflower_photos/sunflowers/5025805406_033cb03475_n.jpg\nflower_photos/sunflowers/18237215308_a158d49f28_n.jpg\nflower_photos/sunflowers/8249000137_eddfffa380_n.jpg\nflower_photos/sunflowers/14646283472_50a3ae1395.jpg\nflower_photos/sunflowers/9246304620_768d1f54d7_n.jpg\nflower_photos/sunflowers/5999024446_5721493894.jpg\nflower_photos/sunflowers/13117907313_86c99c6441.jpg\nflower_photos/sunflowers/20407896403_a50fef58ac_n.jpg\nflower_photos/sunflowers/4746643626_02b2d056a2_n.jpg\nflower_photos/sunflowers/4755075329_1fccc69d4e.jpg\nflower_photos/sunflowers/9339697826_88c9c4dc50.jpg\nflower_photos/sunflowers/20342824594_9740b7b160.jpg\nflower_photos/sunflowers/8071460469_a7c2c34b97_n.jpg\nflower_photos/sunflowers/4814106562_7c3564d2d9_n.jpg\nflower_photos/sunflowers/9555827829_74e6f60f1d_m.jpg\nflower_photos/sunflowers/5139977579_ea2dd6a322_m.jpg\nflower_photos/sunflowers/6204049536_1ac4f09232_n.jpg\nflower_photos/sunflowers/4689061249_6498da5013.jpg\nflower_photos/sunflowers/2067882323_8de6623ffd.jpg\nflower_photos/sunflowers/8021568040_f891223c44_n.jpg\nflower_photos/sunflowers/9460336948_6ae968be93.jpg\nflower_photos/sunflowers/1022552036_67d33d5bd8_n.jpg\nflower_photos/sunflowers/9431896325_23bf6e8761.jpg\nflower_photos/sunflowers/4745985619_249078cafa_n.jpg\nflower_photos/sunflowers/4664737020_b4c61aacd3_n.jpg\nflower_photos/sunflowers/14741812319_e1d32ffb84_n.jpg\nflower_photos/sunflowers/3466923719_b4b6df7f8b_n.jpg\nflower_photos/sunflowers/864957037_c75373d1c5.jpg\nflower_photos/sunflowers/5139977283_530c508603_n.jpg\nflower_photos/sunflowers/6061175433_95fdb12f32_n.jpg\nflower_photos/sunflowers/15241431045_65201cf15a_n.jpg\nflower_photos/sunflowers/20658775992_1619cd0a9b_n.jpg\nflower_photos/sunflowers/215798354_429de28c2d.jpg\nflower_photos/sunflowers/5830614551_e460a1215c.jpg\nflower_photos/sunflowers/18828277053_1493158b28.jpg\nflower_photos/sunflowers/14858674096_ed0fc1a130.jpg\nflower_photos/sunflowers/15380755137_a2e67839ab_m.jpg\nflower_photos/sunflowers/6606749757_b98a4ba403.jpg\nflower_photos/sunflowers/9448615838_04078d09bf_n.jpg\nflower_photos/sunflowers/6495554833_86eb8faa8e_n.jpg\nflower_photos/sunflowers/7728953426_abd179ab63.jpg\nflower_photos/sunflowers/20481273479_d459834a3e_n.jpg\nflower_photos/sunflowers/3912497870_a2f91c3a65_n.jpg\nflower_photos/sunflowers/9610371852_179e7781ce.jpg\nflower_photos/sunflowers/3734999477_7f454081aa_n.jpg\nflower_photos/sunflowers/8192234807_fed4a46f1a_n.jpg\nflower_photos/sunflowers/14646279002_9cdf97be97_n.jpg\nflower_photos/sunflowers/184682095_46f8607278.jpg\nflower_photos/sunflowers/15218871222_c104032ca1.jpg\nflower_photos/sunflowers/5923085671_f81dd1cf6f.jpg\nflower_photos/sunflowers/21349789961_18ba1af5b7_n.jpg\nflower_photos/sunflowers/6606823367_e89dc52a95_n.jpg\nflower_photos/sunflowers/2895404754_6d9f9416d7_n.jpg\nflower_photos/sunflowers/7510285306_ba8f80c382_n.jpg\nflower_photos/sunflowers/2883115621_4837267ea1_m.jpg\nflower_photos/sunflowers/2721638730_34a9b7a78b.jpg\nflower_photos/sunflowers/9216286876_289a4779f7.jpg\nflower_photos/sunflowers/20972862281_5367f4af88.jpg\nflower_photos/sunflowers/8478248531_1a16e232b5.jpg\nflower_photos/sunflowers/6074427492_1b5bab7848_n.jpg\nflower_photos/sunflowers/821368661_4ab4343f5a.jpg\nflower_photos/sunflowers/4806174512_e04475b766_n.jpg\nflower_photos/sunflowers/2443921986_d4582c123a.jpg\nflower_photos/sunflowers/8928614683_6c168edcfc.jpg\nflower_photos/sunflowers/9485002920_59af6f4cac.jpg\nflower_photos/sunflowers/19756232959_17cde3b9f0_m.jpg\nflower_photos/sunflowers/14623719696_1bb7970208_n.jpg\nflower_photos/sunflowers/7820626738_3be6a52e4e_n.jpg\nflower_photos/sunflowers/5043404000_9bc16cb7e5_m.jpg\nflower_photos/sunflowers/21518663809_3d69f5b995_n.jpg\nflower_photos/sunflowers/6606809995_edee55b770_m.jpg\nflower_photos/sunflowers/14925398301_55a180f919_n.jpg\nflower_photos/sunflowers/45045005_57354ee844.jpg\nflower_photos/sunflowers/3001533700_1c62fb8b4a_n.jpg\nflower_photos/sunflowers/16616096711_12375a0260_n.jpg\nflower_photos/sunflowers/14623720226_aeeac66e0a_n.jpg\nflower_photos/sunflowers/15218421476_9d5f38e732_m.jpg\nflower_photos/sunflowers/45045003_30bbd0a142_m.jpg\nflower_photos/sunflowers/2980154410_bffd7a3452_n.jpg\nflower_photos/sunflowers/24459750_eb49f6e4cb_m.jpg\nflower_photos/sunflowers/4804434999_bf2187e96a_n.jpg\nflower_photos/sunflowers/4623843480_23e3fb8dcc_n.jpg\nflower_photos/sunflowers/19915160340_ec904edbdf_n.jpg\nflower_photos/sunflowers/23894449029_bf0f34d35d_n.jpg\nflower_photos/sunflowers/4846786944_2832c5c8b8.jpg\nflower_photos/sunflowers/18828283553_e46504ae38.jpg\nflower_photos/sunflowers/6112510436_9fe06e695a_n.jpg\nflower_photos/sunflowers/6265084065_7a8b30cc6e_n.jpg\nflower_photos/sunflowers/4933230247_a0432f01da.jpg\nflower_photos/sunflowers/4191299785_a4faca9b74_n.jpg\nflower_photos/sunflowers/3858508462_db2b9692d1.jpg\nflower_photos/sunflowers/15118397087_bfb7ea70d5_n.jpg\nflower_photos/sunflowers/3894586562_5dbbdc4354_n.jpg\nflower_photos/sunflowers/2694860538_b95d60122c_m.jpg\nflower_photos/sunflowers/12282924083_fb80aa17d4_n.jpg\nflower_photos/sunflowers/4745980581_a0b7585258_n.jpg\nflower_photos/sunflowers/6606820461_952c450f90_n.jpg\nflower_photos/sunflowers/27465811_9477c9d044.jpg\nflower_photos/sunflowers/2807106374_f422b5f00c.jpg\nflower_photos/sunflowers/5955500463_6c08cb199e.jpg\nflower_photos/sunflowers/4414080766_5116e8084e.jpg\nflower_photos/sunflowers/15122871130_6a7d0b4372_n.jpg\nflower_photos/sunflowers/2588234269_c4bfd0d8b9_n.jpg\nflower_photos/sunflowers/9216286162_6ceefdd1b4_m.jpg\nflower_photos/sunflowers/18843967474_9cb552716b.jpg\nflower_photos/sunflowers/6606753075_72ee32aa30_m.jpg\nflower_photos/sunflowers/6908789145_814d448bb1_n.jpg\nflower_photos/sunflowers/6141199476_9b6d383fd9.jpg\nflower_photos/sunflowers/9558627290_353a14ba0b_m.jpg\nflower_photos/sunflowers/14266917699_91b207888e.jpg\nflower_photos/sunflowers/2883115609_5a69357b5d_m.jpg\nflower_photos/sunflowers/3912497888_e7a5905bc3_n.jpg\nflower_photos/sunflowers/24459548_27a783feda.jpg\nflower_photos/sunflowers/40411019_526f3fc8d9_m.jpg\nflower_photos/sunflowers/2944298800_1984bd4f8a_m.jpg\nflower_photos/sunflowers/5139969871_c9046bdaa7_n.jpg\nflower_photos/sunflowers/5076821914_c21b58fd4c_m.jpg\nflower_photos/sunflowers/8481979626_98c9f88848_n.jpg\nflower_photos/sunflowers/20406385204_469f6749e2_n.jpg\nflower_photos/sunflowers/4933823194_33f6e32c5a.jpg\nflower_photos/sunflowers/9427945592_07a2676945_n.jpg\nflower_photos/sunflowers/1022552002_2b93faf9e7_n.jpg\nflower_photos/sunflowers/5037531593_e2daf4c7f1.jpg\nflower_photos/sunflowers/20258015499_93b9951800_m.jpg\nflower_photos/sunflowers/3001536784_3bfd101b23_n.jpg\nflower_photos/sunflowers/2307673262_e1e1aefd29.jpg\nflower_photos/sunflowers/21984860006_20dfacea1c_m.jpg\nflower_photos/sunflowers/3196753837_411b03682d_n.jpg\nflower_photos/sunflowers/9432335346_e298e47713_n.jpg\nflower_photos/sunflowers/2328600790_90e2942557_n.jpg\nflower_photos/sunflowers/22429146402_332fa2fc72_m.jpg\nflower_photos/sunflowers/15238348741_c2fb12ecf2_m.jpg\nflower_photos/sunflowers/14244273988_a7484f18b7_m.jpg\nflower_photos/sunflowers/14472246629_72373111e6_m.jpg\nflower_photos/sunflowers/5293283002_9b17f085f7_m.jpg\nflower_photos/sunflowers/127192624_afa3d9cb84.jpg\nflower_photos/sunflowers/20156280765_a6baea3176.jpg\nflower_photos/sunflowers/4745991955_6804568ae0_n.jpg\nflower_photos/sunflowers/10386525005_fd0b7d6c55_n.jpg\nflower_photos/sunflowers/5357144886_b78f4782eb.jpg\nflower_photos/sunflowers/13648603305_1268eda8b7_n.jpg\nflower_photos/sunflowers/5951665793_8ae4807cbd_n.jpg\nflower_photos/sunflowers/9375675309_987d32f99e_n.jpg\nflower_photos/sunflowers/20777358950_c63ea569a1.jpg\nflower_photos/sunflowers/7652532108_01ef94c476.jpg\nflower_photos/sunflowers/164670176_9f5b9c7965.jpg\nflower_photos/sunflowers/215798357_3f4bfa27b7.jpg\nflower_photos/sunflowers/2979133707_84aab35b5d.jpg\nflower_photos/sunflowers/3865206264_5d81584bba.jpg\nflower_photos/sunflowers/9588522189_db6166f67f.jpg\nflower_photos/sunflowers/20667988875_6e73ac2879_n.jpg\nflower_photos/sunflowers/22429946721_e17a12cb39_n.jpg\nflower_photos/sunflowers/4932736308_827012cff2.jpg\nflower_photos/sunflowers/15042911059_b6153d94e7_n.jpg\nflower_photos/sunflowers/15072973261_73e2912ef2_n.jpg\nflower_photos/sunflowers/15054752730_fcf54d475e_m.jpg\nflower_photos/sunflowers/184683023_737fec5b18.jpg\nflower_photos/sunflowers/4933823922_911ac40b0d.jpg\nflower_photos/sunflowers/8041242566_752def876e_n.jpg\nflower_photos/sunflowers/18237156988_9ceb46a8de_n.jpg\nflower_photos/sunflowers/6495559397_61d01c0c57.jpg\nflower_photos/sunflowers/5067864967_19928ca94c_m.jpg\nflower_photos/sunflowers/9610374042_bb16cded3d.jpg\nflower_photos/sunflowers/2689228449_e0be72cf00_n.jpg\nflower_photos/sunflowers/6606817351_10f6e43a09.jpg\nflower_photos/sunflowers/164671753_ab36d9cbb7_n.jpg\nflower_photos/sunflowers/15495578821_92c6d14252_n.jpg\nflower_photos/sunflowers/4933822422_4f54fc7cc8.jpg\nflower_photos/sunflowers/7510240282_87554c7418_n.jpg\nflower_photos/sunflowers/418056361_1dfac1c151_n.jpg\nflower_photos/sunflowers/7510262868_cf7d6f6f25_n.jpg\nflower_photos/sunflowers/3311874685_7b9ef10f7e_m.jpg\nflower_photos/sunflowers/22203670478_9ec5c2700b_n.jpg\nflower_photos/sunflowers/16988605969_570329ff20_n.jpg\nflower_photos/sunflowers/50987813_7484bfbcdf.jpg\nflower_photos/sunflowers/4933824130_b99839a80d.jpg\nflower_photos/sunflowers/9399711558_7cb9547cd3_n.jpg\nflower_photos/sunflowers/22755811033_cd17b109e0.jpg\nflower_photos/sunflowers/4895124535_11a2bb704c_m.jpg\nflower_photos/sunflowers/15683877266_42e0fe3782_n.jpg\nflower_photos/sunflowers/5952223760_85972671d6_n.jpg\nflower_photos/sunflowers/9599534035_ae4df582b6.jpg\nflower_photos/sunflowers/14244410747_22691ece4a_n.jpg\nflower_photos/sunflowers/6606813305_c992231d29_m.jpg\nflower_photos/sunflowers/2767658405_1e2043f44c_n.jpg\nflower_photos/sunflowers/4927658421_7eed83bc95_m.jpg\nflower_photos/sunflowers/6141150299_b46a64e4de.jpg\nflower_photos/sunflowers/39271782_b4335d09ae_n.jpg\nflower_photos/sunflowers/9555824387_32b151e9b0_m.jpg\nflower_photos/sunflowers/9497774249_7f5ae70927_m.jpg\nflower_photos/sunflowers/4871455214_8b5fb87ab6_n.jpg\nflower_photos/sunflowers/5027895361_ace3b731e5_n.jpg\nflower_photos/sunflowers/5979111025_3bcae48ae6_n.jpg\nflower_photos/sunflowers/20183028616_beb937e75c_m.jpg\nflower_photos/sunflowers/20410697750_c43973d1eb.jpg\nflower_photos/sunflowers/2706304885_4916102704_n.jpg\nflower_photos/sunflowers/20972866151_e6a928b00a.jpg\nflower_photos/sunflowers/4933229889_c5d9e36392.jpg\nflower_photos/sunflowers/4895122831_83db2ba2d0_n.jpg\nflower_photos/sunflowers/1064662314_c5a7891b9f_m.jpg\nflower_photos/sunflowers/9610098411_f1613c8e14.jpg\nflower_photos/sunflowers/20148493928_9f75a99783.jpg\nflower_photos/sunflowers/8928658373_fdca5ff1b8.jpg\nflower_photos/sunflowers/4933229095_f7e4218b28.jpg\nflower_photos/sunflowers/265450085_6e9f276e2e.jpg\nflower_photos/sunflowers/164668737_aeab0cb55e_n.jpg\nflower_photos/sunflowers/14925397651_97dcddc383_n.jpg\nflower_photos/sunflowers/1880606744_23e3dc4f6b_n.jpg\nflower_photos/sunflowers/6958724008_12259943a7.jpg\nflower_photos/sunflowers/13959937305_2f5c532886_n.jpg\nflower_photos/sunflowers/29972905_4cc537ff4b_n.jpg\nflower_photos/sunflowers/14955545254_324cd4ee75.jpg\nflower_photos/LICENSE.txt\nflower_photos/tulips/\nflower_photos/tulips/3498663243_42b39b4185_m.jpg\nflower_photos/tulips/2936181186_38ff43492e.jpg\nflower_photos/tulips/6931715360_34edc5a372_m.jpg\nflower_photos/tulips/12557176134_ecbf15885b.jpg\nflower_photos/tulips/4590702749_e1df8e0c1b.jpg\nflower_photos/tulips/3909355648_42cb3a5e09_n.jpg\nflower_photos/tulips/14087792403_f34f37ba3b_m.jpg\nflower_photos/tulips/8838347159_746d14e6c1_m.jpg\nflower_photos/tulips/7481215720_73e40f178f_n.jpg\nflower_photos/tulips/13903946578_187f904c9a_n.jpg\nflower_photos/tulips/402525114_eaa60c8341_m.jpg\nflower_photos/tulips/4589624702_b6baa83699_m.jpg\nflower_photos/tulips/14087425312_2b5846b570_n.jpg\nflower_photos/tulips/5661431592_cea1108261_n.jpg\nflower_photos/tulips/14487712670_aebe715525_m.jpg\nflower_photos/tulips/7042692841_f323799c0d.jpg\nflower_photos/tulips/5665708521_799585d229_n.jpg\nflower_photos/tulips/7064813645_f7f48fb527.jpg\nflower_photos/tulips/8750288831_5e49a9f29b.jpg\nflower_photos/tulips/4508346090_a27b988f79_n.jpg\nflower_photos/tulips/3396033831_bb88d93630.jpg\nflower_photos/tulips/14266093711_66d18a1e44_n.jpg\nflower_photos/tulips/14651385476_7ccb20e594_m.jpg\nflower_photos/tulips/13562266594_69b807f90c.jpg\nflower_photos/tulips/4624404489_11e10fcd33_n.jpg\nflower_photos/tulips/14024864234_713158c27f_n.jpg\nflower_photos/tulips/113291410_1bdc718ed8_n.jpg\nflower_photos/tulips/16702188449_3dacce90b2_m.jpg\nflower_photos/tulips/391477275_7c2f50a1a7_m.jpg\nflower_photos/tulips/13539827514_79b60b6c22_n.jpg\nflower_photos/tulips/5565089564_a30c318f44.jpg\nflower_photos/tulips/8614237582_74417799f4_m.jpg\nflower_photos/tulips/17324469461_2b318aff8d_m.jpg\nflower_photos/tulips/17202535346_ab828e779b.jpg\nflower_photos/tulips/17199496791_3caaf5e278_m.jpg\nflower_photos/tulips/11746548_26b3256922_n.jpg\nflower_photos/tulips/5691100579_4a2767360a.jpg\nflower_photos/tulips/13513846963_c3d5e9fb1d_n.jpg\nflower_photos/tulips/4679869990_7c5f28f2fe_n.jpg\nflower_photos/tulips/13561912705_e5eeb41433_z.jpg\nflower_photos/tulips/3485767306_6db7bdf536.jpg\nflower_photos/tulips/13976522214_ccec508fe7.jpg\nflower_photos/tulips/16283125269_4cfae953f1.jpg\nflower_photos/tulips/2425067141_b27043a800_m.jpg\nflower_photos/tulips/2220085701_896054d263_n.jpg\nflower_photos/tulips/3457017604_90e4de7480_m.jpg\nflower_photos/tulips/4117620896_070e5887ae_m.jpg\nflower_photos/tulips/8713357842_9964a93473_n.jpg\nflower_photos/tulips/14094114202_4c1d7f1116.jpg\nflower_photos/tulips/5674132053_b40a7d32ca.jpg\nflower_photos/tulips/4838669164_ffb6f67139.jpg\nflower_photos/tulips/14097366955_84ef6369f2.jpg\nflower_photos/tulips/112951022_4892b1348b_n.jpg\nflower_photos/tulips/107693873_86021ac4ea_n.jpg\nflower_photos/tulips/15090146325_b7e1249e60.jpg\nflower_photos/tulips/8713396140_5af8136136.jpg\nflower_photos/tulips/8712267813_f7a9be2ec5.jpg\nflower_photos/tulips/3422915985_9bf7264d36.jpg\nflower_photos/tulips/134143359_71fa8dd9a4.jpg\nflower_photos/tulips/13513616525_2ee0f049e1.jpg\nflower_photos/tulips/3143110904_66b4851a58_n.jpg\nflower_photos/tulips/471298577_cc7558bcf1.jpg\nflower_photos/tulips/8713397694_bcbcbba2c2_n.jpg\nflower_photos/tulips/2402342888_dd65677013.jpg\nflower_photos/tulips/16937554595_3e1de22f9c.jpg\nflower_photos/tulips/14009216519_b608321cf2_n.jpg\nflower_photos/tulips/5653364300_bc557236c7_n.jpg\nflower_photos/tulips/164578909_51f245d3fa_n.jpg\nflower_photos/tulips/6936168062_a31c28b77c_m.jpg\nflower_photos/tulips/8712230357_1298b8513b.jpg\nflower_photos/tulips/14068295074_cd8b85bffa.jpg\nflower_photos/tulips/14116780333_7836f4448c.jpg\nflower_photos/tulips/1353748522_b9c630b162.jpg\nflower_photos/tulips/8713398114_bc96f1b624_n.jpg\nflower_photos/tulips/16680998737_6f6225fe36.jpg\nflower_photos/tulips/6770436217_281da51e49_n.jpg\nflower_photos/tulips/8713398614_88202e452e_n.jpg\nflower_photos/tulips/13472393854_b2530f7029_n.jpg\nflower_photos/tulips/2256230386_08b54ca760.jpg\nflower_photos/tulips/5730908127_da871df0f8.jpg\nflower_photos/tulips/14022473102_3b24ca08cb_m.jpg\nflower_photos/tulips/16862351376_f0fcc6fc91_n.jpg\nflower_photos/tulips/7166564830_8b34a3fd35_n.jpg\nflower_photos/tulips/8523133474_d2c0845b54.jpg\nflower_photos/tulips/16751015081_af2ef77c9a_n.jpg\nflower_photos/tulips/2087981909_fd468de5c4_n.jpg\nflower_photos/tulips/12916135413_dafcf3089e_n.jpg\nflower_photos/tulips/112334842_3ecf7585dd.jpg\nflower_photos/tulips/5700466891_2bcb17fa68_n.jpg\nflower_photos/tulips/7166552648_28b6dce578.jpg\nflower_photos/tulips/13926327692_a07357ff0d.jpg\nflower_photos/tulips/16303377824_6e9128b4bd.jpg\nflower_photos/tulips/132538273_335240fe5b_n.jpg\nflower_photos/tulips/16582481123_06e8e6b966_n.jpg\nflower_photos/tulips/14278331403_4c475f9a9b.jpg\nflower_photos/tulips/16904202259_8f45d045c3_m.jpg\nflower_photos/tulips/7082608511_f4cf233f59_n.jpg\nflower_photos/tulips/303858799_942b9c09e7_m.jpg\nflower_photos/tulips/13910471347_30c8bf4de1_n.jpg\nflower_photos/tulips/16471277547_a0a5509377_n.jpg\nflower_photos/tulips/17159816388_deafbebdb0.jpg\nflower_photos/tulips/2254152047_d3bf8903cd_n.jpg\nflower_photos/tulips/5529939805_1679b014e1_n.jpg\nflower_photos/tulips/7447655334_e8f805ab95_m.jpg\nflower_photos/tulips/8712282563_3819afb7bc.jpg\nflower_photos/tulips/5810456385_b44358a0ae.jpg\nflower_photos/tulips/4579128789_1561575458_n.jpg\nflower_photos/tulips/14073608876_49db8ac97d_n.jpg\nflower_photos/tulips/6989946990_62c639ff16_n.jpg\nflower_photos/tulips/16265883604_92be82b973.jpg\nflower_photos/tulips/3502685880_f026400dce_n.jpg\nflower_photos/tulips/8688502760_1c8d6de921_m.jpg\nflower_photos/tulips/12616825773_9aa4245b57_n.jpg\nflower_photos/tulips/3511104954_54eace015c_n.jpg\nflower_photos/tulips/65347450_53658c63bd_n.jpg\nflower_photos/tulips/8572847041_d0cc07861f_n.jpg\nflower_photos/tulips/16700863150_ddaa4d89b4_n.jpg\nflower_photos/tulips/2243427551_809b603992_z.jpg\nflower_photos/tulips/13976191172_6f23a0b313.jpg\nflower_photos/tulips/13472141763_f2517e7f0d.jpg\nflower_photos/tulips/6038098425_b3b4fb62cc_m.jpg\nflower_photos/tulips/4573822295_5c5c6a5f6a.jpg\nflower_photos/tulips/3502085373_edc2c36992_n.jpg\nflower_photos/tulips/5674695558_61397a1584.jpg\nflower_photos/tulips/4575963749_2418ff8768.jpg\nflower_photos/tulips/480228053_513791d474.jpg\nflower_photos/tulips/485266837_671def8627.jpg\nflower_photos/tulips/8623173256_3f0eb4c506.jpg\nflower_photos/tulips/8394186551_28eed83a94_m.jpg\nflower_photos/tulips/8712269349_2b933da2b8_n.jpg\nflower_photos/tulips/17066864992_1cbc4fc908.jpg\nflower_photos/tulips/8713407768_f880df361f.jpg\nflower_photos/tulips/2361075034_cf730b8682.jpg\nflower_photos/tulips/7082476907_99beef0dde.jpg\nflower_photos/tulips/3253320570_b617f7fd4b.jpg\nflower_photos/tulips/5632006303_15acd2cf1e_n.jpg\nflower_photos/tulips/16110795216_b3e44697b4_m.jpg\nflower_photos/tulips/155097272_70feb13184.jpg\nflower_photos/tulips/8708209606_d3aede4801.jpg\nflower_photos/tulips/13900235284_32ce563633_n.jpg\nflower_photos/tulips/5637140035_e6c5514f54.jpg\nflower_photos/tulips/17224410762_402455ed8f.jpg\nflower_photos/tulips/4550805310_5f81c9ba08_n.jpg\nflower_photos/tulips/14093907931_dd8f642574.jpg\nflower_photos/tulips/12934201824_1c8b5171fb_m.jpg\nflower_photos/tulips/16644790896_7b296ecd67_n.jpg\nflower_photos/tulips/490541142_c37e2b4191_n.jpg\nflower_photos/tulips/13539404903_cd113e3e9b.jpg\nflower_photos/tulips/16098264209_38fe491093.jpg\nflower_photos/tulips/8697784345_e75913d220.jpg\nflower_photos/tulips/13513851673_9d813dc7b0.jpg\nflower_photos/tulips/17719248689_cfd5d2f228_n.jpg\nflower_photos/tulips/112651128_7b5d39a346_m.jpg\nflower_photos/tulips/2813658587_337eeef124_n.jpg\nflower_photos/tulips/130684927_a05164ba13_m.jpg\nflower_photos/tulips/8554190977_37ac747799_m.jpg\nflower_photos/tulips/5674134129_2db5136cba.jpg\nflower_photos/tulips/14015957646_8317a0f1d9_n.jpg\nflower_photos/tulips/3474066174_8d3b3e8f97_n.jpg\nflower_photos/tulips/5552198702_35856ed8ec.jpg\nflower_photos/tulips/5811004432_266f0f0c6f.jpg\nflower_photos/tulips/5680695867_baff72fc7c.jpg\nflower_photos/tulips/13997627965_22d81601ce_n.jpg\nflower_photos/tulips/4612075317_91eefff68c_n.jpg\nflower_photos/tulips/3254533919_cb0b8af26c.jpg\nflower_photos/tulips/14262354955_cc2ab3b112_m.jpg\nflower_photos/tulips/8711277462_b43df5454b_m.jpg\nflower_photos/tulips/4550091966_7f3e0f8802_n.jpg\nflower_photos/tulips/4522153453_06437ca3af_m.jpg\nflower_photos/tulips/12949131454_4d3392f4dd_n.jpg\nflower_photos/tulips/6267021825_a8316e0dcc_m.jpg\nflower_photos/tulips/4290566894_c7f061583d_m.jpg\nflower_photos/tulips/9870557734_88eb3b9e3b_n.jpg\nflower_photos/tulips/5813495998_64be1b8ab6_n.jpg\nflower_photos/tulips/9947385346_3a8cacea02_n.jpg\nflower_photos/tulips/3404038663_f62cf8eba3_n.jpg\nflower_photos/tulips/17167151059_a53bfe0b02.jpg\nflower_photos/tulips/8690789564_394eb04982_n.jpg\nflower_photos/tulips/7002703410_3e97b29da5_n.jpg\nflower_photos/tulips/7145978709_2d1596f462.jpg\nflower_photos/tulips/5718781677_d5e9267115_n.jpg\nflower_photos/tulips/18270448366_d5676dec64_z.jpg\nflower_photos/tulips/5719416820_3060e3c1f0.jpg\nflower_photos/tulips/14078067903_92f5eb27ff.jpg\nflower_photos/tulips/8838914676_8ef4db7f50_n.jpg\nflower_photos/tulips/12764617214_12211c6a0c_m.jpg\nflower_photos/tulips/17408197905_829c4d7940_m.jpg\nflower_photos/tulips/8677713853_1312f65e71.jpg\nflower_photos/tulips/7166626128_8e0983ac8e_n.jpg\nflower_photos/tulips/5687705933_55a8c2dbac.jpg\nflower_photos/tulips/14068200854_5c13668df9_m.jpg\nflower_photos/tulips/6903831250_a2757fff82_m.jpg\nflower_photos/tulips/7166598930_18d8686ace_n.jpg\nflower_photos/tulips/4644110077_ff252cd7c4.jpg\nflower_photos/tulips/3213012716_b4c0f7db88.jpg\nflower_photos/tulips/2272006181_785f1be94f_n.jpg\nflower_photos/tulips/9976515506_d496c5e72c.jpg\nflower_photos/tulips/738207467_fc59cfcd9b_z.jpg\nflower_photos/tulips/5208680166_c4372477ef_n.jpg\nflower_photos/tulips/13513644515_a51470b899.jpg\nflower_photos/tulips/18378582936_ee7085c850.jpg\nflower_photos/tulips/4522130258_9ee44cf73f_m.jpg\nflower_photos/tulips/483880052_19fdb26a9f.jpg\nflower_photos/tulips/5543457754_89c44c88de_n.jpg\nflower_photos/tulips/141479422_5a6fa1fd1b_m.jpg\nflower_photos/tulips/12548574923_5e90f4ceea.jpg\nflower_photos/tulips/2238626027_058c404b94.jpg\nflower_photos/tulips/10995953955_089572caf0.jpg\nflower_photos/tulips/3510294699_bc4c72cb7d_n.jpg\nflower_photos/tulips/13979098015_e8c98fd34e_n.jpg\nflower_photos/tulips/4681062529_36186617d9.jpg\nflower_photos/tulips/10791227_7168491604.jpg\nflower_photos/tulips/3501996215_1c6d1a3386_n.jpg\nflower_photos/tulips/7481204112_e3c57dd40a_n.jpg\nflower_photos/tulips/15275199229_962387f24d.jpg\nflower_photos/tulips/17844723633_da85357fe3.jpg\nflower_photos/tulips/15147464747_594599c855_m.jpg\nflower_photos/tulips/4263272885_1a49ea5209.jpg\nflower_photos/tulips/15275504998_ca9eb82998.jpg\nflower_photos/tulips/2256214682_130c01d9d9.jpg\nflower_photos/tulips/2336919121_851ebc4754.jpg\nflower_photos/tulips/6948277038_89d7ff42e2_m.jpg\nflower_photos/tulips/5755467567_903c31e3d0.jpg\nflower_photos/tulips/8904780994_8867d64155_n.jpg\nflower_photos/tulips/7136973281_b2a935ce20.jpg\nflower_photos/tulips/5417115048_3b78d6c875_n.jpg\nflower_photos/tulips/7247182064_f8d6759446_n.jpg\nflower_photos/tulips/112428919_f0c5ad7d9d_n.jpg\nflower_photos/tulips/14025589299_eac64c51af_m.jpg\nflower_photos/tulips/5644061265_e02135f028_n.jpg\nflower_photos/tulips/15976769174_1d50f46ca1_m.jpg\nflower_photos/tulips/5691672942_70a93d70fc.jpg\nflower_photos/tulips/15458787091_3edc6cd1eb.jpg\nflower_photos/tulips/15049902081_dd85361f8c_m.jpg\nflower_photos/tulips/3614805920_7a6610aa4b_n.jpg\nflower_photos/tulips/14087860553_bf4f8ec56d.jpg\nflower_photos/tulips/9048307967_40a164a459_m.jpg\nflower_photos/tulips/6958343928_7e596da4ed_m.jpg\nflower_photos/tulips/467770225_e3b41d4dd3_n.jpg\nflower_photos/tulips/3449794006_8c289840aa.jpg\nflower_photos/tulips/12517756805_56b74be742.jpg\nflower_photos/tulips/8712243901_54d686319e_m.jpg\nflower_photos/tulips/13531007054_c88deaf302_n.jpg\nflower_photos/tulips/7068715863_a534ac7884_n.jpg\nflower_photos/tulips/146884869_b1a8fa9c4e_n.jpg\nflower_photos/tulips/8908062479_449200a1b4.jpg\nflower_photos/tulips/19425920580_cdc8f49aed_n.jpg\nflower_photos/tulips/3637371174_a8dfcc1b35.jpg\nflower_photos/tulips/5603625247_e4ff1828af_m.jpg\nflower_photos/tulips/3990746027_338ee436d2_n.jpg\nflower_photos/tulips/9446982168_06c4d71da3_n.jpg\nflower_photos/tulips/6994351792_343e18cbf6_n.jpg\nflower_photos/tulips/497305666_b5d4348826_n.jpg\nflower_photos/tulips/13530796853_e5993f57d6_n.jpg\nflower_photos/tulips/14255917256_84c23c572b.jpg\nflower_photos/tulips/130685040_3c2fcec63e_n.jpg\nflower_photos/tulips/14078067843_3573fcfc85_n.jpg\nflower_photos/tulips/7094271655_79a6f972c1_n.jpg\nflower_photos/tulips/6187740107_9813ccc41e.jpg\nflower_photos/tulips/17318339476_54479b6660_n.jpg\nflower_photos/tulips/5546723510_39a5a10d3a_n.jpg\nflower_photos/tulips/13910678178_25e8b1a5e5.jpg\nflower_photos/tulips/14235021006_dd001ea8ed_n.jpg\nflower_photos/tulips/13510068773_c925c5517c.jpg\nflower_photos/tulips/13910131718_731353d84c_n.jpg\nflower_photos/tulips/14014595475_5892fcda51_n.jpg\nflower_photos/tulips/16711791713_e54bc9c1af_n.jpg\nflower_photos/tulips/17862445825_f7031d6f26.jpg\nflower_photos/tulips/17165583356_38cb1f231d_n.jpg\nflower_photos/tulips/14110616533_e04775e7b1.jpg\nflower_photos/tulips/7166618384_850905fc63_n.jpg\nflower_photos/tulips/8748266132_5298a91dcf_n.jpg\nflower_photos/tulips/420216121_3ee33723d7_m.jpg\nflower_photos/tulips/10686568196_b1915544a8.jpg\nflower_photos/tulips/2351637471_5dd34fd3ac_n.jpg\nflower_photos/tulips/7047408023_6e98fd1e3f.jpg\nflower_photos/tulips/8712266605_3787e346cd_n.jpg\nflower_photos/tulips/8668974855_8389ecbdca_m.jpg\nflower_photos/tulips/503770507_f397245a6a.jpg\nflower_photos/tulips/8713398906_28e59a225a_n.jpg\nflower_photos/tulips/112951086_150a59d499_n.jpg\nflower_photos/tulips/4550117239_5907aaba4c.jpg\nflower_photos/tulips/14097745904_436c4ba1b4_n.jpg\nflower_photos/tulips/4587872443_a86c692cb8.jpg\nflower_photos/tulips/8713390684_041148dd3e_n.jpg\nflower_photos/tulips/133692329_c1150ed811_n.jpg\nflower_photos/tulips/14097676864_4ca8e8b20d_n.jpg\nflower_photos/tulips/7064778965_ddcc6ee9f2.jpg\nflower_photos/tulips/4395433872_e073d8c721_n.jpg\nflower_photos/tulips/13999402743_f563f6b685_n.jpg\nflower_photos/tulips/16930121391_a4092ecf00_n.jpg\nflower_photos/tulips/8712268519_f4c2c39a06_n.jpg\nflower_photos/tulips/2432389721_4d14971060_n.jpg\nflower_photos/tulips/4602809199_d3030cef01_m.jpg\nflower_photos/tulips/14671196461_b725727229_m.jpg\nflower_photos/tulips/12024561754_ce9667e4dc_n.jpg\nflower_photos/tulips/8733586143_3139db6e9e_n.jpg\nflower_photos/tulips/8562853756_73778dac25_n.jpg\nflower_photos/tulips/8622237974_b362574785_n.jpg\nflower_photos/tulips/444963906_e41492b692.jpg\nflower_photos/tulips/466409031_4c10294db5_m.jpg\nflower_photos/tulips/17282288501_e8738c9cfb_n.jpg\nflower_photos/tulips/110147301_ad921e2828.jpg\nflower_photos/tulips/8628453641_6f87755815_m.jpg\nflower_photos/tulips/16765283686_0315ae00a8.jpg\nflower_photos/tulips/16986144192_55e0e6c152.jpg\nflower_photos/tulips/2431737309_1468526f8b.jpg\nflower_photos/tulips/8706523526_a0f161b72b.jpg\nflower_photos/tulips/15516736553_b169b67195_n.jpg\nflower_photos/tulips/14090546015_504c8becd1.jpg\nflower_photos/tulips/13910737760_c71c8b6ff2.jpg\nflower_photos/tulips/2834890466_1cf220fba1.jpg\nflower_photos/tulips/8681825637_837a63513a_n.jpg\nflower_photos/tulips/13514131694_d91da4f4fc.jpg\nflower_photos/tulips/5670916806_df4316006f_n.jpg\nflower_photos/tulips/13530690445_9f1f5cf43a_n.jpg\nflower_photos/tulips/14270573963_f122c40438.jpg\nflower_photos/tulips/6905876618_12732b74de_b.jpg\nflower_photos/tulips/14093565032_a8f1e349d1.jpg\nflower_photos/tulips/5574219476_1f46775487_n.jpg\nflower_photos/tulips/8601596054_33e40c2a7a.jpg\nflower_photos/tulips/13888320717_d2919a879b_m.jpg\nflower_photos/tulips/2481827798_6087d71134.jpg\nflower_photos/tulips/130685347_afbffe3afa_n.jpg\nflower_photos/tulips/574373182_2776669a79_n.jpg\nflower_photos/tulips/2426849837_baefd9a518_n.jpg\nflower_photos/tulips/13910719110_1b21d1fc81.jpg\nflower_photos/tulips/5524946579_307dc74476.jpg\nflower_photos/tulips/3502251824_3be758edc6_m.jpg\nflower_photos/tulips/16594995743_ce72c61201_n.jpg\nflower_photos/tulips/2503489175_f0848d3e8e.jpg\nflower_photos/tulips/3210019014_1bbd8bff20_n.jpg\nflower_photos/tulips/8686013485_3c4dfbfd1f_n.jpg\nflower_photos/tulips/5631861819_f0eb39a357_m.jpg\nflower_photos/tulips/5698944116_fd35fe6bea.jpg\nflower_photos/tulips/112650879_82adc2cc04_n.jpg\nflower_photos/tulips/13514136074_ab1b827e4f.jpg\nflower_photos/tulips/6227136683_262c6be56b.jpg\nflower_photos/tulips/13197345653_0f685b3c97_n.jpg\nflower_photos/tulips/440714501_9f8268e1b0.jpg\nflower_photos/tulips/54895006_55b49052dc.jpg\nflower_photos/tulips/3150964108_24dbec4b23_m.jpg\nflower_photos/tulips/2470177960_7bd67db186_n.jpg\nflower_photos/tulips/14487943607_651e8062a1_m.jpg\nflower_photos/tulips/467702445_b8676f60fb_n.jpg\nflower_photos/tulips/2412250315_a04171da51_n.jpg\nflower_photos/tulips/11746276_de3dec8201.jpg\nflower_photos/tulips/14027372499_30f934d24f_m.jpg\nflower_photos/tulips/8722514702_7ecc68691c.jpg\nflower_photos/tulips/8713387500_6a9138b41b_n.jpg\nflower_photos/tulips/4558912791_084e440365_m.jpg\nflower_photos/tulips/8659691170_09db83d023.jpg\nflower_photos/tulips/8712267391_c756f18ee7_n.jpg\nflower_photos/tulips/4522764992_e9d70b82c1_m.jpg\nflower_photos/tulips/4945315538_97bdd873c4.jpg\nflower_photos/tulips/14866400927_3a59899df3_m.jpg\nflower_photos/tulips/7046815693_f159e96acd_n.jpg\nflower_photos/tulips/14491997336_36ba524713.jpg\nflower_photos/tulips/11746452_5bc1749a36.jpg\nflower_photos/tulips/4561670472_0451888e32_n.jpg\nflower_photos/tulips/12025042086_78bafc0eb6_n.jpg\nflower_photos/tulips/14067456066_87e15792d0.jpg\nflower_photos/tulips/4521037085_70d5802e1d_m.jpg\nflower_photos/tulips/3991742794_edebc6c8a0_n.jpg\nflower_photos/tulips/19413898445_69344f9956_n.jpg\nflower_photos/tulips/4579079143_f65b39dd9f.jpg\nflower_photos/tulips/5635347336_bc1400e939_n.jpg\nflower_photos/tulips/8521597402_4b6169ba05.jpg\nflower_photos/tulips/14061132852_89122de5f9_n.jpg\nflower_photos/tulips/7003964080_4566470798_n.jpg\nflower_photos/tulips/13176576813_50e77cc1d9.jpg\nflower_photos/tulips/2426847695_4b8409402e_n.jpg\nflower_photos/tulips/13509967925_eaaeefa396_m.jpg\nflower_photos/tulips/14068378204_7b26baa30d_n.jpg\nflower_photos/tulips/15029962436_3e50c1f30f_n.jpg\nflower_photos/tulips/13561966423_e5c641fe11.jpg\nflower_photos/tulips/14074147406_7ab87aec79_n.jpg\nflower_photos/tulips/8605564823_7a59d3d92a.jpg\nflower_photos/tulips/14487705209_ea723109e1_m.jpg\nflower_photos/tulips/14957470_6a8c272a87_m.jpg\nflower_photos/tulips/17159349572_c0c51599f7_n.jpg\nflower_photos/tulips/2232289392_9a79a0c5cb_n.jpg\nflower_photos/tulips/10094731133_94a942463c.jpg\nflower_photos/tulips/2535936698_78cc03df3f_n.jpg\nflower_photos/tulips/16677542612_a78a8ca8b3_m.jpg\nflower_photos/tulips/4574785121_5d8ec4626e.jpg\nflower_photos/tulips/2430566689_8543552f9b.jpg\nflower_photos/tulips/8669794378_97dda6036f_n.jpg\nflower_photos/tulips/5891485349_cce7b99549.jpg\nflower_photos/tulips/2813649953_2b0f20fe94_n.jpg\nflower_photos/tulips/8717900362_2aa508e9e5.jpg\nflower_photos/tulips/132538272_63658146d9_n.jpg\nflower_photos/tulips/9378657435_89fabf13c9_n.jpg\nflower_photos/tulips/2229804138_db9cba3443_n.jpg\nflower_photos/tulips/5691090657_2f1e9bf49e_n.jpg\nflower_photos/tulips/4520582070_d14a14f038.jpg\nflower_photos/tulips/470690620_9d3a5bb239.jpg\nflower_photos/tulips/8712270665_57b5bda0a2_n.jpg\nflower_photos/tulips/5674707464_dc18de05b1.jpg\nflower_photos/tulips/7177682195_c29265748d_n.jpg\nflower_photos/tulips/5704726114_a92f753514.jpg\nflower_photos/tulips/17309951996_552d632cbb_n.jpg\nflower_photos/tulips/16677199221_eab3f22378_n.jpg\nflower_photos/tulips/130684941_d1abfa3be6_m.jpg\nflower_photos/tulips/4353419275_79d3904074_n.jpg\nflower_photos/tulips/15147473067_7c5498eb0e_m.jpg\nflower_photos/tulips/18245124970_e68fd3f3c3.jpg\nflower_photos/tulips/5716293002_a8be6a6dd3_n.jpg\nflower_photos/tulips/133960364_d87f883c15_n.jpg\nflower_photos/tulips/3626132563_d955973447_n.jpg\nflower_photos/tulips/14254839301_ffb19c6445_n.jpg\nflower_photos/tulips/6539831765_c21b68910e_n.jpg\nflower_photos/tulips/13509973805_bda5fa8982.jpg\nflower_photos/tulips/13910479407_936fd3122d.jpg\nflower_photos/tulips/6325571510_7544b27e57_n.jpg\nflower_photos/tulips/15275144259_f9a18ec9cb.jpg\nflower_photos/tulips/7166554924_432aaae4b2_n.jpg\nflower_photos/tulips/16938892686_3613ea68e8_n.jpg\nflower_photos/tulips/8484905084_6a18c62b13_m.jpg\nflower_photos/tulips/14054827092_f359f5fcbd_m.jpg\nflower_photos/tulips/13176521023_4d7cc74856_m.jpg\nflower_photos/tulips/14746916178_40403cc57e.jpg\nflower_photos/tulips/13289268363_b9337d751e.jpg\nflower_photos/tulips/8712263493_3db76c5f82.jpg\nflower_photos/tulips/6948239566_0ac0a124ee_n.jpg\nflower_photos/tulips/8712260079_c0ff42e0e2_n.jpg\nflower_photos/tulips/8708856019_f3be2353a4_n.jpg\nflower_photos/tulips/11746367_d23a35b085_n.jpg\nflower_photos/tulips/113902743_8f537f769b_n.jpg\nflower_photos/tulips/176458518_f81d4bff8e.jpg\nflower_photos/tulips/14861513337_4ef0bfa40d.jpg\nflower_photos/tulips/13910126337_53faf1d214_n.jpg\nflower_photos/tulips/9947374414_fdf1d0861c_n.jpg\nflower_photos/tulips/13539384593_23449f7332_n.jpg\nflower_photos/tulips/5666286130_1dc6f66f09_n.jpg\nflower_photos/tulips/450607536_4fd9f5d17c_m.jpg\nflower_photos/tulips/16282277874_b92776b194.jpg\nflower_photos/tulips/113960470_38fab8f2fb_m.jpg\nflower_photos/tulips/11441893003_ab83672800.jpg\nflower_photos/tulips/6998661030_46cbb7892a.jpg\nflower_photos/tulips/13529687904_3d60abb479_n.jpg\nflower_photos/tulips/8673412732_f8fd690ee4_n.jpg\nflower_photos/tulips/5633266048_4f4bfb2cf1_n.jpg\nflower_photos/tulips/7144016605_e159b6c06b_m.jpg\nflower_photos/tulips/6936380780_19c26c918a.jpg\nflower_photos/tulips/14067761295_7cfe6a42e9.jpg\nflower_photos/tulips/4520577328_a94c11e806_n.jpg\nflower_photos/tulips/14171673854_1208c19be3_m.jpg\nflower_photos/tulips/8619064872_dea79a9eb9.jpg\nflower_photos/tulips/14674067742_73c2602aa6_m.jpg\nflower_photos/tulips/13542672763_20c3cb9272.jpg\nflower_photos/tulips/13954659583_03981dea99_n.jpg\nflower_photos/tulips/122450705_9885fff3c4_n.jpg\nflower_photos/tulips/16485607329_e66d5960bc_m.jpg\nflower_photos/tulips/7166606598_5d2cd307c3.jpg\nflower_photos/tulips/16138212287_643bf336e1_m.jpg\nflower_photos/tulips/112428665_d8f3632f36_n.jpg\nflower_photos/tulips/7070694881_e9a331fa4e_n.jpg\nflower_photos/tulips/14087439392_969444f56e.jpg\nflower_photos/tulips/506350421_2ba59e568e_m.jpg\nflower_photos/tulips/14090534565_5857ce4b7c_n.jpg\nflower_photos/tulips/16265876844_0a149c4f76.jpg\nflower_photos/tulips/14099204939_60e6ffa4c3_n.jpg\nflower_photos/tulips/3601085193_de1195d3d7_n.jpg\nflower_photos/tulips/15516715153_08abc9bb20_n.jpg\nflower_photos/tulips/5674167473_ac696c8989_n.jpg\nflower_photos/tulips/16862374316_4135908d4c_m.jpg\nflower_photos/tulips/11614202956_1dcf1c96a1.jpg\nflower_photos/tulips/14088017343_dd158d2eb5.jpg\nflower_photos/tulips/16930105456_8b826dc4a8_n.jpg\nflower_photos/tulips/17469578564_35a8360f58.jpg\nflower_photos/tulips/17012955700_7141d29eee.jpg\nflower_photos/tulips/8673416556_639f5c88f1_n.jpg\nflower_photos/tulips/4521496161_2b41d4182e.jpg\nflower_photos/tulips/3558517884_0c7ca8b862_m.jpg\nflower_photos/tulips/8520488975_a50d377f91.jpg\nflower_photos/tulips/3105702091_f02ce75226.jpg\nflower_photos/tulips/13562271714_d534531374.jpg\nflower_photos/tulips/5682463466_d3e641cb8b.jpg\nflower_photos/tulips/2374855021_21959b40c0_n.jpg\nflower_photos/tulips/14064735842_a946fba1ef_n.jpg\nflower_photos/tulips/3446285408_4be9c0fded_m.jpg\nflower_photos/tulips/6931674908_8e93bd4554.jpg\nflower_photos/tulips/7481217920_6f65766a1c_n.jpg\nflower_photos/tulips/8723767533_9145dec4bd_n.jpg\nflower_photos/tulips/17078716890_68e0723389_n.jpg\nflower_photos/tulips/14054827391_139fb54432.jpg\nflower_photos/tulips/3510799169_0ed6ae9669_n.jpg\nflower_photos/tulips/3540595981_73f14d1227_n.jpg\nflower_photos/tulips/7179796338_05e8b1c87b.jpg\nflower_photos/tulips/14487762578_baba13d16a_m.jpg\nflower_photos/tulips/8623170936_83f4152431.jpg\nflower_photos/tulips/9444202147_405290415b_n.jpg\nflower_photos/tulips/2434178332_7fcf85aa95_n.jpg\nflower_photos/tulips/16862349256_0a1f91ab53.jpg\nflower_photos/tulips/7166539842_43b7e02883.jpg\nflower_photos/tulips/3991423020_9aaf2b5974_n.jpg\nflower_photos/tulips/14067476586_36bcddf111.jpg\nflower_photos/tulips/4571353297_5634177744_n.jpg\nflower_photos/tulips/8713391394_4b679ea1e3_n.jpg\nflower_photos/tulips/13561908485_7e4f8d508b.jpg\nflower_photos/tulips/14044685976_0064faed21.jpg\nflower_photos/tulips/8713389178_66bceb71a8_n.jpg\nflower_photos/tulips/5674704952_9bd225ed9e_n.jpg\nflower_photos/tulips/17720403638_94cfcd8d5c_n.jpg\nflower_photos/tulips/4525067924_177ea3bfb4.jpg\nflower_photos/tulips/3523398585_376960a611_m.jpg\nflower_photos/tulips/4890786831_91bb82a9e4_n.jpg\nflower_photos/tulips/13903937027_44b9f2f5b8.jpg\nflower_photos/tulips/779359602_30abcbf5bb_n.jpg\nflower_photos/tulips/13956300996_07e64a3dbd_n.jpg\nflower_photos/tulips/13923539227_bdab038dc8.jpg\nflower_photos/tulips/17189456156_6fc1067831.jpg\nflower_photos/tulips/3516269489_cef36e87a6.jpg\nflower_photos/tulips/6931708704_fccb06fea8.jpg\nflower_photos/tulips/8838354855_c474fc66a3_m.jpg\nflower_photos/tulips/12025038686_7f10811d4b_n.jpg\nflower_photos/tulips/5757091018_cdfd79dfa6_m.jpg\nflower_photos/tulips/7205145492_baec4dbb94.jpg\nflower_photos/tulips/16717320956_d4b00807f2.jpg\nflower_photos/tulips/5674170543_73e3f403fb.jpg\nflower_photos/tulips/3011223301_09b4e3edb7.jpg\nflower_photos/tulips/16732302779_8aa56f255d_n.jpg\nflower_photos/tulips/4442928974_9672d630b2_n.jpg\nflower_photos/tulips/3516271083_fba63b5861.jpg\nflower_photos/tulips/8454707381_453b4862eb_m.jpg\nflower_photos/tulips/14053292975_fdc1093571_n.jpg\nflower_photos/tulips/3600510954_a51bfc5440_n.jpg\nflower_photos/tulips/3502615974_ef4bd13202_n.jpg\nflower_photos/tulips/7166644048_b00a14f01b.jpg\nflower_photos/tulips/2418823693_72eec80f42_n.jpg\nflower_photos/tulips/6931748252_68f06086b3.jpg\nflower_photos/tulips/13530786873_0d34880300_n.jpg\nflower_photos/tulips/4590703575_6371c0a186_n.jpg\nflower_photos/tulips/12883412424_cb5086b43f_n.jpg\nflower_photos/tulips/14122029097_3e3285ca5c_n.jpg\nflower_photos/tulips/15052586652_56a82de133_m.jpg\nflower_photos/tulips/13976206001_fd1c2cbd60.jpg\nflower_photos/tulips/7448453762_aea8739f1b.jpg\nflower_photos/tulips/8454719295_4276c0e9c5_n.jpg\nflower_photos/tulips/2785458179_9130812eef_m.jpg\nflower_photos/tulips/7166635566_ee240b5408_n.jpg\nflower_photos/tulips/14087892193_653a3ac7ca_n.jpg\nflower_photos/tulips/4578030672_e6aefd45af.jpg\nflower_photos/tulips/15922772266_1167a06620.jpg\nflower_photos/tulips/16074109313_2cc14c7d16.jpg\nflower_photos/tulips/14057246122_8598b665bd.jpg\nflower_photos/tulips/2447151631_7551e6377b_n.jpg\nflower_photos/tulips/13910028149_6c9d5485ef.jpg\nflower_photos/tulips/2481015475_b71a12917d.jpg\nflower_photos/tulips/3282751630_45c2665034_m.jpg\nflower_photos/tulips/175686816_067a8cb4c5.jpg\nflower_photos/tulips/15452909878_0c4941f729_m.jpg\nflower_photos/tulips/8838975946_f54194894e_m.jpg\nflower_photos/tulips/14674389605_df3c0bcfa1_m.jpg\nflower_photos/tulips/3511776685_3635087b12_n.jpg\nflower_photos/tulips/100930342_92e8746431_n.jpg\nflower_photos/tulips/924782410_94ed7913ca_m.jpg\nflower_photos/tulips/17907238905_1ae121f8d9_m.jpg\nflower_photos/tulips/16506668270_b823935dc3.jpg\nflower_photos/tulips/6876631336_54bf150990.jpg\nflower_photos/tulips/14087326141_1906d5a373_n.jpg\nflower_photos/tulips/13903988248_22da33f341.jpg\nflower_photos/tulips/17994129033_bbd0acba62_n.jpg\nflower_photos/tulips/116343334_9cb4acdc57_n.jpg\nflower_photos/tulips/13857267684_d2a4b2630f_n.jpg\nflower_photos/tulips/14084211971_0f921f11fe_n.jpg\nflower_photos/tulips/6799076717_575944af91_m.jpg\nflower_photos/tulips/7166567320_0a2beb6d42.jpg\nflower_photos/tulips/13472387874_d844478dbb.jpg\nflower_photos/tulips/17781940352_a45e4289a5.jpg\nflower_photos/tulips/8713388322_e5ae26263b_n.jpg\nflower_photos/tulips/16309287412_5cc4d58bd1_n.jpg\nflower_photos/tulips/3454461550_64d6e726bf_m.jpg\nflower_photos/tulips/142235237_da662d925c.jpg\nflower_photos/tulips/8520482921_21dd204ebd_n.jpg\nflower_photos/tulips/8762193202_0fbf2f6a81.jpg\nflower_photos/tulips/6931489544_2f35025f7b_m.jpg\nflower_photos/tulips/3459922572_bc8516b5fe_m.jpg\nflower_photos/tulips/15082212714_ff87e8fcb1_m.jpg\nflower_photos/tulips/6982913043_3b873c6a25.jpg\nflower_photos/tulips/13910604778_e5f4588420.jpg\nflower_photos/tulips/3506615859_9850830cf0.jpg\nflower_photos/tulips/6808860548_53796b90ca_n.jpg\nflower_photos/tulips/478765271_6a8ca1cfa1_m.jpg\nflower_photos/tulips/8768645961_8f1e097170_n.jpg\nflower_photos/tulips/5043225469_0aa23f3c8f_n.jpg\nflower_photos/tulips/15881325303_f00807a051_n.jpg\nflower_photos/tulips/443600168_cb08d56511.jpg\nflower_photos/tulips/4591323356_030d8b6967_m.jpg\nflower_photos/tulips/2271507463_15c48d41c4_n.jpg\nflower_photos/tulips/2436998042_4906ea07af.jpg\nflower_photos/tulips/7069622551_348d41c327_n.jpg\nflower_photos/tulips/4042180234_64cd2859c9_m.jpg\nflower_photos/tulips/15632065904_0d9caf174b.jpg\nflower_photos/tulips/3430229687_32645b5738.jpg\nflower_photos/tulips/8603340662_0779bd87fd.jpg\nflower_photos/tulips/4588904196_3c5825c7f4.jpg\nflower_photos/tulips/4565139594_b28d260cb9.jpg\nflower_photos/tulips/12163418275_bd6a1edd61.jpg\nflower_photos/tulips/142235914_5419ff8a4a.jpg\nflower_photos/tulips/4546316433_202cc68c55.jpg\nflower_photos/tulips/17078576150_6f272ce73f_n.jpg\nflower_photos/tulips/13974542496_e4b5d1c913_n.jpg\nflower_photos/tulips/17295127995_62eff434fe_n.jpg\nflower_photos/tulips/4497973347_57480ffee9_m.jpg\nflower_photos/tulips/18378581986_5cd1494d08_n.jpg\nflower_photos/tulips/2344751399_71620039f2_n.jpg\nflower_photos/tulips/14084749296_6143c74c72_m.jpg\nflower_photos/tulips/14097111174_87a2e7e0c7_n.jpg\nflower_photos/tulips/2399982682_16929d1f6d_n.jpg\nflower_photos/tulips/8908097235_c3e746d36e_n.jpg\nflower_photos/tulips/4582198748_20fa7caaa1.jpg\nflower_photos/tulips/14674071872_2df55466d5_m.jpg\nflower_photos/tulips/12916441224_2ed63596f8_n.jpg\nflower_photos/tulips/5674125303_953b0ecf38.jpg\nflower_photos/tulips/8702982836_75222725d7.jpg\nflower_photos/tulips/10128546863_8de70c610d.jpg\nflower_photos/tulips/3421027755_cdb8fef8e8_n.jpg\nflower_photos/tulips/2322670828_34115a7050.jpg\nflower_photos/tulips/489506904_9b68ba211c.jpg\nflower_photos/tulips/5628970369_54eb9ed31c_n.jpg\nflower_photos/tulips/7166646966_41d83cd703.jpg\nflower_photos/tulips/7806320016_fcddfc1f8f_n.jpg\nflower_photos/tulips/142218310_d06005030a_n.jpg\nflower_photos/tulips/14068348874_7b36c99f6a.jpg\nflower_photos/tulips/3524204544_7233737b4f_m.jpg\nflower_photos/tulips/16951623209_00fb7ec1b1_n.jpg\nflower_photos/tulips/2440874162_27a7030402_n.jpg\nflower_photos/tulips/4312181724_16dab26afb_n.jpg\nflower_photos/tulips/6227136437_6117068599_m.jpg\nflower_photos/tulips/5700394524_dc6f8fa9cd_n.jpg\nflower_photos/tulips/4553203984_9cb9312240_n.jpg\nflower_photos/tulips/4955884820_7e4ce4d7e5_m.jpg\nflower_photos/tulips/7166570828_7c26ca5766_n.jpg\nflower_photos/tulips/2294116183_a30d2aa2c1_m.jpg\nflower_photos/tulips/8757486380_90952c5377.jpg\nflower_photos/tulips/16862422576_5226e8d1d0.jpg\nflower_photos/tulips/16139439153_fbdee29a10_n.jpg\nflower_photos/tulips/9030467406_05e93ff171_n.jpg\nflower_photos/tulips/8817622133_a42bb90e38_n.jpg\nflower_photos/tulips/8689672277_b289909f97_n.jpg\nflower_photos/tulips/13979098645_50b9eebc02_n.jpg\nflower_photos/tulips/4572955407_87f4805c7b.jpg\nflower_photos/tulips/16907559551_05ded87fb2_n.jpg\nflower_photos/tulips/3476945045_97ff41e8ec_n.jpg\nflower_photos/tulips/8695367666_0809529eaf_n.jpg\nflower_photos/tulips/18406629611_4d1edcf23b_n.jpg\nflower_photos/tulips/17094167287_865840060d_n.jpg\nflower_photos/tulips/6958342976_a4a9483488_n.jpg\nflower_photos/tulips/4516198427_0e5099cd8e.jpg\nflower_photos/tulips/13510057763_01b832d919.jpg\nflower_photos/tulips/5738195260_b3fc107aa7_n.jpg\nflower_photos/tulips/3433265727_0b8022e091.jpg\nflower_photos/tulips/14017640283_c417141832_n.jpg\nflower_photos/tulips/13530687085_9b515735ef_m.jpg\nflower_photos/tulips/13911047024_8966d70560_n.jpg\nflower_photos/tulips/10163955604_ae0b830975_n.jpg\nflower_photos/tulips/17066862602_7530f21efe.jpg\nflower_photos/tulips/7205698252_b972087cc2.jpg\nflower_photos/tulips/13555215723_cf2c11626b_b.jpg\nflower_photos/tulips/10094729603_eeca3f2cb6.jpg\nflower_photos/tulips/2427626706_ffdf697f84_n.jpg\nflower_photos/tulips/3186520634_30e1c67aa5_n.jpg\nflower_photos/tulips/14127532150_112823a8f6.jpg\nflower_photos/tulips/8555123165_2fe57eff4f.jpg\nflower_photos/tulips/5665080897_0796f726c9_m.jpg\nflower_photos/tulips/12916017805_1cde91a891_n.jpg\nflower_photos/tulips/17908793211_ff0f1f81d3_n.jpg\nflower_photos/tulips/17199499591_67b64b21ed_m.jpg\nflower_photos/tulips/6958243974_8851425ddb_n.jpg\nflower_photos/tulips/14094146241_0b9fa7b3e0.jpg\nflower_photos/tulips/7094415739_6b29e5215c_m.jpg\nflower_photos/tulips/5470898169_52a5ab876c_n.jpg\nflower_photos/tulips/38287568_627de6ca20.jpg\nflower_photos/tulips/8673416166_620fc18e2f_n.jpg\nflower_photos/tulips/7775145448_c42e638a6a_n.jpg\nflower_photos/tulips/14087361621_9fefb8dbef.jpg\nflower_photos/tulips/4550278535_dfdf7b74ef.jpg\nflower_photos/tulips/14026857634_500d7b41d6_m.jpg\nflower_photos/tulips/3529889389_ab4cb6c43b.jpg\nflower_photos/tulips/17198868382_697b23c715_n.jpg\nflower_photos/tulips/13979840624_28466cb3ec_n.jpg\nflower_photos/tulips/8729501081_b993185542_m.jpg\nflower_photos/tulips/16680927427_07ca6e4552_n.jpg\nflower_photos/tulips/6970683464_f70838ca3a_n.jpg\nflower_photos/tulips/8762189906_8223cef62f.jpg\nflower_photos/tulips/5811022098_2523ca4e82.jpg\nflower_photos/tulips/4604272150_0c92385530_n.jpg\nflower_photos/tulips/7342871880_c17fe0eb4f_m.jpg\nflower_photos/tulips/14071516088_b526946e17_n.jpg\nflower_photos/tulips/13923036338_1ce32c6d4f.jpg\nflower_photos/tulips/405035580_94b793e71d.jpg\nflower_photos/tulips/8713397358_0505cc0176_n.jpg\nflower_photos/tulips/251811158_75fa3034ff.jpg\nflower_photos/tulips/212720516_df4965ebda_n.jpg\nflower_photos/tulips/142235017_07816937c6.jpg\nflower_photos/tulips/7266196114_c2a736a15a_m.jpg\nflower_photos/tulips/2333321040_3960b9d67e_n.jpg\nflower_photos/tulips/518256494_368a72db37.jpg\nflower_photos/tulips/135994133_4f306fe4bf_n.jpg\nflower_photos/tulips/3502632842_791dd4be18_n.jpg\nflower_photos/tulips/4497976955_3f8c2a21c1_m.jpg\nflower_photos/tulips/3990989735_59e2751151_n.jpg\nflower_photos/tulips/14093884601_c87b5cd663_n.jpg\nflower_photos/tulips/8892851067_79242a7362_n.jpg\nflower_photos/tulips/14149603605_eedfe9678c_n.jpg\nflower_photos/tulips/8687675254_c93f50d8b0_m.jpg\nflower_photos/tulips/2421740440_f82ced8582.jpg\nflower_photos/tulips/8695372372_302135aeb2.jpg\nflower_photos/tulips/2249756775_02e693beda_n.jpg\nflower_photos/tulips/14064731501_ea14b58161.jpg\nflower_photos/tulips/7166640338_46b15d9ec8_n.jpg\nflower_photos/tulips/4418204816_018375acd0_m.jpg\nflower_photos/tulips/5430796647_f21b7b0fea.jpg\nflower_photos/tulips/16645809126_613b1e3ebe_m.jpg\nflower_photos/tulips/7055500907_dcf2bb50e0.jpg\nflower_photos/tulips/3447650747_8299786b80_n.jpg\nflower_photos/tulips/8511683706_4173683d45_m.jpg\nflower_photos/tulips/14097328354_4f1469a170.jpg\nflower_photos/tulips/8712244311_da8e90bf8e_n.jpg\nflower_photos/tulips/5674127693_1ddbd81097.jpg\nflower_photos/tulips/13953090784_0c7d7a904e.jpg\nflower_photos/tulips/510698601_9f61d6f8d8.jpg\nflower_photos/tulips/4604238410_bcec9da4a0_n.jpg\nflower_photos/tulips/5697471591_200ff951fa_n.jpg\nflower_photos/tulips/14116826873_d4bab623bf_n.jpg\nflower_photos/tulips/16670377091_87987f50a4_n.jpg\nflower_photos/tulips/4555842486_dd214a84d7_n.jpg\nflower_photos/tulips/8838983024_5c1a767878_n.jpg\nflower_photos/tulips/14651383746_419dc73634_m.jpg\nflower_photos/tulips/4209052442_7e754f617c_n.jpg\nflower_photos/tulips/13471563274_471fc1db33_m.jpg\nflower_photos/tulips/13999392173_b1411f8b23_n.jpg\nflower_photos/tulips/4571993204_5b3efe0e78.jpg\nflower_photos/tulips/8690791226_b1f015259f_n.jpg\nflower_photos/tulips/137126311_debe64c6a8_n.jpg\nflower_photos/tulips/8710148289_6fc196a0f8_n.jpg\nflower_photos/tulips/13910544560_9140dd547e.jpg\nflower_photos/tulips/2489638840_72ff3ee527_n.jpg\nflower_photos/tulips/4300258119_b03f2f956e.jpg\nflower_photos/tulips/8668973377_c69527db42_m.jpg\nflower_photos/tulips/17146928665_600fa3a1f1_n.jpg\nflower_photos/tulips/434146736_310a42d9cb_m.jpg\nflower_photos/tulips/16702117379_c25bff70e9.jpg\nflower_photos/tulips/5388013398_09a8a0f166_m.jpg\nflower_photos/tulips/8686332852_c6dcb2e86b.jpg\nflower_photos/tulips/16169741783_deeab1a679_m.jpg\nflower_photos/tulips/8712270243_8512cf4fbd.jpg\nflower_photos/tulips/17189526216_fa24dd541a_n.jpg\nflower_photos/tulips/14066056226_d8564a083e_m.jpg\nflower_photos/tulips/4562423077_00b16240dc_n.jpg\nflower_photos/tulips/3238068295_b2a7b17f48_n.jpg\nflower_photos/tulips/3502974120_9f1eceaf8b_n.jpg\nflower_photos/tulips/15275190769_0ed7bbf490.jpg\nflower_photos/tulips/2280950463_86510c2789_n.jpg\nflower_photos/tulips/14124669683_7fb74f20c3.jpg\nflower_photos/tulips/16062072523_1be3c0b61f.jpg\nflower_photos/tulips/7166550328_de0d73cfa9.jpg\nflower_photos/tulips/14046760909_0c73e84a1f_n.jpg\nflower_photos/tulips/486896118_bcc7b8e1d6.jpg\nflower_photos/tulips/14067778605_0285b7cc3a.jpg\nflower_photos/tulips/14275234071_6e6f473356.jpg\nflower_photos/tulips/13997641965_80d5dab542_n.jpg\nflower_photos/tulips/3455026124_d66cafb9fc.jpg\nflower_photos/tulips/13531001134_72052100e1_m.jpg\nflower_photos/tulips/485415743_eeb5d7c1a5.jpg\nflower_photos/tulips/16680930777_7e7f292fc5_n.jpg\nflower_photos/tulips/14110615113_bd7b3fcb84.jpg\nflower_photos/tulips/5635348214_a4e2b19ffe.jpg\nflower_photos/tulips/15275478257_fbd5850708_n.jpg\nflower_photos/tulips/11746080_963537acdc.jpg\nflower_photos/tulips/5634767665_0ae724774d.jpg\nflower_photos/tulips/4546299243_23cd58eb43.jpg\nflower_photos/tulips/14674388855_2da18e375a_m.jpg\nflower_photos/tulips/4599815420_8ee42c2382.jpg\nflower_photos/tulips/12873145295_438b8197a7_n.jpg\nflower_photos/tulips/10164073235_f29931d91e.jpg\nflower_photos/tulips/15149373026_93aacc65c5.jpg\nflower_photos/tulips/15756524087_823cf86bd8_m.jpg\nflower_photos/tulips/9019694597_2d3bbedb17.jpg\nflower_photos/tulips/14103897845_7986002615.jpg\nflower_photos/tulips/16055807744_000bc07afc_m.jpg\nflower_photos/tulips/8713392604_90631fb809_n.jpg\nflower_photos/tulips/15647243236_2778501cf5_n.jpg\nflower_photos/tulips/4580206494_9386c81ed8_n.jpg\nflower_photos/tulips/14836105101_1d07520932_m.jpg\nflower_photos/tulips/5012813078_99fb977616_n.jpg\nflower_photos/tulips/8713394070_b24561b0a9.jpg\nflower_photos/tulips/13561986193_cf645b2b9a.jpg\nflower_photos/tulips/133858239_3eaa8a91fd_n.jpg\nflower_photos/tulips/495094547_fd2d999c44.jpg\nflower_photos/tulips/12584810723_c97d00fcfd.jpg\nflower_photos/tulips/5433747333_869a2a172d_m.jpg\nflower_photos/tulips/6934951920_d43ff8b78d.jpg\nflower_photos/tulips/430785322_7ddef64c68_m.jpg\nflower_photos/tulips/130685245_dcdd23836f_m.jpg\nflower_photos/tulips/3002863623_cd83d6e634.jpg\nflower_photos/tulips/9831362123_5aac525a99_n.jpg\nflower_photos/daisy/\nflower_photos/daisy/2001380507_19488ff96a_n.jpg\nflower_photos/daisy/14272874304_47c0a46f5a.jpg\nflower_photos/daisy/8708143485_38d084ac8c_n.jpg\nflower_photos/daisy/5434901893_4550be3f84_m.jpg\nflower_photos/daisy/14402451388_56545a374a_n.jpg\nflower_photos/daisy/5874818796_3efbb8769d.jpg\nflower_photos/daisy/2509545845_99e79cb8a2_n.jpg\nflower_photos/daisy/9299302012_958c70564c_n.jpg\nflower_photos/daisy/2838487505_6c3b48efa5_m.jpg\nflower_photos/daisy/754296579_30a9ae018c_n.jpg\nflower_photos/daisy/12701063955_4840594ea6_n.jpg\nflower_photos/daisy/4724713781_d169f98a35.jpg\nflower_photos/daisy/2498632196_e47a472d5a.jpg\nflower_photos/daisy/5135131051_102d4878ca_n.jpg\nflower_photos/daisy/14551098743_2842e7a004_n.jpg\nflower_photos/daisy/4276898893_609d11db8b.jpg\nflower_photos/daisy/3773181799_5def396456.jpg\nflower_photos/daisy/4727955343_0bb23ac4ae.jpg\nflower_photos/daisy/5739768868_9f982684f9_n.jpg\nflower_photos/daisy/4598422221_b37313a3e3_n.jpg\nflower_photos/daisy/3356112863_75da8bca2c_m.jpg\nflower_photos/daisy/2213954589_c7da4b1486.jpg\nflower_photos/daisy/2579018590_74359dcf1a_m.jpg\nflower_photos/daisy/3386988684_bc5a66005e.jpg\nflower_photos/daisy/3861452393_14d2f95157_m.jpg\nflower_photos/daisy/9310226774_d1b8f5d9c9.jpg\nflower_photos/daisy/253426762_9793d43fcd.jpg\nflower_photos/daisy/525271784_013ddccd1b_m.jpg\nflower_photos/daisy/105806915_a9c13e2106_n.jpg\nflower_photos/daisy/14569895116_32f0dcb0f9.jpg\nflower_photos/daisy/10555826524_423eb8bf71_n.jpg\nflower_photos/daisy/14219214466_3ca6104eae_m.jpg\nflower_photos/daisy/16833748795_b681b2839f_n.jpg\nflower_photos/daisy/2573240560_ff7ffdd449.jpg\nflower_photos/daisy/5765646947_82e95a9cc9_n.jpg\nflower_photos/daisy/5014137563_d03eb0ed75_n.jpg\nflower_photos/daisy/7335886184_d06a83f640.jpg\nflower_photos/daisy/5665838969_fe217988b9_m.jpg\nflower_photos/daisy/6148728633_27afc47b0c_m.jpg\nflower_photos/daisy/2632216904_274aa17433.jpg\nflower_photos/daisy/21652746_cc379e0eea_m.jpg\nflower_photos/daisy/3661613900_b15ca1d35d_m.jpg\nflower_photos/daisy/5512287917_9f5d3f0f98_n.jpg\nflower_photos/daisy/5944315415_2be8abeb2f_m.jpg\nflower_photos/daisy/5679288570_b4c52e76d5.jpg\nflower_photos/daisy/1306119996_ab8ae14d72_n.jpg\nflower_photos/daisy/534547364_3f6b7279d2_n.jpg\nflower_photos/daisy/16025261368_911703a536_n.jpg\nflower_photos/daisy/9204730092_a7f2182347.jpg\nflower_photos/daisy/5608389827_a42a46f760.jpg\nflower_photos/daisy/7133935763_82b17c8e1b_n.jpg\nflower_photos/daisy/2578695910_5ab8ee17c1_n.jpg\nflower_photos/daisy/3506866918_61dd5fc53b_n.jpg\nflower_photos/daisy/4278442064_a5a598524b_m.jpg\nflower_photos/daisy/3758221664_b19116d61f.jpg\nflower_photos/daisy/5434742166_35773eba57_m.jpg\nflower_photos/daisy/3483303007_42e3f90da7.jpg\nflower_photos/daisy/20773528301_008fcbc5a1_n.jpg\nflower_photos/daisy/2346726545_2ebce2b2a6.jpg\nflower_photos/daisy/15029936576_8d6f96c72c_n.jpg\nflower_photos/daisy/3764116502_f394428ee0_n.jpg\nflower_photos/daisy/3098641292_76c908ba1f_n.jpg\nflower_photos/daisy/4820415253_15bc3b6833_n.jpg\nflower_photos/daisy/4538877108_3c793f7987_m.jpg\nflower_photos/daisy/8383753520_8391dd80ee_m.jpg\nflower_photos/daisy/14707111433_cce08ee007.jpg\nflower_photos/daisy/8085329197_41d53a21e2_n.jpg\nflower_photos/daisy/459931395_24bf6531fe_n.jpg\nflower_photos/daisy/1344985627_c3115e2d71_n.jpg\nflower_photos/daisy/158869618_f1a6704236_n.jpg\nflower_photos/daisy/8759177308_951790e00d_m.jpg\nflower_photos/daisy/25360380_1a881a5648.jpg\nflower_photos/daisy/1286274236_1d7ac84efb_n.jpg\nflower_photos/daisy/4141147800_813f660b47.jpg\nflower_photos/daisy/8008258043_5457dd254b_n.jpg\nflower_photos/daisy/20329326505_a777c71cc2.jpg\nflower_photos/daisy/10437770546_8bb6f7bdd3_m.jpg\nflower_photos/daisy/3440366251_5b9bdf27c9_m.jpg\nflower_photos/daisy/2538504987_fe524b92a8_n.jpg\nflower_photos/daisy/10770585085_4742b9dac3_n.jpg\nflower_photos/daisy/18901817451_43e2b45f6c.jpg\nflower_photos/daisy/171972704_389cf7a953.jpg\nflower_photos/daisy/19544831049_0d738d4872_m.jpg\nflower_photos/daisy/7410356270_9dff4d0e2e_n.jpg\nflower_photos/daisy/19177263840_6a316ea639.jpg\nflower_photos/daisy/14866200659_6462c723cb_m.jpg\nflower_photos/daisy/2590291468_2635d3e4e0_n.jpg\nflower_photos/daisy/10841136265_af473efc60.jpg\nflower_photos/daisy/799964360_7e07a227ea_n.jpg\nflower_photos/daisy/8063844363_db3f4dea85.jpg\nflower_photos/daisy/15784493690_b1858cdb2b_n.jpg\nflower_photos/daisy/9489270024_1b05f08492_m.jpg\nflower_photos/daisy/4482623536_b9fb5ae41f_n.jpg\nflower_photos/daisy/4694734757_5c563d38dd_n.jpg\nflower_photos/daisy/2551708158_1f10e81e11.jpg\nflower_photos/daisy/517054467_d82d323c33_m.jpg\nflower_photos/daisy/14564545365_1f1d267bf1_n.jpg\nflower_photos/daisy/3494265422_9dba8f2191_n.jpg\nflower_photos/daisy/7669550908_bc5a11276f_n.jpg\nflower_photos/daisy/9158041313_7a6a102f7a_n.jpg\nflower_photos/daisy/5586977262_6b24412805_n.jpg\nflower_photos/daisy/2567033807_8e918c53d8_n.jpg\nflower_photos/daisy/3637428148_a1dcccafa9_n.jpg\nflower_photos/daisy/294451721_5106537b34.jpg\nflower_photos/daisy/2454280137_e1637536ae_n.jpg\nflower_photos/daisy/4281102584_c548a69b81_m.jpg\nflower_photos/daisy/2646438199_b309cffd65_n.jpg\nflower_photos/daisy/8964198962_6d8593b533.jpg\nflower_photos/daisy/2862944799_45bc8e7302.jpg\nflower_photos/daisy/3338077096_3a8ed0e2bc_m.jpg\nflower_photos/daisy/4666648087_b10f376f19.jpg\nflower_photos/daisy/506493250_e9ca42fe3d.jpg\nflower_photos/daisy/14330343061_99478302d4_m.jpg\nflower_photos/daisy/6136947177_47ff445eb4_n.jpg\nflower_photos/daisy/153210866_03cc9f2f36.jpg\nflower_photos/daisy/3504430338_77d6a7fab4_n.jpg\nflower_photos/daisy/8021540573_c56cf9070d_n.jpg\nflower_photos/daisy/12193032636_b50ae7db35_n.jpg\nflower_photos/daisy/3975010332_3209f9f447_m.jpg\nflower_photos/daisy/3379332157_04724f6480.jpg\nflower_photos/daisy/3456403987_5bd5fa6ece_n.jpg\nflower_photos/daisy/5459481183_18d2d49e44_m.jpg\nflower_photos/daisy/446484749_4044affcaf_n.jpg\nflower_photos/daisy/391364010_4b0942d400_m.jpg\nflower_photos/daisy/721595842_bacd80a6ac.jpg\nflower_photos/daisy/8671824531_64b816949e_m.jpg\nflower_photos/daisy/9345273630_af3550031d.jpg\nflower_photos/daisy/521762040_f26f2e08dd.jpg\nflower_photos/daisy/435283392_72e4c5b5d6_m.jpg\nflower_photos/daisy/3720632920_93cf1cc7f3_m.jpg\nflower_photos/daisy/2511306240_9047015f2d_n.jpg\nflower_photos/daisy/11642632_1e7627a2cc.jpg\nflower_photos/daisy/9611923744_013b29e4da_n.jpg\nflower_photos/daisy/54377391_15648e8d18.jpg\nflower_photos/daisy/2556503265_63ae6b9e0e_m.jpg\nflower_photos/daisy/14471433500_cdaa22e3ea_m.jpg\nflower_photos/daisy/2641979584_2b21c3fe29_m.jpg\nflower_photos/daisy/174131220_c853df1287.jpg\nflower_photos/daisy/3939135368_0af5c4982a_n.jpg\nflower_photos/daisy/5434913005_409c1e8b56_n.jpg\nflower_photos/daisy/4657354814_f368762c53_m.jpg\nflower_photos/daisy/4432271543_01c56ca3a9.jpg\nflower_photos/daisy/2641151167_3bf1349606_m.jpg\nflower_photos/daisy/909609509_a05ccb8127.jpg\nflower_photos/daisy/2454280135_ac3aa75cdc_n.jpg\nflower_photos/daisy/8696022686_1f8d62c5cb_m.jpg\nflower_photos/daisy/3588872598_e0f9a1d2a1_m.jpg\nflower_photos/daisy/19653086178_28156b7ce4_m.jpg\nflower_photos/daisy/5773652803_574b51414f_n.jpg\nflower_photos/daisy/3080880039_4f1bd592e5_n.jpg\nflower_photos/daisy/14332947164_9b13513c71_m.jpg\nflower_photos/daisy/3628485766_4ff937954a_n.jpg\nflower_photos/daisy/3848258315_ed2fde4fb4.jpg\nflower_photos/daisy/22873310415_3a5674ec10_m.jpg\nflower_photos/daisy/7320089276_87b544e341.jpg\nflower_photos/daisy/4229503616_9b8a42123c_n.jpg\nflower_photos/daisy/2514748602_343d4727c0_n.jpg\nflower_photos/daisy/18635898912_eb8e058ef0.jpg\nflower_photos/daisy/15813862117_dedcd1c56f_m.jpg\nflower_photos/daisy/4407065098_ef25f1ccac_n.jpg\nflower_photos/daisy/8719756744_34a5a83976_n.jpg\nflower_photos/daisy/163978992_8128b49d3e_n.jpg\nflower_photos/daisy/6299498346_b9774b6500.jpg\nflower_photos/daisy/5435513198_90ce39f1aa_n.jpg\nflower_photos/daisy/11023214096_b5b39fab08.jpg\nflower_photos/daisy/5973488341_50bdf6cee3_n.jpg\nflower_photos/daisy/3703643767_dee82cdef9_n.jpg\nflower_photos/daisy/5435521200_92029bbe2b_n.jpg\nflower_photos/daisy/8446495985_f72d851482.jpg\nflower_photos/daisy/1392946544_115acbb2d9.jpg\nflower_photos/daisy/422094774_28acc69a8b_n.jpg\nflower_photos/daisy/4540555191_3254dc4608_n.jpg\nflower_photos/daisy/3633489595_a037a9b7a4_m.jpg\nflower_photos/daisy/14021430525_e06baf93a9.jpg\nflower_photos/daisy/5853276960_d08f90fff6.jpg\nflower_photos/daisy/512477177_d9004cbcf1_n.jpg\nflower_photos/daisy/12585131704_0f64b17059_m.jpg\nflower_photos/daisy/10993818044_4c19b86c82.jpg\nflower_photos/daisy/15760153042_a2a90e9da5_m.jpg\nflower_photos/daisy/2611119198_9d46b94392.jpg\nflower_photos/daisy/14167534527_781ceb1b7a_n.jpg\nflower_photos/daisy/1354396826_2868631432_m.jpg\nflower_photos/daisy/8489463746_a9839bf7e4.jpg\nflower_photos/daisy/19865728236_a62f8f445b_n.jpg\nflower_photos/daisy/9595857626_979c45e5bf_n.jpg\nflower_photos/daisy/695778683_890c46ebac.jpg\nflower_photos/daisy/433837534_1dbf798b73.jpg\nflower_photos/daisy/835750256_3f91a147ef_n.jpg\nflower_photos/daisy/2713919471_301fcc941f.jpg\nflower_photos/daisy/2627815904_919373e7f5.jpg\nflower_photos/daisy/14921511479_7b0a647795.jpg\nflower_photos/daisy/7749368884_1fc58c67ff_n.jpg\nflower_photos/daisy/15207766_fc2f1d692c_n.jpg\nflower_photos/daisy/20948886919_cac7844f34_n.jpg\nflower_photos/daisy/3711723108_65247a3170.jpg\nflower_photos/daisy/173350276_02817aa8d5.jpg\nflower_photos/daisy/5876455546_32049e5585.jpg\nflower_photos/daisy/13826249325_f61cb15f86_n.jpg\nflower_photos/daisy/14621687774_ec52811acd_n.jpg\nflower_photos/daisy/1396526833_fb867165be_n.jpg\nflower_photos/daisy/14907815010_bff495449f.jpg\nflower_photos/daisy/8619103877_d8c82c5f34_n.jpg\nflower_photos/daisy/5626784099_b36dd3fb11_n.jpg\nflower_photos/daisy/8740807508_0587f5b7b7.jpg\nflower_photos/daisy/2521408074_e6f86daf21_n.jpg\nflower_photos/daisy/175106495_53ebdef092_n.jpg\nflower_photos/daisy/14354051035_1037b30421_n.jpg\nflower_photos/daisy/3750250718_eb61146c5f.jpg\nflower_photos/daisy/3450822975_7e77d67636_n.jpg\nflower_photos/daisy/23095658544_7226386954_n.jpg\nflower_photos/daisy/3491933306_43cfe2cfbe.jpg\nflower_photos/daisy/5547758_eea9edfd54_n.jpg\nflower_photos/daisy/5809489674_5659b3ae5d_n.jpg\nflower_photos/daisy/437859108_173fb33c98.jpg\nflower_photos/daisy/5602738326_97121e007d_n.jpg\nflower_photos/daisy/4318007511_e9f4311936_n.jpg\nflower_photos/daisy/367020749_3c9a652d75.jpg\nflower_photos/daisy/3336704121_cfeb67a7d7.jpg\nflower_photos/daisy/6323721068_3d3394af6d_n.jpg\nflower_photos/daisy/4131565290_0585c4dd5a_n.jpg\nflower_photos/daisy/3900172983_9312fdf39c_n.jpg\nflower_photos/daisy/2349640101_212c275aa7.jpg\nflower_photos/daisy/13977181862_f8237b6b52.jpg\nflower_photos/daisy/5769217520_c90efc3c93_m.jpg\nflower_photos/daisy/16020253176_60f2a6a5ca_n.jpg\nflower_photos/daisy/14221836990_90374e6b34.jpg\nflower_photos/daisy/3957488431_52a447c0e8_m.jpg\nflower_photos/daisy/14421389519_d5fd353eb4.jpg\nflower_photos/daisy/3711892138_b8c953fdc1_z.jpg\nflower_photos/daisy/9529916092_de70623523_n.jpg\nflower_photos/daisy/5684911529_88a7ae32ba_n.jpg\nflower_photos/daisy/8932490012_cc08e690ba_n.jpg\nflower_photos/daisy/18203367608_07a04e98a4_n.jpg\nflower_photos/daisy/4669117051_ce61e91b76.jpg\nflower_photos/daisy/6089825811_80f253fbe1.jpg\nflower_photos/daisy/5110105726_53eb7a93be_m.jpg\nflower_photos/daisy/11023272144_fce94401f2_m.jpg\nflower_photos/daisy/495098110_3a4bb30042_n.jpg\nflower_photos/daisy/6054952060_c88612f3c5_n.jpg\nflower_photos/daisy/3415180846_d7b5cced14_m.jpg\nflower_photos/daisy/22244161124_53e457bb66_n.jpg\nflower_photos/daisy/3475870145_685a19116d.jpg\nflower_photos/daisy/14163875973_467224aaf5_m.jpg\nflower_photos/daisy/4117918318_3c8935289b_m.jpg\nflower_photos/daisy/4268817944_cdbdb226ae.jpg\nflower_photos/daisy/1392131677_116ec04751.jpg\nflower_photos/daisy/4610018126_21f438d2dc_m.jpg\nflower_photos/daisy/12601254324_3cb62c254a_m.jpg\nflower_photos/daisy/7189043225_2fe781439a_n.jpg\nflower_photos/daisy/7633425046_8293e3d0e9_m.jpg\nflower_photos/daisy/5087720485_c0914fb623.jpg\nflower_photos/daisy/3468498624_d082f99e98.jpg\nflower_photos/daisy/5881907044_92a85a05c8_n.jpg\nflower_photos/daisy/18442919723_d1251d3e14_n.jpg\nflower_photos/daisy/1031799732_e7f4008c03.jpg\nflower_photos/daisy/7924174040_444d5bbb8a.jpg\nflower_photos/daisy/2812442552_3eed5fb9f2_m.jpg\nflower_photos/daisy/14716799982_ed6d626a66.jpg\nflower_photos/daisy/10172567486_2748826a8b.jpg\nflower_photos/daisy/5110107234_12ddc0206b_m.jpg\nflower_photos/daisy/14147016029_8d3cf2414e.jpg\nflower_photos/daisy/2617111535_54c2ac8462.jpg\nflower_photos/daisy/11124324295_503f3a0804.jpg\nflower_photos/daisy/21626652132_97e1318bb8_m.jpg\nflower_photos/daisy/452854574_59492f119a_m.jpg\nflower_photos/daisy/1140299375_3aa7024466.jpg\nflower_photos/daisy/12348343085_d4c396e5b5_m.jpg\nflower_photos/daisy/6978826370_7b9aa7c7d5.jpg\nflower_photos/daisy/2561371688_c80a4fe957_n.jpg\nflower_photos/daisy/9120905231_329598304e.jpg\nflower_photos/daisy/20619292635_9857a12d54.jpg\nflower_photos/daisy/3463313493_9497aa47e5_n.jpg\nflower_photos/daisy/2077865117_9ed85191ae_n.jpg\nflower_photos/daisy/13583238844_573df2de8e_m.jpg\nflower_photos/daisy/4890424315_6a59696357_n.jpg\nflower_photos/daisy/172967318_c596d082cc.jpg\nflower_photos/daisy/515112668_a49c69455a.jpg\nflower_photos/daisy/154332674_453cea64f4.jpg\nflower_photos/daisy/3611577717_f3a7a8c416_n.jpg\nflower_photos/daisy/5577555349_2e8490259b.jpg\nflower_photos/daisy/10437929963_bc13eebe0c.jpg\nflower_photos/daisy/169371301_d9b91a2a42.jpg\nflower_photos/daisy/4222584034_8964cbd3de.jpg\nflower_photos/daisy/14674743211_f68b13f6d9.jpg\nflower_photos/daisy/4613992315_143ccc2a10_m.jpg\nflower_photos/daisy/3640845041_80a92c4205_n.jpg\nflower_photos/daisy/2482982436_a2145359e0_n.jpg\nflower_photos/daisy/10391248763_1d16681106_n.jpg\nflower_photos/daisy/5948835387_5a98d39eff_m.jpg\nflower_photos/daisy/2561352120_7961d8263f.jpg\nflower_photos/daisy/16291797949_a1b1b7c2bd_n.jpg\nflower_photos/daisy/14554906452_35f066ffe9_n.jpg\nflower_photos/daisy/2351206867_084e57bd97.jpg\nflower_photos/daisy/5714327423_50af0cffe9.jpg\nflower_photos/daisy/4897587985_f9293ea1ed.jpg\nflower_photos/daisy/8938566373_d129e7af75.jpg\nflower_photos/daisy/8071646795_2fdc89ab7a_n.jpg\nflower_photos/daisy/512177035_70afc925c8.jpg\nflower_photos/daisy/6208851904_9d916ebb32_n.jpg\nflower_photos/daisy/2581171297_b0a249b92b_n.jpg\nflower_photos/daisy/5869147563_66fb88119d.jpg\nflower_photos/daisy/8983779970_9d3a6a3bf2_n.jpg\nflower_photos/daisy/3084924076_4d5c5711af_m.jpg\nflower_photos/daisy/6529588249_d9cbe68aab_n.jpg\nflower_photos/daisy/1299501272_59d9da5510_n.jpg\nflower_photos/daisy/43474673_7bb4465a86.jpg\nflower_photos/daisy/134372449_0f7166d96c_n.jpg\nflower_photos/daisy/3552074420_2a0a7166db_m.jpg\nflower_photos/daisy/1879567877_8ed2a5faa7_n.jpg\nflower_photos/daisy/10555749515_13a12a026e.jpg\nflower_photos/daisy/7568630428_8cf0fc16ff_n.jpg\nflower_photos/daisy/2828733818_1c1ed0089d_n.jpg\nflower_photos/daisy/18474740346_ffdaa18032.jpg\nflower_photos/daisy/162362896_99c7d851c8_n.jpg\nflower_photos/daisy/6596277835_9f86da54bb.jpg\nflower_photos/daisy/3337643329_accc9b5426.jpg\nflower_photos/daisy/4753134939_8e87649db6.jpg\nflower_photos/daisy/7629784968_b953501902_n.jpg\nflower_photos/daisy/7630511450_02d3292e90.jpg\nflower_photos/daisy/19178753159_a471bf4b6b.jpg\nflower_photos/daisy/4534460263_8e9611db3c_n.jpg\nflower_photos/daisy/15760811380_4d686c892b_n.jpg\nflower_photos/daisy/4258408909_b7cc92741c_m.jpg\nflower_photos/daisy/530738000_4df7e4786b.jpg\nflower_photos/daisy/6480809771_b1e14c5cc2_m.jpg\nflower_photos/daisy/19019544592_b64469bf84_n.jpg\nflower_photos/daisy/21805938544_bf6bb0e4bc.jpg\nflower_photos/daisy/9496209717_25a6ebdab6_m.jpg\nflower_photos/daisy/3546455114_cd2dea5e02.jpg\nflower_photos/daisy/14073784469_ffb12f3387_n.jpg\nflower_photos/daisy/8616684075_71923bb771_n.jpg\nflower_photos/daisy/5561775629_a2b709b3a4_n.jpg\nflower_photos/daisy/1150395827_6f94a5c6e4_n.jpg\nflower_photos/daisy/14507818175_05219b051c_m.jpg\nflower_photos/daisy/9346508462_f0af3163f4.jpg\nflower_photos/daisy/1265350143_6e2b276ec9.jpg\nflower_photos/daisy/107592979_aaa9cdfe78_m.jpg\nflower_photos/daisy/5811226952_4650ed70ae_n.jpg\nflower_photos/daisy/8094774544_35465c1c64.jpg\nflower_photos/daisy/498159452_b71afd65ba.jpg\nflower_photos/daisy/5054771689_00dd40b971_n.jpg\nflower_photos/daisy/14088053307_1a13a0bf91_n.jpg\nflower_photos/daisy/9467543719_c4800becbb_m.jpg\nflower_photos/daisy/4851353993_2cbbbd1040_n.jpg\nflower_photos/daisy/2481823240_eab0d86921.jpg\nflower_photos/daisy/7454630692_ab2d67dd18_m.jpg\nflower_photos/daisy/3704305945_a80e60e2f6_m.jpg\nflower_photos/daisy/144603918_b9de002f60_m.jpg\nflower_photos/daisy/10140303196_b88d3d6cec.jpg\nflower_photos/daisy/2539552964_921cf645ba_n.jpg\nflower_photos/daisy/5109508979_68e3530791_m.jpg\nflower_photos/daisy/14333681205_a07c9f1752_m.jpg\nflower_photos/daisy/4923279674_e7f8e70794_n.jpg\nflower_photos/daisy/705422469_ffa28c566d.jpg\nflower_photos/daisy/13901930939_a7733c03f0_n.jpg\nflower_photos/daisy/4286053334_a75541f20b_m.jpg\nflower_photos/daisy/16737503507_431768a927.jpg\nflower_photos/daisy/9054268881_19792c5203_n.jpg\nflower_photos/daisy/3410906335_37e8a24b1c_n.jpg\nflower_photos/daisy/9146733107_98b15d3892_m.jpg\nflower_photos/daisy/506018088_4f7a15a7c5_n.jpg\nflower_photos/daisy/8706810197_17b6c1f1e7.jpg\nflower_photos/daisy/7538403124_f2fc48750a.jpg\nflower_photos/daisy/9286947622_4822f4fc21.jpg\nflower_photos/daisy/19975899671_ebc42b7865_n.jpg\nflower_photos/daisy/11870378973_2ec1919f12.jpg\nflower_photos/daisy/799952628_bf836677fa_n.jpg\nflower_photos/daisy/9321854387_5f77c926cb_n.jpg\nflower_photos/daisy/754248840_95092de274.jpg\nflower_photos/daisy/10437754174_22ec990b77_m.jpg\nflower_photos/daisy/3337536080_1db19964fe.jpg\nflower_photos/daisy/5058708968_8bdcd29e63_n.jpg\nflower_photos/daisy/3326037909_b5ae370722_n.jpg\nflower_photos/daisy/10559679065_50d2b16f6d.jpg\nflower_photos/daisy/4993492878_11fd4f5d12.jpg\nflower_photos/daisy/301964511_fab84ea1c1.jpg\nflower_photos/daisy/14485782498_fb342ec301.jpg\nflower_photos/daisy/9244082319_b1f7e2d8b0_n.jpg\nflower_photos/daisy/13953307149_f8de6a768c_m.jpg\nflower_photos/daisy/2488902131_3417698611_n.jpg\nflower_photos/daisy/7199968650_72afc16d31_m.jpg\nflower_photos/daisy/5981645737_29eceac291_m.jpg\nflower_photos/daisy/506348009_9ecff8b6ef.jpg\nflower_photos/daisy/19834392829_7d697871f6.jpg\nflower_photos/daisy/8882282142_9be2524d38_m.jpg\nflower_photos/daisy/14523675369_97c31d0b5b.jpg\nflower_photos/daisy/413815348_764ae83088.jpg\nflower_photos/daisy/14167543177_cd36b54ac6_n.jpg\nflower_photos/daisy/8742143296_fed9fa007c.jpg\nflower_photos/daisy/20703737132_179560d0fb.jpg\nflower_photos/daisy/2479956481_8d1a9699be_n.jpg\nflower_photos/daisy/2473825306_62fd5f8785_n.jpg\nflower_photos/daisy/4654579740_6671a53627_m.jpg\nflower_photos/daisy/1441939151_b271408c8d_n.jpg\nflower_photos/daisy/676120388_28f03069c3.jpg\nflower_photos/daisy/3780380240_ef9ec1b737_m.jpg\nflower_photos/daisy/14087947408_9779257411_n.jpg\nflower_photos/daisy/7191221492_610035de7c_m.jpg\nflower_photos/daisy/433037739_6a030e5912.jpg\nflower_photos/daisy/450128527_fd35742d44.jpg\nflower_photos/daisy/16482676953_5296227d40_n.jpg\nflower_photos/daisy/2621723097_736febb4a4_n.jpg\nflower_photos/daisy/476856232_7c35952f40_n.jpg\nflower_photos/daisy/14613443462_d4ed356201.jpg\nflower_photos/daisy/3117644024_1cbb59a509_n.jpg\nflower_photos/daisy/6480809573_76a0074b69_n.jpg\nflower_photos/daisy/144099102_bf63a41e4f_n.jpg\nflower_photos/daisy/3962240986_0661edc43a_n.jpg\nflower_photos/daisy/7630517248_98fb8bee1f_n.jpg\nflower_photos/daisy/17249393016_093e915012_n.jpg\nflower_photos/daisy/4757448834_a29a9538c9_n.jpg\nflower_photos/daisy/5623010186_796ca8d29a.jpg\nflower_photos/daisy/10466290366_cc72e33532.jpg\nflower_photos/daisy/10466558316_a7198b87e2.jpg\nflower_photos/daisy/20685027271_0e7306e7c1_n.jpg\nflower_photos/daisy/2057816617_18448093d0_n.jpg\nflower_photos/daisy/14114116486_0bb6649bc1_m.jpg\nflower_photos/daisy/5885826924_38fdc6bcaa_n.jpg\nflower_photos/daisy/4434592930_6610d51fca_m.jpg\nflower_photos/daisy/9515186037_3be48fe68f.jpg\nflower_photos/daisy/16819071290_471d99e166_m.jpg\nflower_photos/daisy/3717746329_53f515c6a6_m.jpg\nflower_photos/daisy/10994032453_ac7f8d9e2e.jpg\nflower_photos/daisy/18195689904_46619b7e16_n.jpg\nflower_photos/daisy/4746633946_23933c0810.jpg\nflower_photos/daisy/2649404904_b7a91991bb_n.jpg\nflower_photos/daisy/10172379554_b296050f82_n.jpg\nflower_photos/daisy/7630520686_e3a61ac763.jpg\nflower_photos/daisy/8348621545_8f02b82662_n.jpg\nflower_photos/daisy/10555815624_dc211569b0.jpg\nflower_photos/daisy/8120563761_ed5620664f_m.jpg\nflower_photos/daisy/14350958832_29bdd3a254.jpg\nflower_photos/daisy/538920244_59899a78f8_n.jpg\nflower_photos/daisy/1355787476_32e9f2a30b.jpg\nflower_photos/daisy/7227973870_806d9d3e42_n.jpg\nflower_photos/daisy/8382667241_0f046cecdb_n.jpg\nflower_photos/daisy/3275951182_d27921af97_n.jpg\nflower_photos/daisy/6095817094_3a5b1d793d.jpg\nflower_photos/daisy/520752848_4b87fb91a4.jpg\nflower_photos/daisy/5574421625_61b1f49b3f_m.jpg\nflower_photos/daisy/2599662355_7782218c83.jpg\nflower_photos/daisy/4544110929_a7de65d65f_n.jpg\nflower_photos/daisy/18354545086_693ea7bc2a.jpg\nflower_photos/daisy/2635314490_e12d3b0f36_m.jpg\nflower_photos/daisy/14264136211_9531fbc144.jpg\nflower_photos/daisy/18023717391_e2c9089e10.jpg\nflower_photos/daisy/4861391074_c3e122dab0_m.jpg\nflower_photos/daisy/14372713423_61e2daae88.jpg\nflower_photos/daisy/11891885265_ccefec7284_n.jpg\nflower_photos/daisy/4563059851_45a9d21a75.jpg\nflower_photos/daisy/4085794721_7cd88e0a6c_m.jpg\nflower_photos/daisy/4646886118_b5c5ceaf6d_n.jpg\nflower_photos/daisy/6910811638_aa6f17df23.jpg\nflower_photos/daisy/19280272025_57de24e940_m.jpg\nflower_photos/daisy/6210664514_f1d211217a.jpg\nflower_photos/daisy/11834945233_a53b7a92ac_m.jpg\nflower_photos/daisy/2331133004_582772d58f_m.jpg\nflower_photos/daisy/18582579815_4c6637e9ff_m.jpg\nflower_photos/daisy/172882635_4cc7b86731_m.jpg\nflower_photos/daisy/10993710036_2033222c91.jpg\nflower_photos/daisy/5665834973_76bd6c6523_m.jpg\nflower_photos/daisy/5795159787_ebb51a5e75.jpg\nflower_photos/daisy/2619413565_61a6cd3ac9_m.jpg\nflower_photos/daisy/16121105382_b96251e506_m.jpg\nflower_photos/daisy/2019064575_7656b9340f_m.jpg\nflower_photos/daisy/4792826628_aa5e5a9804_n.jpg\nflower_photos/daisy/10172636503_21bededa75_n.jpg\nflower_photos/daisy/5796562389_ae43c83317_m.jpg\nflower_photos/daisy/9161647994_e39b65cb9c_n.jpg\nflower_photos/daisy/476857510_d2b30175de_n.jpg\nflower_photos/daisy/14591326135_930703dbed_m.jpg\nflower_photos/daisy/813445367_187ecf080a_n.jpg\nflower_photos/daisy/14600779226_7bbc288d40_m.jpg\nflower_photos/daisy/3695826945_9f374e8a00_m.jpg\nflower_photos/daisy/9242705328_eee8402a8d.jpg\nflower_photos/daisy/5632774792_0fa33d17eb_n.jpg\nflower_photos/daisy/3713290261_8a66de23ab.jpg\nflower_photos/daisy/144076848_57e1d662e3_m.jpg\nflower_photos/daisy/5434914569_e9b982fde0_n.jpg\nflower_photos/daisy/102841525_bd6628ae3c.jpg\nflower_photos/daisy/20580471306_ab5a011b15_n.jpg\nflower_photos/daisy/4668543441_79040ca329_n.jpg\nflower_photos/daisy/3706420943_66f3214862_n.jpg\nflower_photos/daisy/2607132536_d95198e619_n.jpg\nflower_photos/daisy/14698531521_0c2f0c6539.jpg\nflower_photos/daisy/483886997_27ee798327.jpg\nflower_photos/daisy/2642408410_61545fdc83_n.jpg\nflower_photos/daisy/20182559506_40a112f762.jpg\nflower_photos/daisy/7416083788_fcb4c4f27e_n.jpg\nflower_photos/daisy/8710109684_e2c5ef6aeb_n.jpg\nflower_photos/daisy/6776075110_1ea7a09dd4_n.jpg\nflower_photos/daisy/11023277956_8980d53169_m.jpg\nflower_photos/daisy/9180706736_092d43088c.jpg\nflower_photos/daisy/3750771898_cfd50090ba_n.jpg\nflower_photos/daisy/299129811_d6ebda9970.jpg\nflower_photos/daisy/176375506_201859bb92_m.jpg\nflower_photos/daisy/4496202781_1d8e776ff5_n.jpg\nflower_photos/daisy/1656856503_447e5b0f03.jpg\nflower_photos/daisy/3025866885_22fb0b61c6_n.jpg\nflower_photos/daisy/525780443_bba812c26a_m.jpg\nflower_photos/daisy/7702332000_3f21ef4571_n.jpg\nflower_photos/daisy/162362897_1d21b70621_m.jpg\nflower_photos/daisy/5722473541_ffac1ae67e_n.jpg\nflower_photos/daisy/267148092_4bb874af58.jpg\nflower_photos/daisy/2476937534_21b285aa46_n.jpg\nflower_photos/daisy/8127252886_96558c23d1.jpg\nflower_photos/daisy/4581199679_867652c3f1_n.jpg\nflower_photos/daisy/3963330924_6c6a3fa7be_n.jpg\nflower_photos/daisy/14399435971_ea5868c792.jpg\nflower_photos/daisy/21402054779_759366efb0_n.jpg\nflower_photos/daisy/6950173662_5e9473003e_n.jpg\nflower_photos/daisy/4561871220_47f420ca59_m.jpg\nflower_photos/daisy/5435522104_1d6a61b431_n.jpg\nflower_photos/daisy/4683997791_56e7d3c03c_n.jpg\nflower_photos/daisy/5884807222_22f5326ba8_m.jpg\nflower_photos/daisy/3533954656_79156c8473.jpg\nflower_photos/daisy/147068564_32bb4350cc.jpg\nflower_photos/daisy/302782756_d35cb3e468.jpg\nflower_photos/daisy/6884975451_c74f445d69_m.jpg\nflower_photos/daisy/4413849849_b8d2f3bcf1_n.jpg\nflower_photos/daisy/100080576_f52e8ee070_n.jpg\nflower_photos/daisy/4440480869_632ce6aff3_n.jpg\nflower_photos/daisy/8709110478_60d12efcd4_n.jpg\nflower_photos/daisy/6864242336_0d12713fe5_n.jpg\nflower_photos/daisy/4301689054_20519e5b68.jpg\nflower_photos/daisy/3445110406_0c1616d2e3_n.jpg\nflower_photos/daisy/4865691548_00319261b8.jpg\nflower_photos/daisy/3285641623_da0e47f49a.jpg\nflower_photos/daisy/14245834619_153624f836.jpg\nflower_photos/daisy/18622672908_eab6dc9140_n.jpg\nflower_photos/daisy/14307766919_fac3c37a6b_m.jpg\nflower_photos/daisy/4511693548_20f9bd2b9c_m.jpg\nflower_photos/daisy/16527403771_2391f137c4_n.jpg\nflower_photos/daisy/5673551_01d1ea993e_n.jpg\nflower_photos/daisy/2513618768_ff7c004796_m.jpg\nflower_photos/daisy/14816364517_2423021484_m.jpg\nflower_photos/daisy/134409839_71069a95d1_m.jpg\nflower_photos/daisy/20289938802_e16fa9f23d.jpg\nflower_photos/daisy/305160642_53cde0f44f.jpg\nflower_photos/daisy/15853110333_229c439e7f.jpg\nflower_photos/daisy/5693459303_e61d9a9533.jpg\nflower_photos/daisy/510844526_858b8fe4db.jpg\nflower_photos/daisy/3639009391_0f910681b7.jpg\nflower_photos/daisy/5110109540_beed4ed162_m.jpg\nflower_photos/daisy/519880292_7a3a6c6b69.jpg\nflower_photos/daisy/5673728_71b8cb57eb.jpg\nflower_photos/daisy/6299910262_336309ffa5_n.jpg\nflower_photos/daisy/3474942718_c418dae6f1.jpg\nflower_photos/daisy/18711159980_11d3bd5042.jpg\nflower_photos/daisy/1342002397_9503c97b49.jpg\nflower_photos/daisy/2408024540_37f0be7cc0_n.jpg\nflower_photos/daisy/3699235066_fc09a02dfe_m.jpg\nflower_photos/daisy/3627678863_557552c879_m.jpg\nflower_photos/daisy/8887005939_b19e8305ee.jpg\nflower_photos/daisy/1314069875_da8dc023c6_m.jpg\nflower_photos/daisy/3704306975_75b74497d8.jpg\nflower_photos/daisy/4065883015_4bb6010cb7_n.jpg\nflower_photos/daisy/2877860110_a842f8b14a_m.jpg\nflower_photos/daisy/5794839_200acd910c_n.jpg\nflower_photos/daisy/7377004908_5bc0cde347_n.jpg\nflower_photos/daisy/9350942387_5b1d043c26_n.jpg\nflower_photos/daisy/3625257860_33efeef614_m.jpg\nflower_photos/daisy/16492248512_61a57dfec1_m.jpg\nflower_photos/daisy/2666572212_2caca8de9f_n.jpg\nflower_photos/daisy/1374193928_a52320eafa.jpg\nflower_photos/daisy/8694909523_3ca25d449d_n.jpg\nflower_photos/daisy/16161045294_70c76ce846_n.jpg\nflower_photos/daisy/7358085448_b317d11cd5.jpg\nflower_photos/daisy/5794835_d15905c7c8_n.jpg\nflower_photos/daisy/6207492986_0ff91f3296.jpg\nflower_photos/daisy/16401288243_36112bd52f_m.jpg\nflower_photos/daisy/4858518329_7563eb0baa_m.jpg\nflower_photos/daisy/5700781400_65761f3fce.jpg\nflower_photos/daisy/2536529152_33ef3ee078_n.jpg\nflower_photos/daisy/5883162120_dc7274af76_n.jpg\nflower_photos/daisy/17027891179_3edc08f4f6.jpg\nflower_photos/daisy/3310644753_5607eb96a4_m.jpg\nflower_photos/daisy/2480569557_f4e1f0dcb8_n.jpg\nflower_photos/daisy/11439894966_dca877f0cd.jpg\nflower_photos/daisy/8645839873_0151fb92bf_n.jpg\nflower_photos/daisy/19813618946_93818db7aa_m.jpg\nflower_photos/daisy/10712722853_5632165b04.jpg\nflower_photos/daisy/5133243796_44de429de5_m.jpg\nflower_photos/daisy/181007802_7cab5ee78e_n.jpg\nflower_photos/daisy/17101762155_2577a28395.jpg\nflower_photos/daisy/9175280426_40ecc395b8_m.jpg\nflower_photos/daisy/5997702776_c7bc37aa6b_n.jpg\nflower_photos/daisy/99306615_739eb94b9e_m.jpg\nflower_photos/daisy/3598615130_578ed30e5f.jpg\nflower_photos/daisy/7790614422_4557928ab9_n.jpg\nflower_photos/daisy/5626895440_97a0ec04c2_n.jpg\nflower_photos/daisy/8681746439_d6beeefbf9.jpg\nflower_photos/daisy/15306268004_4680ba95e1.jpg\nflower_photos/daisy/14221848160_7f0a37c395.jpg\nflower_photos/daisy/1285423653_18926dc2c8_n.jpg\nflower_photos/daisy/1955336401_fbb206d6ef_n.jpg\nflower_photos/daisy/5973491805_556bba93cc.jpg\nflower_photos/daisy/17357636476_1953c07aa4_n.jpg\nflower_photos/daisy/2365428551_39f83f10bf_n.jpg\nflower_photos/daisy/2087343668_ef4fb95787_n.jpg\nflower_photos/daisy/18679421522_3be9879e32.jpg\nflower_photos/daisy/4837182901_69a6cc782b_n.jpg\nflower_photos/daisy/517054463_036db655a1_m.jpg\nflower_photos/daisy/2045022175_ad087f5f60_n.jpg\nflower_photos/daisy/4814515275_6e25a6c18f.jpg\nflower_photos/daisy/8718637649_87a0d85190_n.jpg\nflower_photos/daisy/18400014056_2e4c601ed5.jpg\nflower_photos/daisy/4694730335_2553e77aa5_z.jpg\nflower_photos/daisy/13491959645_2cd9df44d6_n.jpg\nflower_photos/daisy/4333085242_bbeb3e2841_m.jpg\nflower_photos/daisy/4697206799_19dd2a3193_m.jpg\nflower_photos/daisy/2612704455_efce1c2144_m.jpg\nflower_photos/daisy/18684594849_7dd3634f5e_n.jpg\nflower_photos/daisy/9094631844_1a6abca29e.jpg\nflower_photos/daisy/12891819633_e4c82b51e8.jpg\nflower_photos/daisy/7288989324_c25d9febbf.jpg\nflower_photos/daisy/16360180712_b72695928c_n.jpg\nflower_photos/daisy/2901376034_cdb4bac26b_m.jpg\nflower_photos/daisy/391364011_5beaaa1ae2_m.jpg\nflower_photos/daisy/4565255237_9ba29c4d4e_n.jpg\nflower_photos/daisy/4144275653_7c02d47d9b.jpg\nflower_photos/daisy/9593034725_0062f0d24e_n.jpg\nflower_photos/daisy/9922116524_ab4a2533fe_n.jpg\nflower_photos/daisy/8709535323_a6bea3e43f.jpg\nflower_photos/daisy/2520369272_1dcdb5a892_m.jpg\nflower_photos/daisy/7066602021_2647457985_m.jpg\nflower_photos/daisy/286875003_f7c0e1882d.jpg\nflower_photos/daisy/488202750_c420cbce61.jpg\nflower_photos/daisy/2889325612_f2fc403ff0_m.jpg\nflower_photos/daisy/5110110938_9da91455c4_m.jpg\nflower_photos/daisy/15327813273_06cdf42210.jpg\nflower_photos/daisy/2908212142_5437fa67ff_n.jpg\nflower_photos/daisy/3999978867_c67c79597f_m.jpg\nflower_photos/dandelion/\nflower_photos/dandelion/3539077354_c67aa7168d_m.jpg\nflower_photos/dandelion/9029297232_de50698e2f_n.jpg\nflower_photos/dandelion/4275776457_d04b597cfa_n.jpg\nflower_photos/dandelion/19506262462_d0945c14a6.jpg\nflower_photos/dandelion/5644234724_cb0917ee33_m.jpg\nflower_photos/dandelion/14053397367_75cba846eb_n.jpg\nflower_photos/dandelion/7226987694_34552c3115_n.jpg\nflower_photos/dandelion/14728922673_99086a3818_n.jpg\nflower_photos/dandelion/2444241718_3ca53ce921.jpg\nflower_photos/dandelion/14199664556_188b37e51e.jpg\nflower_photos/dandelion/2670304799_a3f2eef516_m.jpg\nflower_photos/dandelion/1193386857_3ae53574f2_m.jpg\nflower_photos/dandelion/4632757134_40156d7d5b.jpg\nflower_photos/dandelion/570127230_ce409f90f8_n.jpg\nflower_photos/dandelion/284497233_c19801752c.jpg\nflower_photos/dandelion/14076873230_d0bd53b220.jpg\nflower_photos/dandelion/5572197407_a0047238a6.jpg\nflower_photos/dandelion/5863928177_8ae1425e76_n.jpg\nflower_photos/dandelion/129019877_8eea2978ca_m.jpg\nflower_photos/dandelion/1426682852_e62169221f_m.jpg\nflower_photos/dandelion/14554897292_b3e30e52f2.jpg\nflower_photos/dandelion/7162551630_3647eb9254.jpg\nflower_photos/dandelion/19437578578_6ab1b3c984.jpg\nflower_photos/dandelion/6994931380_a7588c1192_m.jpg\nflower_photos/dandelion/7062171343_db61c92737_n.jpg\nflower_photos/dandelion/4847150510_7a5db086fa.jpg\nflower_photos/dandelion/7099259755_1c66420206_n.jpg\nflower_photos/dandelion/2335702923_decb9a860b_m.jpg\nflower_photos/dandelion/16242239484_51286673af.jpg\nflower_photos/dandelion/14884028290_a1344eb446.jpg\nflower_photos/dandelion/14457225751_645a3784fd_n.jpg\nflower_photos/dandelion/506660896_c903cca1f0.jpg\nflower_photos/dandelion/7132676187_7a4265b16f_n.jpg\nflower_photos/dandelion/8935477500_89f22cca03_n.jpg\nflower_photos/dandelion/4562516418_8ccb8c103f.jpg\nflower_photos/dandelion/1080179756_5f05350a59.jpg\nflower_photos/dandelion/16970837587_4a9d8500d7.jpg\nflower_photos/dandelion/14065420729_9b388bf7cb_m.jpg\nflower_photos/dandelion/4675287055_5938ed62c4.jpg\nflower_photos/dandelion/7884440256_91c033732d.jpg\nflower_photos/dandelion/3580437733_9ef51f2981_n.jpg\nflower_photos/dandelion/7247192002_39b79998f0_n.jpg\nflower_photos/dandelion/13910677675_4900fa3dbf_n.jpg\nflower_photos/dandelion/4862011506_4faf6d127e_n.jpg\nflower_photos/dandelion/23891393761_155af6402c.jpg\nflower_photos/dandelion/141935731_d26d600f4f_m.jpg\nflower_photos/dandelion/15358221063_2c6e548e84.jpg\nflower_photos/dandelion/3512879565_88dd8fc269_n.jpg\nflower_photos/dandelion/5605093210_5fecb71c61.jpg\nflower_photos/dandelion/3419176626_512811d3ff.jpg\nflower_photos/dandelion/14070457521_8eb41f65fa.jpg\nflower_photos/dandelion/13675534854_03caf51644_m.jpg\nflower_photos/dandelion/5745882709_fb6fc8f02a_n.jpg\nflower_photos/dandelion/17388674711_6dca8a2e8b_n.jpg\nflower_photos/dandelion/6994938270_bf51d0fe63.jpg\nflower_photos/dandelion/7196683612_6c4cf05b24.jpg\nflower_photos/dandelion/16987075_9a690a2183.jpg\nflower_photos/dandelion/6495802659_98b57e0cca_m.jpg\nflower_photos/dandelion/1443259657_2704fab26e_n.jpg\nflower_photos/dandelion/8980273068_cf7e8b880a_n.jpg\nflower_photos/dandelion/2608937632_cfd93bc7cd.jpg\nflower_photos/dandelion/4624036600_11a4744254_n.jpg\nflower_photos/dandelion/674407101_57676c40fb.jpg\nflower_photos/dandelion/3688128868_031e7b53e1_n.jpg\nflower_photos/dandelion/3357432116_b3dce6fed3_n.jpg\nflower_photos/dandelion/19617501581_606be5f716_n.jpg\nflower_photos/dandelion/5110103388_78dc02558e_n.jpg\nflower_photos/dandelion/315645471_dda66c6338_m.jpg\nflower_photos/dandelion/8733226215_161309f8ec.jpg\nflower_photos/dandelion/9965757055_ff01b5ee6f_n.jpg\nflower_photos/dandelion/12094442595_297494dba4_m.jpg\nflower_photos/dandelion/1469549847_eac61a6802.jpg\nflower_photos/dandelion/3149809654_6a4b31314d_n.jpg\nflower_photos/dandelion/3584415133_a4122ab7b9.jpg\nflower_photos/dandelion/16699732794_5bfd639cf8_n.jpg\nflower_photos/dandelion/23659122395_3467d88c02_n.jpg\nflower_photos/dandelion/4654848357_9549351e0b_n.jpg\nflower_photos/dandelion/10443973_aeb97513fc_m.jpg\nflower_photos/dandelion/8223968_6b51555d2f_n.jpg\nflower_photos/dandelion/3530495617_fd84fb321a_m.jpg\nflower_photos/dandelion/2257649769_deaf97e2c9_n.jpg\nflower_photos/dandelion/2620243133_e801981efe_n.jpg\nflower_photos/dandelion/8719388716_1a392c4c0e_n.jpg\nflower_photos/dandelion/2635422362_a1bf641547_m.jpg\nflower_photos/dandelion/8632704230_ccafc5f7e2.jpg\nflower_photos/dandelion/17135145776_4c2ec21b05_m.jpg\nflower_photos/dandelion/8717161615_4c1e403083.jpg\nflower_photos/dandelion/3383422012_6c9d83671f_n.jpg\nflower_photos/dandelion/2502610598_b9f1b55ebd_n.jpg\nflower_photos/dandelion/6994931102_4667c0352e.jpg\nflower_photos/dandelion/7448453384_fb9caaa9af_n.jpg\nflower_photos/dandelion/4568317687_3f89622f76.jpg\nflower_photos/dandelion/17851831751_35b071f4b0.jpg\nflower_photos/dandelion/3398195641_456872b48b_n.jpg\nflower_photos/dandelion/5675705011_82729927ca_n.jpg\nflower_photos/dandelion/160456948_38c3817c6a_m.jpg\nflower_photos/dandelion/16744522344_8d21b1530d_n.jpg\nflower_photos/dandelion/22190242684_8c3300d4e6.jpg\nflower_photos/dandelion/16713229021_bea2533981_n.jpg\nflower_photos/dandelion/14093744313_b66bc95072.jpg\nflower_photos/dandelion/3419172904_7708414ae9_n.jpg\nflower_photos/dandelion/4571681134_b605a61547_n.jpg\nflower_photos/dandelion/8780964418_7a01a7f48a_n.jpg\nflower_photos/dandelion/2535727910_769c020c0d_n.jpg\nflower_photos/dandelion/3612582808_4503fa1f8b_m.jpg\nflower_photos/dandelion/151385301_153eacf6b5_n.jpg\nflower_photos/dandelion/4632863567_5f9af7de97_n.jpg\nflower_photos/dandelion/2637883118_cf6ce37be4_n.jpg\nflower_photos/dandelion/3696596109_4c4419128a_m.jpg\nflower_photos/dandelion/13290033_ebd7c7abba_n.jpg\nflower_photos/dandelion/15549402199_2890918ddb.jpg\nflower_photos/dandelion/2161283279_02ea3ff8d4.jpg\nflower_photos/dandelion/9533964635_f38e6fa3c3.jpg\nflower_photos/dandelion/19622465055_2a62ebd504_m.jpg\nflower_photos/dandelion/4558536575_d43a611bd4_n.jpg\nflower_photos/dandelion/3823142577_dd5acd5ac6_n.jpg\nflower_photos/dandelion/9010116368_2f51f1e086_n.jpg\nflower_photos/dandelion/5716633491_55e6f02645_n.jpg\nflower_photos/dandelion/14404468648_37903d7025_m.jpg\nflower_photos/dandelion/4657801292_73bef15031.jpg\nflower_photos/dandelion/8966818334_483f4489be_n.jpg\nflower_photos/dandelion/468749497_951c571eff_n.jpg\nflower_photos/dandelion/17276354745_2e312a72b5_n.jpg\nflower_photos/dandelion/10777398353_5a20bb218c.jpg\nflower_photos/dandelion/8220011556_28e0cab67f.jpg\nflower_photos/dandelion/5647842237_b1c5196718_n.jpg\nflower_photos/dandelion/2503875867_2075a9225d_m.jpg\nflower_photos/dandelion/4601270210_60136f2b87_n.jpg\nflower_photos/dandelion/2497301920_91490c42c0.jpg\nflower_photos/dandelion/13651218133_b6eb8e7ed2_m.jpg\nflower_photos/dandelion/480621885_4c8b50fa11_m.jpg\nflower_photos/dandelion/4514343281_26781484df.jpg\nflower_photos/dandelion/14829055_2a2e646a8f_m.jpg\nflower_photos/dandelion/5629940298_634f35125c.jpg\nflower_photos/dandelion/3501368412_358e144d1f.jpg\nflower_photos/dandelion/9853885425_4a82356f1d_m.jpg\nflower_photos/dandelion/8689302980_9bd2f7b9fe_n.jpg\nflower_photos/dandelion/4265711814_9a006ee5b8.jpg\nflower_photos/dandelion/8747223572_dcd9601e99.jpg\nflower_photos/dandelion/3998275481_651205e02d.jpg\nflower_photos/dandelion/2693136371_dde2570813.jpg\nflower_photos/dandelion/4336536446_e635f48f2e.jpg\nflower_photos/dandelion/7004645518_ff0f862eff_n.jpg\nflower_photos/dandelion/8716513637_2ba0c4e6cd_n.jpg\nflower_photos/dandelion/19064700925_b93d474e37.jpg\nflower_photos/dandelion/7355522_b66e5d3078_m.jpg\nflower_photos/dandelion/4598938531_9749b3b56a.jpg\nflower_photos/dandelion/8724252904_db9a5104df_m.jpg\nflower_photos/dandelion/15987457_49dc11bf4b.jpg\nflower_photos/dandelion/16237158409_01913cf918_n.jpg\nflower_photos/dandelion/3509307596_6cfe97867d_n.jpg\nflower_photos/dandelion/7367491658_9eb4dc2384_m.jpg\nflower_photos/dandelion/463736819_f779800165.jpg\nflower_photos/dandelion/17243540220_65b98eb926_n.jpg\nflower_photos/dandelion/3505026222_c760df0035_n.jpg\nflower_photos/dandelion/2019520447_48b2354a20_m.jpg\nflower_photos/dandelion/3483575184_cb8d16a083_n.jpg\nflower_photos/dandelion/2553703483_558d12668c_n.jpg\nflower_photos/dandelion/18243351371_5fda92ac0a_n.jpg\nflower_photos/dandelion/8717157979_05cbc10cc1.jpg\nflower_photos/dandelion/7141019507_4a44c6e888_m.jpg\nflower_photos/dandelion/14335561523_f847f2f4f1.jpg\nflower_photos/dandelion/2938040169_eb38581359.jpg\nflower_photos/dandelion/2392273474_a64cef0eaf_n.jpg\nflower_photos/dandelion/22785985545_95464115b0_m.jpg\nflower_photos/dandelion/7179487220_56e4725195_m.jpg\nflower_photos/dandelion/7280227122_7ea2bef7f4_n.jpg\nflower_photos/dandelion/3675486971_d4c8683b54_n.jpg\nflower_photos/dandelion/62293290_2c463891ff_m.jpg\nflower_photos/dandelion/2622697182_ea4aff29dd_n.jpg\nflower_photos/dandelion/8754822932_948afc7cef.jpg\nflower_photos/dandelion/8194560480_bfc1fb5801.jpg\nflower_photos/dandelion/9613826015_f345354874.jpg\nflower_photos/dandelion/2535769822_513be6bbe9.jpg\nflower_photos/dandelion/18970601002_d70bc883a9.jpg\nflower_photos/dandelion/17344936845_fec4d626b7.jpg\nflower_photos/dandelion/14164392167_650946a169_n.jpg\nflower_photos/dandelion/5744236092_de84b4e38d_n.jpg\nflower_photos/dandelion/5670543216_8c4cb0caa8_m.jpg\nflower_photos/dandelion/2039797043_d5b709f275_n.jpg\nflower_photos/dandelion/2698102820_f15445a3f7.jpg\nflower_photos/dandelion/4552591312_02fe1dcc04_n.jpg\nflower_photos/dandelion/8979062599_86cac547b8.jpg\nflower_photos/dandelion/4893356345_24d67eff9f_m.jpg\nflower_photos/dandelion/486234138_688e01aa9b_n.jpg\nflower_photos/dandelion/14886860069_b84665a073.jpg\nflower_photos/dandelion/4638438929_2ec76083c8_m.jpg\nflower_photos/dandelion/3419166382_a5e4b8fe6d_m.jpg\nflower_photos/dandelion/5757012454_c37f305b73.jpg\nflower_photos/dandelion/15821571649_06c4b9a868_n.jpg\nflower_photos/dandelion/6229634119_af5fec0a22.jpg\nflower_photos/dandelion/4530848609_02a1d9b791.jpg\nflower_photos/dandelion/6060576850_984176cf4f_n.jpg\nflower_photos/dandelion/2469856983_fe8e36ba57.jpg\nflower_photos/dandelion/18276105805_d31d3f7e71.jpg\nflower_photos/dandelion/13897156242_dca5d93075_m.jpg\nflower_photos/dandelion/11545123_50a340b473_m.jpg\nflower_photos/dandelion/18089878729_907ed2c7cd_m.jpg\nflower_photos/dandelion/2470874500_43d8011e75.jpg\nflower_photos/dandelion/5715788902_9dd2b4ef1d.jpg\nflower_photos/dandelion/17075803866_aeeded2637.jpg\nflower_photos/dandelion/13887066460_64156a9021.jpg\nflower_photos/dandelion/8058286066_acdf082487_n.jpg\nflower_photos/dandelion/7249354462_21925f7d95_n.jpg\nflower_photos/dandelion/146242691_44d9c9d6ce_n.jpg\nflower_photos/dandelion/2512148749_261fa9d156.jpg\nflower_photos/dandelion/14455605089_8bbfb41cd7_n.jpg\nflower_photos/dandelion/8475758_4c861ab268_m.jpg\nflower_photos/dandelion/2473862606_291ae74885.jpg\nflower_photos/dandelion/2330339852_fbbdeb7306_n.jpg\nflower_photos/dandelion/9293460423_7fbb1e3c32_n.jpg\nflower_photos/dandelion/3662701865_3ff283a33a_n.jpg\nflower_photos/dandelion/8797114213_103535743c_m.jpg\nflower_photos/dandelion/19600096066_67dc941042.jpg\nflower_photos/dandelion/4844697927_c70d644f40_n.jpg\nflower_photos/dandelion/2540640433_dedd577263.jpg\nflower_photos/dandelion/4721773235_429acdf496_n.jpg\nflower_photos/dandelion/4716316039_044e4d2d1a.jpg\nflower_photos/dandelion/3591588855_b4fd53b000.jpg\nflower_photos/dandelion/5217892384_3edce91761_m.jpg\nflower_photos/dandelion/808239968_318722e4db.jpg\nflower_photos/dandelion/6035460327_4bbb708eab_n.jpg\nflower_photos/dandelion/5768217474_f6b1eef6d5_n.jpg\nflower_photos/dandelion/5600240736_4a90c10579_n.jpg\nflower_photos/dandelion/19961979110_fcd8092388_m.jpg\nflower_photos/dandelion/9301891790_971dcfb35d_m.jpg\nflower_photos/dandelion/14283011_3e7452c5b2_n.jpg\nflower_photos/dandelion/2600382379_5791b0b35a_m.jpg\nflower_photos/dandelion/146023167_f905574d97_m.jpg\nflower_photos/dandelion/2133943140_9fc7bcc9aa.jpg\nflower_photos/dandelion/18001393975_2a6acaabd8.jpg\nflower_photos/dandelion/4575406391_7a62c5f90f_n.jpg\nflower_photos/dandelion/10617162044_8740d4dd9f_n.jpg\nflower_photos/dandelion/19617643201_9922eec796.jpg\nflower_photos/dandelion/7184780734_3baab127c2_m.jpg\nflower_photos/dandelion/22679060358_561ec823ae_m.jpg\nflower_photos/dandelion/7196409186_a59957ce0b_m.jpg\nflower_photos/dandelion/19440910519_cb1162470e.jpg\nflower_photos/dandelion/5996421299_b9bf488c1a_n.jpg\nflower_photos/dandelion/8719032054_9a3ce4f0ff.jpg\nflower_photos/dandelion/14376454225_a1de336c5b.jpg\nflower_photos/dandelion/17175932454_c052e205c1_n.jpg\nflower_photos/dandelion/5598591979_ed9af1b3e9_n.jpg\nflower_photos/dandelion/14070463051_86ab57ab36.jpg\nflower_photos/dandelion/18996965033_1d92e5c99e.jpg\nflower_photos/dandelion/15381511376_fd743b7330_n.jpg\nflower_photos/dandelion/8735646181_fa9787d4e0.jpg\nflower_photos/dandelion/4691257171_23a29aaa33_n.jpg\nflower_photos/dandelion/4489359360_09db62f825.jpg\nflower_photos/dandelion/22274701614_901606ee34_n.jpg\nflower_photos/dandelion/18204150090_fb418bbddb.jpg\nflower_photos/dandelion/130733200_fbe28eea19.jpg\nflower_photos/dandelion/7843447416_847e6ba7f4_m.jpg\nflower_photos/dandelion/8757650550_113d7af3bd.jpg\nflower_photos/dandelion/17367866236_61abd4d243_n.jpg\nflower_photos/dandelion/3998927705_af499a4f29.jpg\nflower_photos/dandelion/458011386_ec89115a19.jpg\nflower_photos/dandelion/10919961_0af657c4e8.jpg\nflower_photos/dandelion/4645161319_c308fc31ef_n.jpg\nflower_photos/dandelion/5676682203_70d797f760.jpg\nflower_photos/dandelion/4574451859_432c856b6e_n.jpg\nflower_photos/dandelion/13900486390_5a25785645_n.jpg\nflower_photos/dandelion/4574736702_b15ecf97d0_m.jpg\nflower_photos/dandelion/19812060274_c432f603db.jpg\nflower_photos/dandelion/2596413098_7ef69b7e1d_m.jpg\nflower_photos/dandelion/20754920332_53b995fc63_n.jpg\nflower_photos/dandelion/5628515159_6b437ff1e5_n.jpg\nflower_photos/dandelion/7015947703_11b30c20c9_n.jpg\nflower_photos/dandelion/16650892835_9228a3ef67_m.jpg\nflower_photos/dandelion/8980164828_04fbf64f79_n.jpg\nflower_photos/dandelion/3476980444_c276bea402_m.jpg\nflower_photos/dandelion/2465573725_d78caca9d4_n.jpg\nflower_photos/dandelion/11465213433_847c4fa261.jpg\nflower_photos/dandelion/3517492544_0fd3ed6a66_m.jpg\nflower_photos/dandelion/2462476884_58c617b26a.jpg\nflower_photos/dandelion/15268682367_5a4512b29f_m.jpg\nflower_photos/dandelion/2512977446_ac498955ee.jpg\nflower_photos/dandelion/3580443099_9a6902ebd8_n.jpg\nflower_photos/dandelion/15005530987_e13b328047_n.jpg\nflower_photos/dandelion/8956863946_f96be02aae_n.jpg\nflower_photos/dandelion/17244252705_328e0bcda6.jpg\nflower_photos/dandelion/14396023703_11c5dd35a9.jpg\nflower_photos/dandelion/17466568484_9128287148.jpg\nflower_photos/dandelion/7425858848_d04dab08dd_n.jpg\nflower_photos/dandelion/2449852402_45d12b9875_n.jpg\nflower_photos/dandelion/8168031302_6e36f39d87.jpg\nflower_photos/dandelion/19004688463_12a8423109.jpg\nflower_photos/dandelion/16863587471_cc3a6ffb29_m.jpg\nflower_photos/dandelion/16041975_2f6c1596e5.jpg\nflower_photos/dandelion/4574447682_40dce530f1.jpg\nflower_photos/dandelion/18999743619_cec3f39bee.jpg\nflower_photos/dandelion/2495749544_679dc7ccef.jpg\nflower_photos/dandelion/2319777940_0cc5476b0d_n.jpg\nflower_photos/dandelion/13968424321_1d89b33a9f_n.jpg\nflower_photos/dandelion/854593001_c57939125f_n.jpg\nflower_photos/dandelion/4574737576_044403a997_n.jpg\nflower_photos/dandelion/16510864164_3afa8ac37f.jpg\nflower_photos/dandelion/148698493_5710e5f472.jpg\nflower_photos/dandelion/5909154147_9da14d1730_n.jpg\nflower_photos/dandelion/18111636378_856027a7b8_n.jpg\nflower_photos/dandelion/5003160931_cf8cbb846f.jpg\nflower_photos/dandelion/14200639491_2a4611916d_n.jpg\nflower_photos/dandelion/14128835667_b6a916222c.jpg\nflower_photos/dandelion/21523597492_39b6765cd7_m.jpg\nflower_photos/dandelion/7280222348_a87725ca77.jpg\nflower_photos/dandelion/17862580326_293070978d_m.jpg\nflower_photos/dandelion/16837594326_1056d875a4_m.jpg\nflower_photos/dandelion/2628514700_b6d5325797_n.jpg\nflower_photos/dandelion/2502613166_2c231b47cb_n.jpg\nflower_photos/dandelion/16096748028_7876887ab2.jpg\nflower_photos/dandelion/8740787470_67230d0609.jpg\nflower_photos/dandelion/6146107825_45f708ecd7_n.jpg\nflower_photos/dandelion/14278605962_d3cce5522f.jpg\nflower_photos/dandelion/19551343814_48f764535f_m.jpg\nflower_photos/dandelion/17649230811_9bdbbacb8c.jpg\nflower_photos/dandelion/6132275522_ce46b33c33_m.jpg\nflower_photos/dandelion/17420983523_2e32d70359.jpg\nflower_photos/dandelion/2597655841_07fb2955a4.jpg\nflower_photos/dandelion/17322195031_c2680809dc_m.jpg\nflower_photos/dandelion/5673112305_02fe19297b_n.jpg\nflower_photos/dandelion/3589816063_50f8de7b64_m.jpg\nflower_photos/dandelion/4574102507_70039c8b28.jpg\nflower_photos/dandelion/17189437699_a9171b6ae3.jpg\nflower_photos/dandelion/1776290427_9d8d5be6ac.jpg\nflower_photos/dandelion/5109501167_2d9bbb0f27_m.jpg\nflower_photos/dandelion/8701999625_8d83138124.jpg\nflower_photos/dandelion/18271576032_d7e2296de4_n.jpg\nflower_photos/dandelion/9152356642_06ae73113f.jpg\nflower_photos/dandelion/2443192475_c64c66d9c2.jpg\nflower_photos/dandelion/4571923094_b9cefa9438_n.jpg\nflower_photos/dandelion/14362539701_cf19e588ca.jpg\nflower_photos/dandelion/23414449869_ee849a80d4.jpg\nflower_photos/dandelion/3954167682_128398bf79_m.jpg\nflower_photos/dandelion/7153497513_076486e26b_n.jpg\nflower_photos/dandelion/8684108_a85764b22d_n.jpg\nflower_photos/dandelion/2517777524_e871ec5291_m.jpg\nflower_photos/dandelion/17047385027_8fd510e164_n.jpg\nflower_photos/dandelion/4254850910_0610224342_n.jpg\nflower_photos/dandelion/11775820493_10fedf4bff_n.jpg\nflower_photos/dandelion/8079778274_f2a400f749_n.jpg\nflower_photos/dandelion/142813254_20a7fd5fb6_n.jpg\nflower_photos/dandelion/7040710179_7f86a17a3c_n.jpg\nflower_photos/dandelion/18183515403_13a9ca6d86_n.jpg\nflower_photos/dandelion/1241011700_261ae180ca.jpg\nflower_photos/dandelion/7132605107_f5e033d725_n.jpg\nflower_photos/dandelion/16949657389_ac0ee80fd1_m.jpg\nflower_photos/dandelion/7132482331_01769e36e9_n.jpg\nflower_photos/dandelion/2494436687_775402e0aa.jpg\nflower_photos/dandelion/5598845098_13e8e9460f.jpg\nflower_photos/dandelion/141652526_2be95f21c3_n.jpg\nflower_photos/dandelion/7808545612_546cfca610_m.jpg\nflower_photos/dandelion/8905148527_ba9f55cd78.jpg\nflower_photos/dandelion/15123503538_8ee984abc6.jpg\nflower_photos/dandelion/20456824132_b1c8fbfa41_m.jpg\nflower_photos/dandelion/9029756865_db8891807a_n.jpg\nflower_photos/dandelion/5726984343_ae124aed97.jpg\nflower_photos/dandelion/18010259565_d6aae33ca7_n.jpg\nflower_photos/dandelion/18995294384_77543e96b6_n.jpg\nflower_photos/dandelion/2477986396_19da36d557_m.jpg\nflower_photos/dandelion/17482158576_86c5ebc2f8.jpg\nflower_photos/dandelion/155646858_9a8b5e8fc8.jpg\nflower_photos/dandelion/2480853696_aacdbb5324.jpg\nflower_photos/dandelion/4633792226_80f89c89ec_m.jpg\nflower_photos/dandelion/6012046444_fd80afb63a_n.jpg\nflower_photos/dandelion/5681951567_d3b03bfd2a_m.jpg\nflower_photos/dandelion/3991962484_085ba2da94.jpg\nflower_photos/dandelion/9617087594_ec2a9b16f6.jpg\nflower_photos/dandelion/461632542_0387557eff.jpg\nflower_photos/dandelion/4155914848_3d57f50fc7.jpg\nflower_photos/dandelion/6104442744_ee2bcd32e7_n.jpg\nflower_photos/dandelion/19397467530_1e8131a7cf.jpg\nflower_photos/dandelion/3198028825_fdfaa1d020.jpg\nflower_photos/dandelion/13471273823_4800ca8eec.jpg\nflower_photos/dandelion/6103898045_e066cdeedf_n.jpg\nflower_photos/dandelion/6208857436_14a65fe4af_n.jpg\nflower_photos/dandelion/19593576916_f5a083d7fe_n.jpg\nflower_photos/dandelion/3475811950_0fb89845f5_n.jpg\nflower_photos/dandelion/7197581386_8a51f1bb12_n.jpg\nflower_photos/dandelion/2470731130_089b8514f6_n.jpg\nflower_photos/dandelion/2753166154_0cb51a127b.jpg\nflower_photos/dandelion/479495978_ee22cf05be.jpg\nflower_photos/dandelion/14019781123_ea0f8722d4_n.jpg\nflower_photos/dandelion/253622055_d72964a7fd_n.jpg\nflower_photos/dandelion/98992760_53ed1d26a9.jpg\nflower_photos/dandelion/13942846777_5571a6b0a1_n.jpg\nflower_photos/dandelion/1798082733_b8080b1173_m.jpg\nflower_photos/dandelion/2387025546_6aecb1b984_n.jpg\nflower_photos/dandelion/3365850019_8158a161a8_n.jpg\nflower_photos/dandelion/2634665077_597910235f_m.jpg\nflower_photos/dandelion/7188112181_571434b058_n.jpg\nflower_photos/dandelion/13920113_f03e867ea7_m.jpg\nflower_photos/dandelion/284497199_93a01f48f6.jpg\nflower_photos/dandelion/515143813_b3afb08bf9.jpg\nflower_photos/dandelion/17821459748_873101edd0_m.jpg\nflower_photos/dandelion/3496258301_ca5f168306.jpg\nflower_photos/dandelion/3464015936_6845f46f64.jpg\nflower_photos/dandelion/477316928_a70a31a704_m.jpg\nflower_photos/dandelion/2457473644_5242844e52_m.jpg\nflower_photos/dandelion/6983105424_f33cc9b08d_m.jpg\nflower_photos/dandelion/17122969189_0ec37cb6c9.jpg\nflower_photos/dandelion/5598014250_684c28bd5c_n.jpg\nflower_photos/dandelion/19599413676_fc9ee2640e.jpg\nflower_photos/dandelion/4610125337_50798408b8_m.jpg\nflower_photos/dandelion/9262004825_710346cde9_n.jpg\nflower_photos/dandelion/13946048982_4e6ec56987.jpg\nflower_photos/dandelion/4622115595_a0de9f2013_n.jpg\nflower_photos/dandelion/7315832212_b0ceeb8de8_n.jpg\nflower_photos/dandelion/4523862714_b41b459c88.jpg\nflower_photos/dandelion/3530500952_9f94fb8b9c_m.jpg\nflower_photos/dandelion/1195255751_d58b3d3076.jpg\nflower_photos/dandelion/8980266062_8387f6cc89.jpg\nflower_photos/dandelion/11595255065_d9550012fc.jpg\nflower_photos/dandelion/2625836599_03e192266f.jpg\nflower_photos/dandelion/2963905796_227d37ff12.jpg\nflower_photos/dandelion/6985099958_5249a4688b.jpg\nflower_photos/dandelion/8687729737_a7fbeded2c_m.jpg\nflower_photos/dandelion/14093789753_f0f1acdb57.jpg\nflower_photos/dandelion/8011324555_375b7b5b0a.jpg\nflower_photos/dandelion/3857059749_fe8ca621a9.jpg\nflower_photos/dandelion/18479635994_83f93f4120.jpg\nflower_photos/dandelion/19437710780_c5f2156438.jpg\nflower_photos/dandelion/19613308325_a67792d889.jpg\nflower_photos/dandelion/19438516548_bbaf350664.jpg\nflower_photos/dandelion/8533312924_ee09412645_n.jpg\nflower_photos/dandelion/4552571121_2677bcdec3.jpg\nflower_photos/dandelion/18482768066_677292a64e.jpg\nflower_photos/dandelion/2326334426_2dc74fceb1.jpg\nflower_photos/dandelion/148180650_19a4b410db.jpg\nflower_photos/dandelion/5642429835_a0cbf1bab7_n.jpg\nflower_photos/dandelion/14202166370_e989588332.jpg\nflower_photos/dandelion/15782158700_3b9bf7d33e_m.jpg\nflower_photos/dandelion/19617425002_b914c1e2ab.jpg\nflower_photos/dandelion/6044710875_0459796d1b_m.jpg\nflower_photos/dandelion/6250363717_17732e992e_n.jpg\nflower_photos/dandelion/19551343954_83bb52f310_m.jpg\nflower_photos/dandelion/142390525_5d81a3659d_m.jpg\nflower_photos/dandelion/4572738670_4787a11058_n.jpg\nflower_photos/dandelion/4151883194_e45505934d_n.jpg\nflower_photos/dandelion/10617191174_9a01753241_n.jpg\nflower_photos/dandelion/6953830582_8525e0423c_n.jpg\nflower_photos/dandelion/5608832856_f5d49de778.jpg\nflower_photos/dandelion/2303491518_f25fee9440.jpg\nflower_photos/dandelion/8727612532_6f3d0904aa_n.jpg\nflower_photos/dandelion/9719816995_8f211abf02_n.jpg\nflower_photos/dandelion/4632761610_768360d425.jpg\nflower_photos/dandelion/2995221296_a6ddaccc39.jpg\nflower_photos/dandelion/7950892504_33142110c2.jpg\nflower_photos/dandelion/6019234426_d25ea1230a_m.jpg\nflower_photos/dandelion/14375349004_68d893254a_n.jpg\nflower_photos/dandelion/4589787911_851cb80157_n.jpg\nflower_photos/dandelion/4082856478_741a411ebb.jpg\nflower_photos/dandelion/14576445793_582aa6446b_m.jpg\nflower_photos/dandelion/5623855601_ecaebdb8fe.jpg\nflower_photos/dandelion/4606893762_c2f26c7e91_n.jpg\nflower_photos/dandelion/4512569988_2b3f802cc6.jpg\nflower_photos/dandelion/2472641499_cbe617a93d.jpg\nflower_photos/dandelion/2116997627_30fed84e53_m.jpg\nflower_photos/dandelion/5110102140_787d325757_n.jpg\nflower_photos/dandelion/14126515096_1134fae695.jpg\nflower_photos/dandelion/13331969914_890082d898_n.jpg\nflower_photos/dandelion/12998979765_3de89e7195_n.jpg\nflower_photos/dandelion/14084345111_8a4cb05a31.jpg\nflower_photos/dandelion/8681169825_19a21c6bf5_m.jpg\nflower_photos/dandelion/2479491210_98e41c4e7d_m.jpg\nflower_photos/dandelion/13560152823_9da5e48c87_m.jpg\nflower_photos/dandelion/14128839257_23def53028.jpg\nflower_photos/dandelion/10828951106_c3cd47983f.jpg\nflower_photos/dandelion/2634666217_d5ef87c9f7_m.jpg\nflower_photos/dandelion/5862288632_1df5eb6dd0.jpg\nflower_photos/dandelion/16716172029_2166d8717f_m.jpg\nflower_photos/dandelion/2697283969_c1f9cbb936.jpg\nflower_photos/dandelion/8842482175_92a14b4934_m.jpg\nflower_photos/dandelion/14368895004_c486a29c1e_n.jpg\nflower_photos/dandelion/14171812905_8b81d50eb9_n.jpg\nflower_photos/dandelion/11768468623_9399b5111b_n.jpg\nflower_photos/dandelion/2476098674_e6f39536f5_n.jpg\nflower_photos/dandelion/5024965767_230f140d60_n.jpg\nflower_photos/dandelion/8980145452_efbd6e3b04.jpg\nflower_photos/dandelion/4944731313_023a0508fd_n.jpg\nflower_photos/dandelion/18282528206_7fb3166041.jpg\nflower_photos/dandelion/4510350093_3700064215.jpg\nflower_photos/dandelion/7243174412_d3628e4cc4_m.jpg\nflower_photos/dandelion/3451079245_2139200d66_n.jpg\nflower_photos/dandelion/3459346147_faffff51c7_n.jpg\nflower_photos/dandelion/3021333497_b927cd8596.jpg\nflower_photos/dandelion/5110104894_a52c685516_n.jpg\nflower_photos/dandelion/344318990_7be3fb0a7d.jpg\nflower_photos/dandelion/2477231067_3aecef1bf8_n.jpg\nflower_photos/dandelion/2521811279_1f7fc353bf_n.jpg\nflower_photos/dandelion/5613466853_e476bb080e.jpg\nflower_photos/dandelion/144040769_c5b805f868.jpg\nflower_photos/dandelion/1074999133_1e4a1e042e.jpg\nflower_photos/dandelion/16159487_3a6615a565_n.jpg\nflower_photos/dandelion/19626311985_58f1a79da3.jpg\nflower_photos/dandelion/2294126841_e478564e77_n.jpg\nflower_photos/dandelion/479115838_0771a6cdff.jpg\nflower_photos/dandelion/5797606814_ccac615312_m.jpg\nflower_photos/dandelion/8681420404_6ae114f036_n.jpg\nflower_photos/dandelion/16817037661_2980d823e1_n.jpg\nflower_photos/dandelion/3730618647_5725c692c3_m.jpg\nflower_photos/dandelion/4676527148_d701b9202f_n.jpg\nflower_photos/dandelion/5607256228_2294c201b3.jpg\nflower_photos/dandelion/7469617666_0e1a014917.jpg\nflower_photos/dandelion/11124381625_24b17662bd_n.jpg\nflower_photos/dandelion/10778387133_9141024b10.jpg\nflower_photos/dandelion/3585220976_5acac92d1c.jpg\nflower_photos/dandelion/4669815582_0a994fb4fd_m.jpg\nflower_photos/dandelion/2598486434_bf349854f2_m.jpg\nflower_photos/dandelion/8978962053_0727b41d26.jpg\nflower_photos/dandelion/8613502159_d9ea67ba63.jpg\nflower_photos/dandelion/8691437509_9ac8441db7_n.jpg\nflower_photos/dandelion/5886830036_2b99899c95.jpg\nflower_photos/dandelion/478851599_25bfd70605_n.jpg\nflower_photos/dandelion/5109496141_8dcf673d43_n.jpg\nflower_photos/dandelion/16462263826_2555edeb74_n.jpg\nflower_photos/dandelion/6918170172_3215766bf4_m.jpg\nflower_photos/dandelion/2389720627_8923180b19.jpg\nflower_photos/dandelion/163702807_e508544acd_n.jpg\nflower_photos/dandelion/14058811536_f29cd7bd58_n.jpg\nflower_photos/dandelion/19613204505_da554eb56a_n.jpg\nflower_photos/dandelion/4632235020_d00ce1e497.jpg\nflower_photos/dandelion/7193058132_36fd883048_m.jpg\nflower_photos/dandelion/9300335851_cdf1cef7a9.jpg\nflower_photos/dandelion/14060367700_fe87e99b6a_m.jpg\nflower_photos/dandelion/17147436650_c94ae24004_n.jpg\nflower_photos/dandelion/151385302_f8980a257f_n.jpg\nflower_photos/dandelion/10779476016_9130714dc0.jpg\nflower_photos/dandelion/18876985840_7531dc8e6a.jpg\nflower_photos/dandelion/8497389500_45636fdd14.jpg\nflower_photos/dandelion/4645101643_9c9d9df13e.jpg\nflower_photos/dandelion/8842317179_d59cf218cb_n.jpg\nflower_photos/dandelion/7270523166_b62fc9e5f1_m.jpg\nflower_photos/dandelion/18687587599_3dd4fdf255.jpg\nflower_photos/dandelion/3487229452_73e3004858.jpg\nflower_photos/dandelion/4669006062_6b3d260037_n.jpg\nflower_photos/dandelion/483097906_2c35054346.jpg\nflower_photos/dandelion/16656127943_2f70926b6c.jpg\nflower_photos/dandelion/921252114_91e334b950.jpg\nflower_photos/dandelion/16953818045_fea21c8bf8.jpg\nflower_photos/dandelion/14292205986_da230467ef.jpg\nflower_photos/dandelion/5749815755_12f9214649_n.jpg\nflower_photos/dandelion/5772194932_60b833091f.jpg\nflower_photos/dandelion/3472437817_7902b3d984_n.jpg\nflower_photos/dandelion/4573204407_babff0dce4_n.jpg\nflower_photos/dandelion/13734221225_0e04edc6b6.jpg\nflower_photos/dandelion/4134441089_c8c1e6132a.jpg\nflower_photos/dandelion/425800274_27dba84fac_n.jpg\nflower_photos/dandelion/19443674130_08db1d9578_m.jpg\nflower_photos/dandelion/9517326597_5d116a0166.jpg\nflower_photos/dandelion/5705695593_d79286ac0d.jpg\nflower_photos/dandelion/138132145_782763b84f_m.jpg\nflower_photos/dandelion/18238604119_a5689980ee_n.jpg\nflower_photos/dandelion/19435491090_7af558e17e.jpg\nflower_photos/dandelion/16241101274_334b54731e.jpg\nflower_photos/dandelion/17080000869_a80e767f4a_m.jpg\nflower_photos/dandelion/5725836812_a7d1c5540d_m.jpg\nflower_photos/dandelion/17457028309_95514c8d02_n.jpg\nflower_photos/dandelion/3461986955_29a1abc621.jpg\nflower_photos/dandelion/14313509432_6f2343d6c8_m.jpg\nflower_photos/dandelion/19691175559_ef12b8b354_n.jpg\nflower_photos/dandelion/4556178143_e0d32c0a86_n.jpg\nflower_photos/dandelion/4258272073_f616d1e575_m.jpg\nflower_photos/dandelion/3533167406_e9f4cf10bb_m.jpg\nflower_photos/dandelion/2481428401_bed64dd043.jpg\nflower_photos/dandelion/8952484062_31d1d97e45.jpg\nflower_photos/dandelion/17220096449_0e535989f0_n.jpg\nflower_photos/dandelion/510677438_73e4b91c95_m.jpg\nflower_photos/dandelion/5740633858_8fd54c23c9_n.jpg\nflower_photos/dandelion/2401343175_d2a892cf25_n.jpg\nflower_photos/dandelion/7291185504_b740bbeba4_m.jpg\nflower_photos/dandelion/8267315764_129f2e1d77_m.jpg\nflower_photos/dandelion/2229906591_e953785d13.jpg\nflower_photos/dandelion/5651310874_c8be336c2b.jpg\nflower_photos/dandelion/4258272381_65bd4b8191_m.jpg\nflower_photos/dandelion/2490828907_5094017933_m.jpg\nflower_photos/dandelion/5654859907_c2be3b0f1e_n.jpg\nflower_photos/dandelion/3856725141_0db85f466d_n.jpg\nflower_photos/dandelion/578938011_34918b1468.jpg\nflower_photos/dandelion/4510938552_6f7bae172a_n.jpg\nflower_photos/dandelion/8981659922_7b1be892e7_m.jpg\nflower_photos/dandelion/8748402330_c00f9fbf7f_n.jpg\nflower_photos/dandelion/7267547016_c8903920bf.jpg\nflower_photos/dandelion/7808430998_31ba639031_n.jpg\nflower_photos/dandelion/14048849371_ec9dbafaeb_m.jpg\nflower_photos/dandelion/5776879272_95008399c3.jpg\nflower_photos/dandelion/14761980161_2d6dbaa4bb_m.jpg\nflower_photos/dandelion/18304194360_2a4a0be631_m.jpg\nflower_photos/dandelion/3005677730_2662753d3f_m.jpg\nflower_photos/dandelion/2516714633_87f28f0314.jpg\nflower_photos/dandelion/17747738311_5014b1f77f.jpg\nflower_photos/dandelion/3664916269_29f07c7c7b.jpg\nflower_photos/dandelion/17388697431_0d84c427d1_n.jpg\nflower_photos/dandelion/14845607659_1be18c5d7f.jpg\nflower_photos/dandelion/2518321294_dde5aa7c20_m.jpg\nflower_photos/dandelion/17029965300_8e755c2214_n.jpg\nflower_photos/dandelion/3458770076_17ed3a1225.jpg\nflower_photos/dandelion/4278757393_bca8415ed4_n.jpg\nflower_photos/dandelion/14306875733_61d71c64c0_n.jpg\nflower_photos/dandelion/8083321316_f62ea76f72_n.jpg\nflower_photos/dandelion/4633323785_20676ff914_m.jpg\nflower_photos/dandelion/3533075436_0954145b9f_m.jpg\nflower_photos/dandelion/16375088191_2bf2916b53.jpg\nflower_photos/dandelion/7116950607_49b19102ba_n.jpg\nflower_photos/dandelion/3584414925_1e6c4b61db_n.jpg\nflower_photos/dandelion/21657726011_2c94e341bc_n.jpg\nflower_photos/dandelion/15819121091_26a5243340_n.jpg\nflower_photos/dandelion/17280886635_e384d91300_n.jpg\nflower_photos/dandelion/19426575569_4b53c0b726.jpg\nflower_photos/dandelion/494108764_e00178af6e.jpg\nflower_photos/dandelion/13967344688_aa629dcdee_n.jpg\nflower_photos/dandelion/7719263062_3c8a307a5d.jpg\nflower_photos/dandelion/17574213074_f5416afd84.jpg\nflower_photos/dandelion/4588529727_4a79c61577.jpg\nflower_photos/dandelion/16691236594_4287cea9d6_n.jpg\nflower_photos/dandelion/10200780773_c6051a7d71_n.jpg\nflower_photos/dandelion/16495282564_d8c34d6a2e_m.jpg\nflower_photos/dandelion/138166590_47c6cb9dd0.jpg\nflower_photos/dandelion/4632251871_9f324a7bb5.jpg\nflower_photos/dandelion/6897671808_57230e04c5_n.jpg\nflower_photos/dandelion/140951103_69847c0b7c.jpg\nflower_photos/dandelion/8475769_3dea463364_m.jpg\nflower_photos/dandelion/7132677385_bcbdcc6001.jpg\nflower_photos/dandelion/8738317694_eca2ce3bfc_n.jpg\nflower_photos/dandelion/8647874151_aac8db2588_m.jpg\nflower_photos/dandelion/7232035352_84a39e99ba_n.jpg\nflower_photos/dandelion/177851662_b2622b4238_n.jpg\nflower_photos/dandelion/8723679596_391a724d4f_m.jpg\nflower_photos/dandelion/4634716478_1cbcbee7ca.jpg\nflower_photos/dandelion/2489438981_4eb60ef98f_m.jpg\nflower_photos/dandelion/2780702427_312333ef33.jpg\nflower_photos/dandelion/2542908888_25a1c78ff0.jpg\nflower_photos/dandelion/19526570282_1d1e71b0f3_m.jpg\nflower_photos/dandelion/22196426956_eca94f6faa_m.jpg\nflower_photos/dandelion/5875763050_82f32f2eed_m.jpg\nflower_photos/dandelion/17047231499_bd66c23641.jpg\nflower_photos/dandelion/13652698934_d258a6ee8c.jpg\nflower_photos/dandelion/1413979148_b40d63db90_m.jpg\nflower_photos/dandelion/4550784336_584d7a65de_m.jpg\nflower_photos/dandelion/10043234166_e6dd915111_n.jpg\nflower_photos/dandelion/5416388641_c66d52d2ff_m.jpg\nflower_photos/dandelion/14805304536_c321a7b061_n.jpg\nflower_photos/dandelion/17346385582_7ba433dbbe.jpg\nflower_photos/dandelion/3554992110_81d8c9b0bd_m.jpg\nflower_photos/dandelion/149782934_21adaf4a21.jpg\nflower_photos/dandelion/4858372040_52216eb0bd.jpg\nflower_photos/dandelion/3451646670_3eff7094b7_n.jpg\nflower_photos/dandelion/3418355347_2bdcca592a.jpg\nflower_photos/dandelion/19440660848_c789227129_m.jpg\nflower_photos/dandelion/3502447188_ab4a5055ac_m.jpg\nflower_photos/dandelion/4226758402_a1b75ce3ac_n.jpg\nflower_photos/dandelion/8720503800_cab5c62a34.jpg\nflower_photos/dandelion/7262863194_682209e9fb.jpg\nflower_photos/dandelion/14886963928_d4856f1eb6_n.jpg\nflower_photos/dandelion/6994933428_307b092ce7_m.jpg\nflower_photos/dandelion/5446666484_365f3be83a_n.jpg\nflower_photos/dandelion/4558562689_c8e2ab9f10.jpg\nflower_photos/dandelion/80846315_d997645bea_n.jpg\nflower_photos/dandelion/3562861685_8b8d747b4d.jpg\nflower_photos/dandelion/4560663938_3557a1f831.jpg\nflower_photos/dandelion/645330051_06b192b7e1.jpg\nflower_photos/dandelion/14648777167_1d92d403c9_n.jpg\nflower_photos/dandelion/4696437766_85952d0196.jpg\nflower_photos/dandelion/6983120596_8b9f084ac2_n.jpg\nflower_photos/dandelion/751941983_58e1ae3957_m.jpg\nflower_photos/dandelion/4684022752_89631bd98e_n.jpg\nflower_photos/dandelion/19602790836_912d38aaa8.jpg\nflower_photos/dandelion/7222962522_36952a67b6_n.jpg\nflower_photos/dandelion/6994925894_030e157fe0.jpg\nflower_photos/dandelion/4635296297_9ce69e4a6e.jpg\nflower_photos/dandelion/1297972485_33266a18d9.jpg\nflower_photos/dandelion/14439618952_470224b89b_n.jpg\nflower_photos/dandelion/8744249948_36cb1969f8_m.jpg\nflower_photos/dandelion/8791577794_7573712cb4_n.jpg\nflower_photos/dandelion/15219268336_f2460fca88_m.jpg\nflower_photos/dandelion/2674176237_e265ea64cc_n.jpg\nflower_photos/dandelion/7368435774_0045b9dc4e.jpg\nflower_photos/dandelion/16766166609_ccb8344c9f_m.jpg\nflower_photos/dandelion/4713958242_fbcfe9a61b_m.jpg\nflower_photos/dandelion/5655177340_78fc36ce59_m.jpg\nflower_photos/dandelion/3761310831_41b5eba622_n.jpg\nflower_photos/dandelion/4708723476_a1b476a373.jpg\nflower_photos/dandelion/15297244181_011883a631_m.jpg\nflower_photos/dandelion/17146644679_11aff3045c.jpg\nflower_photos/dandelion/61242541_a04395e6bc.jpg\nflower_photos/dandelion/4496277750_8c34256e28.jpg\nflower_photos/dandelion/7368449232_c99f49b2e6_n.jpg\nflower_photos/dandelion/13916196427_50a611008f.jpg\nflower_photos/dandelion/176284193_8fa1710431_m.jpg\nflower_photos/dandelion/2538797744_deb53ac253.jpg\nflower_photos/dandelion/8181477_8cb77d2e0f_n.jpg\nflower_photos/dandelion/18243329421_771b4d938e.jpg\nflower_photos/dandelion/3491333876_e3fed43c0d.jpg\nflower_photos/dandelion/8756906129_b05a1b26f2.jpg\nflower_photos/dandelion/3393060921_2328b752f4.jpg\nflower_photos/dandelion/8689302100_be76a16ccc_n.jpg\nflower_photos/dandelion/7950901292_2dea05f9a2_n.jpg\nflower_photos/dandelion/8981828144_4b66b4edb6_n.jpg\nflower_photos/dandelion/3451637528_b245144675_n.jpg\nflower_photos/dandelion/19067907051_16d530c7d2.jpg\nflower_photos/dandelion/7243478942_30bf542a2d_m.jpg\nflower_photos/dandelion/18232119726_cef27eaaac_n.jpg\nflower_photos/dandelion/6400843175_ef07053f8f_m.jpg\nflower_photos/dandelion/2076141453_c63801962a_m.jpg\nflower_photos/dandelion/2522454811_f87af57d8b.jpg\nflower_photos/dandelion/145173479_7d04346c20.jpg\nflower_photos/dandelion/136011860_44ca0b2835_n.jpg\nflower_photos/dandelion/9200211647_be34ce978b.jpg\nflower_photos/dandelion/15139657325_74031c44fc.jpg\nflower_photos/dandelion/9111669902_9471c3a49c_n.jpg\nflower_photos/dandelion/18889216716_cd67aec890_n.jpg\nflower_photos/dandelion/6972675188_37f1f1d6f6.jpg\nflower_photos/dandelion/9726260379_4e8ee66875_m.jpg\nflower_photos/dandelion/8737699225_19e0c9f0fa_m.jpg\nflower_photos/dandelion/10486992895_20b344ce2d_n.jpg\nflower_photos/dandelion/21195621914_a5bdbb203d.jpg\nflower_photos/dandelion/1273326361_b90ea56d0d_m.jpg\nflower_photos/dandelion/8969938579_4c2032dd96_n.jpg\nflower_photos/dandelion/2661585172_94707236be_m.jpg\nflower_photos/dandelion/14373114081_7922bcf765_n.jpg\nflower_photos/dandelion/5643666851_dc3f42399d_m.jpg\nflower_photos/dandelion/15547944931_c1e095b185.jpg\nflower_photos/dandelion/6968202872_cfcb5b77fb.jpg\nflower_photos/dandelion/4500964841_b1142b50fb_n.jpg\nflower_photos/dandelion/4629844753_4e02015d29_m.jpg\nflower_photos/dandelion/9011235009_58c7b244c1_n.jpg\nflower_photos/dandelion/11296320473_1d9261ddcb.jpg\nflower_photos/dandelion/6954604340_d3223ed296_m.jpg\nflower_photos/dandelion/493696003_f93ffb3abd_n.jpg\nflower_photos/dandelion/17020815734_81e8db8008_m.jpg\nflower_photos/dandelion/3499837275_5f24d2f8bf_n.jpg\nflower_photos/dandelion/6901435398_b3192ff7f8_m.jpg\nflower_photos/dandelion/2395009660_295c8ffd67_m.jpg\nflower_photos/dandelion/7295618968_c08a326cc1_m.jpg\nflower_photos/dandelion/8270191872_61e47ae3b8_m.jpg\nflower_photos/dandelion/144686365_d7e96941ee_n.jpg\nflower_photos/dandelion/5623492051_8e5ce438bd.jpg\nflower_photos/dandelion/18996760154_58d3c48604.jpg\nflower_photos/dandelion/18803577858_fd0036e1f5_m.jpg\nflower_photos/dandelion/18996957833_0bd71fbbd4_m.jpg\nflower_photos/dandelion/4573886524_5161482ca7_n.jpg\nflower_photos/dandelion/14185089716_2a48298d17.jpg\nflower_photos/dandelion/9188647508_3b56e62f69.jpg\nflower_photos/dandelion/151979452_9832f08b69.jpg\nflower_photos/dandelion/3454102259_957ecd0a9b.jpg\nflower_photos/dandelion/2330343016_23acc484ee.jpg\nflower_photos/dandelion/8759118120_9eac064e38_n.jpg\nflower_photos/dandelion/8935456132_8dc4d3b679_n.jpg\nflower_photos/dandelion/4560613196_91a04f8dcf_m.jpg\nflower_photos/dandelion/4528742654_99d233223b_m.jpg\nflower_photos/dandelion/15002906952_cab2cb29cf.jpg\nflower_photos/dandelion/3554435478_1a7ab743e9_n.jpg\nflower_photos/dandelion/477207005_6327db8393_m.jpg\nflower_photos/dandelion/3581252194_8c976d333a_n.jpg\nflower_photos/dandelion/20983660733_06b35b9eb8.jpg\nflower_photos/dandelion/5607669502_ccd2a76668_n.jpg\nflower_photos/dandelion/7148085703_b9e8bcd6ca_n.jpg\nflower_photos/dandelion/9595369280_dd88b61814.jpg\nflower_photos/dandelion/5033866477_a77cccba49_m.jpg\nflower_photos/dandelion/14002252932_64d5cbdac7.jpg\nflower_photos/dandelion/7465850028_cdfaae235a_n.jpg\nflower_photos/dandelion/17903104293_9138439e76.jpg\nflower_photos/dandelion/14914603395_b271ffab56_n.jpg\nflower_photos/dandelion/510897767_918260db93.jpg\nflower_photos/dandelion/7280221020_98b473b20d_n.jpg\nflower_photos/dandelion/8880158802_6e10a452c7_m.jpg\nflower_photos/dandelion/4953240903_a121fba81f_m.jpg\nflower_photos/dandelion/5628296138_9031791fab.jpg\nflower_photos/dandelion/14021281124_89cc388eac_n.jpg\nflower_photos/dandelion/1353279846_7e6b87606d.jpg\nflower_photos/dandelion/3446018470_0c40e73ed6_m.jpg\nflower_photos/dandelion/20165867412_fc45d31698_m.jpg\nflower_photos/dandelion/1667963621_c76d570af3_n.jpg\nflower_photos/dandelion/8681388520_c697dee897_n.jpg\nflower_photos/dandelion/8929523512_c87897b84e.jpg\nflower_photos/dandelion/9818247_e2eac18894.jpg\nflower_photos/dandelion/10294487385_92a0676c7d_m.jpg\nflower_photos/dandelion/7218569994_de7045c0c0.jpg\nflower_photos/dandelion/5829610661_8439ba4a77_n.jpg\nflower_photos/dandelion/19443726008_8c9c68efa7_m.jpg\nflower_photos/dandelion/9759608055_9ab623d193.jpg\nflower_photos/dandelion/2831102668_eb65cd40b9_n.jpg\nflower_photos/dandelion/14003401241_543535b385.jpg\nflower_photos/dandelion/141340262_ca2e576490.jpg\nflower_photos/dandelion/8327657321_2cbceec396_n.jpg\nflower_photos/dandelion/2503034372_db7867de51_m.jpg\nflower_photos/dandelion/4164845062_1fd9b3f3b4.jpg\nflower_photos/dandelion/10683189_bd6e371b97.jpg\nflower_photos/dandelion/10477378514_9ffbcec4cf_m.jpg\nflower_photos/dandelion/7164500544_332b75aa3b.jpg\nflower_photos/dandelion/2462379970_6bd5560f4c_m.jpg\nflower_photos/dandelion/4633514720_22e82c5f7c_m.jpg\nflower_photos/dandelion/451965300_619b781dc9_m.jpg\nflower_photos/dandelion/501987276_744448580c_m.jpg\nflower_photos/dandelion/4573204385_9b71e96b35_m.jpg\nflower_photos/dandelion/14469481104_d0e29f7ffd.jpg\nflower_photos/dandelion/151861297_55b10a03a6_n.jpg\nflower_photos/dandelion/7401173270_ebaf04c9b0_n.jpg\nflower_photos/dandelion/2683330456_0f7bbce110_m.jpg\nflower_photos/dandelion/8749577087_dc2521615f_n.jpg\nflower_photos/dandelion/19621170705_30bf8bf0ba.jpg\nflower_photos/dandelion/8663932737_0a603ab718_n.jpg\nflower_photos/dandelion/14053173516_a00150a919_m.jpg\nflower_photos/dandelion/3476759348_a0d34a4b59_n.jpg\nflower_photos/dandelion/459633569_5ddf6bc116_m.jpg\nflower_photos/dandelion/8717787983_c83bdf39fe_n.jpg\nflower_photos/dandelion/8376558865_19c5cd6fd6_n.jpg\nflower_photos/dandelion/7998106328_c3953f70e9_n.jpg\nflower_photos/dandelion/8831808134_315aedb37b.jpg\nflower_photos/dandelion/13386618495_3df1f1330d.jpg\nflower_photos/dandelion/4714026966_93846ddb74_m.jpg\nflower_photos/dandelion/459748276_69101b0cec_n.jpg\nflower_photos/dandelion/5727534342_419604c177_n.jpg\nflower_photos/dandelion/139124974_9e3ba69f6c.jpg\nflower_photos/dandelion/8209318399_ae72aefdb5.jpg\nflower_photos/dandelion/5045509402_6e052ce443.jpg\nflower_photos/dandelion/4557781241_0060cbe723_n.jpg\nflower_photos/dandelion/5767676943_4f9c7323f3_n.jpg\nflower_photos/dandelion/2465442759_d4532a57a3.jpg\nflower_photos/dandelion/10437652486_aa86c14985.jpg\nflower_photos/dandelion/9939430464_5f5861ebab.jpg\nflower_photos/dandelion/5607983792_f8b8766ff7.jpg\nflower_photos/dandelion/3465599902_14729e2b1b_n.jpg\nflower_photos/dandelion/3393564906_f2df184b76_n.jpg\nflower_photos/dandelion/1128626197_3f52424215_n.jpg\nflower_photos/dandelion/2453532367_fc373df4de.jpg\nflower_photos/dandelion/13807932364_673b7f1c1c_n.jpg\nflower_photos/dandelion/6983113346_21551e1b52_n.jpg\nflower_photos/dandelion/3518608454_c3fd3c311c_m.jpg\nflower_photos/dandelion/7308600792_27cff2f73f.jpg\nflower_photos/dandelion/14740350060_a489d9fa06.jpg\nflower_photos/dandelion/510874382_f7e3435043.jpg\nflower_photos/dandelion/8642679391_0805b147cb_m.jpg\nflower_photos/dandelion/15378782362_4161b23af7_m.jpg\nflower_photos/dandelion/2467980325_237b14c737_m.jpg\nflower_photos/dandelion/4523239455_9c31a06aaf_n.jpg\nflower_photos/dandelion/17619402434_15b2ec2d79.jpg\nflower_photos/dandelion/9646730031_f3d5014416_n.jpg\nflower_photos/dandelion/7165651120_2279ebf6d1.jpg\nflower_photos/dandelion/5129135346_3fa8e804d8_n.jpg\nflower_photos/dandelion/4290112545_3528055993_m.jpg\nflower_photos/dandelion/8963359346_65ca69c59d_n.jpg\nflower_photos/dandelion/8915661673_9a1cdc3755_m.jpg\nflower_photos/dandelion/3372748508_e5a4eacfcb_n.jpg\nflower_photos/dandelion/2569516382_9fd7097b9b.jpg\nflower_photos/dandelion/4573886520_09c984ecd8_m.jpg\nflower_photos/dandelion/5140791232_52f2c5b41d_n.jpg\nflower_photos/dandelion/5762590366_5cf7a32b87_n.jpg\nflower_photos/dandelion/8223949_2928d3f6f6_n.jpg\nflower_photos/dandelion/13887031789_97437f246b.jpg\nflower_photos/dandelion/10946896405_81d2d50941_m.jpg\nflower_photos/dandelion/14085038920_2ee4ce8a8d.jpg\nflower_photos/dandelion/8707349105_6d06b543b0.jpg\nflower_photos/dandelion/23192507093_2e6ec77bef_n.jpg\nflower_photos/dandelion/8805314187_1aed702082_n.jpg\nflower_photos/dandelion/4607183665_3472643bc8.jpg\nflower_photos/dandelion/17161833794_e1d92259d2_m.jpg\nflower_photos/dandelion/4586018734_6de9c513c2.jpg\nflower_photos/dandelion/3513200808_390f1d63a7_m.jpg\nflower_photos/dandelion/2521827947_9d237779bb_n.jpg\nflower_photos/dandelion/17077940105_d2cd7b9ec4_n.jpg\nflower_photos/dandelion/8684925862_d736e153bf_n.jpg\nflower_photos/dandelion/5605502523_05acb00ae7_n.jpg\nflower_photos/dandelion/8980460785_b5e6842e59_n.jpg\nflower_photos/dandelion/158988663_6fe055fcb4.jpg\nflower_photos/dandelion/18342918441_b1bb69a2fd_n.jpg\nflower_photos/dandelion/506659320_6fac46551e.jpg\nflower_photos/dandelion/8989067485_aab399460b_n.jpg\nflower_photos/dandelion/136999986_e410a68efb_n.jpg\nflower_photos/dandelion/7280217714_fb9ffccf2d_n.jpg\nflower_photos/dandelion/126012913_edf771c564_n.jpg\nflower_photos/dandelion/11405573_24a8a838cc_n.jpg\nflower_photos/dandelion/14012247974_69ac128799.jpg\nflower_photos/dandelion/8979087213_28f572174c.jpg\nflower_photos/dandelion/3469112805_6cc8640236.jpg\nflower_photos/dandelion/8739657154_6db14796c9.jpg\nflower_photos/dandelion/13881700933_69a750d418_n.jpg\nflower_photos/dandelion/18587334446_ef1021909b_n.jpg\nflower_photos/dandelion/340190928_d77bf4d615.jpg\nflower_photos/dandelion/2502627784_4486978bcf.jpg\nflower_photos/dandelion/18215579866_94b1732f24.jpg\nflower_photos/dandelion/501987288_c69c4e0c90_m.jpg\nflower_photos/dandelion/3844111216_742ea491a0.jpg\nflower_photos/dandelion/19586799286_beb9d684b5.jpg\nflower_photos/dandelion/5760890854_c3e009bc8a_n.jpg\nflower_photos/dandelion/14614655810_9910e6dbd6_n.jpg\nflower_photos/dandelion/9472854850_fc9e1db673.jpg\nflower_photos/dandelion/15644450971_6a28298454_n.jpg\nflower_photos/dandelion/5674707921_1ffd141bab_n.jpg\nflower_photos/dandelion/6888894675_524a6accab_n.jpg\nflower_photos/dandelion/3297108443_0393d04dfc_m.jpg\nflower_photos/dandelion/5596093561_09b0301136_n.jpg\nflower_photos/dandelion/1386449001_5d6da6bde6.jpg\nflower_photos/dandelion/8740218495_23858355d8_n.jpg\nflower_photos/dandelion/2478018280_1be353ca8c_m.jpg\nflower_photos/dandelion/7141013005_d2f168c373.jpg\nflower_photos/dandelion/14211880544_5d1f9d5aa8_n.jpg\nflower_photos/dandelion/455728598_c5f3e7fc71_m.jpg\nflower_photos/dandelion/6900157914_c3387c11d8.jpg\n</code>\n</pre>        <pre><code># All the paths of the flowers\nALL_IMG_PATHS = list(paths.list_images(\"flower_photos\"))\nALL_IMG_PATHS[:5]\n</code></pre>      <pre>\n<code>['flower_photos/tulips/13510057763_01b832d919.jpg',\n 'flower_photos/tulips/486896118_bcc7b8e1d6.jpg',\n 'flower_photos/tulips/7247182064_f8d6759446_n.jpg',\n 'flower_photos/tulips/14093884601_c87b5cd663_n.jpg',\n 'flower_photos/tulips/17224410762_402455ed8f.jpg']</code>\n</pre>"},{"location":"deep_learning/module9/Module9_TFTRT/#visualisation","title":"Visualisation","text":"<pre><code># Visualize images\nplt.figure(figsize=(15,15))\nfor i in range(25):\n    image_path = np.random.choice(ALL_IMG_PATHS)\n    image = plt.imread(image_path)\n    image = cv2.resize(image, (128, 128))\n    # you might want to verify the labels before \n    # you put this to use\n    label = image_path.split(\"/\")[1]\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(True)\n    plt.imshow(image)\n    plt.xlabel(label)\nplt.show()\n</code></pre>"},{"location":"deep_learning/module9/Module9_TFTRT/#train-val-test-datasets","title":"Train, val, test datasets","text":"<pre><code>flower_train, flower_val = train_test_split(ALL_IMG_PATHS, test_size=0.20, random_state=RANDOM_SEED)\nflower_test, flower_val = train_test_split(flower_val, test_size=0.5, random_state=RANDOM_SEED)\nprint(f'images in train : {len(flower_train)}, images in val : {len(flower_val)}, images in test {len(flower_test)}')\n</code></pre>      <pre>\n<code>images in train : 2936, images in val : 367, images in test 367\n</code>\n</pre>        <pre><code>dic = {b'daisy': 0, b'dandelion': 1, b'roses': 2, b'sunflowers': 3, b'tulips': 4}\nlabel_train = [dic[tf.strings.split(filename, '/')[1].numpy()] for filename in flower_train]\nlabel_val = [dic[tf.strings.split(filename, '/')[1].numpy()] for filename in flower_val]\nlabel_test = [dic[tf.strings.split(filename, '/')[1].numpy()] for filename in flower_test]\nprint(f'labels in train : {len(label_train)}, labels in val : {len(label_val)}, labels in test {len(label_test)}')\n</code></pre>      <pre>\n<code>labels in train : 2936, labels in val : 367, labels in test 367\n</code>\n</pre>        <pre><code>NUM_CLASSES = 5\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\ndef parse_image(filename,label):\n  # convert the label to one-hot encoding\n  label = tf.one_hot(label, NUM_CLASSES)\n\n  #decode image\n  image = tf.io.read_file(filename)\n  #Don't use tf.image.decode_image, or the output shape will be undefined\n  image = tf.image.decode_jpeg(image)\n  #This will convert to float values in [0, 1]\n  image = tf.image.convert_image_dtype(image, tf.float32)\n  image = tf.image.resize(image, [224, 224])\n  return image, label\n\ndef train_preprocess(image, label):\n  image = tf.image.random_flip_left_right(image)\n  image = tf.image.random_flip_up_down(image)\n\n  return image, label\n\ndef create_train_dataset(features, labels, batch=32, repet=1, prefetch=1):\n  dataset = tf.data.Dataset.from_tensor_slices((features,labels))\n  dataset = dataset.shuffle(len(features), seed=RANDOM_SEED)\n  dataset = dataset.repeat(repet)\n  dataset = dataset.map(parse_image, num_parallel_calls=AUTOTUNE)\n  dataset = dataset.map(train_preprocess, num_parallel_calls=AUTOTUNE)\n  dataset = dataset.batch(batch)\n  dataset = dataset.prefetch(prefetch)\n  return dataset\n\ndef create_val_dataset(features, labels, batch=32, repet=1, prefetch=1):\n  dataset = tf.data.Dataset.from_tensor_slices((features,labels))\n  dataset = dataset.shuffle(len(features), seed=RANDOM_SEED)\n  dataset = dataset.repeat(repet)\n  dataset = dataset.map(parse_image, num_parallel_calls=AUTOTUNE)\n  dataset = dataset.batch(batch)\n  dataset = dataset.prefetch(prefetch)\n  return dataset\n</code></pre>     <pre><code>ds_train = create_train_dataset(flower_train, label_train)\nds_val = create_val_dataset(flower_val, label_val)\nds_test = create_val_dataset(flower_test, label_test)\n</code></pre>"},{"location":"deep_learning/module9/Module9_TFTRT/#construction-du-modele-entrainement-du-modele","title":"Construction du mod\u00e8le &amp; entra\u00eenement du mod\u00e8le","text":""},{"location":"deep_learning/module9/Module9_TFTRT/#construction","title":"Construction","text":"<pre><code># Load the MobileNetV2 model but exclude the classification layers\nconv_base = MobileNetV2(weights=\"imagenet\", \n                        include_top=False,\n                        input_shape=(224, 224, 3))\n</code></pre>     <pre><code>def get_training_model():\n    # We are fine-tuning the extractor model\n    conv_base.trainable = True\n    # Construct the head of the model that will be placed on top of the\n    # the base model\n    class_head = conv_base.output\n    class_head = GlobalAveragePooling2D()(class_head)\n    class_head = Dense(512, activation=\"relu\")(class_head)\n    class_head = Dropout(0.5)(class_head)\n    class_head = Dense(5, activation=\"softmax\")(class_head)\n\n    # Create the new model\n    classifier = Model(inputs=conv_base.input, outputs=class_head)\n\n    # Compile and return the model\n    classifier.compile(loss=\"categorical_crossentropy\", \n                          optimizer=\"adam\",\n                          metrics=[\"accuracy\"])\n\n    return classifier\n</code></pre>"},{"location":"deep_learning/module9/Module9_TFTRT/#entrainement","title":"Entra\u00eenement","text":"<pre><code># LR schedule configuration\nstart_lr = 0.00001\nmin_lr = 0.00001\nmax_lr = 0.00005\nrampup_epochs = 5\nsustain_epochs = 0\nexp_decay = .8\n</code></pre>     <pre><code># LR schedule\ndef lrfn(epoch):\n    def lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay):\n        if epoch &lt; rampup_epochs:\n            lr = (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n        elif epoch &lt; rampup_epochs + sustain_epochs:\n            lr = max_lr\n        else:\n            lr = (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n        return lr\n    return lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay)\n\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=True)\n</code></pre>     <pre><code># How does the LR schedule looks like?\nrng = [i for i in range(50)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, [lrfn(x) for x in rng])\nprint(y[0], y[-1])\n</code></pre>      <pre>\n<code>1e-05 1.0002177807148294e-05\n</code>\n</pre>             <pre><code># Train the model\nflower_model = get_training_model()\nstart = time.time()\nhistory = flower_model.fit(ds_train,\n                           validation_data=ds_val,\n                           epochs=10,\n                           callbacks=[lr_callback])\nprint(\"Total training time: \",time.time()-start)\n</code></pre>      <pre>\n<code>Train for 92 steps, validate for 12 steps\n\nEpoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\nEpoch 1/10\n92/92 [==============================] - 27s 291ms/step - loss: 1.6304 - accuracy: 0.3348 - val_loss: 1.2459 - val_accuracy: 0.5123\n\nEpoch 00002: LearningRateScheduler reducing learning rate to 1.8000000000000004e-05.\nEpoch 2/10\n92/92 [==============================] - 19s 208ms/step - loss: 0.9983 - accuracy: 0.6117 - val_loss: 0.8482 - val_accuracy: 0.6621\n\nEpoch 00003: LearningRateScheduler reducing learning rate to 2.6000000000000002e-05.\nEpoch 3/10\n92/92 [==============================] - 19s 210ms/step - loss: 0.6268 - accuracy: 0.7847 - val_loss: 0.5672 - val_accuracy: 0.7820\n\nEpoch 00004: LearningRateScheduler reducing learning rate to 3.4000000000000007e-05.\nEpoch 4/10\n92/92 [==============================] - 20s 212ms/step - loss: 0.4487 - accuracy: 0.8341 - val_loss: 0.4797 - val_accuracy: 0.8202\n\nEpoch 00005: LearningRateScheduler reducing learning rate to 4.2000000000000004e-05.\nEpoch 5/10\n92/92 [==============================] - 20s 215ms/step - loss: 0.3306 - accuracy: 0.8845 - val_loss: 0.4430 - val_accuracy: 0.8392\n\nEpoch 00006: LearningRateScheduler reducing learning rate to 5e-05.\nEpoch 6/10\n92/92 [==============================] - 20s 218ms/step - loss: 0.2585 - accuracy: 0.9091 - val_loss: 0.4133 - val_accuracy: 0.8365\n\nEpoch 00007: LearningRateScheduler reducing learning rate to 4.2000000000000004e-05.\nEpoch 7/10\n92/92 [==============================] - 20s 220ms/step - loss: 0.2015 - accuracy: 0.9275 - val_loss: 0.3557 - val_accuracy: 0.8638\n\nEpoch 00008: LearningRateScheduler reducing learning rate to 3.5600000000000005e-05.\nEpoch 8/10\n92/92 [==============================] - 20s 217ms/step - loss: 0.1766 - accuracy: 0.9356 - val_loss: 0.3505 - val_accuracy: 0.8610\n\nEpoch 00009: LearningRateScheduler reducing learning rate to 3.0480000000000006e-05.\nEpoch 9/10\n92/92 [==============================] - 20s 216ms/step - loss: 0.1449 - accuracy: 0.9499 - val_loss: 0.3420 - val_accuracy: 0.8719\n\nEpoch 00010: LearningRateScheduler reducing learning rate to 2.6384000000000004e-05.\nEpoch 10/10\n92/92 [==============================] - 20s 217ms/step - loss: 0.1382 - accuracy: 0.9547 - val_loss: 0.2870 - val_accuracy: 0.8828\nTotal training time:  204.63329792022705\n</code>\n</pre>"},{"location":"deep_learning/module9/Module9_TFTRT/#evaluation","title":"Evaluation","text":"<pre><code>pd.DataFrame(history.history).plot(figsize=(12,8))\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()\n</code></pre>              <pre><code>_, accuracy = flower_model.evaluate(ds_test)\n\ncomp = comp.append({'model': 'unoptim_model', 'pr\u00e9cision' : accuracy}, ignore_index=True)\nprint(comp)\n</code></pre>      <pre>\n<code>12/12 [==============================] - 1s 69ms/step - loss: 0.3031 - accuracy: 0.8965\n           model  pr\u00e9cision\n0  unoptim_model   0.896458\n</code>\n</pre>"},{"location":"deep_learning/module9/Module9_TFTRT/#sauvegarde","title":"Sauvegarde","text":"<p>Pour convertir un mod\u00e8le, que ce soit avec TF Lite ou avec TensorRT, on doit le sauvegarder sous la format <code>SavedModel</code> qui est le format standard de TensorFlow. La m\u00e9thode classique o\u00f9 l'on utilise le format <code>.h5</code> est elle propre \u00e0 <code>tf.keras</code>.</p>      <pre><code># Unoptimized (SavedModel format)\nflower_model.save(\"base_model/flower_model_no_op\")\n</code></pre>      <pre>\n<code>WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nINFO:tensorflow:Assets written to: base_model/flower_model_no_op/assets\n</code>\n</pre>        <pre><code># Check size\n!du --all -h base_model/flower_model_no_op\n</code></pre>      <pre>\n<code>4.4M  base_model/flower_model_no_op/saved_model.pb\n34M base_model/flower_model_no_op/variables/variables.data-00000-of-00001\n140K    base_model/flower_model_no_op/variables/variables.data-00000-of-00002\n44K base_model/flower_model_no_op/variables/variables.index\n34M base_model/flower_model_no_op/variables/variables.data-00001-of-00002\n67M base_model/flower_model_no_op/variables\n4.0K    base_model/flower_model_no_op/assets\n71M base_model/flower_model_no_op\n</code>\n</pre>         <p>Charger un mod\u00e8le se fait de la m\u00eame fa\u00e7on, que l'on utilise le format <code>SavedModel</code> ou <code>.h5</code>.</p>      <pre><code>flower_model = tf.keras.models.load_model('base_model/flower_model_no_op')\n\n# Check its architecture\nflower_model.summary()\n</code></pre>      <pre>\n<code>Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n__________________________________________________________________________________________________\nConv1_pad (ZeroPadding2D)       (None, 225, 225, 3)  0           input_1[0][0]                    \n__________________________________________________________________________________________________\nConv1 (Conv2D)                  (None, 112, 112, 32) 864         Conv1_pad[0][0]                  \n__________________________________________________________________________________________________\nbn_Conv1 (BatchNormalization)   (None, 112, 112, 32) 128         Conv1[0][0]                      \n__________________________________________________________________________________________________\nConv1_relu (ReLU)               (None, 112, 112, 32) 0           bn_Conv1[0][0]                   \n__________________________________________________________________________________________________\nexpanded_conv_depthwise (Depthw (None, 112, 112, 32) 288         Conv1_relu[0][0]                 \n__________________________________________________________________________________________________\nexpanded_conv_depthwise_BN (Bat (None, 112, 112, 32) 128         expanded_conv_depthwise[0][0]    \n__________________________________________________________________________________________________\nexpanded_conv_depthwise_relu (R (None, 112, 112, 32) 0           expanded_conv_depthwise_BN[0][0] \n__________________________________________________________________________________________________\nexpanded_conv_project (Conv2D)  (None, 112, 112, 16) 512         expanded_conv_depthwise_relu[0][0\n__________________________________________________________________________________________________\nexpanded_conv_project_BN (Batch (None, 112, 112, 16) 64          expanded_conv_project[0][0]      \n__________________________________________________________________________________________________\nblock_1_expand (Conv2D)         (None, 112, 112, 96) 1536        expanded_conv_project_BN[0][0]   \n__________________________________________________________________________________________________\nblock_1_expand_BN (BatchNormali (None, 112, 112, 96) 384         block_1_expand[0][0]             \n__________________________________________________________________________________________________\nblock_1_expand_relu (ReLU)      (None, 112, 112, 96) 0           block_1_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_1_pad (ZeroPadding2D)     (None, 113, 113, 96) 0           block_1_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_1_depthwise (DepthwiseCon (None, 56, 56, 96)   864         block_1_pad[0][0]                \n__________________________________________________________________________________________________\nblock_1_depthwise_BN (BatchNorm (None, 56, 56, 96)   384         block_1_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_1_depthwise_relu (ReLU)   (None, 56, 56, 96)   0           block_1_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_1_project (Conv2D)        (None, 56, 56, 24)   2304        block_1_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_1_project_BN (BatchNormal (None, 56, 56, 24)   96          block_1_project[0][0]            \n__________________________________________________________________________________________________\nblock_2_expand (Conv2D)         (None, 56, 56, 144)  3456        block_1_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_2_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_2_expand[0][0]             \n__________________________________________________________________________________________________\nblock_2_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_2_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_2_depthwise (DepthwiseCon (None, 56, 56, 144)  1296        block_2_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_2_depthwise_BN (BatchNorm (None, 56, 56, 144)  576         block_2_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_2_depthwise_relu (ReLU)   (None, 56, 56, 144)  0           block_2_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_2_project (Conv2D)        (None, 56, 56, 24)   3456        block_2_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_2_project_BN (BatchNormal (None, 56, 56, 24)   96          block_2_project[0][0]            \n__________________________________________________________________________________________________\nblock_2_add (Add)               (None, 56, 56, 24)   0           block_1_project_BN[0][0]         \n                                                                 block_2_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_3_expand (Conv2D)         (None, 56, 56, 144)  3456        block_2_add[0][0]                \n__________________________________________________________________________________________________\nblock_3_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_3_expand[0][0]             \n__________________________________________________________________________________________________\nblock_3_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_3_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_3_pad (ZeroPadding2D)     (None, 57, 57, 144)  0           block_3_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_3_depthwise (DepthwiseCon (None, 28, 28, 144)  1296        block_3_pad[0][0]                \n__________________________________________________________________________________________________\nblock_3_depthwise_BN (BatchNorm (None, 28, 28, 144)  576         block_3_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_3_depthwise_relu (ReLU)   (None, 28, 28, 144)  0           block_3_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_3_project (Conv2D)        (None, 28, 28, 32)   4608        block_3_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_3_project_BN (BatchNormal (None, 28, 28, 32)   128         block_3_project[0][0]            \n__________________________________________________________________________________________________\nblock_4_expand (Conv2D)         (None, 28, 28, 192)  6144        block_3_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_4_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_4_expand[0][0]             \n__________________________________________________________________________________________________\nblock_4_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_4_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_4_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_4_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_4_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_4_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_4_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_4_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_4_project (Conv2D)        (None, 28, 28, 32)   6144        block_4_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_4_project_BN (BatchNormal (None, 28, 28, 32)   128         block_4_project[0][0]            \n__________________________________________________________________________________________________\nblock_4_add (Add)               (None, 28, 28, 32)   0           block_3_project_BN[0][0]         \n                                                                 block_4_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_5_expand (Conv2D)         (None, 28, 28, 192)  6144        block_4_add[0][0]                \n__________________________________________________________________________________________________\nblock_5_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_5_expand[0][0]             \n__________________________________________________________________________________________________\nblock_5_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_5_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_5_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_5_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_5_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_5_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_5_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_5_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_5_project (Conv2D)        (None, 28, 28, 32)   6144        block_5_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_5_project_BN (BatchNormal (None, 28, 28, 32)   128         block_5_project[0][0]            \n__________________________________________________________________________________________________\nblock_5_add (Add)               (None, 28, 28, 32)   0           block_4_add[0][0]                \n                                                                 block_5_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_6_expand (Conv2D)         (None, 28, 28, 192)  6144        block_5_add[0][0]                \n__________________________________________________________________________________________________\nblock_6_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_6_expand[0][0]             \n__________________________________________________________________________________________________\nblock_6_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_6_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_6_pad (ZeroPadding2D)     (None, 29, 29, 192)  0           block_6_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_6_depthwise (DepthwiseCon (None, 14, 14, 192)  1728        block_6_pad[0][0]                \n__________________________________________________________________________________________________\nblock_6_depthwise_BN (BatchNorm (None, 14, 14, 192)  768         block_6_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_6_depthwise_relu (ReLU)   (None, 14, 14, 192)  0           block_6_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_6_project (Conv2D)        (None, 14, 14, 64)   12288       block_6_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_6_project_BN (BatchNormal (None, 14, 14, 64)   256         block_6_project[0][0]            \n__________________________________________________________________________________________________\nblock_7_expand (Conv2D)         (None, 14, 14, 384)  24576       block_6_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_7_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_7_expand[0][0]             \n__________________________________________________________________________________________________\nblock_7_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_7_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_7_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_7_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_7_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_7_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_7_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_7_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_7_project (Conv2D)        (None, 14, 14, 64)   24576       block_7_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_7_project_BN (BatchNormal (None, 14, 14, 64)   256         block_7_project[0][0]            \n__________________________________________________________________________________________________\nblock_7_add (Add)               (None, 14, 14, 64)   0           block_6_project_BN[0][0]         \n                                                                 block_7_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_8_expand (Conv2D)         (None, 14, 14, 384)  24576       block_7_add[0][0]                \n__________________________________________________________________________________________________\nblock_8_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_8_expand[0][0]             \n__________________________________________________________________________________________________\nblock_8_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_8_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_8_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_8_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_8_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_8_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_8_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_8_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_8_project (Conv2D)        (None, 14, 14, 64)   24576       block_8_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_8_project_BN (BatchNormal (None, 14, 14, 64)   256         block_8_project[0][0]            \n__________________________________________________________________________________________________\nblock_8_add (Add)               (None, 14, 14, 64)   0           block_7_add[0][0]                \n                                                                 block_8_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_9_expand (Conv2D)         (None, 14, 14, 384)  24576       block_8_add[0][0]                \n__________________________________________________________________________________________________\nblock_9_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_9_expand[0][0]             \n__________________________________________________________________________________________________\nblock_9_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_9_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_9_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_9_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_9_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_9_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_9_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_9_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_9_project (Conv2D)        (None, 14, 14, 64)   24576       block_9_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_9_project_BN (BatchNormal (None, 14, 14, 64)   256         block_9_project[0][0]            \n__________________________________________________________________________________________________\nblock_9_add (Add)               (None, 14, 14, 64)   0           block_8_add[0][0]                \n                                                                 block_9_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_10_expand (Conv2D)        (None, 14, 14, 384)  24576       block_9_add[0][0]                \n__________________________________________________________________________________________________\nblock_10_expand_BN (BatchNormal (None, 14, 14, 384)  1536        block_10_expand[0][0]            \n__________________________________________________________________________________________________\nblock_10_expand_relu (ReLU)     (None, 14, 14, 384)  0           block_10_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_10_depthwise (DepthwiseCo (None, 14, 14, 384)  3456        block_10_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_10_depthwise_BN (BatchNor (None, 14, 14, 384)  1536        block_10_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           block_10_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_10_project (Conv2D)       (None, 14, 14, 96)   36864       block_10_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_10_project_BN (BatchNorma (None, 14, 14, 96)   384         block_10_project[0][0]           \n__________________________________________________________________________________________________\nblock_11_expand (Conv2D)        (None, 14, 14, 576)  55296       block_10_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_11_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_11_expand[0][0]            \n__________________________________________________________________________________________________\nblock_11_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_11_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_11_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_11_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_11_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_11_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_11_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_11_project (Conv2D)       (None, 14, 14, 96)   55296       block_11_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_11_project_BN (BatchNorma (None, 14, 14, 96)   384         block_11_project[0][0]           \n__________________________________________________________________________________________________\nblock_11_add (Add)              (None, 14, 14, 96)   0           block_10_project_BN[0][0]        \n                                                                 block_11_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_12_expand (Conv2D)        (None, 14, 14, 576)  55296       block_11_add[0][0]               \n__________________________________________________________________________________________________\nblock_12_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_12_expand[0][0]            \n__________________________________________________________________________________________________\nblock_12_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_12_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_12_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_12_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_12_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_12_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_12_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_12_project (Conv2D)       (None, 14, 14, 96)   55296       block_12_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_12_project_BN (BatchNorma (None, 14, 14, 96)   384         block_12_project[0][0]           \n__________________________________________________________________________________________________\nblock_12_add (Add)              (None, 14, 14, 96)   0           block_11_add[0][0]               \n                                                                 block_12_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_13_expand (Conv2D)        (None, 14, 14, 576)  55296       block_12_add[0][0]               \n__________________________________________________________________________________________________\nblock_13_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_13_expand[0][0]            \n__________________________________________________________________________________________________\nblock_13_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_13_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_13_pad (ZeroPadding2D)    (None, 15, 15, 576)  0           block_13_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_13_depthwise (DepthwiseCo (None, 7, 7, 576)    5184        block_13_pad[0][0]               \n__________________________________________________________________________________________________\nblock_13_depthwise_BN (BatchNor (None, 7, 7, 576)    2304        block_13_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_13_depthwise_relu (ReLU)  (None, 7, 7, 576)    0           block_13_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_13_project (Conv2D)       (None, 7, 7, 160)    92160       block_13_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_13_project_BN (BatchNorma (None, 7, 7, 160)    640         block_13_project[0][0]           \n__________________________________________________________________________________________________\nblock_14_expand (Conv2D)        (None, 7, 7, 960)    153600      block_13_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_14_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_14_expand[0][0]            \n__________________________________________________________________________________________________\nblock_14_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_14_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_14_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_14_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_14_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_14_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_14_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_14_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_14_project (Conv2D)       (None, 7, 7, 160)    153600      block_14_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_14_project_BN (BatchNorma (None, 7, 7, 160)    640         block_14_project[0][0]           \n__________________________________________________________________________________________________\nblock_14_add (Add)              (None, 7, 7, 160)    0           block_13_project_BN[0][0]        \n                                                                 block_14_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_15_expand (Conv2D)        (None, 7, 7, 960)    153600      block_14_add[0][0]               \n__________________________________________________________________________________________________\nblock_15_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_15_expand[0][0]            \n__________________________________________________________________________________________________\nblock_15_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_15_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_15_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_15_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_15_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_15_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_15_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_15_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_15_project (Conv2D)       (None, 7, 7, 160)    153600      block_15_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_15_project_BN (BatchNorma (None, 7, 7, 160)    640         block_15_project[0][0]           \n__________________________________________________________________________________________________\nblock_15_add (Add)              (None, 7, 7, 160)    0           block_14_add[0][0]               \n                                                                 block_15_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_16_expand (Conv2D)        (None, 7, 7, 960)    153600      block_15_add[0][0]               \n__________________________________________________________________________________________________\nblock_16_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_16_expand[0][0]            \n__________________________________________________________________________________________________\nblock_16_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_16_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_16_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_16_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_16_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_16_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_16_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_16_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_16_project (Conv2D)       (None, 7, 7, 320)    307200      block_16_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_16_project_BN (BatchNorma (None, 7, 7, 320)    1280        block_16_project[0][0]           \n__________________________________________________________________________________________________\nConv_1 (Conv2D)                 (None, 7, 7, 1280)   409600      block_16_project_BN[0][0]        \n__________________________________________________________________________________________________\nConv_1_bn (BatchNormalization)  (None, 7, 7, 1280)   5120        Conv_1[0][0]                     \n__________________________________________________________________________________________________\nout_relu (ReLU)                 (None, 7, 7, 1280)   0           Conv_1_bn[0][0]                  \n__________________________________________________________________________________________________\nglobal_average_pooling2d (Globa (None, 1280)         0           out_relu[0][0]                   \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 512)          655872      global_average_pooling2d[0][0]   \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 512)          0           dense[0][0]                      \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 5)            2565        dropout[0][0]                    \n==================================================================================================\nTotal params: 2,916,421\nTrainable params: 2,882,309\nNon-trainable params: 34,112\n__________________________________________________________________________________________________\n</code>\n</pre>"},{"location":"deep_learning/module9/Module9_TFTRT/#tensorrt","title":"TensorRT","text":"<ul> <li> <p>https://github.com/tensorflow/tensorrt/blob/master/tftrt/examples/image-classification/Colab-TF-TRT-inference-from-Keras-saved-model.ipynb</p> </li> <li> <p>https://github.com/tensorflow/tensorrt/blob/master/tftrt/examples/image-classification/TF-TRT-inference-from-saved-model.ipynb</p> </li> <li> <p>https://github.com/tensorflow/tensorrt/tree/master/tftrt/examples/image-classification</p> </li> <li> <p>https://sayak.dev/tf.keras/tensorrt/tensorflow/2020/07/01/accelerated-inference-trt.html</p> </li> </ul>"},{"location":"deep_learning/module9/Module9_TFTRT/#installation-tensorrt-converter-runtime","title":"Installation TensorRT Converter &amp; Runtime","text":"<p>Actuellement, on a deux versions de Jetpack (le kit de d\u00e9veloppement de NVidia) disponibles : la 4.3 et la 4.4. </p> <p>La version 4.4 fonctionne avec une version Tensorflow 2.2 optimis\u00e9e pour Jetson Nano, et la 4.3 avec une version de Tensorflow 2.1 optimis\u00e9e. Chacune de ces deux version a une versio diff\u00e9rente de TensorRT :</p> <ul> <li>Jetpack 4.4 poss\u00e8de la version TensorRT 7.1.3,</li> <li>Jetpack 4.3 poss\u00e8de la version TensorRT 6.0.1.</li> </ul> <p>Pour savoir quelle version l'on doit utiliser, il faut v\u00e9rifier la version de cuDNN utilis\u00e9e (le driver deep learning de Nvidia).</p> <ul> <li>Jetpack 4.4 supporte cuDNN 8.0, avec une version minimale de Cuda 10.2,</li> <li>Jetpack 4.3 supporte cuDNN 7.6.3, avec une version minimale de Cuda 10.0.</li> </ul> <p>Pour voir les versions utilis\u00e9es sur Colab (ou sur n'importe quel pc Linux), on lance les commandes suivantes.</p>      <pre><code>!nvcc -V\n</code></pre>      <pre>\n<code>nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2019 NVIDIA Corporation\nBuilt on Sun_Jul_28_19:07:16_PDT_2019\nCuda compilation tools, release 10.1, V10.1.243\n</code>\n</pre>        <pre><code>!cat /usr/include/cudnn.h | grep CUDNN_MAJOR -A 2\n</code></pre>      <pre>\n<code>#define CUDNN_MAJOR 7\n#define CUDNN_MINOR 6\n#define CUDNN_PATCHLEVEL 5\n--\n#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\n\n#include \"driver_types.h\"\n</code>\n</pre>         <p>on est donc donc sous Cuda 10.1 avec cuDNN 7.6.5, e qui fait que l'on doit uiliser la version pr\u00e9sente dans Jetpack 4.3.</p>       <p>Pour avoir des versions compatibles de TensorFlow &amp; TensorRT, on va downgrader vers la version \\(2.1\\) de TensorFlow.</p>      <pre><code>!pip install tensorflow-gpu==2.1.0\n</code></pre>      <pre>\n<code>Collecting tensorflow-gpu==2.1.0\n  Downloading https://files.pythonhosted.org/packages/0a/93/c7bca39b23aae45cd2e85ad3871c81eccc63b9c5276e926511e2e5b0879d/tensorflow_gpu-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 421.8MB 36kB/s \nRequirement already satisfied: opt-einsum&gt;=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0) (3.3.0)\nRequirement already satisfied: grpcio&gt;=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0) (1.31.0)\nRequirement already satisfied: six&gt;=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0) (1.15.0)\nCollecting keras-applications&gt;=1.0.8\n  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 6.7MB/s \nRequirement already satisfied: wheel&gt;=0.26; python_version &gt;= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0) (0.35.1)\nRequirement already satisfied: termcolor&gt;=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0) (1.1.0)\nRequirement already satisfied: absl-py&gt;=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0) (0.8.1)\nRequirement already satisfied: protobuf&gt;=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0) (3.12.4)\nCollecting tensorboard&lt;2.2.0,&gt;=2.1.0\n  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.9MB 56.8MB/s \nRequirement already satisfied: keras-preprocessing&gt;=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0) (1.1.2)\nCollecting tensorflow-estimator&lt;2.2.0,&gt;=2.1.0rc0\n  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 450kB 36.2MB/s \nRequirement already satisfied: astor&gt;=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0) (0.8.1)\nCollecting gast==0.2.2\n  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\nRequirement already satisfied: wrapt&gt;=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0) (1.12.1)\nRequirement already satisfied: numpy&lt;2.0,&gt;=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0) (1.18.5)\nRequirement already satisfied: google-pasta&gt;=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0) (0.2.0)\nRequirement already satisfied: scipy==1.4.1; python_version &gt;= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0) (1.4.1)\nRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications&gt;=1.0.8-&gt;tensorflow-gpu==2.1.0) (2.10.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf&gt;=3.8.0-&gt;tensorflow-gpu==2.1.0) (49.6.0)\nRequirement already satisfied: werkzeug&gt;=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow-gpu==2.1.0) (1.0.1)\nRequirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow-gpu==2.1.0) (0.4.1)\nRequirement already satisfied: requests&lt;3,&gt;=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow-gpu==2.1.0) (2.23.0)\nRequirement already satisfied: google-auth&lt;2,&gt;=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow-gpu==2.1.0) (1.17.2)\nRequirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow-gpu==2.1.0) (3.2.2)\nRequirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow-gpu==2.1.0) (1.3.0)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow-gpu==2.1.0) (1.24.3)\nRequirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow-gpu==2.1.0) (3.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow-gpu==2.1.0) (2020.6.20)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow-gpu==2.1.0) (2.10)\nRequirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow-gpu==2.1.0) (0.2.8)\nRequirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow-gpu==2.1.0) (4.1.1)\nRequirement already satisfied: rsa&lt;5,&gt;=3.1.4; python_version &gt;= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow-gpu==2.1.0) (4.6)\nRequirement already satisfied: importlib-metadata; python_version &lt; \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown&gt;=2.6.8-&gt;tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow-gpu==2.1.0) (1.7.0)\nRequirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow-gpu==2.1.0) (3.1.0)\nRequirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow-gpu==2.1.0) (0.4.8)\nRequirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version &lt; \"3.8\"-&gt;markdown&gt;=2.6.8-&gt;tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow-gpu==2.1.0) (3.1.0)\nBuilding wheels for collected packages: gast\n  Building wheel for gast (setup.py) ... done\n  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=3bc75e9d32a7cb7e7f51cf622fb8940d62e0b24ed504ee3cbe5e28f7cf90b7bc\n  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\nSuccessfully built gast\nERROR: tensorflow 2.3.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\nERROR: tensorflow 2.3.0 has requirement tensorboard&lt;3,&gt;=2.3.0, but you'll have tensorboard 2.1.1 which is incompatible.\nERROR: tensorflow 2.3.0 has requirement tensorflow-estimator&lt;2.4.0,&gt;=2.3.0, but you'll have tensorflow-estimator 2.1.0 which is incompatible.\nERROR: tensorflow-probability 0.11.0 has requirement gast&gt;=0.3.2, but you'll have gast 0.2.2 which is incompatible.\nInstalling collected packages: keras-applications, tensorboard, tensorflow-estimator, gast, tensorflow-gpu\n  Found existing installation: tensorboard 2.3.0\n    Uninstalling tensorboard-2.3.0:\n      Successfully uninstalled tensorboard-2.3.0\n  Found existing installation: tensorflow-estimator 2.3.0\n    Uninstalling tensorflow-estimator-2.3.0:\n      Successfully uninstalled tensorflow-estimator-2.3.0\n  Found existing installation: gast 0.3.3\n    Uninstalling gast-0.3.3:\n      Successfully uninstalled gast-0.3.3\nSuccessfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-2.1.1 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0\n</code>\n</pre>         <pre><code>%%bash\nwget https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/libnvinfer6_6.0.1-1+cuda10.1_amd64.deb\ndpkg -i libnvinfer6_6.0.1-1+cuda10.1_amd64.deb\n</code></pre>      <pre>\n<code>Selecting previously unselected package libnvinfer6.\n(Reading database ... 144579 files and directories currently installed.)\nPreparing to unpack libnvinfer6_6.0.1-1+cuda10.1_amd64.deb ...\nUnpacking libnvinfer6 (6.0.1-1+cuda10.1) ...\nSetting up libnvinfer6 (6.0.1-1+cuda10.1) ...\nProcessing triggers for libc-bin (2.27-3ubuntu1) ...\n</code>\n</pre>     <pre>\n<code>--2020-09-01 08:47:40--  https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/libnvinfer6_6.0.1-1+cuda10.1_amd64.deb\nResolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.195.19.142\nConnecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.195.19.142|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 71245796 (68M) [application/x-deb]\nSaving to: \u2018libnvinfer6_6.0.1-1+cuda10.1_amd64.deb\u2019\n\n     0K .......... .......... .......... .......... ..........  0% 2.22M 31s\n    50K .......... .......... .......... .......... ..........  0% 7.17M 20s\n   100K .......... .......... .......... .......... ..........  0% 2.04M 24s\n   150K .......... .......... .......... .......... ..........  0% 53.5M 19s\n   200K .......... .......... .......... .......... ..........  0% 8.94M 16s\n   250K .......... .......... .......... .......... ..........  0% 3.40M 17s\n   300K .......... .......... .......... .......... ..........  0% 4.78M 17s\n   350K .......... .......... .......... .......... ..........  0%  155M 15s\n   400K .......... .......... .......... .......... ..........  0%  132M 13s\n   450K .......... .......... .......... .......... ..........  0%  171M 12s\n   500K .......... .......... .......... .......... ..........  0% 9.15M 11s\n   550K .......... .......... .......... .......... ..........  0% 3.34M 12s\n   600K .......... .......... .......... .......... ..........  0%  197M 11s\n   650K .......... .......... .......... .......... ..........  1% 4.93M 11s\n   700K .......... .......... .......... .......... ..........  1%  171M 11s\n   750K .......... .......... .......... .......... ..........  1%  118M 10s\n   800K .......... .......... .......... .......... ..........  1%  138M 9s\n   850K .......... .......... .......... .......... ..........  1%  176M 9s\n   900K .......... .......... .......... .......... ..........  1%  180M 8s\n   950K .......... .......... .......... .......... ..........  1% 2.54M 9s\n  1000K .......... .......... .......... .......... ..........  1% 4.84M 10s\n  1050K .......... .......... .......... .......... ..........  1%  149M 9s\n  1100K .......... .......... .......... .......... ..........  1%  145M 9s\n  1150K .......... .......... .......... .......... ..........  1%  106M 8s\n  1200K .......... .......... .......... .......... ..........  1%  127M 8s\n  1250K .......... .......... .......... .......... ..........  1%  198M 8s\n  1300K .......... .......... .......... .......... ..........  1% 2.61M 8s\n  1350K .......... .......... .......... .......... ..........  2% 4.68M 9s\n  1400K .......... .......... .......... .......... ..........  2%  171M 8s\n  1450K .......... .......... .......... .......... ..........  2%  149M 8s\n  1500K .......... .......... .......... .......... ..........  2%  160M 8s\n  1550K .......... .......... .......... .......... ..........  2%  107M 8s\n  1600K .......... .......... .......... .......... ..........  2%  177M 7s\n  1650K .......... .......... .......... .......... ..........  2% 2.62M 8s\n  1700K .......... .......... .......... .......... ..........  2%  142M 8s\n  1750K .......... .......... .......... .......... ..........  2% 4.68M 8s\n  1800K .......... .......... .......... .......... ..........  2%  154M 8s\n  1850K .......... .......... .......... .......... ..........  2%  110M 7s\n  1900K .......... .......... .......... .......... ..........  2% 92.8M 7s\n  1950K .......... .......... .......... .......... ..........  2% 82.4M 7s\n  2000K .......... .......... .......... .......... ..........  2% 2.66M 8s\n  2050K .......... .......... .......... .......... ..........  3%  156M 7s\n  2100K .......... .......... .......... .......... ..........  3% 4.76M 7s\n  2150K .......... .......... .......... .......... ..........  3%  128M 7s\n  2200K .......... .......... .......... .......... ..........  3%  138M 7s\n  2250K .......... .......... .......... .......... ..........  3%  106M 7s\n  2300K .......... .......... .......... .......... ..........  3% 99.6M 7s\n  2350K .......... .......... .......... .......... ..........  3% 67.0M 7s\n  2400K .......... .......... .......... .......... ..........  3% 2.66M 7s\n  2450K .......... .......... .......... .......... ..........  3% 4.89M 7s\n  2500K .......... .......... .......... .......... ..........  3% 95.0M 7s\n  2550K .......... .......... .......... .......... ..........  3%  117M 7s\n  2600K .......... .......... .......... .......... ..........  3% 60.5M 7s\n  2650K .......... .......... .......... .......... ..........  3%  165M 7s\n  2700K .......... .......... .......... .......... ..........  3% 36.2M 7s\n  2750K .......... .......... .......... .......... ..........  4% 2.80M 7s\n  2800K .......... .......... .......... .......... ..........  4% 5.04M 7s\n  2850K .......... .......... .......... .......... ..........  4% 75.9M 7s\n  2900K .......... .......... .......... .......... ..........  4% 86.4M 7s\n  2950K .......... .......... .......... .......... ..........  4%  105M 7s\n  3000K .......... .......... .......... .......... ..........  4% 21.6M 7s\n  3050K .......... .......... .......... .......... ..........  4%  195M 7s\n  3100K .......... .......... .......... .......... ..........  4%  241M 6s\n  3150K .......... .......... .......... .......... ..........  4% 2.89M 7s\n  3200K .......... .......... .......... .......... ..........  4% 4.84M 7s\n  3250K .......... .......... .......... .......... ..........  4% 57.2M 7s\n  3300K .......... .......... .......... .......... ..........  4%  175M 7s\n  3350K .......... .......... .......... .......... ..........  4%  126M 6s\n  3400K .......... .......... .......... .......... ..........  4% 83.6M 6s\n  3450K .......... .......... .......... .......... ..........  5% 53.0M 6s\n  3500K .......... .......... .......... .......... ..........  5% 2.77M 7s\n  3550K .......... .......... .......... .......... ..........  5% 4.85M 7s\n  3600K .......... .......... .......... .......... ..........  5% 90.3M 7s\n  3650K .......... .......... .......... .......... ..........  5% 62.5M 6s\n  3700K .......... .......... .......... .......... ..........  5%  213M 6s\n  3750K .......... .......... .......... .......... ..........  5% 94.5M 6s\n  3800K .......... .......... .......... .......... ..........  5% 57.9M 6s\n  3850K .......... .......... .......... .......... ..........  5% 73.7M 6s\n  3900K .......... .......... .......... .......... ..........  5% 2.82M 6s\n  3950K .......... .......... .......... .......... ..........  5% 4.79M 6s\n  4000K .......... .......... .......... .......... ..........  5% 93.7M 6s\n  4050K .......... .......... .......... .......... ..........  5% 79.1M 6s\n  4100K .......... .......... .......... .......... ..........  5% 55.6M 6s\n  4150K .......... .......... .......... .......... ..........  6%  177M 6s\n  4200K .......... .......... .......... .......... ..........  6% 63.1M 6s\n  4250K .......... .......... .......... .......... ..........  6% 2.78M 6s\n  4300K .......... .......... .......... .......... ..........  6%  157M 6s\n  4350K .......... .......... .......... .......... ..........  6% 4.78M 6s\n  4400K .......... .......... .......... .......... ..........  6% 88.1M 6s\n  4450K .......... .......... .......... .......... ..........  6% 99.6M 6s\n  4500K .......... .......... .......... .......... ..........  6% 74.3M 6s\n  4550K .......... .......... .......... .......... ..........  6% 60.6M 6s\n  4600K .......... .......... .......... .......... ..........  6% 74.4M 6s\n  4650K .......... .......... .......... .......... ..........  6% 2.82M 6s\n  4700K .......... .......... .......... .......... ..........  6% 4.91M 6s\n  4750K .......... .......... .......... .......... ..........  6% 81.3M 6s\n  4800K .......... .......... .......... .......... ..........  6% 84.2M 6s\n  4850K .......... .......... .......... .......... ..........  7% 88.6M 6s\n  4900K .......... .......... .......... .......... ..........  7% 75.6M 6s\n  4950K .......... .......... .......... .......... ..........  7% 57.8M 6s\n  5000K .......... .......... .......... .......... ..........  7% 2.76M 6s\n  5050K .......... .......... .......... .......... ..........  7% 4.78M 6s\n  5100K .......... .......... .......... .......... ..........  7% 91.0M 6s\n  5150K .......... .......... .......... .......... ..........  7% 49.5M 6s\n  5200K .......... .......... .......... .......... ..........  7% 40.7M 6s\n  5250K .......... .......... .......... .......... ..........  7% 2.75M 6s\n  5300K .......... .......... .......... .......... ..........  7% 4.81M 6s\n  5350K .......... .......... .......... .......... ..........  7% 71.9M 6s\n  5400K .......... .......... .......... .......... ..........  7% 57.5M 6s\n  5450K .......... .......... .......... .......... ..........  7% 54.8M 6s\n  5500K .......... .......... .......... .......... ..........  7% 47.3M 6s\n  5550K .......... .......... .......... .......... ..........  8% 2.76M 6s\n  5600K .......... .......... .......... .......... ..........  8% 4.70M 6s\n  5650K .......... .......... .......... .......... ..........  8% 81.7M 6s\n  5700K .......... .......... .......... .......... ..........  8% 71.0M 6s\n  5750K .......... .......... .......... .......... ..........  8% 36.8M 6s\n  5800K .......... .......... .......... .......... ..........  8% 2.75M 6s\n  5850K .......... .......... .......... .......... ..........  8% 4.81M 6s\n  5900K .......... .......... .......... .......... ..........  8% 87.4M 6s\n  5950K .......... .......... .......... .......... ..........  8% 44.0M 6s\n  6000K .......... .......... .......... .......... ..........  8% 48.1M 6s\n  6050K .......... .......... .......... .......... ..........  8% 2.74M 6s\n  6100K .......... .......... .......... .......... ..........  8%  118M 6s\n  6150K .......... .......... .......... .......... ..........  8% 4.63M 6s\n  6200K .......... .......... .......... .......... ..........  8% 74.8M 6s\n  6250K .......... .......... .......... .......... ..........  9% 54.9M 6s\n  6300K .......... .......... .......... .......... ..........  9% 47.5M 6s\n  6350K .......... .......... .......... .......... ..........  9% 2.75M 6s\n  6400K .......... .......... .......... .......... ..........  9% 4.74M 6s\n  6450K .......... .......... .......... .......... ..........  9% 82.6M 6s\n  6500K .......... .......... .......... .......... ..........  9% 56.8M 6s\n  6550K .......... .......... .......... .......... ..........  9% 41.4M 6s\n  6600K .......... .......... .......... .......... ..........  9% 2.76M 6s\n  6650K .......... .......... .......... .......... ..........  9% 4.90M 6s\n  6700K .......... .......... .......... .......... ..........  9% 58.2M 6s\n  6750K .......... .......... .......... .......... ..........  9% 26.4M 6s\n  6800K .......... .......... .......... .......... ..........  9%  225M 6s\n  6850K .......... .......... .......... .......... ..........  9% 57.9M 6s\n  6900K .......... .......... .......... .......... ..........  9% 2.81M 6s\n  6950K .......... .......... .......... .......... .......... 10% 4.71M 6s\n  7000K .......... .......... .......... .......... .......... 10% 73.9M 6s\n  7050K .......... .......... .......... .......... .......... 10% 53.5M 6s\n  7100K .......... .......... .......... .......... .......... 10% 59.8M 6s\n  7150K .......... .......... .......... .......... .......... 10% 2.68M 6s\n  7200K .......... .......... .......... .......... .......... 10% 4.90M 6s\n  7250K .......... .......... .......... .......... .......... 10% 71.5M 6s\n  7300K .......... .......... .......... .......... .......... 10% 60.3M 6s\n  7350K .......... .......... .......... .......... .......... 10% 56.6M 6s\n  7400K .......... .......... .......... .......... .......... 10% 45.8M 6s\n  7450K .......... .......... .......... .......... .......... 10% 2.76M 6s\n  7500K .......... .......... .......... .......... .......... 10% 4.85M 6s\n  7550K .......... .......... .......... .......... .......... 10% 57.0M 6s\n  7600K .......... .......... .......... .......... .......... 10% 57.6M 6s\n  7650K .......... .......... .......... .......... .......... 11% 47.7M 6s\n  7700K .......... .......... .......... .......... .......... 11% 2.78M 6s\n  7750K .......... .......... .......... .......... .......... 11% 4.80M 6s\n  7800K .......... .......... .......... .......... .......... 11% 63.1M 6s\n  7850K .......... .......... .......... .......... .......... 11% 69.5M 6s\n  7900K .......... .......... .......... .......... .......... 11% 55.8M 6s\n  7950K .......... .......... .......... .......... .......... 11% 37.9M 6s\n  8000K .......... .......... .......... .......... .......... 11% 2.79M 6s\n  8050K .......... .......... .......... .......... .......... 11% 4.77M 6s\n  8100K .......... .......... .......... .......... .......... 11% 77.0M 6s\n  8150K .......... .......... .......... .......... .......... 11% 55.5M 6s\n  8200K .......... .......... .......... .......... .......... 11% 57.1M 6s\n  8250K .......... .......... .......... .......... .......... 11% 49.1M 6s\n  8300K .......... .......... .......... .......... .......... 12% 2.75M 6s\n  8350K .......... .......... .......... .......... .......... 12% 4.82M 6s\n  8400K .......... .......... .......... .......... .......... 12% 47.1M 6s\n  8450K .......... .......... .......... .......... .......... 12% 88.4M 6s\n  8500K .......... .......... .......... .......... .......... 12% 47.9M 6s\n  8550K .......... .......... .......... .......... .......... 12% 2.78M 6s\n  8600K .......... .......... .......... .......... .......... 12% 4.81M 6s\n  8650K .......... .......... .......... .......... .......... 12% 73.2M 6s\n  8700K .......... .......... .......... .......... .......... 12% 64.3M 6s\n  8750K .......... .......... .......... .......... .......... 12% 47.2M 6s\n  8800K .......... .......... .......... .......... .......... 12% 2.72M 6s\n  8850K .......... .......... .......... .......... .......... 12% 90.5M 6s\n  8900K .......... .......... .......... .......... .......... 12% 4.65M 6s\n  8950K .......... .......... .......... .......... .......... 12%  134M 6s\n  9000K .......... .......... .......... .......... .......... 13% 46.2M 6s\n  9050K .......... .......... .......... .......... .......... 13% 57.7M 6s\n  9100K .......... .......... .......... .......... .......... 13% 2.76M 6s\n  9150K .......... .......... .......... .......... .......... 13% 4.65M 6s\n  9200K .......... .......... .......... .......... .......... 13% 79.8M 6s\n  9250K .......... .......... .......... .......... .......... 13% 67.9M 6s\n  9300K .......... .......... .......... .......... .......... 13% 56.5M 6s\n  9350K .......... .......... .......... .......... .......... 13% 44.3M 6s\n  9400K .......... .......... .......... .......... .......... 13% 2.79M 6s\n  9450K .......... .......... .......... .......... .......... 13% 4.83M 6s\n  9500K .......... .......... .......... .......... .......... 13% 65.2M 6s\n  9550K .......... .......... .......... .......... .......... 13% 47.2M 6s\n  9600K .......... .......... .......... .......... .......... 13% 50.1M 6s\n  9650K .......... .......... .......... .......... .......... 13% 2.80M 6s\n  9700K .......... .......... .......... .......... .......... 14% 63.5M 6s\n  9750K .......... .......... .......... .......... .......... 14% 4.76M 6s\n  9800K .......... .......... .......... .......... .......... 14% 36.0M 6s\n  9850K .......... .......... .......... .......... .......... 14%  169M 6s\n  9900K .......... .......... .......... .......... .......... 14% 59.4M 6s\n  9950K .......... .......... .......... .......... .......... 14% 2.74M 6s\n 10000K .......... .......... .......... .......... .......... 14% 4.82M 6s\n 10050K .......... .......... .......... .......... .......... 14% 70.0M 6s\n 10100K .......... .......... .......... .......... .......... 14% 56.8M 6s\n 10150K .......... .......... .......... .......... .......... 14% 53.9M 6s\n 10200K .......... .......... .......... .......... .......... 14% 48.0M 6s\n 10250K .......... .......... .......... .......... .......... 14% 2.81M 6s\n 10300K .......... .......... .......... .......... .......... 14% 4.75M 6s\n 10350K .......... .......... .......... .......... .......... 14% 58.7M 6s\n 10400K .......... .......... .......... .......... .......... 15% 58.4M 6s\n 10450K .......... .......... .......... .......... .......... 15% 64.1M 6s\n 10500K .......... .......... .......... .......... .......... 15% 2.73M 6s\n 10550K .......... .......... .......... .......... .......... 15% 69.8M 6s\n 10600K .......... .......... .......... .......... .......... 15% 4.85M 6s\n 10650K .......... .......... .......... .......... .......... 15% 76.8M 6s\n 10700K .......... .......... .......... .......... .......... 15% 54.9M 6s\n 10750K .......... .......... .......... .......... .......... 15% 12.4M 6s\n 10800K .......... .......... .......... .......... .......... 15% 3.29M 6s\n 10850K .......... .......... .......... .......... .......... 15% 4.81M 6s\n 10900K .......... .......... .......... .......... .......... 15% 65.3M 6s\n 10950K .......... .......... .......... .......... .......... 15% 79.4M 6s\n 11000K .......... .......... .......... .......... .......... 15% 50.2M 6s\n 11050K .......... .......... .......... .......... .......... 15% 13.2M 6s\n 11100K .......... .......... .......... .......... .......... 16% 3.31M 6s\n 11150K .......... .......... .......... .......... .......... 16% 4.73M 6s\n 11200K .......... .......... .......... .......... .......... 16% 59.9M 6s\n 11250K .......... .......... .......... .......... .......... 16% 70.2M 6s\n 11300K .......... .......... .......... .......... .......... 16% 56.3M 6s\n 11350K .......... .......... .......... .......... .......... 16% 13.6M 6s\n 11400K .......... .......... .......... .......... .......... 16% 3.29M 6s\n 11450K .......... .......... .......... .......... .......... 16% 4.83M 6s\n 11500K .......... .......... .......... .......... .......... 16% 59.8M 6s\n 11550K .......... .......... .......... .......... .......... 16% 51.1M 6s\n 11600K .......... .......... .......... .......... .......... 16% 51.0M 6s\n 11650K .......... .......... .......... .......... .......... 16% 2.76M 6s\n 11700K .......... .......... .......... .......... .......... 16% 92.3M 6s\n 11750K .......... .......... .......... .......... .......... 16% 4.58M 6s\n 11800K .......... .......... .......... .......... .......... 17%  112M 6s\n 11850K .......... .......... .......... .......... .......... 17% 74.4M 6s\n 11900K .......... .......... .......... .......... .......... 17% 49.3M 6s\n 11950K .......... .......... .......... .......... .......... 17% 2.78M 6s\n 12000K .......... .......... .......... .......... .......... 17% 4.83M 6s\n 12050K .......... .......... .......... .......... .......... 17% 46.8M 6s\n 12100K .......... .......... .......... .......... .......... 17% 86.0M 6s\n 12150K .......... .......... .......... .......... .......... 17% 54.1M 6s\n 12200K .......... .......... .......... .......... .......... 17% 12.8M 6s\n 12250K .......... .......... .......... .......... .......... 17% 3.37M 6s\n 12300K .......... .......... .......... .......... .......... 17% 4.67M 6s\n 12350K .......... .......... .......... .......... .......... 17% 58.7M 6s\n 12400K .......... .......... .......... .......... .......... 17% 76.5M 6s\n 12450K .......... .......... .......... .......... .......... 17% 33.5M 6s\n 12500K .......... .......... .......... .......... .......... 18% 16.2M 6s\n 12550K .......... .......... .......... .......... .......... 18% 3.28M 6s\n 12600K .......... .......... .......... .......... .......... 18% 4.92M 6s\n 12650K .......... .......... .......... .......... .......... 18% 51.0M 6s\n 12700K .......... .......... .......... .......... .......... 18% 73.1M 6s\n 12750K .......... .......... .......... .......... .......... 18% 43.0M 6s\n 12800K .......... .......... .......... .......... .......... 18% 14.2M 6s\n 12850K .......... .......... .......... .......... .......... 18% 3.28M 6s\n 12900K .......... .......... .......... .......... .......... 18% 4.89M 6s\n 12950K .......... .......... .......... .......... .......... 18% 55.4M 6s\n 13000K .......... .......... .......... .......... .......... 18% 67.2M 6s\n 13050K .......... .......... .......... .......... .......... 18% 52.1M 6s\n 13100K .......... .......... .......... .......... .......... 18% 14.0M 6s\n 13150K .......... .......... .......... .......... .......... 18% 3.28M 6s\n 13200K .......... .......... .......... .......... .......... 19% 4.76M 6s\n 13250K .......... .......... .......... .......... .......... 19% 59.4M 6s\n 13300K .......... .......... .......... .......... .......... 19% 34.7M 6s\n 13350K .......... .......... .......... .......... .......... 19%  136M 6s\n 13400K .......... .......... .......... .......... .......... 19% 2.79M 6s\n 13450K .......... .......... .......... .......... .......... 19% 71.0M 6s\n 13500K .......... .......... .......... .......... .......... 19% 4.91M 6s\n 13550K .......... .......... .......... .......... .......... 19% 50.9M 6s\n 13600K .......... .......... .......... .......... .......... 19% 56.4M 6s\n 13650K .......... .......... .......... .......... .......... 19% 12.4M 6s\n 13700K .......... .......... .......... .......... .......... 19% 3.34M 6s\n 13750K .......... .......... .......... .......... .......... 19% 68.6M 6s\n 13800K .......... .......... .......... .......... .......... 19% 4.85M 6s\n 13850K .......... .......... .......... .......... .......... 19% 64.1M 6s\n 13900K .......... .......... .......... .......... .......... 20% 58.1M 6s\n 13950K .......... .......... .......... .......... .......... 20% 12.3M 6s\n 14000K .......... .......... .......... .......... .......... 20% 3.34M 6s\n 14050K .......... .......... .......... .......... .......... 20% 4.77M 6s\n 14100K .......... .......... .......... .......... .......... 20% 90.9M 6s\n 14150K .......... .......... .......... .......... .......... 20% 57.5M 6s\n 14200K .......... .......... .......... .......... .......... 20% 58.2M 6s\n 14250K .......... .......... .......... .......... .......... 20% 13.0M 6s\n 14300K .......... .......... .......... .......... .......... 20% 3.32M 6s\n 14350K .......... .......... .......... .......... .......... 20% 4.82M 6s\n 14400K .......... .......... .......... .......... .......... 20% 60.7M 6s\n 14450K .......... .......... .......... .......... .......... 20% 70.1M 6s\n 14500K .......... .......... .......... .......... .......... 20% 53.4M 6s\n 14550K .......... .......... .......... .......... .......... 20% 12.3M 6s\n 14600K .......... .......... .......... .......... .......... 21% 3.38M 6s\n 14650K .......... .......... .......... .......... .......... 21% 4.83M 6s\n 14700K .......... .......... .......... .......... .......... 21% 85.0M 6s\n 14750K .......... .......... .......... .......... .......... 21% 49.1M 6s\n 14800K .......... .......... .......... .......... .......... 21% 51.5M 6s\n 14850K .......... .......... .......... .......... .......... 21% 13.9M 6s\n 14900K .......... .......... .......... .......... .......... 21% 3.32M 6s\n 14950K .......... .......... .......... .......... .......... 21% 4.86M 6s\n 15000K .......... .......... .......... .......... .......... 21% 59.7M 6s\n 15050K .......... .......... .......... .......... .......... 21% 60.5M 6s\n 15100K .......... .......... .......... .......... .......... 21% 57.4M 6s\n 15150K .......... .......... .......... .......... .......... 21% 12.8M 6s\n 15200K .......... .......... .......... .......... .......... 21% 3.34M 6s\n 15250K .......... .......... .......... .......... .......... 21% 4.80M 6s\n 15300K .......... .......... .......... .......... .......... 22% 68.8M 6s\n 15350K .......... .......... .......... .......... .......... 22% 70.7M 6s\n 15400K .......... .......... .......... .......... .......... 22% 50.8M 6s\n 15450K .......... .......... .......... .......... .......... 22% 13.6M 6s\n 15500K .......... .......... .......... .......... .......... 22% 3.29M 6s\n 15550K .......... .......... .......... .......... .......... 22% 4.75M 6s\n 15600K .......... .......... .......... .......... .......... 22% 79.2M 6s\n 15650K .......... .......... .......... .......... .......... 22% 59.9M 6s\n 15700K .......... .......... .......... .......... .......... 22% 54.7M 6s\n 15750K .......... .......... .......... .......... .......... 22% 13.9M 6s\n 15800K .......... .......... .......... .......... .......... 22% 3.22M 6s\n 15850K .......... .......... .......... .......... .......... 22% 5.08M 6s\n 15900K .......... .......... .......... .......... .......... 22% 66.0M 6s\n 15950K .......... .......... .......... .......... .......... 22% 42.9M 6s\n 16000K .......... .......... .......... .......... .......... 23% 56.3M 6s\n 16050K .......... .......... .......... .......... .......... 23% 14.5M 6s\n 16100K .......... .......... .......... .......... .......... 23% 3.33M 6s\n 16150K .......... .......... .......... .......... .......... 23% 4.75M 6s\n 16200K .......... .......... .......... .......... .......... 23% 64.3M 6s\n 16250K .......... .......... .......... .......... .......... 23% 76.7M 6s\n 16300K .......... .......... .......... .......... .......... 23% 54.5M 6s\n 16350K .......... .......... .......... .......... .......... 23% 12.7M 6s\n 16400K .......... .......... .......... .......... .......... 23% 3.35M 6s\n 16450K .......... .......... .......... .......... .......... 23% 4.78M 6s\n 16500K .......... .......... .......... .......... .......... 23% 64.9M 6s\n 16550K .......... .......... .......... .......... .......... 23% 82.4M 5s\n 16600K .......... .......... .......... .......... .......... 23% 47.8M 5s\n 16650K .......... .......... .......... .......... .......... 24% 13.9M 5s\n 16700K .......... .......... .......... .......... .......... 24% 3.20M 5s\n 16750K .......... .......... .......... .......... .......... 24% 5.02M 6s\n 16800K .......... .......... .......... .......... .......... 24% 68.3M 5s\n 16850K .......... .......... .......... .......... .......... 24% 62.1M 5s\n 16900K .......... .......... .......... .......... .......... 24% 55.4M 5s\n 16950K .......... .......... .......... .......... .......... 24% 13.8M 5s\n 17000K .......... .......... .......... .......... .......... 24% 3.27M 5s\n 17050K .......... .......... .......... .......... .......... 24% 4.96M 5s\n 17100K .......... .......... .......... .......... .......... 24% 67.8M 5s\n 17150K .......... .......... .......... .......... .......... 24% 21.2M 5s\n 17200K .......... .......... .......... .......... .......... 24%  175M 5s\n 17250K .......... .......... .......... .......... .......... 24% 17.7M 5s\n 17300K .......... .......... .......... .......... .......... 24% 3.25M 5s\n 17350K .......... .......... .......... .......... .......... 25% 4.99M 5s\n 17400K .......... .......... .......... .......... .......... 25% 61.6M 5s\n 17450K .......... .......... .......... .......... .......... 25% 24.5M 5s\n 17500K .......... .......... .......... .......... .......... 25%  176M 5s\n 17550K .......... .......... .......... .......... .......... 25% 15.4M 5s\n 17600K .......... .......... .......... .......... .......... 25% 3.29M 5s\n 17650K .......... .......... .......... .......... .......... 25% 4.94M 5s\n 17700K .......... .......... .......... .......... .......... 25% 68.9M 5s\n 17750K .......... .......... .......... .......... .......... 25% 25.4M 5s\n 17800K .......... .......... .......... .......... .......... 25%  146M 5s\n 17850K .......... .......... .......... .......... .......... 25% 16.6M 5s\n 17900K .......... .......... .......... .......... .......... 25% 3.23M 5s\n 17950K .......... .......... .......... .......... .......... 25% 4.94M 5s\n 18000K .......... .......... .......... .......... .......... 25% 61.4M 5s\n 18050K .......... .......... .......... .......... .......... 26% 26.4M 5s\n 18100K .......... .......... .......... .......... .......... 26%  183M 5s\n 18150K .......... .......... .......... .......... .......... 26% 15.9M 5s\n 18200K .......... .......... .......... .......... .......... 26% 3.21M 5s\n 18250K .......... .......... .......... .......... .......... 26% 5.11M 5s\n 18300K .......... .......... .......... .......... .......... 26% 76.5M 5s\n 18350K .......... .......... .......... .......... .......... 26% 21.0M 5s\n 18400K .......... .......... .......... .......... .......... 26%  147M 5s\n 18450K .......... .......... .......... .......... .......... 26% 17.7M 5s\n 18500K .......... .......... .......... .......... .......... 26% 3.25M 5s\n 18550K .......... .......... .......... .......... .......... 26% 4.97M 5s\n 18600K .......... .......... .......... .......... .......... 26% 82.8M 5s\n 18650K .......... .......... .......... .......... .......... 26% 59.4M 5s\n 18700K .......... .......... .......... .......... .......... 26% 28.0M 5s\n 18750K .......... .......... .......... .......... .......... 27% 16.7M 5s\n 18800K .......... .......... .......... .......... .......... 27% 3.31M 5s\n 18850K .......... .......... .......... .......... .......... 27% 4.82M 5s\n 18900K .......... .......... .......... .......... .......... 27% 84.6M 5s\n 18950K .......... .......... .......... .......... .......... 27% 67.2M 5s\n 19000K .......... .......... .......... .......... .......... 27% 25.7M 5s\n 19050K .......... .......... .......... .......... .......... 27% 18.4M 5s\n 19100K .......... .......... .......... .......... .......... 27% 3.33M 5s\n 19150K .......... .......... .......... .......... .......... 27% 4.72M 5s\n 19200K .......... .......... .......... .......... .......... 27% 77.6M 5s\n 19250K .......... .......... .......... .......... .......... 27% 65.9M 5s\n 19300K .......... .......... .......... .......... .......... 27% 28.0M 5s\n 19350K .......... .......... .......... .......... .......... 27% 18.0M 5s\n 19400K .......... .......... .......... .......... .......... 27% 3.28M 5s\n 19450K .......... .......... .......... .......... .......... 28% 75.6M 5s\n 19500K .......... .......... .......... .......... .......... 28% 4.88M 5s\n 19550K .......... .......... .......... .......... .......... 28% 68.8M 5s\n 19600K .......... .......... .......... .......... .......... 28% 23.7M 5s\n 19650K .......... .......... .......... .......... .......... 28% 19.2M 5s\n 19700K .......... .......... .......... .......... .......... 28% 3.25M 5s\n 19750K .......... .......... .......... .......... .......... 28%  101M 5s\n 19800K .......... .......... .......... .......... .......... 28% 4.86M 5s\n 19850K .......... .......... .......... .......... .......... 28% 80.3M 5s\n 19900K .......... .......... .......... .......... .......... 28% 24.0M 5s\n 19950K .......... .......... .......... .......... .......... 28%  102M 5s\n 20000K .......... .......... .......... .......... .......... 28% 2.86M 5s\n 20050K .......... .......... .......... .......... .......... 28% 70.4M 5s\n 20100K .......... .......... .......... .......... .......... 28% 4.88M 5s\n 20150K .......... .......... .......... .......... .......... 29% 76.7M 5s\n 20200K .......... .......... .......... .......... .......... 29% 23.7M 5s\n 20250K .......... .......... .......... .......... .......... 29%  146M 5s\n 20300K .......... .......... .......... .......... .......... 29% 18.1M 5s\n 20350K .......... .......... .......... .......... .......... 29% 3.23M 5s\n 20400K .......... .......... .......... .......... .......... 29% 4.88M 5s\n 20450K .......... .......... .......... .......... .......... 29% 57.8M 5s\n 20500K .......... .......... .......... .......... .......... 29% 25.5M 5s\n 20550K .......... .......... .......... .......... .......... 29%  124M 5s\n 20600K .......... .......... .......... .......... .......... 29% 19.0M 5s\n 20650K .......... .......... .......... .......... .......... 29% 3.28M 5s\n 20700K .......... .......... .......... .......... .......... 29% 4.86M 5s\n 20750K .......... .......... .......... .......... .......... 29% 53.6M 5s\n 20800K .......... .......... .......... .......... .......... 29% 22.8M 5s\n 20850K .......... .......... .......... .......... .......... 30% 88.6M 5s\n 20900K .......... .......... .......... .......... .......... 30% 21.2M 5s\n 20950K .......... .......... .......... .......... .......... 30% 3.29M 5s\n 21000K .......... .......... .......... .......... .......... 30% 4.82M 5s\n 21050K .......... .......... .......... .......... .......... 30% 42.8M 5s\n 21100K .......... .......... .......... .......... .......... 30%  112M 5s\n 21150K .......... .......... .......... .......... .......... 30% 27.0M 5s\n 21200K .......... .......... .......... .......... .......... 30% 19.4M 5s\n 21250K .......... .......... .......... .......... .......... 30% 3.25M 5s\n 21300K .......... .......... .......... .......... .......... 30% 85.5M 5s\n 21350K .......... .......... .......... .......... .......... 30% 4.88M 5s\n 21400K .......... .......... .......... .......... .......... 30% 57.7M 5s\n 21450K .......... .......... .......... .......... .......... 30% 24.7M 5s\n 21500K .......... .......... .......... .......... .......... 30%  122M 5s\n 21550K .......... .......... .......... .......... .......... 31% 2.87M 5s\n 21600K .......... .......... .......... .......... .......... 31% 80.1M 5s\n 21650K .......... .......... .......... .......... .......... 31% 4.88M 5s\n 21700K .......... .......... .......... .......... .......... 31% 56.5M 5s\n 21750K .......... .......... .......... .......... .......... 31% 27.7M 5s\n 21800K .......... .......... .......... .......... .......... 31% 84.3M 5s\n 21850K .......... .......... .......... .......... .......... 31% 18.6M 5s\n 21900K .......... .......... .......... .......... .......... 31% 3.30M 5s\n 21950K .......... .......... .......... .......... .......... 31% 4.80M 5s\n 22000K .......... .......... .......... .......... .......... 31% 59.9M 5s\n 22050K .......... .......... .......... .......... .......... 31% 25.8M 5s\n 22100K .......... .......... .......... .......... .......... 31% 89.8M 5s\n 22150K .......... .......... .......... .......... .......... 31% 20.4M 5s\n 22200K .......... .......... .......... .......... .......... 31% 3.21M 5s\n 22250K .......... .......... .......... .......... .......... 32% 4.93M 5s\n 22300K .......... .......... .......... .......... .......... 32% 67.3M 5s\n 22350K .......... .......... .......... .......... .......... 32% 21.4M 5s\n 22400K .......... .......... .......... .......... .......... 32%  155M 5s\n 22450K .......... .......... .......... .......... .......... 32% 20.0M 5s\n 22500K .......... .......... .......... .......... .......... 32% 3.27M 5s\n 22550K .......... .......... .......... .......... .......... 32% 4.90M 5s\n 22600K .......... .......... .......... .......... .......... 32% 58.6M 5s\n 22650K .......... .......... .......... .......... .......... 32% 67.5M 5s\n 22700K .......... .......... .......... .......... .......... 32% 27.5M 5s\n 22750K .......... .......... .......... .......... .......... 32% 18.3M 5s\n 22800K .......... .......... .......... .......... .......... 32% 3.30M 5s\n 22850K .......... .......... .......... .......... .......... 32% 4.81M 5s\n 22900K .......... .......... .......... .......... .......... 32% 74.1M 5s\n 22950K .......... .......... .......... .......... .......... 33% 65.8M 5s\n 23000K .......... .......... .......... .......... .......... 33% 26.9M 5s\n 23050K .......... .......... .......... .......... .......... 33% 20.5M 5s\n 23100K .......... .......... .......... .......... .......... 33% 64.8M 5s\n 23150K .......... .......... .......... .......... .......... 33% 3.22M 5s\n 23200K .......... .......... .......... .......... .......... 33% 4.91M 5s\n 23250K .......... .......... .......... .......... .......... 33% 66.3M 5s\n 23300K .......... .......... .......... .......... .......... 33% 8.79M 5s\n 23350K .......... .......... .......... .......... .......... 33%  186M 5s\n 23400K .......... .......... .......... .......... .......... 33%  222M 5s\n 23450K .......... .......... .......... .......... .......... 33% 3.48M 5s\n 23500K .......... .......... .......... .......... .......... 33% 5.06M 5s\n 23550K .......... .......... .......... .......... .......... 33% 44.0M 5s\n 23600K .......... .......... .......... .......... .......... 33% 26.1M 5s\n 23650K .......... .......... .......... .......... .......... 34%  104M 5s\n 23700K .......... .......... .......... .......... .......... 34% 20.2M 5s\n 23750K .......... .......... .......... .......... .......... 34% 3.24M 5s\n 23800K .......... .......... .......... .......... .......... 34% 4.84M 5s\n 23850K .......... .......... .......... .......... .......... 34% 83.0M 5s\n 23900K .......... .......... .......... .......... .......... 34% 65.3M 5s\n 23950K .......... .......... .......... .......... .......... 34% 24.2M 5s\n 24000K .......... .......... .......... .......... .......... 34% 21.2M 5s\n 24050K .......... .......... .......... .......... .......... 34% 3.23M 5s\n 24100K .......... .......... .......... .......... .......... 34% 4.99M 5s\n 24150K .......... .......... .......... .......... .......... 34% 57.1M 5s\n 24200K .......... .......... .......... .......... .......... 34% 77.8M 5s\n 24250K .......... .......... .......... .......... .......... 34% 24.7M 5s\n 24300K .......... .......... .......... .......... .......... 34% 19.9M 5s\n 24350K .......... .......... .......... .......... .......... 35% 3.25M 5s\n 24400K .......... .......... .......... .......... .......... 35% 66.5M 5s\n 24450K .......... .......... .......... .......... .......... 35% 4.99M 5s\n 24500K .......... .......... .......... .......... .......... 35% 62.1M 5s\n 24550K .......... .......... .......... .......... .......... 35% 23.8M 5s\n 24600K .......... .......... .......... .......... .......... 35%  104M 5s\n 24650K .......... .......... .......... .......... .......... 35% 19.9M 5s\n 24700K .......... .......... .......... .......... .......... 35% 3.23M 5s\n 24750K .......... .......... .......... .......... .......... 35% 4.89M 5s\n 24800K .......... .......... .......... .......... .......... 35% 58.2M 5s\n 24850K .......... .......... .......... .......... .......... 35% 25.6M 5s\n 24900K .......... .......... .......... .......... .......... 35%  105M 5s\n 24950K .......... .......... .......... .......... .......... 35% 19.9M 5s\n 25000K .......... .......... .......... .......... .......... 36% 3.28M 5s\n 25050K .......... .......... .......... .......... .......... 36% 4.89M 5s\n 25100K .......... .......... .......... .......... .......... 36% 74.5M 5s\n 25150K .......... .......... .......... .......... .......... 36% 20.3M 5s\n 25200K .......... .......... .......... .......... .......... 36%  116M 5s\n 25250K .......... .......... .......... .......... .......... 36% 20.3M 5s\n 25300K .......... .......... .......... .......... .......... 36% 3.28M 5s\n 25350K .......... .......... .......... .......... .......... 36% 53.9M 5s\n 25400K .......... .......... .......... .......... .......... 36% 4.93M 5s\n 25450K .......... .......... .......... .......... .......... 36% 84.6M 5s\n 25500K .......... .......... .......... .......... .......... 36% 24.2M 5s\n 25550K .......... .......... .......... .......... .......... 36% 19.7M 5s\n 25600K .......... .......... .......... .......... .......... 36% 3.30M 5s\n 25650K .......... .......... .......... .......... .......... 36% 58.2M 5s\n 25700K .......... .......... .......... .......... .......... 37% 4.94M 5s\n 25750K .......... .......... .......... .......... .......... 37% 64.0M 4s\n 25800K .......... .......... .......... .......... .......... 37% 23.8M 4s\n 25850K .......... .......... .......... .......... .......... 37%  106M 4s\n 25900K .......... .......... .......... .......... .......... 37% 19.8M 4s\n 25950K .......... .......... .......... .......... .......... 37% 3.31M 4s\n 26000K .......... .......... .......... .......... .......... 37% 4.81M 4s\n 26050K .......... .......... .......... .......... .......... 37% 54.2M 4s\n 26100K .......... .......... .......... .......... .......... 37% 25.1M 4s\n 26150K .......... .......... .......... .......... .......... 37% 77.5M 4s\n 26200K .......... .......... .......... .......... .......... 37% 21.2M 4s\n 26250K .......... .......... .......... .......... .......... 37% 3.30M 4s\n 26300K .......... .......... .......... .......... .......... 37% 44.0M 4s\n 26350K .......... .......... .......... .......... .......... 37% 5.03M 4s\n 26400K .......... .......... .......... .......... .......... 38% 57.5M 4s\n 26450K .......... .......... .......... .......... .......... 38% 25.0M 4s\n 26500K .......... .......... .......... .......... .......... 38% 21.1M 4s\n 26550K .......... .......... .......... .......... .......... 38% 99.3M 4s\n 26600K .......... .......... .......... .......... .......... 38% 3.24M 4s\n 26650K .......... .......... .......... .......... .......... 38% 4.97M 4s\n 26700K .......... .......... .......... .......... .......... 38% 59.2M 4s\n 26750K .......... .......... .......... .......... .......... 38% 22.7M 4s\n 26800K .......... .......... .......... .......... .......... 38% 83.8M 4s\n 26850K .......... .......... .......... .......... .......... 38% 21.5M 4s\n 26900K .......... .......... .......... .......... .......... 38% 3.29M 4s\n 26950K .......... .......... .......... .......... .......... 38% 4.91M 4s\n 27000K .......... .......... .......... .......... .......... 38% 74.4M 4s\n 27050K .......... .......... .......... .......... .......... 38% 55.7M 4s\n 27100K .......... .......... .......... .......... .......... 39% 23.5M 4s\n 27150K .......... .......... .......... .......... .......... 39% 19.7M 4s\n 27200K .......... .......... .......... .......... .......... 39% 3.24M 4s\n 27250K .......... .......... .......... .......... .......... 39%  125M 4s\n 27300K .......... .......... .......... .......... .......... 39% 4.95M 4s\n 27350K .......... .......... .......... .......... .......... 39% 65.0M 4s\n 27400K .......... .......... .......... .......... .......... 39% 23.9M 4s\n 27450K .......... .......... .......... .......... .......... 39% 82.7M 4s\n 27500K .......... .......... .......... .......... .......... 39% 19.8M 4s\n 27550K .......... .......... .......... .......... .......... 39% 3.27M 4s\n 27600K .......... .......... .......... .......... .......... 39% 4.97M 4s\n 27650K .......... .......... .......... .......... .......... 39% 55.7M 4s\n 27700K .......... .......... .......... .......... .......... 39% 24.5M 4s\n 27750K .......... .......... .......... .......... .......... 39% 93.7M 4s\n 27800K .......... .......... .......... .......... .......... 40% 18.9M 4s\n 27850K .......... .......... .......... .......... .......... 40% 3.28M 4s\n 27900K .......... .......... .......... .......... .......... 40%  102M 4s\n 27950K .......... .......... .......... .......... .......... 40% 4.89M 4s\n 28000K .......... .......... .......... .......... .......... 40% 63.5M 4s\n 28050K .......... .......... .......... .......... .......... 40% 23.3M 4s\n 28100K .......... .......... .......... .......... .......... 40% 20.7M 4s\n 28150K .......... .......... .......... .......... .......... 40% 87.2M 4s\n 28200K .......... .......... .......... .......... .......... 40% 3.27M 4s\n 28250K .......... .......... .......... .......... .......... 40% 4.99M 4s\n 28300K .......... .......... .......... .......... .......... 40% 66.1M 4s\n 28350K .......... .......... .......... .......... .......... 40% 21.7M 4s\n 28400K .......... .......... .......... .......... .......... 40% 83.6M 4s\n 28450K .......... .......... .......... .......... .......... 40% 20.2M 4s\n 28500K .......... .......... .......... .......... .......... 41% 3.25M 4s\n 28550K .......... .......... .......... .......... .......... 41% 4.97M 4s\n 28600K .......... .......... .......... .......... .......... 41%  117M 4s\n 28650K .......... .......... .......... .......... .......... 41% 58.2M 4s\n 28700K .......... .......... .......... .......... .......... 41% 23.7M 4s\n 28750K .......... .......... .......... .......... .......... 41% 20.7M 4s\n 28800K .......... .......... .......... .......... .......... 41% 3.21M 4s\n 28850K .......... .......... .......... .......... .......... 41% 69.6M 4s\n 28900K .......... .......... .......... .......... .......... 41% 5.12M 4s\n 28950K .......... .......... .......... .......... .......... 41% 51.6M 4s\n 29000K .......... .......... .......... .......... .......... 41% 25.5M 4s\n 29050K .......... .......... .......... .......... .......... 41% 98.8M 4s\n 29100K .......... .......... .......... .......... .......... 41% 20.2M 4s\n 29150K .......... .......... .......... .......... .......... 41% 3.21M 4s\n 29200K .......... .......... .......... .......... .......... 42% 4.97M 4s\n 29250K .......... .......... .......... .......... .......... 42%  111M 4s\n 29300K .......... .......... .......... .......... .......... 42% 59.7M 4s\n 29350K .......... .......... .......... .......... .......... 42% 22.5M 4s\n 29400K .......... .......... .......... .......... .......... 42% 20.6M 4s\n 29450K .......... .......... .......... .......... .......... 42%  116M 4s\n 29500K .......... .......... .......... .......... .......... 42% 3.24M 4s\n 29550K .......... .......... .......... .......... .......... 42% 5.00M 4s\n 29600K .......... .......... .......... .......... .......... 42% 54.0M 4s\n 29650K .......... .......... .......... .......... .......... 42% 24.7M 4s\n 29700K .......... .......... .......... .......... .......... 42% 86.6M 4s\n 29750K .......... .......... .......... .......... .......... 42% 19.6M 4s\n 29800K .......... .......... .......... .......... .......... 42% 3.29M 4s\n 29850K .......... .......... .......... .......... .......... 42% 4.96M 4s\n 29900K .......... .......... .......... .......... .......... 43%  115M 4s\n 29950K .......... .......... .......... .......... .......... 43% 21.4M 4s\n 30000K .......... .......... .......... .......... .......... 43% 55.0M 4s\n 30050K .......... .......... .......... .......... .......... 43% 22.7M 4s\n 30100K .......... .......... .......... .......... .......... 43% 3.30M 4s\n 30150K .......... .......... .......... .......... .......... 43% 52.2M 4s\n 30200K .......... .......... .......... .......... .......... 43% 5.05M 4s\n 30250K .......... .......... .......... .......... .......... 43% 76.9M 4s\n 30300K .......... .......... .......... .......... .......... 43% 21.2M 4s\n 30350K .......... .......... .......... .......... .......... 43% 64.8M 4s\n 30400K .......... .......... .......... .......... .......... 43% 21.2M 4s\n 30450K .......... .......... .......... .......... .......... 43% 3.29M 4s\n 30500K .......... .......... .......... .......... .......... 43% 4.96M 4s\n 30550K .......... .......... .......... .......... .......... 43% 97.4M 4s\n 30600K .......... .......... .......... .......... .......... 44% 55.4M 4s\n 30650K .......... .......... .......... .......... .......... 44% 24.3M 4s\n 30700K .......... .......... .......... .......... .......... 44% 65.3M 4s\n 30750K .......... .......... .......... .......... .......... 44% 2.91M 4s\n 30800K .......... .......... .......... .......... .......... 44% 68.5M 4s\n 30850K .......... .......... .......... .......... .......... 44% 5.14M 4s\n 30900K .......... .......... .......... .......... .......... 44% 77.4M 4s\n 30950K .......... .......... .......... .......... .......... 44% 21.5M 4s\n 31000K .......... .......... .......... .......... .......... 44% 75.8M 4s\n 31050K .......... .......... .......... .......... .......... 44% 21.9M 4s\n 31100K .......... .......... .......... .......... .......... 44% 3.23M 4s\n 31150K .......... .......... .......... .......... .......... 44% 4.96M 4s\n 31200K .......... .......... .......... .......... .......... 44%  125M 4s\n 31250K .......... .......... .......... .......... .......... 44% 60.0M 4s\n 31300K .......... .......... .......... .......... .......... 45% 24.3M 4s\n 31350K .......... .......... .......... .......... .......... 45% 62.3M 4s\n 31400K .......... .......... .......... .......... .......... 45% 22.9M 4s\n 31450K .......... .......... .......... .......... .......... 45% 3.28M 4s\n 31500K .......... .......... .......... .......... .......... 45% 4.88M 4s\n 31550K .......... .......... .......... .......... .......... 45% 76.0M 4s\n 31600K .......... .......... .......... .......... .......... 45% 23.1M 4s\n 31650K .......... .......... .......... .......... .......... 45% 58.8M 4s\n 31700K .......... .......... .......... .......... .......... 45% 23.9M 4s\n 31750K .......... .......... .......... .......... .......... 45% 3.22M 4s\n 31800K .......... .......... .......... .......... .......... 45% 87.3M 4s\n 31850K .......... .......... .......... .......... .......... 45% 5.10M 4s\n 31900K .......... .......... .......... .......... .......... 45% 68.4M 4s\n 31950K .......... .......... .......... .......... .......... 45% 20.7M 4s\n 32000K .......... .......... .......... .......... .......... 46% 69.1M 4s\n 32050K .......... .......... .......... .......... .......... 46% 22.9M 4s\n 32100K .......... .......... .......... .......... .......... 46% 3.28M 4s\n 32150K .......... .......... .......... .......... .......... 46% 4.84M 4s\n 32200K .......... .......... .......... .......... .......... 46%  168M 4s\n 32250K .......... .......... .......... .......... .......... 46% 52.7M 4s\n 32300K .......... .......... .......... .......... .......... 46% 25.8M 4s\n 32350K .......... .......... .......... .......... .......... 46% 58.7M 4s\n 32400K .......... .......... .......... .......... .......... 46% 2.93M 4s\n 32450K .......... .......... .......... .......... .......... 46% 67.5M 4s\n 32500K .......... .......... .......... .......... .......... 46% 5.04M 4s\n 32550K .......... .......... .......... .......... .......... 46% 84.7M 4s\n 32600K .......... .......... .......... .......... .......... 46% 23.6M 4s\n 32650K .......... .......... .......... .......... .......... 46% 58.5M 4s\n 32700K .......... .......... .......... .......... .......... 47% 23.1M 4s\n 32750K .......... .......... .......... .......... .......... 47% 3.27M 4s\n 32800K .......... .......... .......... .......... .......... 47% 55.6M 4s\n 32850K .......... .......... .......... .......... .......... 47% 5.00M 4s\n 32900K .......... .......... .......... .......... .......... 47% 72.5M 4s\n 32950K .......... .......... .......... .......... .......... 47% 24.5M 4s\n 33000K .......... .......... .......... .......... .......... 47% 65.6M 4s\n 33050K .......... .......... .......... .......... .......... 47% 22.4M 4s\n 33100K .......... .......... .......... .......... .......... 47% 3.26M 4s\n 33150K .......... .......... .......... .......... .......... 47% 5.01M 4s\n 33200K .......... .......... .......... .......... .......... 47% 61.9M 4s\n 33250K .......... .......... .......... .......... .......... 47% 57.2M 4s\n 33300K .......... .......... .......... .......... .......... 47% 25.3M 4s\n 33350K .......... .......... .......... .......... .......... 48% 23.3M 4s\n 33400K .......... .......... .......... .......... .......... 48% 70.6M 4s\n 33450K .......... .......... .......... .......... .......... 48% 3.27M 4s\n 33500K .......... .......... .......... .......... .......... 48% 4.94M 4s\n 33550K .......... .......... .......... .......... .......... 48% 57.0M 4s\n 33600K .......... .......... .......... .......... .......... 48% 23.6M 4s\n 33650K .......... .......... .......... .......... .......... 48% 80.1M 4s\n 33700K .......... .......... .......... .......... .......... 48% 24.5M 4s\n 33750K .......... .......... .......... .......... .......... 48% 3.19M 4s\n 33800K .......... .......... .......... .......... .......... 48% 78.0M 4s\n 33850K .......... .......... .......... .......... .......... 48% 5.04M 4s\n 33900K .......... .......... .......... .......... .......... 48% 72.0M 4s\n 33950K .......... .......... .......... .......... .......... 48% 23.7M 4s\n 34000K .......... .......... .......... .......... .......... 48% 69.8M 4s\n 34050K .......... .......... .......... .......... .......... 49% 22.0M 4s\n 34100K .......... .......... .......... .......... .......... 49% 3.30M 4s\n 34150K .......... .......... .......... .......... .......... 49% 4.90M 4s\n 34200K .......... .......... .......... .......... .......... 49% 95.7M 4s\n 34250K .......... .......... .......... .......... .......... 49% 54.3M 4s\n 34300K .......... .......... .......... .......... .......... 49% 26.1M 4s\n 34350K .......... .......... .......... .......... .......... 49% 20.5M 4s\n 34400K .......... .......... .......... .......... .......... 49% 3.27M 4s\n 34450K .......... .......... .......... .......... .......... 49% 79.1M 4s\n 34500K .......... .......... .......... .......... .......... 49% 4.91M 4s\n 34550K .......... .......... .......... .......... .......... 49%  102M 4s\n 34600K .......... .......... .......... .......... .......... 49% 50.0M 4s\n 34650K .......... .......... .......... .......... .......... 49% 25.7M 4s\n 34700K .......... .......... .......... .......... .......... 49% 23.6M 3s\n 34750K .......... .......... .......... .......... .......... 50% 3.19M 4s\n 34800K .......... .......... .......... .......... .......... 50% 80.3M 3s\n 34850K .......... .......... .......... .......... .......... 50% 5.09M 3s\n 34900K .......... .......... .......... .......... .......... 50% 78.1M 3s\n 34950K .......... .......... .......... .......... .......... 50% 18.5M 3s\n 35000K .......... .......... .......... .......... .......... 50%  207M 3s\n 35050K .......... .......... .......... .......... .......... 50% 24.5M 3s\n 35100K .......... .......... .......... .......... .......... 50% 3.27M 3s\n 35150K .......... .......... .......... .......... .......... 50% 42.2M 3s\n 35200K .......... .......... .......... .......... .......... 50% 5.24M 3s\n 35250K .......... .......... .......... .......... .......... 50% 46.5M 3s\n 35300K .......... .......... .......... .......... .......... 50% 26.0M 3s\n 35350K .......... .......... .......... .......... .......... 50% 64.0M 3s\n 35400K .......... .......... .......... .......... .......... 50% 23.7M 3s\n 35450K .......... .......... .......... .......... .......... 51% 3.29M 3s\n 35500K .......... .......... .......... .......... .......... 51% 50.6M 3s\n 35550K .......... .......... .......... .......... .......... 51% 4.95M 3s\n 35600K .......... .......... .......... .......... .......... 51% 50.5M 3s\n 35650K .......... .......... .......... .......... .......... 51% 27.7M 3s\n 35700K .......... .......... .......... .......... .......... 51% 23.2M 3s\n 35750K .......... .......... .......... .......... .......... 51% 73.8M 3s\n 35800K .......... .......... .......... .......... .......... 51% 3.31M 3s\n 35850K .......... .......... .......... .......... .......... 51% 4.03M 3s\n 35900K .......... .......... .......... .......... .......... 51%  211M 3s\n 35950K .......... .......... .......... .......... .......... 51%  116M 3s\n 36000K .......... .......... .......... .......... .......... 51% 62.4M 3s\n 36050K .......... .......... .......... .......... .......... 51% 24.1M 3s\n 36100K .......... .......... .......... .......... .......... 51% 3.34M 3s\n 36150K .......... .......... .......... .......... .......... 52% 58.6M 3s\n 36200K .......... .......... .......... .......... .......... 52% 4.89M 3s\n 36250K .......... .......... .......... .......... .......... 52%  100M 3s\n 36300K .......... .......... .......... .......... .......... 52% 25.8M 3s\n 36350K .......... .......... .......... .......... .......... 52% 27.8M 3s\n 36400K .......... .......... .......... .......... .......... 52% 35.2M 3s\n 36450K .......... .......... .......... .......... .......... 52% 3.29M 3s\n 36500K .......... .......... .......... .......... .......... 52% 53.3M 3s\n 36550K .......... .......... .......... .......... .......... 52% 5.01M 3s\n 36600K .......... .......... .......... .......... .......... 52% 88.1M 3s\n 36650K .......... .......... .......... .......... .......... 52% 24.2M 3s\n 36700K .......... .......... .......... .......... .......... 52% 53.2M 3s\n 36750K .......... .......... .......... .......... .......... 52% 24.2M 3s\n 36800K .......... .......... .......... .......... .......... 52% 3.24M 3s\n 36850K .......... .......... .......... .......... .......... 53% 57.4M 3s\n 36900K .......... .......... .......... .......... .......... 53% 5.19M 3s\n 36950K .......... .......... .......... .......... .......... 53% 58.3M 3s\n 37000K .......... .......... .......... .......... .......... 53% 24.2M 3s\n 37050K .......... .......... .......... .......... .......... 53% 52.1M 3s\n 37100K .......... .......... .......... .......... .......... 53% 20.1M 3s\n 37150K .......... .......... .......... .......... .......... 53% 3.39M 3s\n 37200K .......... .......... .......... .......... .......... 53% 4.82M 3s\n 37250K .......... .......... .......... .......... .......... 53%  196M 3s\n 37300K .......... .......... .......... .......... .......... 53% 56.4M 3s\n 37350K .......... .......... .......... .......... .......... 53% 24.5M 3s\n 37400K .......... .......... .......... .......... .......... 53% 62.5M 3s\n 37450K .......... .......... .......... .......... .......... 53% 22.6M 3s\n 37500K .......... .......... .......... .......... .......... 53% 3.34M 3s\n 37550K .......... .......... .......... .......... .......... 54% 4.71M 3s\n 37600K .......... .......... .......... .......... .......... 54%  205M 3s\n 37650K .......... .......... .......... .......... .......... 54% 51.3M 3s\n 37700K .......... .......... .......... .......... .......... 54% 24.8M 3s\n 37750K .......... .......... .......... .......... .......... 54% 23.1M 3s\n 37800K .......... .......... .......... .......... .......... 54% 54.1M 3s\n 37850K .......... .......... .......... .......... .......... 54% 3.39M 3s\n 37900K .......... .......... .......... .......... .......... 54% 4.89M 3s\n 37950K .......... .......... .......... .......... .......... 54% 66.0M 3s\n 38000K .......... .......... .......... .......... .......... 54% 54.5M 3s\n 38050K .......... .......... .......... .......... .......... 54% 22.2M 3s\n 38100K .......... .......... .......... .......... .......... 54% 25.3M 3s\n 38150K .......... .......... .......... .......... .......... 54% 47.2M 3s\n 38200K .......... .......... .......... .......... .......... 54% 3.43M 3s\n 38250K .......... .......... .......... .......... .......... 55% 4.85M 3s\n 38300K .......... .......... .......... .......... .......... 55%  144M 3s\n 38350K .......... .......... .......... .......... .......... 55% 21.8M 3s\n 38400K .......... .......... .......... .......... .......... 55% 35.5M 3s\n 38450K .......... .......... .......... .......... .......... 55% 28.7M 3s\n 38500K .......... .......... .......... .......... .......... 55% 3.27M 3s\n 38550K .......... .......... .......... .......... .......... 55%  137M 3s\n 38600K .......... .......... .......... .......... .......... 55% 4.92M 3s\n 38650K .......... .......... .......... .......... .......... 55%  101M 3s\n 38700K .......... .......... .......... .......... .......... 55% 45.1M 3s\n 38750K .......... .......... .......... .......... .......... 55% 23.0M 3s\n 38800K .......... .......... .......... .......... .......... 55% 24.5M 3s\n 38850K .......... .......... .......... .......... .......... 55% 3.32M 3s\n 38900K .......... .......... .......... .......... .......... 55% 51.8M 3s\n 38950K .......... .......... .......... .......... .......... 56% 5.03M 3s\n 39000K .......... .......... .......... .......... .......... 56% 79.6M 3s\n 39050K .......... .......... .......... .......... .......... 56% 26.7M 3s\n 39100K .......... .......... .......... .......... .......... 56% 42.6M 3s\n 39150K .......... .......... .......... .......... .......... 56% 23.8M 3s\n 39200K .......... .......... .......... .......... .......... 56% 3.28M 3s\n 39250K .......... .......... .......... .......... .......... 56% 75.6M 3s\n 39300K .......... .......... .......... .......... .......... 56% 5.05M 3s\n 39350K .......... .......... .......... .......... .......... 56% 74.8M 3s\n 39400K .......... .......... .......... .......... .......... 56% 22.4M 3s\n 39450K .......... .......... .......... .......... .......... 56% 39.3M 3s\n 39500K .......... .......... .......... .......... .......... 56% 31.3M 3s\n 39550K .......... .......... .......... .......... .......... 56% 3.31M 3s\n 39600K .......... .......... .......... .......... .......... 56% 42.3M 3s\n 39650K .......... .......... .......... .......... .......... 57% 5.19M 3s\n 39700K .......... .......... .......... .......... .......... 57% 84.7M 3s\n 39750K .......... .......... .......... .......... .......... 57% 23.5M 3s\n 39800K .......... .......... .......... .......... .......... 57% 36.2M 3s\n 39850K .......... .......... .......... .......... .......... 57% 29.6M 3s\n 39900K .......... .......... .......... .......... .......... 57% 3.29M 3s\n 39950K .......... .......... .......... .......... .......... 57% 52.4M 3s\n 40000K .......... .......... .......... .......... .......... 57% 5.09M 3s\n 40050K .......... .......... .......... .......... .......... 57% 87.7M 3s\n 40100K .......... .......... .......... .......... .......... 57% 25.2M 3s\n 40150K .......... .......... .......... .......... .......... 57% 33.5M 3s\n 40200K .......... .......... .......... .......... .......... 57% 32.1M 3s\n 40250K .......... .......... .......... .......... .......... 57% 3.34M 3s\n 40300K .......... .......... .......... .......... .......... 57% 47.8M 3s\n 40350K .......... .......... .......... .......... .......... 58% 4.95M 3s\n 40400K .......... .......... .......... .......... .......... 58% 97.3M 3s\n 40450K .......... .......... .......... .......... .......... 58% 25.8M 3s\n 40500K .......... .......... .......... .......... .......... 58% 22.7M 3s\n 40550K .......... .......... .......... .......... .......... 58% 61.6M 3s\n 40600K .......... .......... .......... .......... .......... 58% 3.29M 3s\n 40650K .......... .......... .......... .......... .......... 58% 60.2M 3s\n 40700K .......... .......... .......... .......... .......... 58% 4.99M 3s\n 40750K .......... .......... .......... .......... .......... 58% 57.7M 3s\n 40800K .......... .......... .......... .......... .......... 58% 28.2M 3s\n 40850K .......... .......... .......... .......... .......... 58% 21.7M 3s\n 40900K .......... .......... .......... .......... .......... 58% 65.4M 3s\n 40950K .......... .......... .......... .......... .......... 58% 3.31M 3s\n 41000K .......... .......... .......... .......... .......... 59% 60.1M 3s\n 41050K .......... .......... .......... .......... .......... 59% 4.93M 3s\n 41100K .......... .......... .......... .......... .......... 59%  116M 3s\n 41150K .......... .......... .......... .......... .......... 59% 22.6M 3s\n 41200K .......... .......... .......... .......... .......... 59% 19.8M 3s\n 41250K .......... .......... .......... .......... .......... 59% 36.0M 3s\n 41300K .......... .......... .......... .......... .......... 59% 3.52M 3s\n 41350K .......... .......... .......... .......... .......... 59% 60.0M 3s\n 41400K .......... .......... .......... .......... .......... 59% 5.01M 3s\n 41450K .......... .......... .......... .......... .......... 59% 86.6M 3s\n 41500K .......... .......... .......... .......... .......... 59% 27.6M 3s\n 41550K .......... .......... .......... .......... .......... 59% 19.8M 3s\n 41600K .......... .......... .......... .......... .......... 59% 25.2M 3s\n 41650K .......... .......... .......... .......... .......... 59% 3.62M 3s\n 41700K .......... .......... .......... .......... .......... 60% 57.5M 3s\n 41750K .......... .......... .......... .......... .......... 60% 5.03M 3s\n 41800K .......... .......... .......... .......... .......... 60% 74.4M 3s\n 41850K .......... .......... .......... .......... .......... 60% 26.8M 3s\n 41900K .......... .......... .......... .......... .......... 60% 26.3M 3s\n 41950K .......... .......... .......... .......... .......... 60% 22.3M 3s\n 42000K .......... .......... .......... .......... .......... 60% 3.57M 3s\n 42050K .......... .......... .......... .......... .......... 60% 50.3M 3s\n 42100K .......... .......... .......... .......... .......... 60% 5.04M 3s\n 42150K .......... .......... .......... .......... .......... 60% 75.4M 3s\n 42200K .......... .......... .......... .......... .......... 60% 27.6M 3s\n 42250K .......... .......... .......... .......... .......... 60% 29.7M 3s\n 42300K .......... .......... .......... .......... .......... 60% 35.4M 3s\n 42350K .......... .......... .......... .......... .......... 60% 3.28M 3s\n 42400K .......... .......... .......... .......... .......... 61% 50.3M 3s\n 42450K .......... .......... .......... .......... .......... 61% 5.00M 3s\n 42500K .......... .......... .......... .......... .......... 61% 98.8M 3s\n 42550K .......... .......... .......... .......... .......... 61% 60.8M 3s\n 42600K .......... .......... .......... .......... .......... 61% 22.1M 3s\n 42650K .......... .......... .......... .......... .......... 61% 29.5M 3s\n 42700K .......... .......... .......... .......... .......... 61% 34.2M 3s\n 42750K .......... .......... .......... .......... .......... 61% 3.37M 3s\n 42800K .......... .......... .......... .......... .......... 61% 5.10M 3s\n 42850K .......... .......... .......... .......... .......... 61% 58.1M 3s\n 42900K .......... .......... .......... .......... .......... 61% 81.8M 3s\n 42950K .......... .......... .......... .......... .......... 61% 23.1M 3s\n 43000K .......... .......... .......... .......... .......... 61% 26.9M 3s\n 43050K .......... .......... .......... .......... .......... 61% 33.1M 3s\n 43100K .......... .......... .......... .......... .......... 62% 3.48M 3s\n 43150K .......... .......... .......... .......... .......... 62% 4.91M 3s\n 43200K .......... .......... .......... .......... .......... 62% 65.6M 3s\n 43250K .......... .......... .......... .......... .......... 62% 58.3M 3s\n 43300K .......... .......... .......... .......... .......... 62% 23.6M 3s\n 43350K .......... .......... .......... .......... .......... 62% 27.3M 3s\n 43400K .......... .......... .......... .......... .......... 62% 33.4M 3s\n 43450K .......... .......... .......... .......... .......... 62% 3.46M 3s\n 43500K .......... .......... .......... .......... .......... 62% 48.2M 3s\n 43550K .......... .......... .......... .......... .......... 62% 5.15M 3s\n 43600K .......... .......... .......... .......... .......... 62% 47.3M 3s\n 43650K .......... .......... .......... .......... .......... 62% 25.7M 3s\n 43700K .......... .......... .......... .......... .......... 62% 26.4M 3s\n 43750K .......... .......... .......... .......... .......... 62% 35.1M 3s\n 43800K .......... .......... .......... .......... .......... 63% 3.50M 3s\n 43850K .......... .......... .......... .......... .......... 63% 33.1M 3s\n 43900K .......... .......... .......... .......... .......... 63% 5.32M 3s\n 43950K .......... .......... .......... .......... .......... 63% 57.6M 3s\n 44000K .......... .......... .......... .......... .......... 63% 22.5M 3s\n 44050K .......... .......... .......... .......... .......... 63% 26.3M 2s\n 44100K .......... .......... .......... .......... .......... 63% 32.5M 2s\n 44150K .......... .......... .......... .......... .......... 63% 3.54M 2s\n 44200K .......... .......... .......... .......... .......... 63% 46.3M 2s\n 44250K .......... .......... .......... .......... .......... 63% 4.89M 2s\n 44300K .......... .......... .......... .......... .......... 63%  117M 2s\n 44350K .......... .......... .......... .......... .......... 63% 25.4M 2s\n 44400K .......... .......... .......... .......... .......... 63% 21.2M 2s\n 44450K .......... .......... .......... .......... .......... 63% 38.1M 2s\n 44500K .......... .......... .......... .......... .......... 64% 3.50M 2s\n 44550K .......... .......... .......... .......... .......... 64% 59.0M 2s\n 44600K .......... .......... .......... .......... .......... 64% 5.07M 2s\n 44650K .......... .......... .......... .......... .......... 64% 54.4M 2s\n 44700K .......... .......... .......... .......... .......... 64% 35.1M 2s\n 44750K .......... .......... .......... .......... .......... 64% 19.0M 2s\n 44800K .......... .......... .......... .......... .......... 64% 31.4M 2s\n 44850K .......... .......... .......... .......... .......... 64% 3.48M 2s\n 44900K .......... .......... .......... .......... .......... 64% 62.7M 2s\n 44950K .......... .......... .......... .......... .......... 64% 5.10M 2s\n 45000K .......... .......... .......... .......... .......... 64% 57.9M 2s\n 45050K .......... .......... .......... .......... .......... 64% 61.0M 2s\n 45100K .......... .......... .......... .......... .......... 64% 26.6M 2s\n 45150K .......... .......... .......... .......... .......... 64% 25.6M 2s\n 45200K .......... .......... .......... .......... .......... 65% 3.23M 2s\n 45250K .......... .......... .......... .......... .......... 65%  110M 2s\n 45300K .......... .......... .......... .......... .......... 65% 4.92M 2s\n 45350K .......... .......... .......... .......... .......... 65% 66.2M 2s\n 45400K .......... .......... .......... .......... .......... 65% 59.2M 2s\n 45450K .......... .......... .......... .......... .......... 65% 27.8M 2s\n 45500K .......... .......... .......... .......... .......... 65% 25.8M 2s\n 45550K .......... .......... .......... .......... .......... 65% 3.24M 2s\n 45600K .......... .......... .......... .......... .......... 65% 86.3M 2s\n 45650K .......... .......... .......... .......... .......... 65% 4.75M 2s\n 45700K .......... .......... .......... .......... .......... 65% 40.9M 2s\n 45750K .......... .......... .......... .......... .......... 65%  162M 2s\n 45800K .......... .......... .......... .......... .......... 65% 37.6M 2s\n 45850K .......... .......... .......... .......... .......... 65% 19.1M 2s\n 45900K .......... .......... .......... .......... .......... 66% 43.9M 2s\n 45950K .......... .......... .......... .......... .......... 66% 3.50M 2s\n 46000K .......... .......... .......... .......... .......... 66% 42.4M 2s\n 46050K .......... .......... .......... .......... .......... 66% 5.03M 2s\n 46100K .......... .......... .......... .......... .......... 66% 79.4M 2s\n 46150K .......... .......... .......... .......... .......... 66% 28.4M 2s\n 46200K .......... .......... .......... .......... .......... 66% 25.1M 2s\n 46250K .......... .......... .......... .......... .......... 66% 32.3M 2s\n 46300K .......... .......... .......... .......... .......... 66% 3.48M 2s\n 46350K .......... .......... .......... .......... .......... 66% 45.9M 2s\n 46400K .......... .......... .......... .......... .......... 66% 4.86M 2s\n 46450K .......... .......... .......... .......... .......... 66%  147M 2s\n 46500K .......... .......... .......... .......... .......... 66% 29.2M 2s\n 46550K .......... .......... .......... .......... .......... 66% 32.1M 2s\n 46600K .......... .......... .......... .......... .......... 67% 34.4M 2s\n 46650K .......... .......... .......... .......... .......... 67% 3.34M 2s\n 46700K .......... .......... .......... .......... .......... 67% 66.3M 2s\n 46750K .......... .......... .......... .......... .......... 67% 4.87M 2s\n 46800K .......... .......... .......... .......... .......... 67% 60.6M 2s\n 46850K .......... .......... .......... .......... .......... 67% 51.8M 2s\n 46900K .......... .......... .......... .......... .......... 67% 30.0M 2s\n 46950K .......... .......... .......... .......... .......... 67% 26.1M 2s\n 47000K .......... .......... .......... .......... .......... 67% 33.9M 2s\n 47050K .......... .......... .......... .......... .......... 67% 3.48M 2s\n 47100K .......... .......... .......... .......... .......... 67% 48.7M 2s\n 47150K .......... .......... .......... .......... .......... 67% 4.43M 2s\n 47200K .......... .......... .......... .......... .......... 67%  217M 2s\n 47250K .......... .......... .......... .......... .......... 67% 26.7M 2s\n 47300K .......... .......... .......... .......... .......... 68% 18.9M 2s\n 47350K .......... .......... .......... .......... .......... 68%  120M 2s\n 47400K .......... .......... .......... .......... .......... 68% 3.57M 2s\n 47450K .......... .......... .......... .......... .......... 68% 64.5M 2s\n 47500K .......... .......... .......... .......... .......... 68% 4.78M 2s\n 47550K .......... .......... .......... .......... .......... 68% 37.3M 2s\n 47600K .......... .......... .......... .......... .......... 68% 37.8M 2s\n 47650K .......... .......... .......... .......... .......... 68% 30.6M 2s\n 47700K .......... .......... .......... .......... .......... 68% 30.8M 2s\n 47750K .......... .......... .......... .......... .......... 68% 3.48M 2s\n 47800K .......... .......... .......... .......... .......... 68% 72.7M 2s\n 47850K .......... .......... .......... .......... .......... 68% 5.06M 2s\n 47900K .......... .......... .......... .......... .......... 68% 27.8M 2s\n 47950K .......... .......... .......... .......... .......... 68% 30.0M 2s\n 48000K .......... .......... .......... .......... .......... 69% 33.7M 2s\n 48050K .......... .......... .......... .......... .......... 69% 50.0M 2s\n 48100K .......... .......... .......... .......... .......... 69% 3.41M 2s\n 48150K .......... .......... .......... .......... .......... 69% 67.3M 2s\n 48200K .......... .......... .......... .......... .......... 69% 4.96M 2s\n 48250K .......... .......... .......... .......... .......... 69% 49.1M 2s\n 48300K .......... .......... .......... .......... .......... 69% 32.8M 2s\n 48350K .......... .......... .......... .......... .......... 69% 38.0M 2s\n 48400K .......... .......... .......... .......... .......... 69% 22.4M 2s\n 48450K .......... .......... .......... .......... .......... 69% 39.5M 2s\n 48500K .......... .......... .......... .......... .......... 69% 3.60M 2s\n 48550K .......... .......... .......... .......... .......... 69% 38.5M 2s\n 48600K .......... .......... .......... .......... .......... 69% 4.96M 2s\n 48650K .......... .......... .......... .......... .......... 69% 38.8M 2s\n 48700K .......... .......... .......... .......... .......... 70% 37.0M 2s\n 48750K .......... .......... .......... .......... .......... 70% 27.4M 2s\n 48800K .......... .......... .......... .......... .......... 70% 33.6M 2s\n 48850K .......... .......... .......... .......... .......... 70% 3.55M 2s\n 48900K .......... .......... .......... .......... .......... 70% 50.3M 2s\n 48950K .......... .......... .......... .......... .......... 70% 5.14M 2s\n 49000K .......... .......... .......... .......... .......... 70% 25.1M 2s\n 49050K .......... .......... .......... .......... .......... 70% 70.1M 2s\n 49100K .......... .......... .......... .......... .......... 70% 32.5M 2s\n 49150K .......... .......... .......... .......... .......... 70% 26.6M 2s\n 49200K .......... .......... .......... .......... .......... 70% 3.41M 2s\n 49250K .......... .......... .......... .......... .......... 70% 65.5M 2s\n 49300K .......... .......... .......... .......... .......... 70% 4.99M 2s\n 49350K .......... .......... .......... .......... .......... 71% 46.8M 2s\n 49400K .......... .......... .......... .......... .......... 71% 41.6M 2s\n 49450K .......... .......... .......... .......... .......... 71% 34.6M 2s\n 49500K .......... .......... .......... .......... .......... 71% 30.3M 2s\n 49550K .......... .......... .......... .......... .......... 71% 25.9M 2s\n 49600K .......... .......... .......... .......... .......... 71% 3.57M 2s\n 49650K .......... .......... .......... .......... .......... 71% 46.6M 2s\n 49700K .......... .......... .......... .......... .......... 71% 4.87M 2s\n 49750K .......... .......... .......... .......... .......... 71% 41.0M 2s\n 49800K .......... .......... .......... .......... .......... 71% 35.2M 2s\n 49850K .......... .......... .......... .......... .......... 71% 34.1M 2s\n 49900K .......... .......... .......... .......... .......... 71% 40.6M 2s\n 49950K .......... .......... .......... .......... .......... 71% 3.38M 2s\n 50000K .......... .......... .......... .......... .......... 71% 67.7M 2s\n 50050K .......... .......... .......... .......... .......... 72% 5.09M 2s\n 50100K .......... .......... .......... .......... .......... 72% 29.2M 2s\n 50150K .......... .......... .......... .......... .......... 72% 85.1M 2s\n 50200K .......... .......... .......... .......... .......... 72% 34.9M 2s\n 50250K .......... .......... .......... .......... .......... 72% 21.6M 2s\n 50300K .......... .......... .......... .......... .......... 72% 40.1M 2s\n 50350K .......... .......... .......... .......... .......... 72% 3.53M 2s\n 50400K .......... .......... .......... .......... .......... 72% 4.99M 2s\n 50450K .......... .......... .......... .......... .......... 72% 44.5M 2s\n 50500K .......... .......... .......... .......... .......... 72% 40.5M 2s\n 50550K .......... .......... .......... .......... .......... 72% 35.3M 2s\n 50600K .......... .......... .......... .......... .......... 72% 32.1M 2s\n 50650K .......... .......... .......... .......... .......... 72% 30.9M 2s\n 50700K .......... .......... .......... .......... .......... 72% 3.49M 2s\n 50750K .......... .......... .......... .......... .......... 73% 62.8M 2s\n 50800K .......... .......... .......... .......... .......... 73% 5.02M 2s\n 50850K .......... .......... .......... .......... .......... 73% 28.4M 2s\n 50900K .......... .......... .......... .......... .......... 73% 65.3M 2s\n 50950K .......... .......... .......... .......... .......... 73% 30.8M 2s\n 51000K .......... .......... .......... .......... .......... 73% 27.5M 2s\n 51050K .......... .......... .......... .......... .......... 73% 43.7M 2s\n 51100K .......... .......... .......... .......... .......... 73% 3.57M 2s\n 51150K .......... .......... .......... .......... .......... 73% 4.82M 2s\n 51200K .......... .......... .......... .......... .......... 73% 43.5M 2s\n 51250K .......... .......... .......... .......... .......... 73% 47.8M 2s\n 51300K .......... .......... .......... .......... .......... 73% 35.6M 2s\n 51350K .......... .......... .......... .......... .......... 73% 30.7M 2s\n 51400K .......... .......... .......... .......... .......... 73% 32.9M 2s\n 51450K .......... .......... .......... .......... .......... 74% 3.53M 2s\n 51500K .......... .......... .......... .......... .......... 74% 54.6M 2s\n 51550K .......... .......... .......... .......... .......... 74% 5.07M 2s\n 51600K .......... .......... .......... .......... .......... 74% 25.0M 2s\n 51650K .......... .......... .......... .......... .......... 74% 38.1M 2s\n 51700K .......... .......... .......... .......... .......... 74% 51.5M 2s\n 51750K .......... .......... .......... .......... .......... 74% 27.6M 2s\n 51800K .......... .......... .......... .......... .......... 74% 36.1M 2s\n 51850K .......... .......... .......... .......... .......... 74% 3.63M 2s\n 51900K .......... .......... .......... .......... .......... 74% 43.6M 2s\n 51950K .......... .......... .......... .......... .......... 74% 4.96M 2s\n 52000K .......... .......... .......... .......... .......... 74% 40.4M 2s\n 52050K .......... .......... .......... .......... .......... 74% 34.1M 2s\n 52100K .......... .......... .......... .......... .......... 74% 22.3M 2s\n 52150K .......... .......... .......... .......... .......... 75% 38.8M 2s\n 52200K .......... .......... .......... .......... .......... 75% 3.56M 2s\n 52250K .......... .......... .......... .......... .......... 75% 70.7M 2s\n 52300K .......... .......... .......... .......... .......... 75% 5.15M 2s\n 52350K .......... .......... .......... .......... .......... 75% 21.8M 2s\n 52400K .......... .......... .......... .......... .......... 75% 43.5M 2s\n 52450K .......... .......... .......... .......... .......... 75% 27.3M 2s\n 52500K .......... .......... .......... .......... .......... 75% 56.9M 2s\n 52550K .......... .......... .......... .......... .......... 75% 33.8M 2s\n 52600K .......... .......... .......... .......... .......... 75% 3.62M 2s\n 52650K .......... .......... .......... .......... .......... 75% 4.91M 2s\n 52700K .......... .......... .......... .......... .......... 75% 54.0M 2s\n 52750K .......... .......... .......... .......... .......... 75% 39.0M 2s\n 52800K .......... .......... .......... .......... .......... 75% 35.1M 2s\n 52850K .......... .......... .......... .......... .......... 76% 21.8M 2s\n 52900K .......... .......... .......... .......... .......... 76% 36.6M 2s\n 52950K .......... .......... .......... .......... .......... 76% 3.60M 2s\n 53000K .......... .......... .......... .......... .......... 76% 65.1M 2s\n 53050K .......... .......... .......... .......... .......... 76% 5.14M 2s\n 53100K .......... .......... .......... .......... .......... 76% 26.3M 2s\n 53150K .......... .......... .......... .......... .......... 76% 35.3M 2s\n 53200K .......... .......... .......... .......... .......... 76% 30.0M 2s\n 53250K .......... .......... .......... .......... .......... 76% 42.8M 2s\n 53300K .......... .......... .......... .......... .......... 76% 35.2M 2s\n 53350K .......... .......... .......... .......... .......... 76% 3.63M 2s\n 53400K .......... .......... .......... .......... .......... 76% 4.97M 2s\n 53450K .......... .......... .......... .......... .......... 76% 43.4M 2s\n 53500K .......... .......... .......... .......... .......... 76% 39.9M 2s\n 53550K .......... .......... .......... .......... .......... 77% 32.5M 2s\n 53600K .......... .......... .......... .......... .......... 77% 22.8M 2s\n 53650K .......... .......... .......... .......... .......... 77% 38.3M 2s\n 53700K .......... .......... .......... .......... .......... 77% 3.49M 2s\n 53750K .......... .......... .......... .......... .......... 77%  109M 2s\n 53800K .......... .......... .......... .......... .......... 77% 5.13M 2s\n 53850K .......... .......... .......... .......... .......... 77% 26.9M 1s\n 53900K .......... .......... .......... .......... .......... 77% 56.8M 1s\n 53950K .......... .......... .......... .......... .......... 77% 23.1M 1s\n 54000K .......... .......... .......... .......... .......... 77% 44.5M 1s\n 54050K .......... .......... .......... .......... .......... 77% 36.5M 1s\n 54100K .......... .......... .......... .......... .......... 77% 3.63M 1s\n 54150K .......... .......... .......... .......... .......... 77% 44.4M 1s\n 54200K .......... .......... .......... .......... .......... 77% 4.84M 1s\n 54250K .......... .......... .......... .......... .......... 78% 44.9M 1s\n 54300K .......... .......... .......... .......... .......... 78% 37.9M 1s\n 54350K .......... .......... .......... .......... .......... 78% 17.3M 1s\n 54400K .......... .......... .......... .......... .......... 78% 64.7M 1s\n 54450K .......... .......... .......... .......... .......... 78% 3.39M 1s\n 54500K .......... .......... .......... .......... .......... 78%  171M 1s\n 54550K .......... .......... .......... .......... .......... 78% 5.21M 1s\n 54600K .......... .......... .......... .......... .......... 78% 33.2M 1s\n 54650K .......... .......... .......... .......... .......... 78% 43.4M 1s\n 54700K .......... .......... .......... .......... .......... 78% 40.3M 1s\n 54750K .......... .......... .......... .......... .......... 78% 22.2M 1s\n 54800K .......... .......... .......... .......... .......... 78% 26.4M 1s\n 54850K .......... .......... .......... .......... .......... 78% 3.70M 1s\n 54900K .......... .......... .......... .......... .......... 78% 58.5M 1s\n 54950K .......... .......... .......... .......... .......... 79% 5.00M 1s\n 55000K .......... .......... .......... .......... .......... 79% 41.6M 1s\n 55050K .......... .......... .......... .......... .......... 79% 38.1M 1s\n 55100K .......... .......... .......... .......... .......... 79% 32.8M 1s\n 55150K .......... .......... .......... .......... .......... 79% 19.7M 1s\n 55200K .......... .......... .......... .......... .......... 79% 3.62M 1s\n 55250K .......... .......... .......... .......... .......... 79%  175M 1s\n 55300K .......... .......... .......... .......... .......... 79% 5.01M 1s\n 55350K .......... .......... .......... .......... .......... 79% 37.9M 1s\n 55400K .......... .......... .......... .......... .......... 79% 47.3M 1s\n 55450K .......... .......... .......... .......... .......... 79% 21.0M 1s\n 55500K .......... .......... .......... .......... .......... 79% 79.8M 1s\n 55550K .......... .......... .......... .......... .......... 79% 21.6M 1s\n 55600K .......... .......... .......... .......... .......... 79% 3.56M 1s\n 55650K .......... .......... .......... .......... .......... 80%  131M 1s\n 55700K .......... .......... .......... .......... .......... 80% 5.04M 1s\n 55750K .......... .......... .......... .......... .......... 80% 32.1M 1s\n 55800K .......... .......... .......... .......... .......... 80% 50.9M 1s\n 55850K .......... .......... .......... .......... .......... 80% 33.2M 1s\n 55900K .......... .......... .......... .......... .......... 80% 24.0M 1s\n 55950K .......... .......... .......... .......... .......... 80% 39.6M 1s\n 56000K .......... .......... .......... .......... .......... 80% 3.58M 1s\n 56050K .......... .......... .......... .......... .......... 80% 5.18M 1s\n 56100K .......... .......... .......... .......... .......... 80% 39.6M 1s\n 56150K .......... .......... .......... .......... .......... 80% 41.0M 1s\n 56200K .......... .......... .......... .......... .......... 80% 41.7M 1s\n 56250K .......... .......... .......... .......... .......... 80% 27.9M 1s\n 56300K .......... .......... .......... .......... .......... 80% 33.2M 1s\n 56350K .......... .......... .......... .......... .......... 81% 3.22M 1s\n 56400K .......... .......... .......... .......... .......... 81%  219M 1s\n 56450K .......... .......... .......... .......... .......... 81% 5.49M 1s\n 56500K .......... .......... .......... .......... .......... 81% 32.1M 1s\n 56550K .......... .......... .......... .......... .......... 81% 21.7M 1s\n 56600K .......... .......... .......... .......... .......... 81% 54.2M 1s\n 56650K .......... .......... .......... .......... .......... 81% 32.3M 1s\n 56700K .......... .......... .......... .......... .......... 81% 37.6M 1s\n 56750K .......... .......... .......... .......... .......... 81% 3.59M 1s\n 56800K .......... .......... .......... .......... .......... 81% 86.4M 1s\n 56850K .......... .......... .......... .......... .......... 81% 5.14M 1s\n 56900K .......... .......... .......... .......... .......... 81% 29.0M 1s\n 56950K .......... .......... .......... .......... .......... 81% 51.8M 1s\n 57000K .......... .......... .......... .......... .......... 81% 12.5M 1s\n 57050K .......... .......... .......... .......... .......... 82%  246M 1s\n 57100K .......... .......... .......... .......... .......... 82% 76.2M 1s\n 57150K .......... .......... .......... .......... .......... 82% 3.57M 1s\n 57200K .......... .......... .......... .......... .......... 82% 5.20M 1s\n 57250K .......... .......... .......... .......... .......... 82% 38.9M 1s\n 57300K .......... .......... .......... .......... .......... 82% 44.8M 1s\n 57350K .......... .......... .......... .......... .......... 82% 23.5M 1s\n 57400K .......... .......... .......... .......... .......... 82% 54.4M 1s\n 57450K .......... .......... .......... .......... .......... 82% 37.0M 1s\n 57500K .......... .......... .......... .......... .......... 82% 3.37M 1s\n 57550K .......... .......... .......... .......... .......... 82% 90.7M 1s\n 57600K .......... .......... .......... .......... .......... 82% 5.17M 1s\n 57650K .......... .......... .......... .......... .......... 82% 41.0M 1s\n 57700K .......... .......... .......... .......... .......... 83% 33.1M 1s\n 57750K .......... .......... .......... .......... .......... 83% 28.1M 1s\n 57800K .......... .......... .......... .......... .......... 83% 27.4M 1s\n 57850K .......... .......... .......... .......... .......... 83% 37.3M 1s\n 57900K .......... .......... .......... .......... .......... 83% 3.63M 1s\n 57950K .......... .......... .......... .......... .......... 83%  101M 1s\n 58000K .......... .......... .......... .......... .......... 83% 5.01M 1s\n 58050K .......... .......... .......... .......... .......... 83% 31.1M 1s\n 58100K .......... .......... .......... .......... .......... 83% 45.0M 1s\n 58150K .......... .......... .......... .......... .......... 83% 31.3M 1s\n 58200K .......... .......... .......... .......... .......... 83% 27.5M 1s\n 58250K .......... .......... .......... .......... .......... 83% 39.6M 1s\n 58300K .......... .......... .......... .......... .......... 83% 3.59M 1s\n 58350K .......... .......... .......... .......... .......... 83% 57.8M 1s\n 58400K .......... .......... .......... .......... .......... 84% 4.93M 1s\n 58450K .......... .......... .......... .......... .......... 84% 38.3M 1s\n 58500K .......... .......... .......... .......... .......... 84% 37.7M 1s\n 58550K .......... .......... .......... .......... .......... 84% 31.4M 1s\n 58600K .......... .......... .......... .......... .......... 84% 36.7M 1s\n 58650K .......... .......... .......... .......... .......... 84% 30.1M 1s\n 58700K .......... .......... .......... .......... .......... 84% 3.57M 1s\n 58750K .......... .......... .......... .......... .......... 84% 5.31M 1s\n 58800K .......... .......... .......... .......... .......... 84% 45.6M 1s\n 58850K .......... .......... .......... .......... .......... 84% 42.7M 1s\n 58900K .......... .......... .......... .......... .......... 84% 23.5M 1s\n 58950K .......... .......... .......... .......... .......... 84% 56.7M 1s\n 59000K .......... .......... .......... .......... .......... 84% 33.4M 1s\n 59050K .......... .......... .......... .......... .......... 84% 3.40M 1s\n 59100K .......... .......... .......... .......... .......... 85%  146M 1s\n 59150K .......... .......... .......... .......... .......... 85% 4.96M 1s\n 59200K .......... .......... .......... .......... .......... 85% 32.6M 1s\n 59250K .......... .......... .......... .......... .......... 85% 46.3M 1s\n 59300K .......... .......... .......... .......... .......... 85% 29.4M 1s\n 59350K .......... .......... .......... .......... .......... 85% 26.3M 1s\n 59400K .......... .......... .......... .......... .......... 85% 43.4M 1s\n 59450K .......... .......... .......... .......... .......... 85% 3.61M 1s\n 59500K .......... .......... .......... .......... .......... 85% 95.5M 1s\n 59550K .......... .......... .......... .......... .......... 85% 4.97M 1s\n 59600K .......... .......... .......... .......... .......... 85% 31.0M 1s\n 59650K .......... .......... .......... .......... .......... 85% 52.3M 1s\n 59700K .......... .......... .......... .......... .......... 85% 29.2M 1s\n 59750K .......... .......... .......... .......... .......... 85% 25.7M 1s\n 59800K .......... .......... .......... .......... .......... 86% 44.8M 1s\n 59850K .......... .......... .......... .......... .......... 86% 3.52M 1s\n 59900K .......... .......... .......... .......... .......... 86%  197M 1s\n 59950K .......... .......... .......... .......... .......... 86% 4.71M 1s\n 60000K .......... .......... .......... .......... .......... 86% 46.4M 1s\n 60050K .......... .......... .......... .......... .......... 86% 36.4M 1s\n 60100K .......... .......... .......... .......... .......... 86% 28.8M 1s\n 60150K .......... .......... .......... .......... .......... 86% 38.7M 1s\n 60200K .......... .......... .......... .......... .......... 86% 35.6M 1s\n 60250K .......... .......... .......... .......... .......... 86% 3.58M 1s\n 60300K .......... .......... .......... .......... .......... 86%  181M 1s\n 60350K .......... .......... .......... .......... .......... 86% 4.85M 1s\n 60400K .......... .......... .......... .......... .......... 86% 36.6M 1s\n 60450K .......... .......... .......... .......... .......... 86% 25.4M 1s\n 60500K .......... .......... .......... .......... .......... 87% 24.3M 1s\n 60550K .......... .......... .......... .......... .......... 87%  127M 1s\n 60600K .......... .......... .......... .......... .......... 87% 3.44M 1s\n 60650K .......... .......... .......... .......... .......... 87% 78.6M 1s\n 60700K .......... .......... .......... .......... .......... 87% 5.21M 1s\n 60750K .......... .......... .......... .......... .......... 87% 40.6M 1s\n 60800K .......... .......... .......... .......... .......... 87% 36.5M 1s\n 60850K .......... .......... .......... .......... .......... 87% 18.0M 1s\n 60900K .......... .......... .......... .......... .......... 87% 42.0M 1s\n 60950K .......... .......... .......... .......... .......... 87% 49.5M 1s\n 61000K .......... .......... .......... .......... .......... 87% 3.51M 1s\n 61050K .......... .......... .......... .......... .......... 87%  219M 1s\n 61100K .......... .......... .......... .......... .......... 87% 5.14M 1s\n 61150K .......... .......... .......... .......... .......... 87% 30.8M 1s\n 61200K .......... .......... .......... .......... .......... 88% 41.7M 1s\n 61250K .......... .......... .......... .......... .......... 88% 19.5M 1s\n 61300K .......... .......... .......... .......... .......... 88% 48.8M 1s\n 61350K .......... .......... .......... .......... .......... 88% 38.7M 1s\n 61400K .......... .......... .......... .......... .......... 88% 3.61M 1s\n 61450K .......... .......... .......... .......... .......... 88%  118M 1s\n 61500K .......... .......... .......... .......... .......... 88% 5.07M 1s\n 61550K .......... .......... .......... .......... .......... 88% 28.9M 1s\n 61600K .......... .......... .......... .......... .......... 88% 52.9M 1s\n 61650K .......... .......... .......... .......... .......... 88% 21.3M 1s\n 61700K .......... .......... .......... .......... .......... 88% 17.7M 1s\n 61750K .......... .......... .......... .......... .......... 88%  227M 1s\n 61800K .......... .......... .......... .......... .......... 88% 3.74M 1s\n 61850K .......... .......... .......... .......... .......... 88%  117M 1s\n 61900K .......... .......... .......... .......... .......... 89% 5.05M 1s\n 61950K .......... .......... .......... .......... .......... 89% 29.9M 1s\n 62000K .......... .......... .......... .......... .......... 89% 48.2M 1s\n 62050K .......... .......... .......... .......... .......... 89% 22.6M 1s\n 62100K .......... .......... .......... .......... .......... 89% 36.7M 1s\n 62150K .......... .......... .......... .......... .......... 89% 40.4M 1s\n 62200K .......... .......... .......... .......... .......... 89% 3.58M 1s\n 62250K .......... .......... .......... .......... .......... 89% 64.4M 1s\n 62300K .......... .......... .......... .......... .......... 89% 5.27M 1s\n 62350K .......... .......... .......... .......... .......... 89% 28.2M 1s\n 62400K .......... .......... .......... .......... .......... 89% 40.4M 1s\n 62450K .......... .......... .......... .......... .......... 89% 26.5M 1s\n 62500K .......... .......... .......... .......... .......... 89% 33.3M 1s\n 62550K .......... .......... .......... .......... .......... 89% 48.6M 1s\n 62600K .......... .......... .......... .......... .......... 90% 3.58M 1s\n 62650K .......... .......... .......... .......... .......... 90%  130M 1s\n 62700K .......... .......... .......... .......... .......... 90% 5.00M 1s\n 62750K .......... .......... .......... .......... .......... 90% 28.8M 1s\n 62800K .......... .......... .......... .......... .......... 90% 38.3M 1s\n 62850K .......... .......... .......... .......... .......... 90% 19.4M 1s\n 62900K .......... .......... .......... .......... .......... 90% 65.7M 1s\n 62950K .......... .......... .......... .......... .......... 90% 47.3M 1s\n 63000K .......... .......... .......... .......... .......... 90% 3.58M 1s\n 63050K .......... .......... .......... .......... .......... 90%  184M 1s\n 63100K .......... .......... .......... .......... .......... 90% 4.92M 1s\n 63150K .......... .......... .......... .......... .......... 90% 32.4M 1s\n 63200K .......... .......... .......... .......... .......... 90% 28.9M 1s\n 63250K .......... .......... .......... .......... .......... 90% 22.1M 1s\n 63300K .......... .......... .......... .......... .......... 91%  101M 1s\n 63350K .......... .......... .......... .......... .......... 91% 3.46M 1s\n 63400K .......... .......... .......... .......... .......... 91% 82.1M 1s\n 63450K .......... .......... .......... .......... .......... 91% 78.7M 1s\n 63500K .......... .......... .......... .......... .......... 91% 4.97M 1s\n 63550K .......... .......... .......... .......... .......... 91% 13.4M 1s\n 63600K .......... .......... .......... .......... .......... 91%  159M 1s\n 63650K .......... .......... .......... .......... .......... 91% 36.8M 1s\n 63700K .......... .......... .......... .......... .......... 91%  103M 1s\n 63750K .......... .......... .......... .......... .......... 91% 3.38M 1s\n 63800K .......... .......... .......... .......... .......... 91% 64.9M 1s\n 63850K .......... .......... .......... .......... .......... 91%  133M 1s\n 63900K .......... .......... .......... .......... .......... 91% 4.99M 1s\n 63950K .......... .......... .......... .......... .......... 91% 15.0M 1s\n 64000K .......... .......... .......... .......... .......... 92%  224M 1s\n 64050K .......... .......... .......... .......... .......... 92% 28.4M 1s\n 64100K .......... .......... .......... .......... .......... 92%  106M 1s\n 64150K .......... .......... .......... .......... .......... 92% 3.41M 1s\n 64200K .......... .......... .......... .......... .......... 92%  140M 0s\n 64250K .......... .......... .......... .......... .......... 92% 87.7M 0s\n 64300K .......... .......... .......... .......... .......... 92% 4.92M 0s\n 64350K .......... .......... .......... .......... .......... 92% 43.5M 0s\n 64400K .......... .......... .......... .......... .......... 92% 21.0M 0s\n 64450K .......... .......... .......... .......... .......... 92% 26.7M 0s\n 64500K .......... .......... .......... .......... .......... 92%  120M 0s\n 64550K .......... .......... .......... .......... .......... 92% 3.39M 0s\n 64600K .......... .......... .......... .......... .......... 92%  125M 0s\n 64650K .......... .......... .......... .......... .......... 92% 5.17M 0s\n 64700K .......... .......... .......... .......... .......... 93% 44.0M 0s\n 64750K .......... .......... .......... .......... .......... 93% 15.9M 0s\n 64800K .......... .......... .......... .......... .......... 93% 75.0M 0s\n 64850K .......... .......... .......... .......... .......... 93% 35.3M 0s\n 64900K .......... .......... .......... .......... .......... 93% 40.4M 0s\n 64950K .......... .......... .......... .......... .......... 93% 3.61M 0s\n 65000K .......... .......... .......... .......... .......... 93% 95.7M 0s\n 65050K .......... .......... .......... .......... .......... 93% 5.22M 0s\n 65100K .......... .......... .......... .......... .......... 93% 40.4M 0s\n 65150K .......... .......... .......... .......... .......... 93% 15.6M 0s\n 65200K .......... .......... .......... .......... .......... 93% 74.1M 0s\n 65250K .......... .......... .......... .......... .......... 93% 34.8M 0s\n 65300K .......... .......... .......... .......... .......... 93% 32.8M 0s\n 65350K .......... .......... .......... .......... .......... 93% 3.71M 0s\n 65400K .......... .......... .......... .......... .......... 94% 74.3M 0s\n 65450K .......... .......... .......... .......... .......... 94% 5.32M 0s\n 65500K .......... .......... .......... .......... .......... 94% 41.3M 0s\n 65550K .......... .......... .......... .......... .......... 94% 14.9M 0s\n 65600K .......... .......... .......... .......... .......... 94% 88.5M 0s\n 65650K .......... .......... .......... .......... .......... 94% 34.0M 0s\n 65700K .......... .......... .......... .......... .......... 94% 42.7M 0s\n 65750K .......... .......... .......... .......... .......... 94% 3.64M 0s\n 65800K .......... .......... .......... .......... .......... 94% 71.7M 0s\n 65850K .......... .......... .......... .......... .......... 94% 4.85M 0s\n 65900K .......... .......... .......... .......... .......... 94%  129M 0s\n 65950K .......... .......... .......... .......... .......... 94% 15.6M 0s\n 66000K .......... .......... .......... .......... .......... 94%  136M 0s\n 66050K .......... .......... .......... .......... .......... 95% 28.0M 0s\n 66100K .......... .......... .......... .......... .......... 95% 73.1M 0s\n 66150K .......... .......... .......... .......... .......... 95% 3.49M 0s\n 66200K .......... .......... .......... .......... .......... 95% 72.9M 0s\n 66250K .......... .......... .......... .......... .......... 95% 4.82M 0s\n 66300K .......... .......... .......... .......... .......... 95%  154M 0s\n 66350K .......... .......... .......... .......... .......... 95% 16.0M 0s\n 66400K .......... .......... .......... .......... .......... 95%  122M 0s\n 66450K .......... .......... .......... .......... .......... 95% 27.0M 0s\n 66500K .......... .......... .......... .......... .......... 95% 86.2M 0s\n 66550K .......... .......... .......... .......... .......... 95% 3.49M 0s\n 66600K .......... .......... .......... .......... .......... 95% 82.8M 0s\n 66650K .......... .......... .......... .......... .......... 95%  104M 0s\n 66700K .......... .......... .......... .......... .......... 95% 5.00M 0s\n 66750K .......... .......... .......... .......... .......... 96% 15.6M 0s\n 66800K .......... .......... .......... .......... .......... 96%  145M 0s\n 66850K .......... .......... .......... .......... .......... 96% 25.7M 0s\n 66900K .......... .......... .......... .......... .......... 96% 80.9M 0s\n 66950K .......... .......... .......... .......... .......... 96% 3.44M 0s\n 67000K .......... .......... .......... .......... .......... 96%  105M 0s\n 67050K .......... .......... .......... .......... .......... 96%  109M 0s\n 67100K .......... .......... .......... .......... .......... 96% 4.93M 0s\n 67150K .......... .......... .......... .......... .......... 96% 16.5M 0s\n 67200K .......... .......... .......... .......... .......... 96%  106M 0s\n 67250K .......... .......... .......... .......... .......... 96% 27.6M 0s\n 67300K .......... .......... .......... .......... .......... 96% 77.0M 0s\n 67350K .......... .......... .......... .......... .......... 96% 3.50M 0s\n 67400K .......... .......... .......... .......... .......... 96% 63.4M 0s\n 67450K .......... .......... .......... .......... .......... 97%  154M 0s\n 67500K .......... .......... .......... .......... .......... 97% 4.91M 0s\n 67550K .......... .......... .......... .......... .......... 97% 16.8M 0s\n 67600K .......... .......... .......... .......... .......... 97% 95.8M 0s\n 67650K .......... .......... .......... .......... .......... 97% 27.1M 0s\n 67700K .......... .......... .......... .......... .......... 97% 82.0M 0s\n 67750K .......... .......... .......... .......... .......... 97% 41.8M 0s\n 67800K .......... .......... .......... .......... .......... 97% 3.60M 0s\n 67850K .......... .......... .......... .......... .......... 97%  104M 0s\n 67900K .......... .......... .......... .......... .......... 97% 4.90M 0s\n 67950K .......... .......... .......... .......... .......... 97% 37.8M 0s\n 68000K .......... .......... .......... .......... .......... 97% 23.5M 0s\n 68050K .......... .......... .......... .......... .......... 97% 31.2M 0s\n 68100K .......... .......... .......... .......... .......... 97% 67.4M 0s\n 68150K .......... .......... .......... .......... .......... 98% 50.9M 0s\n 68200K .......... .......... .......... .......... .......... 98% 3.55M 0s\n 68250K .......... .......... .......... .......... .......... 98% 94.7M 0s\n 68300K .......... .......... .......... .......... .......... 98% 4.91M 0s\n 68350K .......... .......... .......... .......... .......... 98% 36.5M 0s\n 68400K .......... .......... .......... .......... .......... 98% 24.4M 0s\n 68450K .......... .......... .......... .......... .......... 98% 36.2M 0s\n 68500K .......... .......... .......... .......... .......... 98% 50.4M 0s\n 68550K .......... .......... .......... .......... .......... 98% 46.4M 0s\n 68600K .......... .......... .......... .......... .......... 98% 3.59M 0s\n 68650K .......... .......... .......... .......... .......... 98% 98.5M 0s\n 68700K .......... .......... .......... .......... .......... 98% 4.88M 0s\n 68750K .......... .......... .......... .......... .......... 98% 35.5M 0s\n 68800K .......... .......... .......... .......... .......... 98% 20.2M 0s\n 68850K .......... .......... .......... .......... .......... 99% 51.2M 0s\n 68900K .......... .......... .......... .......... .......... 99% 22.8M 0s\n 68950K .......... .......... .......... .......... .......... 99%  144M 0s\n 69000K .......... .......... .......... .......... .......... 99% 3.72M 0s\n 69050K .......... .......... .......... .......... .......... 99% 77.4M 0s\n 69100K .......... .......... .......... .......... .......... 99% 4.99M 0s\n 69150K .......... .......... .......... .......... .......... 99% 32.3M 0s\n 69200K .......... .......... .......... .......... .......... 99% 20.7M 0s\n 69250K .......... .......... .......... .......... .......... 99%  100M 0s\n 69300K .......... .......... .......... .......... .......... 99% 34.9M 0s\n 69350K .......... .......... .......... .......... .......... 99% 32.1M 0s\n 69400K .......... .......... .......... .......... .......... 99% 3.58M 0s\n 69450K .......... .......... .......... .......... .......... 99%  164M 0s\n 69500K .......... .......... .......... .......... .......... 99% 5.04M 0s\n 69550K .......... .......... .....                           100% 87.0M=6.4s\n\n2020-09-01 08:47:46 (10.6 MB/s) - \u2018libnvinfer6_6.0.1-1+cuda10.1_amd64.deb\u2019 saved [71245796/71245796]\n\n/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n\n</code>\n</pre>        <pre><code>!apt install libnvinfer-plugin6=6.0.1-1+cuda10.1\n</code></pre>      <pre>\n<code>Reading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following package was automatically installed and is no longer required:\n  libnvidia-common-440\nUse 'apt autoremove' to remove it.\nThe following NEW packages will be installed:\n  libnvinfer-plugin6\n0 upgraded, 1 newly installed, 0 to remove and 40 not upgraded.\nNeed to get 1,751 kB of archives.\nAfter this operation, 4,508 kB of additional disk space will be used.\nGet:1 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  libnvinfer-plugin6 6.0.1-1+cuda10.1 [1,751 kB]\nFetched 1,751 kB in 0s (7,666 kB/s)\nSelecting previously unselected package libnvinfer-plugin6.\n(Reading database ... 144584 files and directories currently installed.)\nPreparing to unpack .../libnvinfer-plugin6_6.0.1-1+cuda10.1_amd64.deb ...\nUnpacking libnvinfer-plugin6 (6.0.1-1+cuda10.1) ...\nSetting up libnvinfer-plugin6 (6.0.1-1+cuda10.1) ...\nProcessing triggers for libc-bin (2.27-3ubuntu1) ...\n/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n\n</code>\n</pre>        <pre><code># check TensorRT version\nprint(\"TensorRT version: \")\n!dpkg -l | grep nvinfer\n</code></pre>      <pre>\n<code>TensorRT version: \nii  libnvinfer-plugin6                     6.0.1-1+cuda10.1                                  amd64        TensorRT plugin libraries\nii  libnvinfer6                            6.0.1-1+cuda10.1                                  amd64        TensorRT runtime libraries\n</code>\n</pre>        <pre><code>from tensorflow.compiler.tf2tensorrt.wrap_py_utils import get_linked_tensorrt_version\nfrom tensorflow.compiler.tf2tensorrt.wrap_py_utils import get_loaded_tensorrt_version\n\nprint(f\"Linked TensorRT version {get_linked_tensorrt_version()}\")\nprint(f\"Loaded TensorRT version {get_loaded_tensorrt_version()}\")\n</code></pre>      <pre>\n<code>Linked TensorRT version (6, 0, 1)\nLoaded TensorRT version (6, 0, 1)\n</code>\n</pre>         <p>Les deux versions de Tensorflow et TensorRT install\u00e9es sont maintenant compatibles. Lan\u00e7ons la conversion.</p>       <p>https://developer.ibm.com/components/ibm-power/tutorials/introducing-tensorflow-with-tensorrt/</p> <p>https://github.com/tensorflow/tensorrt/tree/master/tftrt/examples/image-classification</p>      <pre><code>from tensorflow.python.compiler.tensorrt import trt_convert as trt\n</code></pre>      <ul> <li>TensorFlow TensorRT (TF-TRT) prend en entr\u00e9e pour optimiser le mod\u00e8le un mod\u00e8le entra\u00een\u00e9 via <code>tf.keras</code> sous la forme <code>SavedModel</code>, ie un <code>.pb</code>.</li> </ul>       <p>D\u00e9finisoons un dataset de validation, comme pour Tensorflow Lite.</p>      <pre><code># Prepare validation sets\n# Extract the image paths from the train set\n\n# Empty labels for storing images and labels\nval_images = []\n\n# Iterate over the image paths\nfor image in flower_val:\n    # Read the image from the current path, change the datatype, resize the image,\n    # add batch dimension, normalize the pixel values\n    image_pixels = plt.imread(image).astype(\"float32\")\n    image_pixels = cv2.resize(image_pixels, (224, 224))\n    image_pixels = np.expand_dims(image_pixels, 0)\n    image_pixels = image_pixels / 255.\n\n    # Append to the list\n    val_images.append(image_pixels)\n    val_labels = label_val\n\n# Create NumPy array\nval_images = np.array(val_images)\n</code></pre>"},{"location":"deep_learning/module9/Module9_TFTRT/#tf-trt-fp32-modele","title":"TF-TRT FP32 mod\u00e8le","text":"<p>On aura besoin de renseigner l'adresse <code>input_saved_model_dir=dir</code>, o\u00f9 <code>dir/saved_model.pb</code> existe. Passons l\u00e0 en variable.</p>      <pre><code>dir = 'base_model/flower_model_no_op/'\n</code></pre>     <pre><code>!saved_model_cli show --all --dir base_model/flower_model_no_op/\n</code></pre>      <pre>\n<code>2020-09-01 09:36:58.234629: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\n2020-09-01 09:36:58.379139: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\n\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['__saved_model_init_op']:\n  The given SavedModel SignatureDef contains the following input(s):\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['__saved_model_init_op'] tensor_info:\n        dtype: DT_INVALID\n        shape: unknown_rank\n        name: NoOp\n  Method name is: \n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['input_1'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 224, 224, 3)\n        name: serving_default_input_1:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['dense_1'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 5)\n        name: StatefulPartitionedCall:0\n  Method name is: tensorflow/serving/predict\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\n\nDefined Functions:\n  Function Name: '__call__'\n    Option #1\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')\n        Argument #2\n          DType: bool\n          Value: False\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #2\n      Callable with:\n        Argument #1\n          inputs: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n        Argument #2\n          DType: bool\n          Value: False\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #3\n      Callable with:\n        Argument #1\n          inputs: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n        Argument #2\n          DType: bool\n          Value: True\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #4\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')\n        Argument #2\n          DType: bool\n          Value: True\n        Argument #3\n          DType: NoneType\n          Value: None\n\n  Function Name: '_default_save_signature'\n    Option #1\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')\n\n  Function Name: 'call_and_return_all_conditional_losses'\n    Option #1\n      Callable with:\n        Argument #1\n          inputs: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n        Argument #2\n          DType: bool\n          Value: True\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #2\n      Callable with:\n        Argument #1\n          inputs: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n        Argument #2\n          DType: bool\n          Value: False\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #3\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')\n        Argument #2\n          DType: bool\n          Value: True\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #4\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')\n        Argument #2\n          DType: bool\n          Value: False\n        Argument #3\n          DType: NoneType\n          Value: None\n</code>\n</pre>         <p>Conversion parameters</p> <ul> <li> <p>There are additional parameters that can be passed to <code>saved_model_cli</code> and <code>TrtGraphConverterV2</code>:</p> <ul> <li><code>precision_mode</code> : The precision mode to use (FP32, FP16, or INT8)</li> <li><code>minimum_segment_size</code> : The minimum number of TensorFlow nodes required for a TensorRT subgraph to be valid.</li> <li><code>is_dynamic_op</code> : TensorRT engines are converted and built at model run time instead of during the <code>converter.convert()</code> call. This is required if there are tensors with unknown or dynamic shapes.</li> <li><code>use_calibration</code> : Only used if <code>precision_mode='INT8'</code>. If True, a calibration graph will be created, and <code>converter.calibrate()</code> should be called. This is the recommended option. If False, all tensors that will not be fused must have quantization nodes. See NVIDIA\u2019s INT8 Quantization for details.</li> <li><code>max_batch_size</code> : Used when <code>is_dynamic_op=False</code>. This is the maximum batch size for TensorRT engines. At run time, smaller batch sizes can be used, but a larger batch size will result in an error.</li> <li><code>maximum_cached_engines</code> : Used when <code>is_dynamic_op=True</code>. This limits the number of TensorRT engines that are cached, per TRTEngineOp.</li> </ul> </li> </ul>      <pre><code>print('Converting to TF-TRT FP32...')\nconversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode=trt.TrtPrecisionMode.FP32)\n\nconverter = trt.TrtGraphConverterV2(input_saved_model_dir=dir,\n                                    conversion_params=conversion_params)\nconverter.convert()\nconverter.save(output_saved_model_dir='dir_saved_model_TFTRT_FP32')\nprint('Done Converting to TF-TRT FP32')\n</code></pre>      <pre>\n<code>Converting to TF-TRT FP32...\nINFO:tensorflow:Linked TensorRT version: (6, 0, 1)\nINFO:tensorflow:Loaded TensorRT version: (6, 0, 1)\nINFO:tensorflow:Could not find TRTEngineOp_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\nINFO:tensorflow:Assets written to: dir_saved_model_TFTRT_FP32/assets\nDone Converting to TF-TRT FP32\n</code>\n</pre>        <pre><code>!saved_model_cli show --all --dir dir_saved_model_TFTRT_FP32\n</code></pre>      <pre>\n<code>2020-09-01 09:34:25.612510: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\n2020-09-01 09:34:25.612670: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n2020-09-01 09:34:25.612690: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['__saved_model_init_op']:\n  The given SavedModel SignatureDef contains the following input(s):\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['__saved_model_init_op'] tensor_info:\n        dtype: DT_INVALID\n        shape: unknown_rank\n        name: NoOp\n  Method name is: \n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['input_1'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 224, 224, 3)\n        name: serving_default_input_1:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['dense_1'] tensor_info:\n        dtype: DT_FLOAT\n        shape: unknown_rank\n        name: PartitionedCall:0\n  Method name is: tensorflow/serving/predict\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\n\nDefined Functions:\n  Function Name: '__call__'\n    Option #1\n      Callable with:\n        Argument #1\n          inputs: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n        Argument #2\n          DType: bool\n          Value: True\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #2\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')\n        Argument #2\n          DType: bool\n          Value: True\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #3\n      Callable with:\n        Argument #1\n          inputs: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n        Argument #2\n          DType: bool\n          Value: False\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #4\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')\n        Argument #2\n          DType: bool\n          Value: False\n        Argument #3\n          DType: NoneType\n          Value: None\n\n  Function Name: '_default_save_signature'\n    Option #1\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')\n\n  Function Name: 'call_and_return_all_conditional_losses'\n    Option #1\n      Callable with:\n        Argument #1\n          inputs: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n        Argument #2\n          DType: bool\n          Value: False\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #2\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')\n        Argument #2\n          DType: bool\n          Value: False\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #3\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')\n        Argument #2\n          DType: bool\n          Value: True\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #4\n      Callable with:\n        Argument #1\n          inputs: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n        Argument #2\n          DType: bool\n          Value: True\n        Argument #3\n          DType: NoneType\n          Value: None\n</code>\n</pre>         <p>On charge le mod\u00e8le converti est on lance une pr\u00e9diction.</p>      <pre><code>CLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.python.saved_model import tag_constants\n</code></pre>     <pre><code>def predict_tftrt(input_saved_model, img_path_i):\n  \"\"\"Runs prediction on a single image and shows the result.\n  input_saved_model (string): Name of the input model stored in the current dir\n  \"\"\"\n  img_path = ALL_IMG_PATHS[img_path_i]\n  img = image.load_img(img_path, target_size=(224, 224))\n  x = image.img_to_array(img)\n  x = np.expand_dims(x, axis=0)\n  #x = preprocess_input(x)\n  x = tf.constant(x)\n\n  saved_model_loaded = tf.saved_model.load(input_saved_model, tags=[tag_constants.SERVING])\n  signature_keys = list(saved_model_loaded.signatures.keys())\n  print(signature_keys)\n\n  infer = saved_model_loaded.signatures['serving_default']\n  print(infer.structured_outputs)\n\n  labeling = infer(x)\n  preds = labeling['dense_1'].numpy()\n  print(f'True: {image_path.split(\"/\")[1]} - Predicted: {CLASSES[np.argmax(preds)]}')\n  plt.subplot(2,2,1)\n  plt.imshow(img);\n  plt.axis('off');\n  plt.title('pred')\n</code></pre>     <pre><code>dataset = tf.data.Dataset.from_tensor_slices((val_images,val_labels))\n\ndef evaluate_tftrt_model(input_saved_model):\n  acc = tf.keras.metrics.Accuracy()\n\n  # Load TF TRT model\n  saved_model_loaded = tf.saved_model.load(input_saved_model, tags=[tag_constants.SERVING])\n  signature_keys = list(saved_model_loaded.signatures.keys())  \n  infer = saved_model_loaded.signatures['serving_default']\n\n  # Run predictions on every image in the \"test\" dataset.\n  predictions = []\n\n  for val_image, val_label in dataset:\n    # Run inference.\n\n    #print(infer.structured_outputs)\n\n    labeling = infer(val_image)\n\n    # Post-processing: remove batch dimension and find the digit with highest\n    # probability.\n    preds = labeling['dense_1'].numpy()\n    flower_id = np.argmax(preds)\n    predictions.append(flower_id)\n\n    # Compare prediction results with ground truth labels to calculate accuracy.\n\n  acc.update_state(val_labels, predictions)\n  acc = acc.result().numpy()\n\n  return accuracy\n</code></pre>     <pre><code>def benchmark_saved_model(input_saved_model, BATCH_SIZE=32):\n  # Load TF TRT model\n  saved_model_loaded = tf.saved_model.load(input_saved_model, tags=[tag_constants.SERVING])\n  signature_keys = list(saved_model_loaded.signatures.keys())  \n  print(signature_keys)\n\n  infer = saved_model_loaded.signatures['serving_default']\n  print(infer.structured_outputs)\n\n  print('Warming up for 50 batches...')\n  cnt = 0\n  for x, y in dataset:\n      labeling = infer(x)\n      cnt += 1\n      if cnt == 50:\n          break\n\n  print('Benchmarking inference engine...')\n  num_hits = 0\n  num_predict = 0\n  start_time = time.time()\n  for x, y in dataset:\n      labeling = infer(x)\n      preds = labeling['dense_1'].numpy()\n      num_hits += np.sum(preds == y)\n      num_predict += preds.shape[0]\n\n  print(f'Accuracy : {100*num_hits/num_predict:.2f%%}')\n  print(f'Inference speed: {num_predict/(time.time()-start_time):.2f} samples/s')\n</code></pre>     <pre><code>acc = evaluate_tftrt_model('dir_saved_model_TFTRT_FP32')\n\nprint(acc)\n</code></pre>      <pre>\n<code>0.8964578\n</code>\n</pre>        <pre><code>def get_gzipped_model_size(file):\n  # Returns size of gzipped model, in bytes.\n  import os\n  import zipfile\n  import tempfile\n\n  _, zipped_file = tempfile.mkstemp('.zip')\n  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n    f.write(file)\n\n  return os.path.getsize(zipped_file)\n\nprint(f\"Size of gzipped TF-TRT FP32 model: {get_gzipped_model_size('dir_saved_model_TFTRT_FP32/saved_model.pb')/1e6:.2f} Mb\")\n</code></pre>      <pre>\n<code>Size of gzipped pruned model trained from scratch: 22.02 Mb\n</code>\n</pre>        <pre><code>predict_tftrt('dir_saved_model_TFTRT_FP32')\n</code></pre>      <pre>\n<code>['serving_default']\n{'dense_1': TensorSpec(shape=&lt;unknown&gt;, dtype=tf.float32, name='dense_1')}\nflower_photos/tulips/7094415739_6b29e5215c_m.jpg - Predicted: dandelion\n</code>\n</pre>             <pre><code># Extract the image paths from the test set, shuffle them, and \n# choose 100 images\nrandom.shuffle(flower_test)\n</code></pre>     <pre><code>batch_size = 32\nbatched_input = np.zeros((batch_size, 224, 224, 3), dtype=np.float32)\n\nfor i in range(batch_size):\n  img_path = flower_test[i]\n  img = image.load_img(img_path, target_size=(224, 224))\n  x = image.img_to_array(img)\n  x = np.expand_dims(x, axis=0)\n  #x = preprocess_input(x)\n  batched_input[i, :] = x\nbatched_input = tf.constant(batched_input)\nprint('batched_input shape: ', batched_input.shape)\n</code></pre>      <pre>\n<code>batched_input shape:  (32, 224, 224, 3)\n</code>\n</pre>        <pre><code>saved_model_loaded = tf.saved_model.load('dir_saved_model_TFTRT_FP32', tags=[tag_constants.SERVING])\ninfer = saved_model_loaded.signatures['serving_default']\n\nlabeling = infer(batched_input)\n</code></pre>     <pre><code>labeling\n</code></pre>      <pre>\n<code>{'dense_1': &lt;tf.Tensor: shape=(32, 5), dtype=float32, numpy=\n array([[0.0088747 , 0.9767918 , 0.00127777, 0.01128097, 0.00177473],\n        [0.03964839, 0.47872713, 0.02123511, 0.37978247, 0.08060697],\n        [0.6408535 , 0.23330621, 0.04096965, 0.02476202, 0.06010861],\n        [0.03127632, 0.95195925, 0.00256521, 0.00962751, 0.0045717 ],\n        [0.23046839, 0.33052164, 0.03368737, 0.07395303, 0.33136958],\n        [0.07995716, 0.5434051 , 0.04632825, 0.15429737, 0.17601213],\n        [0.05205392, 0.90882397, 0.00295803, 0.02629342, 0.0098706 ],\n        [0.21822473, 0.67236066, 0.02545933, 0.03455472, 0.04940057],\n        [0.02356102, 0.96842057, 0.0019952 , 0.0044627 , 0.00156046],\n        [0.09176064, 0.86290795, 0.00751013, 0.01719821, 0.02062304],\n        [0.23318937, 0.46196407, 0.02395418, 0.10345814, 0.17743419],\n        [0.04559918, 0.8493675 , 0.00347995, 0.0747133 , 0.02684013],\n        [0.02022551, 0.9180966 , 0.01522244, 0.02123133, 0.02522412],\n        [0.02450896, 0.9172821 , 0.00481094, 0.02517896, 0.02821913],\n        [0.43404263, 0.36382154, 0.04026186, 0.05822622, 0.10364769],\n        [0.30016962, 0.5991007 , 0.06361873, 0.01602487, 0.0210861 ],\n        [0.02560495, 0.8583455 , 0.006536  , 0.03781543, 0.07169813],\n        [0.41791597, 0.38264656, 0.04154864, 0.06038012, 0.09750871],\n        [0.31952348, 0.5942062 , 0.01773698, 0.03286676, 0.03566658],\n        [0.05328519, 0.86068314, 0.00503189, 0.06404767, 0.01695201],\n        [0.22895539, 0.5240973 , 0.03053663, 0.08612359, 0.13028705],\n        [0.04869621, 0.82155716, 0.02925042, 0.05661807, 0.04387806],\n        [0.10426427, 0.7885772 , 0.01618585, 0.02916272, 0.06181002],\n        [0.30164632, 0.57752544, 0.03456168, 0.03043949, 0.05582702],\n        [0.23405264, 0.53193617, 0.02175272, 0.09940863, 0.11284982],\n        [0.17902733, 0.6940101 , 0.02832778, 0.05573856, 0.04289621],\n        [0.22064471, 0.58679503, 0.06802008, 0.01932614, 0.10521407],\n        [0.03830879, 0.83624995, 0.02486008, 0.04252513, 0.05805605],\n        [0.18920578, 0.76590884, 0.00939537, 0.02154765, 0.01394244],\n        [0.2575585 , 0.6262969 , 0.05659823, 0.02013459, 0.03941176],\n        [0.33415112, 0.5626648 , 0.01746169, 0.03285992, 0.05286248],\n        [0.14745806, 0.7814531 , 0.01624616, 0.02162713, 0.03321562]],\n       dtype=float32)&gt;}</code>\n</pre>        <pre><code>def benchmark_tftrt(input_saved_model):\n    saved_model_loaded = tf.saved_model.load(input_saved_model, tags=[tag_constants.SERVING])\n    infer = saved_model_loaded.signatures['serving_default']\n\n    N_warmup_run = 50\n    N_run = 1000\n    elapsed_time = []\n\n    for i in range(N_warmup_run):\n      labeling = infer(batched_input)\n\n    for i in range(N_run):\n      start_time = time.time()\n      labeling = infer(batched_input)\n      #prob = labeling['probs'].numpy()\n      end_time = time.time()\n      elapsed_time = np.append(elapsed_time, end_time - start_time)\n      if i % 50 == 0:\n        print(f'Step {i}: {(elapsed_time[-50:].mean() * 1000):4.1f}ms')\n\n    print(f'Throughput: {N_run * batch_size / elapsed_time.sum():.0f} images/s')\n</code></pre>     <pre><code>benchmark_tftrt('dir_saved_model_TFTRT_FP32')\n</code></pre>"},{"location":"deep_learning/module9/Module9_TFTRT/#tf-trt-fp16-modele","title":"TF-TRT FP16 mod\u00e8le","text":"<pre><code>print('Converting to TF-TRT FP16...')\nconversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n    precision_mode=trt.TrtPrecisionMode.FP16)\n\nconverter = trt.TrtGraphConverterV2(input_saved_model_dir=dir,\n                                    conversion_params=conversion_params)\n\nconverter.convert()\n\nconverter.save(output_saved_model_dir='dir_saved_model_TFTRT_FP16')\nprint('Done Converting to TF-TRT FP16')\n</code></pre>      <pre>\n<code>Converting to TF-TRT FP16...\nINFO:tensorflow:Linked TensorRT version: (6, 0, 1)\nINFO:tensorflow:Loaded TensorRT version: (6, 0, 1)\nINFO:tensorflow:Assets written to: dir_saved_model_TFTRT_FP16/assets\nDone Converting to TF-TRT FP16\n</code>\n</pre>        <pre><code>acc = evaluate_tftrt_model('dir_saved_model_TFTRT_FP16')\n\nprint(acc)\n</code></pre>      <pre>\n<code>0.8964578\n</code>\n</pre>        <pre><code>print(f\"Size of gzipped TF-TRT FP16 model: {get_gzipped_model_size('dir_saved_model_TFTRT_FP16/saved_model.pb')/1e6:.2f} Mb\")\n</code></pre>      <pre>\n<code>Size of gzipped TF-TRT FP16 model: 22.02 Mb\n</code>\n</pre>"},{"location":"deep_learning/module9/Module9_TFTRT/#tf-trt-int8-modele","title":"TF-TRT INT8 mod\u00e8le","text":"<pre><code>batch_size = 32\nbatched_input = np.zeros((batch_size, 224, 224, 3), dtype=np.float32)\n\nfor i in range(batch_size):\n  img_path = flower_test[i]\n  img = image.load_img(img_path, target_size=(224, 224))\n  x = image.img_to_array(img)\n  x = np.expand_dims(x, axis=0)\n  #x = preprocess_input(x)\n  batched_input[i, :] = x\nbatched_input = tf.constant(batched_input)\nprint('batched_input shape: ', batched_input.shape)\n</code></pre>      <pre>\n<code>batched_input shape:  (32, 224, 224, 3)\n</code>\n</pre>        <pre><code>print('Converting to TF-TRT INT8...')\nconversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n    precision_mode=trt.TrtPrecisionMode.INT8, \n    use_calibration=True)\nconverter = trt.TrtGraphConverterV2(\n    input_saved_model_dir=dir, \n    conversion_params=conversion_params)\n\ndef calibration_input_fn():\n    yield (batched_input, )\nconverter.convert(calibration_input_fn=calibration_input_fn)\n\nconverter.save(output_saved_model_dir='dir_saved_model_TFTRT_INT8')\nprint('Done Converting to TF-TRT INT8')\n</code></pre>      <pre>\n<code>Converting to TF-TRT INT8...\n</code>\n</pre>     <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-1-aca72c532aa7&gt; in &lt;module&gt;()\n      1 print('Converting to TF-TRT INT8...')\n----&gt; 2 conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n      3     precision_mode=trt.TrtPrecisionMode.INT8,\n      4     use_calibration=True)\n      5 converter = trt.TrtGraphConverterV2(\n\nNameError: name 'trt' is not defined</pre>        <pre><code>print(f\"Size of gzipped TF-TRT INT8 model: {get_gzipped_model_size('dir_saved_model_TFTRT_INT8/saved_model.pb')/1e6:.2f} Mb\")\n</code></pre>      <pre>\n<code>Size of gzipped pruned model trained from scratch: 22.02 Mb\n</code>\n</pre>        <pre><code>acc = evaluate_tftrt_model('dir_saved_model_TFTRT_INT8')\n\nprint(acc)\n</code></pre>      <pre>\n<code>0.8746594005449592\n</code>\n</pre>        <pre><code>benchmark_tftrt('dir_saved_model_TFTRT_INT8')\n</code></pre>      <pre>\n<code>Step 0: 81.2ms\nStep 50: 81.2ms\nStep 100: 81.2ms\nStep 150: 80.9ms\nStep 200: 80.9ms\nStep 250: 80.8ms\nStep 300: 80.8ms\nStep 350: 80.6ms\nStep 400: 80.5ms\nStep 450: 80.7ms\nStep 500: 80.9ms\nStep 550: 80.6ms\nStep 600: 80.8ms\nStep 650: 80.8ms\nStep 700: 80.8ms\nStep 750: 80.9ms\nStep 800: 80.8ms\nStep 850: 80.7ms\nStep 900: 80.6ms\nStep 950: 80.7ms\nThroughput: 396 images/s\n</code>\n</pre>"},{"location":"deep_learning/module9/bnn/","title":"Les r\u00e9seaux de neurones binaires","text":"<p>Librairie Larq pour TensorFlow</p>"},{"location":"deep_learning/module9/fusion/","title":"Annexe, RepVGG","text":"(function() {   function addWidgetsRenderer() {     var requireJsScript = document.createElement('script');     requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js';      var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]');     var jupyterWidgetsScript = document.createElement('script');     var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js';     var widgetState;      // Fallback for older version:     try {       widgetState = mimeElement &amp;&amp; JSON.parse(mimeElement.innerHTML);        if (widgetState &amp;&amp; (widgetState.version_major &lt; 2 || !widgetState.version_major)) {         widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js';       }     } catch(e) {}      jupyterWidgetsScript.src = widgetRendererSrc;      document.body.appendChild(requireJsScript);     document.body.appendChild(jupyterWidgetsScript);   }    document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }());"},{"location":"deep_learning/module9/fusion/#fusion-conv-bn-et-repvgg","title":"Fusion Conv-BN et RepVGG","text":"<pre><code>import tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import GlobalAvgPool2D, Flatten, ReLU, Softmax, Dense\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Add\nimport numpy as np\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend\n</code></pre>     <pre><code>img_shape = 32, 32, 3\n</code></pre>"},{"location":"deep_learning/module9/fusion/#etude-des-poids-des-conv-3-times-3-et-des-batchnorm","title":"Etude des poids des Conv \\(3 \\times 3\\) et des BatchNorm","text":"<p>Regardons comment s'articulent les poids dans les couches de convolutions et de batchnormalisation.</p>"},{"location":"deep_learning/module9/fusion/#initialisation-dun-modele","title":"Initialisation d'un mod\u00e8le","text":"<pre><code>input = Input(img_shape)\nx= Conv2D(filters = 16, kernel_size=3, padding='same', use_bias=True, kernel_initializer='he_uniform', name='testing_conv_init')(input)\nx= BatchNormalization(name=f'testing_bn_init')(x)\nmodel = Model(input, x)\nmodel.summary()\n</code></pre>      <pre>\n<code>Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n_________________________________________________________________\ntesting_conv_init (Conv2D)   (None, 32, 32, 16)        448       \n_________________________________________________________________\ntesting_bn_init (BatchNormal (None, 32, 32, 16)        64        \n=================================================================\nTotal params: 512\nTrainable params: 480\nNon-trainable params: 32\n_________________________________________________________________\n</code>\n</pre>"},{"location":"deep_learning/module9/fusion/#etude-de-la-couche-convolutive","title":"Etude de la couche convolutive","text":"<pre><code>weights_conv = model.get_layer(\"testing_conv_init\").get_weights()\nweights_conv\n</code></pre>      <pre>\n<code>[array([[[[-0.09736294,  0.01110744,  0.38817808,  0.02365676,\n            0.37265095,  0.22067323,  0.44893858, -0.4457162 ,\n            0.2723634 ,  0.21101996, -0.42767352,  0.39105812,\n           -0.38641602, -0.39619464, -0.12856498,  0.00230291],\n          [-0.15622476,  0.08576027, -0.39533868,  0.14336786,\n            0.09569708,  0.05594608,  0.05045763, -0.15595007,\n           -0.05612397, -0.19001147,  0.2724487 ,  0.3459774 ,\n            0.01586419,  0.08192965,  0.32559904,  0.04557905],\n          [-0.37503266,  0.05977681, -0.05365878,  0.34279034,\n           -0.22699383, -0.20862746,  0.13931164, -0.20776296,\n            0.12117836,  0.06501201, -0.28448707,  0.2668244 ,\n            0.2704514 , -0.34608564, -0.35193035, -0.3525829 ]],\n\n         [[ 0.11737838,  0.14824674, -0.00563487,  0.3061553 ,\n            0.01097953,  0.23561516, -0.4535907 , -0.4175086 ,\n            0.4607807 , -0.37212858, -0.30806714,  0.14160755,\n            0.24837866,  0.12601265,  0.2622976 ,  0.3263745 ],\n          [-0.34101892,  0.31189194, -0.11391068,  0.14759234,\n           -0.30657652, -0.13771534,  0.45230624,  0.22417751,\n            0.0407432 ,  0.07712099, -0.2991236 , -0.1449846 ,\n           -0.00576615,  0.26184592, -0.2169587 , -0.2828556 ],\n          [-0.20167324, -0.1357314 , -0.29285318,  0.33294716,\n            0.23413381, -0.00896761, -0.31519616,  0.14762929,\n           -0.18309683, -0.32602823,  0.10732868, -0.15018934,\n           -0.27001262,  0.0079852 , -0.20946455, -0.10388163]],\n\n         [[ 0.2783232 ,  0.05878171, -0.35913152,  0.40182415,\n            0.092141  , -0.13399044,  0.03964153,  0.06443217,\n            0.46447167, -0.41376618,  0.10037312,  0.42862818,\n            0.17965165, -0.13864604,  0.04373595, -0.13044247],\n          [-0.45564812, -0.33004287,  0.3405182 ,  0.39920047,\n           -0.25907567,  0.15346971, -0.02812734, -0.19346526,\n           -0.12104025, -0.4278583 ,  0.23774037,  0.3617756 ,\n           -0.0419741 ,  0.09765604, -0.26489764, -0.43987206],\n          [ 0.07685599,  0.04646841, -0.06539023,  0.02657837,\n            0.44154963,  0.21664849,  0.4601402 ,  0.21062568,\n            0.34529766,  0.30855897,  0.40019926, -0.26326853,\n           -0.04224968, -0.46108606, -0.37318596, -0.42175823]]],\n\n\n        [[[-0.3455502 ,  0.46076384, -0.15374777,  0.29009756,\n           -0.1667389 ,  0.31607136,  0.26514402,  0.3037739 ,\n           -0.0812335 ,  0.23732015,  0.07654181, -0.0679315 ,\n           -0.31959674, -0.28597334, -0.10885993,  0.408551  ],\n          [ 0.14297041, -0.09827566,  0.40382537, -0.4178524 ,\n           -0.01861295, -0.35064167,  0.45108077, -0.12225789,\n           -0.29017037,  0.23471412,  0.14587566, -0.36702543,\n           -0.3979674 ,  0.29743996, -0.39642572, -0.0245271 ],\n          [-0.0504581 ,  0.46684763,  0.37567046,  0.20170256,\n           -0.36068565, -0.17110959,  0.30896595,  0.09373525,\n           -0.21380827, -0.395913  ,  0.24522027,  0.36945108,\n           -0.06558827, -0.4528262 ,  0.08612862,  0.3769413 ]],\n\n         [[-0.06688267,  0.11741361, -0.14751217, -0.01143798,\n            0.30352488,  0.23946908,  0.15358987, -0.11050937,\n           -0.05894232, -0.22810066,  0.3403059 ,  0.23961261,\n           -0.16434652,  0.11093548,  0.00398877, -0.11645409],\n          [-0.37574512,  0.3296124 , -0.05067682, -0.09595928,\n           -0.2214837 ,  0.35080925, -0.02972579,  0.1532239 ,\n            0.18751535, -0.02314931, -0.18395177, -0.03616032,\n            0.27149966, -0.4180094 ,  0.28113106,  0.4416559 ],\n          [-0.2833048 ,  0.03007612,  0.34248617, -0.24044934,\n            0.16455218,  0.15701613,  0.20851544, -0.25038487,\n           -0.21328565, -0.23982422,  0.37252668,  0.15518335,\n            0.28911796,  0.44327244,  0.14157644,  0.26580074]],\n\n         [[ 0.28428563, -0.18229985, -0.20275667,  0.1955097 ,\n           -0.3543805 , -0.14616191,  0.28929475,  0.14749238,\n            0.2464734 , -0.46400836, -0.02009585,  0.3317866 ,\n           -0.20362425, -0.42040807, -0.17440939, -0.01986095],\n          [ 0.44240263,  0.1606206 , -0.4160647 ,  0.27185783,\n            0.06790957, -0.32414603, -0.23126818, -0.29003426,\n            0.12488225,  0.16395304, -0.3259102 , -0.38798535,\n           -0.43922648, -0.3790248 ,  0.12847778,  0.13634267],\n          [-0.02119684,  0.28930375, -0.4681574 ,  0.28300878,\n            0.40201846,  0.1442084 ,  0.19728747, -0.02722415,\n            0.42741737, -0.21063939,  0.42403385,  0.3592575 ,\n           -0.20373034, -0.4468028 , -0.08656737, -0.20087367]]],\n\n\n        [[[ 0.02782702,  0.01967528,  0.07292607,  0.389165  ,\n            0.36987802,  0.18188533,  0.04504755,  0.41210625,\n            0.01592982, -0.11140019, -0.11948159,  0.10680953,\n           -0.11324027,  0.39144352, -0.35009858,  0.26030532],\n          [ 0.09090939, -0.09534436,  0.03001484, -0.1512312 ,\n           -0.3052565 ,  0.3308485 , -0.24254534, -0.10194659,\n            0.00120181,  0.38111654,  0.21856287,  0.32130632,\n           -0.33506405,  0.44324157,  0.32223514, -0.2681678 ],\n          [ 0.4114515 , -0.40250486,  0.3429939 , -0.2685476 ,\n            0.37336466,  0.18623039, -0.23775014,  0.18628749,\n           -0.07814869,  0.15818772, -0.21979165, -0.44143543,\n           -0.09982735, -0.17906868, -0.11419335, -0.10601136]],\n\n         [[-0.04968157,  0.33098873, -0.35580167,  0.19270661,\n            0.33699194,  0.41285995,  0.23392567,  0.4191365 ,\n           -0.04616675,  0.22136435, -0.29591143,  0.20710972,\n           -0.3502412 , -0.10524371, -0.32441103,  0.35304728],\n          [-0.45383817, -0.21678528,  0.13484439, -0.23379093,\n           -0.038737  ,  0.02832112, -0.36996582, -0.19088644,\n            0.32281145,  0.14254984,  0.24848273,  0.3904991 ,\n           -0.41010976, -0.4470079 , -0.13003865, -0.41824162],\n          [ 0.14215276,  0.29101995, -0.40439036, -0.43802816,\n            0.33089224, -0.34787324,  0.32901898, -0.11648187,\n           -0.07495171,  0.4408585 ,  0.24113259, -0.08504415,\n            0.0623028 ,  0.23243883, -0.19044554,  0.3553057 ]],\n\n         [[-0.09105429,  0.30605808,  0.41014054,  0.33675548,\n           -0.05541334,  0.1684458 , -0.12973395, -0.1533834 ,\n           -0.00280687, -0.07284549,  0.35922107,  0.09879085,\n            0.43200687,  0.02035648,  0.36917636, -0.4091605 ],\n          [-0.22962008,  0.13721845, -0.12232229, -0.2520702 ,\n            0.07350466,  0.042968  ,  0.31936017,  0.36257067,\n           -0.10858962, -0.18460804, -0.1590521 , -0.47047156,\n           -0.1503287 , -0.04001006, -0.11763006, -0.24719194],\n          [-0.32062727,  0.13543853,  0.0306963 , -0.16442779,\n           -0.37330386,  0.13838956,  0.03568128, -0.2321063 ,\n            0.07501194, -0.426333  ,  0.00162551,  0.04697415,\n           -0.20748636, -0.30767232,  0.43708238, -0.16146705]]]],\n       dtype=float32),\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       dtype=float32)]</code>\n</pre>         <p>Les poids forment une liste de deux \u00e9lements : les poids des noyaux de convolutions et les biais. La m\u00e9thode d'initalisation utilis\u00e9e ici est <code>he_uniform</code>, d\u00e9velopp\u00e9e dans l'article Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</p>      <pre><code>type(weights_conv)\n</code></pre>      <pre>\n<code>list</code>\n</pre>        <pre><code>len(weights_conv)\n</code></pre>      <pre>\n<code>2</code>\n</pre>         <p>Les poids dans une couche convolutive sont une liste de deux \u00e9l\u00e9ments :  - <code>weights[0]</code> correspond aux poids des noyaux de convolution, - <code>weights[1]</code> correspond aux biais.</p>      <pre><code>type(weights_conv[0])\n</code></pre>      <pre>\n<code>numpy.ndarray</code>\n</pre>        <pre><code>weights_conv[0].shape \n</code></pre>      <pre>\n<code>(3, 3, 3, 16)</code>\n</pre>         <p>Les axes du tenseur de poids suivent les dimensions suivantes :</p> <ul> <li>kernel_size1 : hauteur du kernel,</li> <li>kernel_size2 : largeur du kernel,</li> <li>channels_in : nombre des feature maps en entr\u00e9e, </li> <li>channels_out : nombres de features maps (filters) en sortie.</li> </ul> <p><code>channels_out</code> est d\u00e9finie dans la couche convolutive via le param\u00e8tres <code>filters</code>, alors que la valeur <code>channels_in</code> est elle directement d\u00e9termin\u00e9e par le tenseur en entr\u00e9e. C'est une diff\u00e9rence de TensorFlow par rapport \u00e0 Pytorch o\u00f9 <code>channels_in</code> et <code>channels_out</code> sont tous les deux des param\u00e8tres des couches convolutives.</p> <p>Ainsi, si l'on veut voir les poids du noyau de convolution par rapport au canal \\(0\\) en la feature map de sortie \\(5\\), on les obtient en regardant :</p>      <pre><code>weights_conv[0][:,:,0,5]\n</code></pre>      <pre>\n<code>array([[ 0.22067323,  0.23561516, -0.13399044],\n       [ 0.31607136,  0.23946908, -0.14616191],\n       [ 0.18188533,  0.41285995,  0.1684458 ]], dtype=float32)</code>\n</pre>         <p>Par d\u00e9faut, les biais des couches de convolutions sont tous initialis\u00e9s \u00e0 z\u00e9ro.</p>      <pre><code>weights_conv[1]\n</code></pre>      <pre>\n<code>array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)</code>\n</pre>"},{"location":"deep_learning/module9/fusion/#etude-de-la-batchnorm","title":"Etude de la batchnorm","text":"<pre><code>weights_bn = model.get_layer('testing_bn_init').get_weights()\nweights_bn\n</code></pre>      <pre>\n<code>[array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       dtype=float32),\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       dtype=float32),\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       dtype=float32),\n array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       dtype=float32)]</code>\n</pre>        <pre><code>type(weights_bn)\n</code></pre>      <pre>\n<code>list</code>\n</pre>        <pre><code>len(weights_bn)\n</code></pre>      <pre>\n<code>4</code>\n</pre>         <p>Dans une couche de Batchnormalization, on a 4 types de poids.</p> <ul> <li>Les deux param\u00e8tres de scaling \\(\\gamma\\) et de biais \\(\\beta\\).</li> <li>Les deux param\u00e8tres correspondant \u00e0 la moyenne \\(\\mu\\) et la variance \\(\\sigma\\).</li> </ul>       <p>Tous ces param\u00e8tres ne sont pas entra\u00eenables, comme on peut le voir dans la liste suivante.</p>      <pre><code>[(var.name, var.trainable) for var in model.get_layer('testing_bn_init').variables]\n</code></pre>     <pre><code>backend.shape(model.get_layer('testing_bn_init').get_weights())\n</code></pre>      <pre>\n<code>&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([ 4, 16], dtype=int32)&gt;</code>\n</pre>         <p>Les \\(4\\) param\u00e8tres sont tous des vecteurs de dimension \\(16\\), ce qui correspond au nombre de feature maps en sortie de la couche convolutive. </p>"},{"location":"deep_learning/module9/fusion/#fusion-dune-convolution-et-dune-batchnorm","title":"Fusion d'une Convolution et d'une batchnorm","text":"<p>La fusion d'une couche de convolution avec une couche de batchnorm ressort les poids et biais d'une nouvelle couche de convolution avec les noyaux de convolutions de m\u00eame dimension.</p> <p>Etant donn\u00e9 le tenseur \\(W\\) de poids des noyaux de convolution d'une couche convolutive et le tenseur de \\(4\\) param\u00e8tres \\(B=(\\gamma, \\beta, \\mu, \\sigma)\\) d'une couche de batchnormalization, on obtient les nouveaux poids et poids de la nouvelle couche convolutive via les formules suivantes.</p> \\[ \\widehat{W}_{:,:,:,j} := \\frac{\\gamma_{j} \\cdot W_{:,:,:,j}}{\\sqrt{\\sigma_{j} + \\epsilon}} \\] \\[ b_{j} = \\beta_{j} - \\frac{\\mu_{j}\\cdot\\gamma_{j}}{\\sqrt{\\sigma_{j} + \\epsilon}}  \\] <p>On remarque ici que le biais de la nouvelle couche de convolution ne d\u00e9pend que des param\u00e8tres de la couche de batchnorm. Ce qui est coh\u00e9rent avec la pratique de ne jamais mettre de biais dans une couche de convolution lorsqu'elle est suivie par une couche de batchnorm.</p> <p>Remarque : le \\(\\epsilon\\) pr\u00e9sent ici est pour s'assurer que l'on ne divise jamais pas z\u00e9ro, dans la pratique il est fix\u00e9 \u00e0 \\(0,001\\).</p> <p>Ce qui nous donne, dans la pratique la fonction suivante.</p>      <pre><code># https://scortex.io/batch-norm-folding-an-easy-way-to-improve-your-network-speed/\n# https://github.com/DingXiaoH/RepVGG/blob/4da799e33c890c624bfb484b2c35abafd327ba40/repvgg.py#L68\n\ndef fuse_bn_conv(weights_conv, weights_bn, eps=0.001):\n    gamma = np.reshape(weights_bn[0], (1,1,1,weights_bn[0].shape[0]))\n    beta = weights_bn[1]\n    mean = weights_bn[2]\n    variance = np.reshape(weights_bn[3], (1,1,1,weights_bn[3].shape[0]))\n\n    new_weights = (weights_conv[0]*gamma) / np.sqrt(variance + eps)\n    new_bias = beta - mean*gamma/np.sqrt(variance+eps)\n\n    new_bias = np.reshape(new_bias, weights_bn[3].shape[0])\n\n    return new_weights, new_bias\n\n# In the code above, the reshaping is necessary to prevent a mistake if the dimension of the output O was the same as the dimension of the input I. \n\n# def get_equivalent_kernel_bias(self):\n#    kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)\n#    kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\n#    kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)\n#    return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\n</code></pre>      <p>D\u00e9taillons la fonction ci dessus.</p>"},{"location":"deep_learning/module9/fusion/#nouveau-tenseur-de-poids","title":"Nouveau tenseur de poids","text":"<p>Discutons premi\u00e8rement de la formulation du nouveau tenseur de poids, et voyons pourquoi on modifie la forme de vecteurs \\(\\gamma\\) et \\(\\sigma\\).</p> <p>\\(W_{:,:,:,j}\\) correspond dans la formule au noyau de convolution complet de la \\(j\\)-i\u00e8me feature map de sortie.</p>      <pre><code>weights_conv = model.get_layer(\"testing_conv_init\").get_weights()\nweights_conv[0].shape\n</code></pre>      <pre>\n<code>(3, 3, 3, 16)</code>\n</pre>         <p>On a \\(16\\) noyaux de convolution, chacun de dimensions \\((3,3,3)\\). Par exemple, pour \\(j=1\\).</p>      <pre><code>weights_conv[0][:,:,:,1].shape\n</code></pre>      <pre>\n<code>(3, 3, 3)</code>\n</pre>         <p>Les vecteur \\(\\gamma\\) et \\(\\sigma\\) \u00e9tant des vecteurs de dimension \\(16\\), on va les \"transformer en tenseur\" de dimensions \\((1,1,1,16)\\) pour bien faire correspondre le produit suivant chaque axe.</p>      <pre><code>variance = np.reshape(weights_bn[3], (1,1,1,weights_bn[3].shape[0]))\nvariance.shape\n</code></pre>      <pre>\n<code>(1, 1, 1, 16)</code>\n</pre>        <pre><code>gamma = np.reshape(weights_bn[0], (1,1,1,weights_bn[0].shape[0]))\ngamma.shape\n</code></pre>      <pre>\n<code>(1, 1, 1, 16)</code>\n</pre>                <p>Au final, la formule</p> <pre><code>new_weights = (weights_conv[0]*gamma) / np.sqrt(variance + eps)\n</code></pre> <p>r\u00e9sume tout cela, tous les tenseurs ayant le nombre d'axes, les op\u00e9rations sont vectoris\u00e9es et se font axe par axe.</p>"},{"location":"deep_learning/module9/fusion/#nouveau-tenseur-de-biais","title":"Nouveau tenseur de biais","text":"<p>Le op\u00e9rations de <code>reshape</code> n'ont pas ajouter de nouveaux scalaires, juste des axes, le calcul du biais se fait alors \u00e9l\u00e9ment par \u00e9l\u00e9ment pour tout \\(j\\).</p>"},{"location":"deep_learning/module9/fusion/#verification-via-les-developpements-limites","title":"V\u00e9rification via les d\u00e9veloppements limit\u00e9s","text":"<p>Cr\u00e9ons un tenseur de poids \\(W\\) rep\u00e9resentatif du noyau d'une convolution et un tenseur de poids \\(B=(\\gamma, \\beta, \\mu, \\sigma)\\) repr\u00e9sentatif des coefficients d'une batchnormalization.</p> <p>Pour v\u00e9rifier si tout marche bien, fixons volontairement le tenseur poids comme un tenseur de dimensions \\((3,3,4,5)\\), la dimension du noyau est toujours fix\u00e9 \u00e0 \\((3,3)\\) dans RepVGG, seules les dimensions <code>channels_in</code> et <code>channels_out</code> peuvent changer.</p> <p>Tous les coefficients du tenseur de poids seront fix\u00e9s \u00e0 \\(1\\).</p>      <pre><code>conv_weights = np.ones(3*3*4*5).reshape((3,3,4,5))\nconv_weights.shape\n</code></pre>      <pre>\n<code>(3, 3, 4, 5)</code>\n</pre>         <p>La dimension <code>channels_out</code> ayant \u00e9t\u00e9 fix\u00e9e \u00e0 \\(5\\), les vecteurs de la batchnormalization seront tous des vecteurs de dimension \\(5\\). Fixons les coefficients suivants.</p>      <pre><code>def batchnorm_variables(gamma_coef: float, beta_coef: float, mu_coef: float, sigma_coef: float, channels: int):\n    gamma = gamma_coef*np.ones(channels)\n    beta = beta_coef*np.ones(channels)\n    mu = mu_coef*np.ones(channels)\n    sigma = sigma_coef*np.ones(channels)\n\n    return [gamma, beta, mu, sigma]\n</code></pre>     <pre><code>conv, bn = fuse_bn_conv([conv_weights], batchnorm_variables(1,2,1,4,5))\n</code></pre>      <p>Par d\u00e9finition, le nouveau tenseur de poids \\(\\widehat{W}\\) de la convolution r\u00e9sultant de la fusion de l'ancienne convolution et de la batchnorm est donn\u00e9 par formule suivante.</p> \\[ \\widehat{W}_{:,:,:,j} := \\frac{\\gamma_{j} \\cdot W_{:,:,:,j}}{\\sqrt{\\sigma_{j} + \\epsilon}} \\] <p>De fa\u00e7on g\u00e9n\u00e9rale, pour \\(\\gamma_{j}, \\sigma_{j}\\), on a le d\u00e9veloppement limit\u00e9 suivant.</p> \\[ \\widehat{W}_{:,:,:,j} := \\frac{\\gamma_{j} \\cdot W_{:,:,:,j}}{\\sqrt{\\sigma_{j} + \\epsilon}} = \\frac{\\gamma_{j}}{\\sqrt{\\sigma_{j}}}\\left[1- \\frac{1}{2\\sigma_{j}}\\epsilon + o(\\epsilon^{2})\\right]W_{:,:,:,j}  \\] <p>Dans notre cas, \\(\\forall j, \\gamma_{j} = 1, \\sigma_{j} = 4\\) d'o\u00f9</p> \\[ \\widehat{W}_{:,:,:,j} := \\frac{W_{:,:,:,j}}{\\sqrt{4 + \\epsilon}} = \\left[\\frac{1}{2}- \\frac{1}{16}\\epsilon + o(\\epsilon^{2})\\right]W_{:,:,:,j} \\simeq \\left[\\frac{1}{2}- \\frac{1}{16}\\epsilon\\right]W_{:,:,:,j} \\]      <pre><code>def compute_scaling_weight_factor(gamma, sigma):\n    return gamma/np.sqrt(sigma)*(1-0.001/(2*sigma))\n</code></pre>     <pre><code>scale = compute_scaling_weight_factor(1,4)\nscale\n</code></pre>      <pre>\n<code>0.4999375</code>\n</pre>        <pre><code>conv[:,:,:,4]\n</code></pre>      <pre>\n<code>array([[[0.49993751, 0.49993751, 0.49993751, 0.49993751],\n        [0.49993751, 0.49993751, 0.49993751, 0.49993751],\n        [0.49993751, 0.49993751, 0.49993751, 0.49993751]],\n\n       [[0.49993751, 0.49993751, 0.49993751, 0.49993751],\n        [0.49993751, 0.49993751, 0.49993751, 0.49993751],\n        [0.49993751, 0.49993751, 0.49993751, 0.49993751]],\n\n       [[0.49993751, 0.49993751, 0.49993751, 0.49993751],\n        [0.49993751, 0.49993751, 0.49993751, 0.49993751],\n        [0.49993751, 0.49993751, 0.49993751, 0.49993751]]])</code>\n</pre>         <p>Ce qui correspond bien \u00e0 l'approximation obtenue par d\u00e9veloppement limit\u00e9. On peut par exemple v\u00e9rifier si \\(\\widehat{W}\\) est approximativement \u00e9gal \u00e0 <code>conv</code> \u00e0 \\(10^{-3}\\) avec la commande <code>np.isclose</code>.</p>      <pre><code>conv_weights_real = scale*np.ones(3*3*4*5).reshape((3,3,4,5))\nconv_weights_real[:,:,:,0]\n</code></pre>      <pre>\n<code>array([[[0.4999375, 0.4999375, 0.4999375, 0.4999375],\n        [0.4999375, 0.4999375, 0.4999375, 0.4999375],\n        [0.4999375, 0.4999375, 0.4999375, 0.4999375]],\n\n       [[0.4999375, 0.4999375, 0.4999375, 0.4999375],\n        [0.4999375, 0.4999375, 0.4999375, 0.4999375],\n        [0.4999375, 0.4999375, 0.4999375, 0.4999375]],\n\n       [[0.4999375, 0.4999375, 0.4999375, 0.4999375],\n        [0.4999375, 0.4999375, 0.4999375, 0.4999375],\n        [0.4999375, 0.4999375, 0.4999375, 0.4999375]]])</code>\n</pre>         <p>Si <code>np.mean(...)</code> \\(&lt; 1\\) alors le calcul est faux.</p>      <pre><code>np.mean(np.isclose(conv, conv_weights_real, rtol=1e-3))\n</code></pre>      <pre>\n<code>1.0</code>\n</pre>         <p>Pour le biais, on a la formule suivante.</p> \\[ b_{j} = \\beta_{j} - \\frac{\\mu_{j}\\cdot\\gamma_{j}}{\\sqrt{\\sigma_{j} + \\epsilon}} = \\beta_{j} - \\frac{\\mu_{j}\\cdot\\gamma_{j}}{\\sqrt{\\sigma_{j}}}\\left[1- \\frac{1}{2\\sigma_{j}}\\epsilon + o(\\epsilon^{2})\\right] \\] <p>dans notre cas, on a :</p> <ul> <li>\\(\\beta_{j} = 2\\),</li> <li>\\(\\gamma_{j} = 1\\),</li> <li>\\(\\mu_{j} = 1\\),</li> <li>\\(\\sigma_{j} = 4\\).</li> </ul> \\[ b_{j} = 2 - \\frac{1}{2}\\left[1- \\frac{1}{8}\\epsilon + o(\\epsilon^{2})\\right] \\simeq 2 - \\frac{1}{2} - \\frac{1}{16}\\epsilon \\]      <pre><code>def compute_scaling_bias_factor(gamma, beta, mu, sigma):\n    a = (mu*gamma)/np.sqrt(sigma)\n    b = 1 - 0.001/(2*sigma)\n\n    return beta-a*b\n</code></pre>     <pre><code>bias_scale = compute_scaling_bias_factor(1,2,1,4)\nbias_scale\n</code></pre>     <pre><code>bn\n</code></pre>      <pre>\n<code>array([1.50006249, 1.50006249, 1.50006249, 1.50006249, 1.50006249])</code>\n</pre>        <pre><code>bn_real = bias_scale*np.ones(5)\nnp.mean(np.isclose(bn_real, bn, rtol=1e-3))\n</code></pre>      <pre>\n<code>1.0</code>\n</pre>"},{"location":"deep_learning/module9/fusion/#repvgg","title":"RepVGG","text":"<p>Les couches convolutives dans RepVGG n'ayant que des noyaux \\(3\\times3\\) ou \\(1\\times1\\), on ne se pr\u00e9occupe que de cela dans la suite.</p>"},{"location":"deep_learning/module9/fusion/#fusion-dune-conv-3times3-avec-une-batchnorm-puis-transfert-de-poids","title":"Fusion d'une Conv \\(3\\times3\\) avec une batchnorm puis transfert de poids","text":"<p>Cr\u00e9ons un mod\u00e8le simple : une couche convolutive suivi d'une couche de batchnormalisation, pour simplifier on ne condi\u00e8re aucune couche d'activation (qui de toute fa\u00e7on ne rentre pas en jeu). Nous allons :</p> <ol> <li>Fusionner les deux couches pour cr\u00e9er un nouveau tenseur (poids, biais)</li> <li>Transf\u00e9rer ce nouveau tensor dans un mod\u00e8le plus simple <code>model_after_fusion</code>.</li> </ol> <p>Remarque : la convolution dans <code>model_after_fusion</code> utilise elle bien un biais (<code>use_bias = True</code>).</p>      <pre><code>input = Input(img_shape)\nx= Conv2D(filters = 16, kernel_size=3, padding='same', use_bias=False, kernel_initializer='he_uniform', name='conv')(input)\nx= BatchNormalization(name='bn')(x)\nmodel_before_fusion = Model(input, x)\nmodel_before_fusion.summary()\n</code></pre>      <pre>\n<code>Model: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n_________________________________________________________________\nconv (Conv2D)                (None, 32, 32, 16)        432       \n_________________________________________________________________\nbn (BatchNormalization)      (None, 32, 32, 16)        64        \n=================================================================\nTotal params: 496\nTrainable params: 464\nNon-trainable params: 32\n_________________________________________________________________\n</code>\n</pre>        <pre><code>input = Input(img_shape)\nx= Conv2D(filters = 16, kernel_size=3, padding='same', use_bias=True, kernel_initializer='he_normal', name='conv')(input)\nmodel_after_fusion = Model(input, x)\nmodel_after_fusion.summary()\n</code></pre>      <pre>\n<code>Model: \"model_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_3 (InputLayer)         [(None, 32, 32, 3)]       0         \n_________________________________________________________________\nconv (Conv2D)                (None, 32, 32, 16)        448       \n=================================================================\nTotal params: 448\nTrainable params: 448\nNon-trainable params: 0\n_________________________________________________________________\n</code>\n</pre>        <pre><code>weights_1 = model_before_fusion.get_layer('conv').get_weights()[0]\nweights_2 = model_after_fusion.get_layer('conv').get_weights()[0]\n</code></pre>     <pre><code>np.mean(weights_1-weights_2)\n</code></pre>      <pre>\n<code>0.0023266133</code>\n</pre>        <pre><code>conv = model_before_fusion.get_layer(\"conv\")\nbn = model_before_fusion.get_layer(\"bn\")\n\nconv_weights, conv_biases = fuse_bn_conv(conv.get_weights(), bn.get_weights())\nmodel_after_fusion.get_layer(f\"conv\").set_weights([conv_weights, conv_biases])\n</code></pre>      <p>V\u00e9rifions que la mise en place des nouveaux poids s'est bien pass\u00e9e, ie que l'op\u00e9ration <code>set_weights()</code> n'a rien ajout\u00e9 de suppl\u00e9mtentaire. Si tout se passe bien, <code>np.mean</code> ne devrait renvoyer que des <code>1.0</code>.</p>      <pre><code>w0, b0 = fuse_bn_conv(model_before_fusion.get_layer(\"conv\").get_weights(), model_before_fusion.get_layer(\"bn\").get_weights())\n\nw1, b1 = model_after_fusion.get_layer(\"conv\").get_weights()\n</code></pre>     <pre><code>np.mean(w0 == w1)\n</code></pre>      <pre>\n<code>1.0</code>\n</pre>        <pre><code>np.mean(b0 == b1)\n</code></pre>      <pre>\n<code>1.0</code>\n</pre>         <p>Donc tout s'est bien pass\u00e9. Reste maintenant \u00e0 g\u00e9n\u00e9raliser cette transformation.</p>       <p>L'id\u00e9e de RepVGG est d'utiliser une architecture \u00e0 la ResNet pour l'entra\u00eenement, avec des skips connections, puis lors du d\u00e9ploiement du mod\u00e8le de reparam\u00e9trer les skips connections via des fusions Conv-BN afin de plus avoir qu'une architecture lin\u00e9aire \u00e0 la VGG, beaucoup plus rapide en inf\u00e9rence qu'une architecture \u00e0 la ResNet.</p> <p>En plus de fusionner des \\(\\mathrm{Conv} 3 \\times 3\\) avec des \\(\\mathrm{BN}\\), il est aussi n\u00e9cessaire de savoir faire les op\u00e9rations suivantes.</p> <ol> <li>Convertir une \\(\\mathrm{Conv} 1 \\times 1\\) en \\(\\mathrm{Conv} 3 \\times 3\\) puis la fusionner avec la \\(\\mathrm{BN}\\) correspondante.</li> <li>Convertir une \\(\\mathrm{id}\\) en \\(\\mathrm{Conv} 3 \\times 3\\) puis la fusionner avec la \\(\\mathrm{BN}\\) correspondante.</li> </ol>"},{"location":"deep_learning/module9/fusion/#conversion-dune-conv-1-times-1-en-3-times-3-puis-fusion-avec-la-batchnorm","title":"Conversion d'une Conv \\(1 \\times 1\\) en \\(3 \\times 3\\) puis fusion avec la batchnorm.","text":"<p>Pour convertir une conv 1x1 en conv 3x3 les nombres de canaux en entr\u00e9e et en sortie importe peu, ce qu'il faut c'est modifier la dimension des noyaux de convolutions pour passer d'une dimension 1x1 \u00e0 3x3, et pour cela on utilise un padding.</p>      <pre><code>input = Input(img_shape)\nx= Conv2D(filters = 16, kernel_size=1, padding='same', use_bias=False, kernel_initializer='he_uniform', name='conv')(input)\nx= BatchNormalization(name='bn')(x)\nmodel_before_fusion_conv1 = Model(input, x)\nmodel_before_fusion_conv1.summary()\n</code></pre>      <pre>\n<code>Model: \"model_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_4 (InputLayer)         [(None, 32, 32, 3)]       0         \n_________________________________________________________________\nconv (Conv2D)                (None, 32, 32, 16)        48        \n_________________________________________________________________\nbn (BatchNormalization)      (None, 32, 32, 16)        64        \n=================================================================\nTotal params: 112\nTrainable params: 80\nNon-trainable params: 32\n_________________________________________________________________\n</code>\n</pre>        <pre><code>input = Input(img_shape)\nx= Conv2D(filters = 16, kernel_size=3, padding='same', use_bias=True, kernel_initializer='he_normal', name='conv')(input)\nmodel_after_fusion_conv1 = Model(input, x)\nmodel_after_fusion_conv1.summary()\n</code></pre>      <pre>\n<code>Model: \"model_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_5 (InputLayer)         [(None, 32, 32, 3)]       0         \n_________________________________________________________________\nconv (Conv2D)                (None, 32, 32, 16)        448       \n=================================================================\nTotal params: 448\nTrainable params: 448\nNon-trainable params: 0\n_________________________________________________________________\n</code>\n</pre>        <pre><code>weights_conv1 = model_before_fusion_conv1.get_layer('conv')\nweights_bn1 = model_before_fusion_conv1.get_layer('bn')\n</code></pre>     <pre><code>weights_conv1.get_weights()[0].shape\n</code></pre>      <pre>\n<code>(1, 1, 3, 16)</code>\n</pre>         <p>La premi\u00e8re chose \u00e0 faire, c'est de transformer les noyaux de convolution \\(1\\times1\\) en des noyaux de convolution \\(3\\times3\\). Pour faire cela, on utilise la notion de \"padding\", d\u00e9j\u00e0 utilis\u00e9e dans le cas des convolutions.</p>      <pre><code>weights_conv1.get_weights()[0][:,:,1,1]\n</code></pre>      <pre>\n<code>array([[-1.227452]], dtype=float32)</code>\n</pre>         <p>On a deux fonctions possibles pour faire \u00e7a. On peut utiliser soit la fonction de tensorflow.</p> <pre><code>padded_conv1 = tf.pad(weights_conv1.get_weights()[0], [[1,1], [1, 1], [0,0], [0,0]], \"CONSTANT\")\n</code></pre> <p>Soit la fonction de numpy.</p> <pre><code>padded_conv1 = np.pad(weights_conv1.get_weights()[0], pad_width=[[1,1], [1, 1], [0,0], [0,0]], mode='constant', constant_values=0)\n</code></pre> <p>Dans les deux cas, on a un param\u00e8tre donnant la taille du padding : <code>[[1,1], [1, 1], [0,0], [0,0]]</code>, c'est une liste de longueur le nombre d'axes du tenseur que l'on souhaite modifier, chaque \u00e9l\u00e9ment de la liste nous dit de combien on doit agrandir au d\u00e9but et \u00e0 la fin.</p> <p><code>[[1,1], [1, 1], [0,0], [0,0]] = [[pad_avant_axe1, pad_arri\u00e8re_axe1], [pad_avant_axe2, pad_arri\u00e8re_axe2], [pad_avant_axe3, pad_arri\u00e8re_axe3], [pad_avant_axe4, pad_arri\u00e8re_axe4]]</code></p> <p>Le dernier param\u00e8tre nous dit quoi rajouter aux endroits o\u00f9 l'on a agrandi, ici des constantes : la valeur \\(0\\).</p>      <pre><code>padded_conv1_tf = tf.pad(weights_conv1.get_weights()[0], [[1,1], [1, 1], [0,0], [0,0]], \"CONSTANT\")\npadded_conv1_np = np.pad(weights_conv1.get_weights()[0], pad_width=[[1,1], [1, 1], [0,0], [0,0]], mode='constant', constant_values=0)\n</code></pre>      <p>Les deux fonctions donnent le m\u00eame r\u00e9sultat.</p>      <pre><code>np.mean(padded_conv1_tf.numpy()==padded_conv1_np)==1\n</code></pre>      <pre>\n<code>True</code>\n</pre>         <p>Comme la fonction <code>set_weights()</code> demande d'utiliser des <code>np.array</code>, on va utiliser la fonction de numpy.</p>      <pre><code>def pad_size_one_kernel(conv_weights):    \n    return np.pad(conv_weights[0], pad_width=[[1,1], [1, 1], [0,0], [0,0]], mode='constant', constant_values=0)\n</code></pre>     <pre><code>padded_weights_conv1 = pad_size_one_kernel(weights_conv1.get_weights())\npadded_weights_conv1.shape\n</code></pre>      <pre>\n<code>(3, 3, 3, 16)</code>\n</pre>"},{"location":"deep_learning/module9/fusion/#verification","title":"V\u00e9rification","text":"<p>On a transform\u00e9 tous les noyaux de convolutions \\(1\\times1\\) en noyaux \\(3\\times3\\), chacun des <code>padded_weights_conv1[:,:,i,j]</code> pour \\(0 \\leq i \\leq 2\\) et \\(0 \\leq j \\leq 15\\) doit \u00eatre une matrice \\(3\\times3\\) o\u00f9 tous les \u00e9l\u00e9ments sont nuls sauf possiblement celui du milieu.</p>      <pre><code>def test_padded_kernel_conv(padded_kernel):\n    for i in range(3):\n        for j in range(16):\n            print(f'Matrix of size 3x3 : {padded_kernel[:,:,i,j].shape == (3,3)}')\n            squared_sum = 0\n            for k in range(3):\n                for l in range(3):\n                    if k != 1 and l != 1:\n                        squared_sum += padded_kernel[:,:,i,j][k,l]**2\n            print(f'Squared sum is : {squared_sum}')\n</code></pre>     <pre><code>test_padded_kernel_conv(padded_weights_conv1)\n</code></pre>      <pre>\n<code>Matrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\n</code>\n</pre>        <pre><code>dummy_conv = np.ones(1*1*3*16).reshape((1,1,3,16))\n</code></pre>     <pre><code>padded_dummy_conv=pad_size_one_kernel([dummy_conv])\ntest_padded_kernel_conv(padded_dummy_conv)\n</code></pre>      <pre>\n<code>Matrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nSquared sum is : 0.0\n</code>\n</pre>         <p>Comme pr\u00e9c\u00e9demment, on v\u00e9rifie via les d\u00e9veloppements limit\u00e9s que \u00e7a fonctionne.</p>      <pre><code>conv, bn = fuse_bn_conv([padded_dummy_conv], batchnorm_variables(1,2,1,4,16))\n</code></pre>     <pre><code>scale = compute_scaling_weight_factor(1,4)\nscale\n</code></pre>      <pre>\n<code>0.4999375</code>\n</pre>        <pre><code>np.mean(np.isclose(conv, scale*padded_dummy_conv, rtol=1e-3))\n</code></pre>      <pre>\n<code>1.0</code>\n</pre>        <pre><code>bias_scale = compute_scaling_bias_factor(1,2,1,4)\nbias_scale\n</code></pre>      <pre>\n<code>1.5000625</code>\n</pre>        <pre><code>bn_real = bias_scale*np.ones(16)\nnp.mean(np.isclose(bn_real, bn, rtol=1e-3))\n</code></pre>      <pre>\n<code>1.0</code>\n</pre>        <pre><code>weights_conv1 = model_before_fusion_conv1.get_layer('conv')\nweights_bn1 = model_before_fusion_conv1.get_layer('bn')\n\npadded_weights_conv1 = pad_size_one_kernel(weights_conv1.get_weights())\nconv_weights, conv_bias = fuse_bn_conv([padded_weights_conv1], weights_bn1.get_weights())\n\nmodel_after_fusion_conv1.get_layer(\"conv\").set_weights([conv_weights, conv_bias])\n</code></pre>"},{"location":"deep_learning/module9/fusion/#conversion-dune-mathrmid-en-mathrmconv-3-times-3-puis-fusion-avec-la-batchnorm","title":"Conversion d'une \\(\\mathrm{id}\\) en \\(\\mathrm{Conv} 3 \\times 3\\) puis fusion avec la batchnorm.","text":"<p>Les branches id ne sont utilis\u00e9es dans l'architecture de RepVGG que lorsque la conditions <code>channels_in</code> = <code>channels_out</code> est v\u00e9rifi\u00e9e, c'est \u00e0 dire \u00e0 l'int\u00e9rieur de chaque stage entre 2 blocs convolutifs avec un stride de 2.</p>      <pre><code># Fixons le nombre de channels, peut importe le nombre.\nchannels = 4\n</code></pre>      <p>An identity mapping can be viewed as a \\(1\\times1\\) conv with an identity matrix as the kernel.</p>      <pre><code>def size_three_kernel_from_id(channels):\n    kernel = np.ones(channels)\n    kernel = np.diag(kernel)\n    kernel = np.reshape(kernel, (1,1,channels,channels))\n    kernel = np.pad(kernel, pad_width=[[1,1], [1, 1], [0,0], [0,0]], mode='constant', constant_values=0)\n\n    return kernel\n</code></pre>     <pre><code>conv_from_id = size_three_kernel_from_id(4)\nconv_from_id.shape\n</code></pre>      <pre>\n<code>(3, 3, 4, 4)</code>\n</pre>        <pre><code>def test_padded_kernel_from_id(padded_kernel):\n    for i in range(3):\n        for j in range(16):\n            print(f'Matrix of size 3x3 : {padded_kernel[:,:,i,j].shape == (3,3)}')\n            squared_sum = 0\n            for k in range(3):\n                for l in range(3):\n                    if (k,l) != (1,1):\n                        squared_sum += padded_kernel[:,:,i,j][k,l]**2\n                    else:\n                        print(f'Middle element is 1 : {padded_kernel[:,:,i,j][k,l]==1}')\n            print(f'Squared sum is : {squared_sum}')\n</code></pre>     <pre><code>test_padded_kernel_from_id(conv)\n</code></pre>      <pre>\n<code>Matrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\nMatrix of size 3x3 : True\nMiddle element is 1 : False\nSquared sum is : 0.0\n</code>\n</pre>        <pre><code>conv, bn = fuse_bn_conv([conv_from_id], batchnorm_variables(1,2,1,4,channels))\n</code></pre>     <pre><code>scale = compute_scaling_weight_factor(1,4)\nscale\n</code></pre>      <pre>\n<code>0.4999375</code>\n</pre>        <pre><code>np.mean(np.isclose(conv, scale*conv_from_id, rtol=1e-3))\n</code></pre>      <pre>\n<code>1.0</code>\n</pre>        <pre><code>bias_scale = compute_scaling_bias_factor(1,2,1,4)\nbias_scale\n</code></pre>      <pre>\n<code>1.5000625</code>\n</pre>        <pre><code>bn_real = bias_scale*np.ones(channels)\nnp.mean(np.isclose(bn_real, bn, rtol=1e-3))\n</code></pre>      <pre>\n<code>1.0</code>\n</pre>"},{"location":"deep_learning/module9/fusion/#verification_1","title":"V\u00e9rification","text":""},{"location":"deep_learning/module9/fusion/#test-grandeur-reelle","title":"Test grandeur r\u00e9elle","text":"<pre><code>def repvgg_block(tensor, filters, num_layer):\n\n    # main stream\n    x = Conv2D(\n        filters=filters,\n        kernel_size=(3,3),\n        strides=(2,2),\n        padding=\"same\",\n        kernel_initializer=\"he_normal\",\n        use_bias=False,\n        name=f'block_{num_layer}_conv_main'\n    )(tensor)\n    x = BatchNormalization(name=f'block_{num_layer}_bn_main')(x)\n\n    # conv1x1 stream\n\n    y = Conv2D(\n        filters=filters,\n        kernel_size=(1,1),\n        strides=(2,2),\n        padding=\"same\",\n        kernel_initializer=\"he_normal\",\n        use_bias=False,\n        name=f'block_{num_layer}_conv_alt'\n    )(tensor)\n    y = BatchNormalization(name=f'block_{num_layer}_bn_alt')(y)\n\n    z = Add()([x,y])\n\n    return z\n</code></pre>     <pre><code>def repvgg_block_with_id(tensor, filters, num_layer):\n\n    # main stream\n    x = Conv2D(\n        filters=filters,\n        kernel_size=(3, 3),\n        strides=(1, 1),\n        padding=\"same\",\n        kernel_initializer=\"he_normal\",\n        use_bias=False,\n        name=f\"block_{num_layer}_conv_main\",\n    )(tensor)\n    x = BatchNormalization(name=f\"block_{num_layer}_bn_main\")(x)\n\n    # conv1x1 stream\n\n    y = Conv2D(\n        filters=filters,\n        kernel_size=(1, 1),\n        strides=(1, 1),\n        padding=\"same\",\n        kernel_initializer=\"he_normal\",\n        use_bias=False,\n        name=f\"block_{num_layer}_conv_alt\",\n    )(tensor)\n    y = BatchNormalization(name=f\"block_{num_layer}_bn_alt\")(y)\n\n    # id_conv branch\n    z = BatchNormalization(name=f\"block_{num_layer}_bn_id\")(tensor)\n\n    return Add()([x, y, z])\n</code></pre>     <pre><code>def get_model(img_shape):\n\n    input = Input(img_shape)\n\n    x = repvgg_block(input, filters=64, num_layer=0)\n    x = ReLU()(x)\n    x = repvgg_block(x, filters=64, num_layer=1)\n    x = ReLU()(x)\n    x = repvgg_block_with_id(x, filters=64, num_layer=2)\n    x = ReLU()(x)\n    x = Flatten()(x)\n    x = Dense(10, name='dense')(x)\n    x = Softmax()(x)\n    model = Model(input, x)\n    return model\n\ndef get_inference_model(img_shape):\n    input = Input(img_shape)\n\n    x = Conv2D(filters=64, kernel_size=(3,3), strides=(2,2), padding='same', name='conv_0')(input)\n    x = ReLU()(x)\n    x = Conv2D(filters=64, kernel_size=(3,3), strides=(2,2), padding='same', name='conv_1')(x)\n    x = ReLU()(x)\n    x = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='same', name='conv_2')(x)\n    x = ReLU()(x)\n    x = Flatten()(x)\n    x = Dense(10, name='dense')(x)\n    x = Softmax()(x)\n    model = Model(input, x)\n    return model\n</code></pre>     <pre><code>training_model = get_model([32,32,3])\ntraining_model.summary()\n</code></pre>      <pre>\n<code>Model: \"model_11\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_14 (InputLayer)           [(None, 32, 32, 3)]  0                                            \n__________________________________________________________________________________________________\nblock_0_conv_main (Conv2D)      (None, 16, 16, 64)   1728        input_14[0][0]                   \n__________________________________________________________________________________________________\nblock_0_conv_alt (Conv2D)       (None, 16, 16, 64)   192         input_14[0][0]                   \n__________________________________________________________________________________________________\nblock_0_bn_main (BatchNormaliza (None, 16, 16, 64)   256         block_0_conv_main[0][0]          \n__________________________________________________________________________________________________\nblock_0_bn_alt (BatchNormalizat (None, 16, 16, 64)   256         block_0_conv_alt[0][0]           \n__________________________________________________________________________________________________\nadd_13 (Add)                    (None, 16, 16, 64)   0           block_0_bn_main[0][0]            \n                                                                 block_0_bn_alt[0][0]             \n__________________________________________________________________________________________________\nre_lu_20 (ReLU)                 (None, 16, 16, 64)   0           add_13[0][0]                     \n__________________________________________________________________________________________________\nblock_1_conv_main (Conv2D)      (None, 8, 8, 64)     36864       re_lu_20[0][0]                   \n__________________________________________________________________________________________________\nblock_1_conv_alt (Conv2D)       (None, 8, 8, 64)     4096        re_lu_20[0][0]                   \n__________________________________________________________________________________________________\nblock_1_bn_main (BatchNormaliza (None, 8, 8, 64)     256         block_1_conv_main[0][0]          \n__________________________________________________________________________________________________\nblock_1_bn_alt (BatchNormalizat (None, 8, 8, 64)     256         block_1_conv_alt[0][0]           \n__________________________________________________________________________________________________\nadd_14 (Add)                    (None, 8, 8, 64)     0           block_1_bn_main[0][0]            \n                                                                 block_1_bn_alt[0][0]             \n__________________________________________________________________________________________________\nre_lu_21 (ReLU)                 (None, 8, 8, 64)     0           add_14[0][0]                     \n__________________________________________________________________________________________________\nblock_2_conv_main (Conv2D)      (None, 8, 8, 64)     36864       re_lu_21[0][0]                   \n__________________________________________________________________________________________________\nblock_2_conv_alt (Conv2D)       (None, 8, 8, 64)     4096        re_lu_21[0][0]                   \n__________________________________________________________________________________________________\nblock_2_bn_main (BatchNormaliza (None, 8, 8, 64)     256         block_2_conv_main[0][0]          \n__________________________________________________________________________________________________\nblock_2_bn_alt (BatchNormalizat (None, 8, 8, 64)     256         block_2_conv_alt[0][0]           \n__________________________________________________________________________________________________\nblock_2_bn_id (BatchNormalizati (None, 8, 8, 64)     256         re_lu_21[0][0]                   \n__________________________________________________________________________________________________\nadd_15 (Add)                    (None, 8, 8, 64)     0           block_2_bn_main[0][0]            \n                                                                 block_2_bn_alt[0][0]             \n                                                                 block_2_bn_id[0][0]              \n__________________________________________________________________________________________________\nre_lu_22 (ReLU)                 (None, 8, 8, 64)     0           add_15[0][0]                     \n__________________________________________________________________________________________________\nflatten_6 (Flatten)             (None, 4096)         0           re_lu_22[0][0]                   \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 10)           40970       flatten_6[0][0]                  \n__________________________________________________________________________________________________\nsoftmax_6 (Softmax)             (None, 10)           0           dense[0][0]                      \n==================================================================================================\nTotal params: 126,602\nTrainable params: 125,706\nNon-trainable params: 896\n__________________________________________________________________________________________________\n</code>\n</pre>        <pre><code>inference_model = get_inference_model([32,32,3])\ninference_model.summary()\n</code></pre>      <pre>\n<code>Model: \"model_12\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_15 (InputLayer)        [(None, 32, 32, 3)]       0         \n_________________________________________________________________\nconv_0 (Conv2D)              (None, 16, 16, 64)        1792      \n_________________________________________________________________\nre_lu_23 (ReLU)              (None, 16, 16, 64)        0         \n_________________________________________________________________\nconv_1 (Conv2D)              (None, 8, 8, 64)          36928     \n_________________________________________________________________\nre_lu_24 (ReLU)              (None, 8, 8, 64)          0         \n_________________________________________________________________\nconv_2 (Conv2D)              (None, 8, 8, 64)          36928     \n_________________________________________________________________\nre_lu_25 (ReLU)              (None, 8, 8, 64)          0         \n_________________________________________________________________\nflatten_7 (Flatten)          (None, 4096)              0         \n_________________________________________________________________\ndense (Dense)                (None, 10)                40970     \n_________________________________________________________________\nsoftmax_7 (Softmax)          (None, 10)                0         \n=================================================================\nTotal params: 116,618\nTrainable params: 116,618\nNon-trainable params: 0\n_________________________________________________________________\n</code>\n</pre>        <pre><code>def from_repvgg_to_vgg(training_model, inference_model, depth):\n    model = training_model\n    inference_model = inference_model\n\n    for i in range(depth):\n        print(f\"Fusion Conv-BN from main branch at depth {i}\")\n        conv_main = model.get_layer(f\"block_{i}_conv_main\")\n        bn_main = model.get_layer(f\"block_{i}_bn_main\")\n\n        conv_weights_main, conv_biases_main = fuse_bn_conv(\n            conv_main.get_weights(), bn_main.get_weights()\n        )\n\n        print(f\"Fusion Conv-BN from alt branch at depth {i}\")\n        conv_alt_one_by_one = model.get_layer(f\"block_{i}_conv_alt\")\n        bn_alt = model.get_layer(f\"block_{i}_bn_alt\")\n\n        conv_alt = pad_size_one_kernel(conv_alt_one_by_one.get_weights())\n\n        conv_weights_alt, conv_biases_alt = fuse_bn_conv([conv_alt], bn_alt.get_weights())\n\n        if i==3:\n            print(f\"Fusion Conv-BN from id branch at depth {i}\")\n            bn_id = model.get_layer(f\"block_{i}_bn_id\")\n            channels = backend.int_shape(bn_id.get_weights()[0])[-1]\n\n            conv_id = size_three_kernel_from_id(channels)\n            conv_weights_id, conv_biases_id = fuse_bn_conv([conv_id], bn_id.get_weights())\n\n            conv_weights = conv_weights_main + conv_weights_alt + conv_weights_id\n            conv_biases = conv_biases_main + conv_biases_alt + conv_biases_id\n        else:\n            conv_weights = conv_weights_main + conv_weights_alt\n            conv_biases = conv_biases_main + conv_biases_alt\n\n\n        print(f\"Setting weights on inference model at depth {i}\")\n        inference_model.get_layer(f\"conv_{i}\").set_weights([conv_weights, conv_biases])\n\n    dense_weights = model.get_layer(f\"dense\").get_weights()\n    inference_model.get_layer(f\"dense\").set_weights(dense_weights)\n\n    return inference_model\n</code></pre>     <pre><code>from tensorflow.keras import datasets\nfrom sklearn.model_selection import train_test_split\n\n(X_train,y_train), (X_test,y_test)  = tf.keras.datasets.cifar10.load_data()\n\n# Normalize pixel values to be between 0 and 1\nX_train, X_test = X_train / 255.0, X_test / 255.0\n\nX_train = X_train.reshape(-1, 32, 32, 3).astype('float32')\nX_test = X_test.reshape(-1, 32, 32, 3).astype('float32')\n\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, random_state=42)\n\ny_train_oh = tf.keras.utils.to_categorical(y_train, num_classes=10)\ny_test_oh = tf.keras.utils.to_categorical(y_test, num_classes=10)\ny_valid_oh = tf.keras.utils.to_categorical(y_valid, num_classes=10)\n</code></pre>     <pre><code>training_model.compile(loss = 'categorical_crossentropy',\n             optimizer=tf.keras.optimizers.Adam(lr=0.001),\n             metrics=['accuracy'])\n\ntraining_model.fit(X_train, y_train_oh,\n                     epochs = 200,\n                     batch_size=128,\n                     validation_data=(X_valid, y_valid_oh))\n</code></pre>      <pre>\n<code>Epoch 1/200\n293/293 [==============================] - 2s 4ms/step - loss: 1.9390 - accuracy: 0.3690 - val_loss: 1.4326 - val_accuracy: 0.4882\nEpoch 2/200\n293/293 [==============================] - 1s 3ms/step - loss: 1.1761 - accuracy: 0.5805 - val_loss: 1.2776 - val_accuracy: 0.5559\nEpoch 3/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.9771 - accuracy: 0.6608 - val_loss: 1.2784 - val_accuracy: 0.5692\nEpoch 4/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.8457 - accuracy: 0.7047 - val_loss: 1.1538 - val_accuracy: 0.6070\nEpoch 5/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.7411 - accuracy: 0.7415 - val_loss: 1.1523 - val_accuracy: 0.6082\nEpoch 6/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.6627 - accuracy: 0.7721 - val_loss: 1.2050 - val_accuracy: 0.6056\nEpoch 7/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.5730 - accuracy: 0.8017 - val_loss: 1.3360 - val_accuracy: 0.5908\nEpoch 8/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.5037 - accuracy: 0.8267 - val_loss: 1.2697 - val_accuracy: 0.6086\nEpoch 9/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.4315 - accuracy: 0.8549 - val_loss: 1.6375 - val_accuracy: 0.5710\nEpoch 10/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.3744 - accuracy: 0.8732 - val_loss: 1.4501 - val_accuracy: 0.6078\nEpoch 11/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.3140 - accuracy: 0.8947 - val_loss: 1.3964 - val_accuracy: 0.6189\nEpoch 12/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.2657 - accuracy: 0.9140 - val_loss: 1.4346 - val_accuracy: 0.6252\nEpoch 13/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.2328 - accuracy: 0.9247 - val_loss: 1.7435 - val_accuracy: 0.6030\nEpoch 14/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.1914 - accuracy: 0.9402 - val_loss: 1.6220 - val_accuracy: 0.6197\nEpoch 15/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.1661 - accuracy: 0.9486 - val_loss: 2.0838 - val_accuracy: 0.5826\nEpoch 16/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.1485 - accuracy: 0.9539 - val_loss: 1.6913 - val_accuracy: 0.6221\nEpoch 17/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.1283 - accuracy: 0.9603 - val_loss: 1.7873 - val_accuracy: 0.6273\nEpoch 18/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.1113 - accuracy: 0.9667 - val_loss: 1.9872 - val_accuracy: 0.6094\nEpoch 19/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.1026 - accuracy: 0.9691 - val_loss: 2.0423 - val_accuracy: 0.6062\nEpoch 20/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.1021 - accuracy: 0.9666 - val_loss: 2.0206 - val_accuracy: 0.6166\nEpoch 21/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0920 - accuracy: 0.9714 - val_loss: 2.0964 - val_accuracy: 0.6199\nEpoch 22/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0794 - accuracy: 0.9750 - val_loss: 2.4125 - val_accuracy: 0.5951\nEpoch 23/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0647 - accuracy: 0.9814 - val_loss: 2.2896 - val_accuracy: 0.6095\nEpoch 24/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0907 - accuracy: 0.9686 - val_loss: 2.3337 - val_accuracy: 0.6104\nEpoch 25/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0774 - accuracy: 0.9755 - val_loss: 2.7223 - val_accuracy: 0.5977\nEpoch 26/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0615 - accuracy: 0.9804 - val_loss: 2.3225 - val_accuracy: 0.6226\nEpoch 27/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0616 - accuracy: 0.9815 - val_loss: 2.3531 - val_accuracy: 0.6187\nEpoch 28/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0510 - accuracy: 0.9862 - val_loss: 2.5024 - val_accuracy: 0.6199\nEpoch 29/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0528 - accuracy: 0.9838 - val_loss: 2.9272 - val_accuracy: 0.5823\nEpoch 30/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0715 - accuracy: 0.9745 - val_loss: 2.4670 - val_accuracy: 0.6142\nEpoch 31/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0780 - accuracy: 0.9733 - val_loss: 2.6590 - val_accuracy: 0.6194\nEpoch 32/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0636 - accuracy: 0.9776 - val_loss: 2.5846 - val_accuracy: 0.6173\nEpoch 33/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0364 - accuracy: 0.9898 - val_loss: 2.5448 - val_accuracy: 0.6227\nEpoch 34/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0308 - accuracy: 0.9915 - val_loss: 2.6284 - val_accuracy: 0.6238\nEpoch 35/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0307 - accuracy: 0.9914 - val_loss: 2.8331 - val_accuracy: 0.6170\nEpoch 36/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0561 - accuracy: 0.9809 - val_loss: 3.3461 - val_accuracy: 0.5866\nEpoch 37/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0740 - accuracy: 0.9740 - val_loss: 2.9504 - val_accuracy: 0.6025\nEpoch 38/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0706 - accuracy: 0.9749 - val_loss: 2.7457 - val_accuracy: 0.6227\nEpoch 39/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0456 - accuracy: 0.9849 - val_loss: 2.7979 - val_accuracy: 0.6226\nEpoch 40/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0269 - accuracy: 0.9929 - val_loss: 2.8238 - val_accuracy: 0.6207\nEpoch 41/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0178 - accuracy: 0.9957 - val_loss: 2.8020 - val_accuracy: 0.6298\nEpoch 42/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0161 - accuracy: 0.9959 - val_loss: 2.8443 - val_accuracy: 0.6261\nEpoch 43/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0377 - accuracy: 0.9870 - val_loss: 4.3773 - val_accuracy: 0.5440\nEpoch 44/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.1039 - accuracy: 0.9646 - val_loss: 3.0375 - val_accuracy: 0.6030\nEpoch 45/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0617 - accuracy: 0.9799 - val_loss: 2.8326 - val_accuracy: 0.6314\nEpoch 46/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0328 - accuracy: 0.9899 - val_loss: 2.8987 - val_accuracy: 0.6274\nEpoch 47/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0190 - accuracy: 0.9945 - val_loss: 2.9066 - val_accuracy: 0.6258\nEpoch 48/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0134 - accuracy: 0.9962 - val_loss: 3.0005 - val_accuracy: 0.6267\nEpoch 49/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0098 - accuracy: 0.9980 - val_loss: 2.9672 - val_accuracy: 0.6337\nEpoch 50/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0150 - accuracy: 0.9957 - val_loss: 3.4715 - val_accuracy: 0.5854\nEpoch 51/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.1082 - accuracy: 0.9634 - val_loss: 3.1194 - val_accuracy: 0.6069\nEpoch 52/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0703 - accuracy: 0.9746 - val_loss: 3.4825 - val_accuracy: 0.6022\nEpoch 53/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0372 - accuracy: 0.9875 - val_loss: 3.1542 - val_accuracy: 0.6277\nEpoch 54/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0176 - accuracy: 0.9953 - val_loss: 3.0385 - val_accuracy: 0.6305\nEpoch 55/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0099 - accuracy: 0.9978 - val_loss: 3.0361 - val_accuracy: 0.6351\nEpoch 56/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0067 - accuracy: 0.9987 - val_loss: 3.0368 - val_accuracy: 0.6361\nEpoch 57/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0042 - accuracy: 0.9995 - val_loss: 3.1196 - val_accuracy: 0.6257\nEpoch 58/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0070 - accuracy: 0.9984 - val_loss: 3.2946 - val_accuracy: 0.6184\nEpoch 59/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0844 - accuracy: 0.9726 - val_loss: 3.6981 - val_accuracy: 0.5838\nEpoch 60/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.1068 - accuracy: 0.9615 - val_loss: 3.3662 - val_accuracy: 0.6150\nEpoch 61/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0331 - accuracy: 0.9884 - val_loss: 3.1085 - val_accuracy: 0.6268\nEpoch 62/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0139 - accuracy: 0.9960 - val_loss: 3.2636 - val_accuracy: 0.6290\nEpoch 63/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0095 - accuracy: 0.9980 - val_loss: 3.1850 - val_accuracy: 0.6310\nEpoch 64/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0063 - accuracy: 0.9987 - val_loss: 3.2820 - val_accuracy: 0.6345\nEpoch 65/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0031 - accuracy: 0.9997 - val_loss: 3.1574 - val_accuracy: 0.6397\nEpoch 66/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.1854 - val_accuracy: 0.6414\nEpoch 67/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.1196 - val_accuracy: 0.6458\nEpoch 68/200\n293/293 [==============================] - 1s 4ms/step - loss: 9.6865e-04 - accuracy: 1.0000 - val_loss: 3.1958 - val_accuracy: 0.6469\nEpoch 69/200\n293/293 [==============================] - 1s 4ms/step - loss: 8.6404e-04 - accuracy: 1.0000 - val_loss: 3.1414 - val_accuracy: 0.6472\nEpoch 70/200\n293/293 [==============================] - 1s 4ms/step - loss: 5.8577e-04 - accuracy: 1.0000 - val_loss: 3.1648 - val_accuracy: 0.6446\nEpoch 71/200\n293/293 [==============================] - 1s 4ms/step - loss: 5.2701e-04 - accuracy: 1.0000 - val_loss: 3.2118 - val_accuracy: 0.6478\nEpoch 72/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.1478 - accuracy: 0.9627 - val_loss: 3.3232 - val_accuracy: 0.5995\nEpoch 73/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.1219 - accuracy: 0.9585 - val_loss: 3.0879 - val_accuracy: 0.6179\nEpoch 74/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0325 - accuracy: 0.9888 - val_loss: 3.1610 - val_accuracy: 0.6290\nEpoch 75/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0124 - accuracy: 0.9973 - val_loss: 3.0288 - val_accuracy: 0.6348\nEpoch 76/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0045 - accuracy: 0.9997 - val_loss: 3.0254 - val_accuracy: 0.6465\nEpoch 77/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0027 - accuracy: 0.9999 - val_loss: 3.0743 - val_accuracy: 0.6470\nEpoch 78/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0019 - accuracy: 0.9999 - val_loss: 3.1198 - val_accuracy: 0.6430\nEpoch 79/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.0889 - val_accuracy: 0.6466\nEpoch 80/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.1780 - val_accuracy: 0.6393\nEpoch 81/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.1530 - val_accuracy: 0.6427\nEpoch 82/200\n293/293 [==============================] - 1s 4ms/step - loss: 9.9041e-04 - accuracy: 1.0000 - val_loss: 3.1667 - val_accuracy: 0.6454\nEpoch 83/200\n293/293 [==============================] - 1s 4ms/step - loss: 9.6558e-04 - accuracy: 1.0000 - val_loss: 3.1835 - val_accuracy: 0.6430\nEpoch 84/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.1062 - accuracy: 0.9710 - val_loss: 4.0625 - val_accuracy: 0.5772\nEpoch 85/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.1073 - accuracy: 0.9622 - val_loss: 3.1034 - val_accuracy: 0.6308\nEpoch 86/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0338 - accuracy: 0.9883 - val_loss: 3.1997 - val_accuracy: 0.6258\nEpoch 87/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0141 - accuracy: 0.9965 - val_loss: 3.2102 - val_accuracy: 0.6319\nEpoch 88/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0064 - accuracy: 0.9991 - val_loss: 3.1613 - val_accuracy: 0.6379\nEpoch 89/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9996 - val_loss: 3.2062 - val_accuracy: 0.6387\nEpoch 90/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0022 - accuracy: 0.9999 - val_loss: 3.1607 - val_accuracy: 0.6414\nEpoch 91/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.1905 - val_accuracy: 0.6446\nEpoch 92/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 3.2038 - val_accuracy: 0.6442\nEpoch 93/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 3.2732 - val_accuracy: 0.6409\nEpoch 94/200\n293/293 [==============================] - 1s 4ms/step - loss: 7.5633e-04 - accuracy: 1.0000 - val_loss: 3.2499 - val_accuracy: 0.6442\nEpoch 95/200\n293/293 [==============================] - 1s 4ms/step - loss: 7.7323e-04 - accuracy: 1.0000 - val_loss: 3.2885 - val_accuracy: 0.6450\nEpoch 96/200\n293/293 [==============================] - 1s 4ms/step - loss: 9.9023e-04 - accuracy: 1.0000 - val_loss: 3.4087 - val_accuracy: 0.6306\nEpoch 97/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0088 - accuracy: 0.9979 - val_loss: 5.3897 - val_accuracy: 0.5452\nEpoch 98/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.3004 - accuracy: 0.9175 - val_loss: 3.8454 - val_accuracy: 0.5955\nEpoch 99/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0424 - accuracy: 0.9852 - val_loss: 3.4719 - val_accuracy: 0.6225\nEpoch 100/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0128 - accuracy: 0.9972 - val_loss: 3.4143 - val_accuracy: 0.6262\nEpoch 101/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0070 - accuracy: 0.9984 - val_loss: 3.2489 - val_accuracy: 0.6374\nEpoch 102/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0036 - accuracy: 0.9996 - val_loss: 3.2455 - val_accuracy: 0.6399\nEpoch 103/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.2159 - val_accuracy: 0.6434\nEpoch 104/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.2219 - val_accuracy: 0.6451\nEpoch 105/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.2600 - val_accuracy: 0.6425\nEpoch 106/200\n293/293 [==============================] - 1s 4ms/step - loss: 8.9068e-04 - accuracy: 1.0000 - val_loss: 3.2839 - val_accuracy: 0.6430\nEpoch 107/200\n293/293 [==============================] - 1s 3ms/step - loss: 8.8232e-04 - accuracy: 1.0000 - val_loss: 3.2854 - val_accuracy: 0.6457\nEpoch 108/200\n293/293 [==============================] - 1s 3ms/step - loss: 6.3356e-04 - accuracy: 1.0000 - val_loss: 3.3021 - val_accuracy: 0.6451\nEpoch 109/200\n293/293 [==============================] - 1s 3ms/step - loss: 6.9747e-04 - accuracy: 1.0000 - val_loss: 3.3147 - val_accuracy: 0.6457\nEpoch 110/200\n293/293 [==============================] - 1s 3ms/step - loss: 5.3339e-04 - accuracy: 1.0000 - val_loss: 3.3380 - val_accuracy: 0.6454\nEpoch 111/200\n293/293 [==============================] - 1s 4ms/step - loss: 4.3812e-04 - accuracy: 1.0000 - val_loss: 3.3765 - val_accuracy: 0.6447\nEpoch 112/200\n293/293 [==============================] - 1s 3ms/step - loss: 4.1380e-04 - accuracy: 1.0000 - val_loss: 3.3854 - val_accuracy: 0.6460\nEpoch 113/200\n293/293 [==============================] - 1s 3ms/step - loss: 4.5256e-04 - accuracy: 1.0000 - val_loss: 3.4450 - val_accuracy: 0.6454\nEpoch 114/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0543 - accuracy: 0.9867 - val_loss: 3.9023 - val_accuracy: 0.5873\nEpoch 115/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.1773 - accuracy: 0.9468 - val_loss: 3.3988 - val_accuracy: 0.6180\nEpoch 116/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0298 - accuracy: 0.9901 - val_loss: 3.3156 - val_accuracy: 0.6344\nEpoch 117/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0111 - accuracy: 0.9966 - val_loss: 3.2564 - val_accuracy: 0.6354\nEpoch 118/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0056 - accuracy: 0.9991 - val_loss: 3.3034 - val_accuracy: 0.6414\nEpoch 119/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0030 - accuracy: 0.9996 - val_loss: 3.3015 - val_accuracy: 0.6431\nEpoch 120/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0021 - accuracy: 0.9999 - val_loss: 3.3338 - val_accuracy: 0.6422\nEpoch 121/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.3000 - val_accuracy: 0.6458\nEpoch 122/200\n293/293 [==============================] - 1s 3ms/step - loss: 9.2647e-04 - accuracy: 1.0000 - val_loss: 3.3178 - val_accuracy: 0.6457\nEpoch 123/200\n293/293 [==============================] - 1s 3ms/step - loss: 8.6534e-04 - accuracy: 1.0000 - val_loss: 3.3308 - val_accuracy: 0.6482\nEpoch 124/200\n293/293 [==============================] - 1s 3ms/step - loss: 6.3539e-04 - accuracy: 1.0000 - val_loss: 3.3759 - val_accuracy: 0.6479\nEpoch 125/200\n293/293 [==============================] - 1s 3ms/step - loss: 6.1540e-04 - accuracy: 1.0000 - val_loss: 3.3861 - val_accuracy: 0.6474\nEpoch 126/200\n293/293 [==============================] - 1s 3ms/step - loss: 5.1956e-04 - accuracy: 1.0000 - val_loss: 3.3955 - val_accuracy: 0.6460\nEpoch 127/200\n293/293 [==============================] - 1s 3ms/step - loss: 4.9951e-04 - accuracy: 1.0000 - val_loss: 3.5187 - val_accuracy: 0.6467\nEpoch 128/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0084 - accuracy: 0.9978 - val_loss: 4.4000 - val_accuracy: 0.5709\nEpoch 129/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.2555 - accuracy: 0.9294 - val_loss: 3.7346 - val_accuracy: 0.6086\nEpoch 130/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0358 - accuracy: 0.9878 - val_loss: 3.4413 - val_accuracy: 0.6241\nEpoch 131/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0122 - accuracy: 0.9971 - val_loss: 3.4731 - val_accuracy: 0.6366\nEpoch 132/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 3.3815 - val_accuracy: 0.6391\nEpoch 133/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0040 - accuracy: 0.9993 - val_loss: 3.4015 - val_accuracy: 0.6413\nEpoch 134/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 3.4036 - val_accuracy: 0.6428\nEpoch 135/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.4494 - val_accuracy: 0.6402\nEpoch 136/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 3.4177 - val_accuracy: 0.6470\nEpoch 137/200\n293/293 [==============================] - 1s 4ms/step - loss: 8.6482e-04 - accuracy: 1.0000 - val_loss: 3.4614 - val_accuracy: 0.6452\nEpoch 138/200\n293/293 [==============================] - 1s 4ms/step - loss: 6.2557e-04 - accuracy: 1.0000 - val_loss: 3.4649 - val_accuracy: 0.6465\nEpoch 139/200\n293/293 [==============================] - 1s 4ms/step - loss: 6.4032e-04 - accuracy: 1.0000 - val_loss: 3.4694 - val_accuracy: 0.6444\nEpoch 140/200\n293/293 [==============================] - 1s 4ms/step - loss: 5.6854e-04 - accuracy: 1.0000 - val_loss: 3.5629 - val_accuracy: 0.6450\nEpoch 141/200\n293/293 [==============================] - 1s 4ms/step - loss: 5.7389e-04 - accuracy: 1.0000 - val_loss: 3.5976 - val_accuracy: 0.6391\nEpoch 142/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0730 - accuracy: 0.9803 - val_loss: 3.6825 - val_accuracy: 0.6078\nEpoch 143/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.1055 - accuracy: 0.9660 - val_loss: 3.4045 - val_accuracy: 0.6313\nEpoch 144/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0229 - accuracy: 0.9920 - val_loss: 3.5080 - val_accuracy: 0.6366\nEpoch 145/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0088 - accuracy: 0.9976 - val_loss: 3.4096 - val_accuracy: 0.6414\nEpoch 146/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0040 - accuracy: 0.9994 - val_loss: 3.4737 - val_accuracy: 0.6436\nEpoch 147/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 3.5510 - val_accuracy: 0.6415\nEpoch 148/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 3.5113 - val_accuracy: 0.6454\nEpoch 149/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.4949 - val_accuracy: 0.6440\nEpoch 150/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 3.5232 - val_accuracy: 0.6479\nEpoch 151/200\n293/293 [==============================] - 1s 4ms/step - loss: 9.0308e-04 - accuracy: 1.0000 - val_loss: 3.5569 - val_accuracy: 0.6500\nEpoch 152/200\n293/293 [==============================] - 1s 4ms/step - loss: 6.7638e-04 - accuracy: 1.0000 - val_loss: 3.5668 - val_accuracy: 0.6485\nEpoch 153/200\n293/293 [==============================] - 1s 3ms/step - loss: 6.3473e-04 - accuracy: 1.0000 - val_loss: 3.5986 - val_accuracy: 0.6502\nEpoch 154/200\n293/293 [==============================] - 1s 3ms/step - loss: 4.5041e-04 - accuracy: 1.0000 - val_loss: 3.5845 - val_accuracy: 0.6482\nEpoch 155/200\n293/293 [==============================] - 1s 3ms/step - loss: 4.0400e-04 - accuracy: 1.0000 - val_loss: 3.6154 - val_accuracy: 0.6446\nEpoch 156/200\n293/293 [==============================] - 1s 3ms/step - loss: 6.3062e-04 - accuracy: 1.0000 - val_loss: 3.7590 - val_accuracy: 0.6394\nEpoch 157/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.1702 - accuracy: 0.9545 - val_loss: 3.6780 - val_accuracy: 0.6218\nEpoch 158/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0543 - accuracy: 0.9818 - val_loss: 3.5992 - val_accuracy: 0.6316\nEpoch 159/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0184 - accuracy: 0.9940 - val_loss: 3.7416 - val_accuracy: 0.6269\nEpoch 160/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0061 - accuracy: 0.9988 - val_loss: 3.5675 - val_accuracy: 0.6382\nEpoch 161/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 3.6239 - val_accuracy: 0.6394\nEpoch 162/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 3.5901 - val_accuracy: 0.6389\nEpoch 163/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 3.6193 - val_accuracy: 0.6441\nEpoch 164/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 3.6285 - val_accuracy: 0.6438\nEpoch 165/200\n293/293 [==============================] - 1s 3ms/step - loss: 7.3676e-04 - accuracy: 1.0000 - val_loss: 3.6237 - val_accuracy: 0.6454\nEpoch 166/200\n293/293 [==============================] - 1s 3ms/step - loss: 5.4252e-04 - accuracy: 1.0000 - val_loss: 3.6783 - val_accuracy: 0.6441\nEpoch 167/200\n293/293 [==============================] - 1s 4ms/step - loss: 5.7206e-04 - accuracy: 1.0000 - val_loss: 3.6587 - val_accuracy: 0.6457\nEpoch 168/200\n293/293 [==============================] - 1s 3ms/step - loss: 5.1652e-04 - accuracy: 1.0000 - val_loss: 3.6578 - val_accuracy: 0.6467\nEpoch 169/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0366 - accuracy: 0.9905 - val_loss: 4.2009 - val_accuracy: 0.5858\nEpoch 170/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0951 - accuracy: 0.9697 - val_loss: 3.5935 - val_accuracy: 0.6283\nEpoch 171/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0229 - accuracy: 0.9921 - val_loss: 3.6487 - val_accuracy: 0.6332\nEpoch 172/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0066 - accuracy: 0.9983 - val_loss: 3.8405 - val_accuracy: 0.6329\nEpoch 173/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0045 - accuracy: 0.9989 - val_loss: 3.6509 - val_accuracy: 0.6421\nEpoch 174/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 3.6520 - val_accuracy: 0.6394\nEpoch 175/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.6577 - val_accuracy: 0.6406\nEpoch 176/200\n293/293 [==============================] - 1s 4ms/step - loss: 6.8113e-04 - accuracy: 1.0000 - val_loss: 3.6778 - val_accuracy: 0.6417\nEpoch 177/200\n293/293 [==============================] - 1s 4ms/step - loss: 8.0387e-04 - accuracy: 1.0000 - val_loss: 3.6704 - val_accuracy: 0.6414\nEpoch 178/200\n293/293 [==============================] - 1s 3ms/step - loss: 6.4948e-04 - accuracy: 1.0000 - val_loss: 3.7070 - val_accuracy: 0.6417\nEpoch 179/200\n293/293 [==============================] - 1s 4ms/step - loss: 4.2156e-04 - accuracy: 1.0000 - val_loss: 3.7131 - val_accuracy: 0.6404\nEpoch 180/200\n293/293 [==============================] - 1s 4ms/step - loss: 5.7091e-04 - accuracy: 1.0000 - val_loss: 3.8784 - val_accuracy: 0.6346\nEpoch 181/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0651 - accuracy: 0.9804 - val_loss: 4.3902 - val_accuracy: 0.5985\nEpoch 182/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0633 - accuracy: 0.9788 - val_loss: 4.0795 - val_accuracy: 0.6162\nEpoch 183/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0194 - accuracy: 0.9931 - val_loss: 4.0404 - val_accuracy: 0.6267\nEpoch 184/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0076 - accuracy: 0.9979 - val_loss: 3.7312 - val_accuracy: 0.6357\nEpoch 185/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 3.7239 - val_accuracy: 0.6436\nEpoch 186/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 3.7600 - val_accuracy: 0.6344\nEpoch 187/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 3.7408 - val_accuracy: 0.6391\nEpoch 188/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 3.8037 - val_accuracy: 0.6379\nEpoch 189/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 3.8553 - val_accuracy: 0.6378\nEpoch 190/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0050 - accuracy: 0.9984 - val_loss: 4.3736 - val_accuracy: 0.6046\nEpoch 191/200\n293/293 [==============================] - 1s 3ms/step - loss: 0.0759 - accuracy: 0.9757 - val_loss: 4.1114 - val_accuracy: 0.6099\nEpoch 192/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0357 - accuracy: 0.9879 - val_loss: 4.0255 - val_accuracy: 0.6281\nEpoch 193/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0121 - accuracy: 0.9959 - val_loss: 3.9506 - val_accuracy: 0.6345\nEpoch 194/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0050 - accuracy: 0.9989 - val_loss: 3.9339 - val_accuracy: 0.6302\nEpoch 195/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 3.8998 - val_accuracy: 0.6373\nEpoch 196/200\n293/293 [==============================] - 1s 4ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 3.8751 - val_accuracy: 0.6438\nEpoch 197/200\n293/293 [==============================] - 1s 4ms/step - loss: 9.7056e-04 - accuracy: 1.0000 - val_loss: 3.8705 - val_accuracy: 0.6424\nEpoch 198/200\n293/293 [==============================] - 1s 3ms/step - loss: 5.4642e-04 - accuracy: 1.0000 - val_loss: 3.8781 - val_accuracy: 0.6441\nEpoch 199/200\n293/293 [==============================] - 1s 4ms/step - loss: 5.2847e-04 - accuracy: 1.0000 - val_loss: 3.8988 - val_accuracy: 0.6422\nEpoch 200/200\n293/293 [==============================] - 1s 3ms/step - loss: 5.0220e-04 - accuracy: 1.0000 - val_loss: 3.9146 - val_accuracy: 0.6449\n</code>\n</pre>     <pre>\n<code>&lt;tensorflow.python.keras.callbacks.History at 0x7fc892648ca0&gt;</code>\n</pre>        <pre><code>training_model.evaluate(X_test, y_test_oh)\n</code></pre>      <pre>\n<code>313/313 [==============================] - 0s 1ms/step - loss: 4.0375 - accuracy: 0.6408\n</code>\n</pre>     <pre>\n<code>[4.037524223327637, 0.6407999992370605]</code>\n</pre>        <pre><code>model = from_repvgg_to_vgg(training_model, inference_model, 3)\nmodel.summary()\n</code></pre>      <pre>\n<code>Fusion Conv-BN from main branch at depth 0\nFusion Conv-BN from alt branch at depth 0\nSetting weights on inference model at depth 0\nFusion Conv-BN from main branch at depth 1\nFusion Conv-BN from alt branch at depth 1\nSetting weights on inference model at depth 1\nFusion Conv-BN from main branch at depth 2\nFusion Conv-BN from alt branch at depth 2\nSetting weights on inference model at depth 2\nModel: \"model_12\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_15 (InputLayer)        [(None, 32, 32, 3)]       0         \n_________________________________________________________________\nconv_0 (Conv2D)              (None, 16, 16, 64)        1792      \n_________________________________________________________________\nre_lu_23 (ReLU)              (None, 16, 16, 64)        0         \n_________________________________________________________________\nconv_1 (Conv2D)              (None, 8, 8, 64)          36928     \n_________________________________________________________________\nre_lu_24 (ReLU)              (None, 8, 8, 64)          0         \n_________________________________________________________________\nconv_2 (Conv2D)              (None, 8, 8, 64)          36928     \n_________________________________________________________________\nre_lu_25 (ReLU)              (None, 8, 8, 64)          0         \n_________________________________________________________________\nflatten_7 (Flatten)          (None, 4096)              0         \n_________________________________________________________________\ndense (Dense)                (None, 10)                40970     \n_________________________________________________________________\nsoftmax_7 (Softmax)          (None, 10)                0         \n=================================================================\nTotal params: 116,618\nTrainable params: 116,618\nNon-trainable params: 0\n_________________________________________________________________\n</code>\n</pre>        <pre><code>model.compile(loss = 'categorical_crossentropy',\n             optimizer=tf.keras.optimizers.SGD(lr=2e-9),\n             metrics=['accuracy'])\n</code></pre>     <pre><code>model.evaluate(X_test, y_test_oh)\n</code></pre>      <pre>\n<code>313/313 [==============================] - 0s 1ms/step - loss: 7.8139 - accuracy: 0.3986\n</code>\n</pre>     <pre>\n<code>[7.867744445800781, 0.39640000462532043]</code>\n</pre>"},{"location":"deep_learning/module9/module9/","title":"Module9 : D\u00e9ployer son r\u00e9seau de neurones sur de l'embarqu\u00e9","text":"<p>But de l'optimisation des mod\u00e8les :</p> <ul> <li>R\u00e9duire la taille du mod\u00e8le.</li> <li>Acc\u00e9l\u00e9rer le temps d'inf\u00e9rence.</li> <li>R\u00e9duire la consommation \u00e9nerg\u00e9tique du mod\u00e8le.</li> </ul> <p>Quelles sont les diff\u00e9rentes fa\u00e7on d'optimiser une mod\u00e8le ?</p> <ul> <li>Tous les param\u00e8tres contribuent ils \u00e0 la performance du mod\u00e8le ? Pruning</li> <li>R\u00e9duire sa pr\u00e9cision num\u00e9rique : les param\u00e8tres et fonctions d'activations d'un mod\u00e8le sont le plus souvent repr\u00e9sent\u00e9es en <code>float32</code>. Est-ce n\u00e9cessaire ? Quantification</li> <li>Toutes les op\u00e9rations du graphe d'un mod\u00e8le sont elles n\u00e9c\u00e9ssaires durant l'inf\u00e9rence ? Fusion des couches</li> <li>Am\u00e9liorer les allers-retours entre GPU et CPU.</li> </ul>"},{"location":"deep_learning/module9/module9/#elagage","title":"Elagage","text":"<p>TLDR</p> <p>Une des premi\u00e8re alternatives lors de l'optimisation des mod\u00e8les et de se poser la question des param\u00e8tres (ie poids &amp; biais). Tous les param\u00e8tres n'ont pas la m\u00eame importance.</p> <p>L'id\u00e9e ici est qu'un r\u00e9seau entra\u00een\u00e9 peut \u00eatre r\u00e9duit \u00e0 un r\u00e9seau plus petit en supprimant les poids inutiles. En pratique, cela signifie que les pond\u00e9rations \"inutiles\" sont fix\u00e9es \u00e0 z\u00e9ro. En mettant les pond\u00e9rations inutiles \u00e0 z\u00e9ro, l'inf\u00e9rence ou la pr\u00e9diction peut \u00eatre acc\u00e9l\u00e9r\u00e9e.</p> <p>De plus, les mod\u00e8les \u00e9lagu\u00e9s peuvent \u00eatre compress\u00e9s en mod\u00e8les de taille plus petite, car des pond\u00e9rations peu nombreuses entra\u00eenent des taux de compression plus \u00e9lev\u00e9s.</p> <p>Dans le cas de mod\u00e8les cr\u00e9\u00e9s via Tensoflow, cela se fait via la librairie Tensorflow model optimization.</p>  <p>La derni\u00e8re d\u00e9cennie a montr\u00e9 qu'en g\u00e9n\u00e9ral, les grands r\u00e9seaux de neurones donnent de meilleurs r\u00e9sultats (avec par exemple l'arriv\u00e9e des architetcures de type ResNet &amp; les connexions r\u00e9siduelles, qui ont compl\u00e8tement chang\u00e9 les m\u00e9thodes de cr\u00e9ation des mod\u00e8les). Mais les grands mod\u00e8les d'apprentissage profond ont un co\u00fbt \u00e9norme. Par exemple, pour entra\u00eener le mod\u00e8le GPT-3 d'OpenAI, qui compte 175 milliards de param\u00e8tres (700Go pour des poids en Float32), il faut avoir acc\u00e8s \u00e0 d'\u00e9normes grappes de serveurs dot\u00e9s de cartes graphiques tr\u00e8s puissantes, et les co\u00fbts peuvent atteindre plusieurs millions de dollars. En outre, vous avez besoin de centaines de gigaoctets de VRAM et d'un serveur puissant pour ex\u00e9cuter le mod\u00e8le.</p> <p>https://blog.dataiku.com/making-neural-networks-smaller-for-better-deployment-solving-the-size-problem-of-cnns-using-network-pruning-with-keras</p> <p>L'\u00e9lagage des r\u00e9seaux de neurones est une vieille id\u00e9e qui remonte \u00e0 1990 (avec les travaux de Yan LeCun sur les l\u00e9sions c\u00e9r\u00e9brales optimales) et avant. L'id\u00e9e est que parmi les nombreux param\u00e8tres du r\u00e9seau, certains sont redondants et ne contribuent pas beaucoup \u00e0 la sortie.</p>  <p>Optimal Brain Damage</p> <p>Article</p> <p>We have used information-theoretic ideas to derive a  class of practical and nearly optimal schemes for adapting the size of a  neural network. By removing unimportant weights from a  network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a  tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.</p>  <p>Si vous pouviez classer les neurones du r\u00e9seau en fonction de leur contribution, vous pourriez alors supprimer les neurones de rang inf\u00e9rieur du r\u00e9seau, ce qui permettrait d'obtenir un r\u00e9seau plus petit et plus rapide.</p> <p>L'obtention de r\u00e9seaux plus rapides/petits est importante pour l'ex\u00e9cution de ces r\u00e9seaux d'apprentissage profond sur les appareils mobiles.</p> <p>Le classement peut \u00eatre effectu\u00e9 en fonction de la moyenne \\(L_1\\)/\\(L_2\\) des poids des neurones, de leurs activations moyennes, du nombre de fois o\u00f9 un neurone n'\u00e9tait pas nul sur un ensemble de validation, et d'autres m\u00e9thodes cr\u00e9atives. Apr\u00e8s l'\u00e9lagage, la pr\u00e9cision diminue (pas trop, esp\u00e9rons-le, si le classement est intelligent) et le r\u00e9seau est g\u00e9n\u00e9ralement entra\u00een\u00e9 davantage pour r\u00e9cup\u00e9rer.</p>  <p>Question</p> <p>Eh bien on n'a qu'\u00e0 \u00e9laguer GPT-3 apr\u00e8s son entra\u00eenement et il tournera sur un smartphone non ?</p>  <p>Le probl\u00e8me de l'\u00e9lagage des r\u00e9seaux de neurones apr\u00e8s l'entra\u00eenement est qu'il ne r\u00e9duit pas les co\u00fbts de r\u00e9glage de tous les param\u00e8tres excessifs. M\u00eame si vous parvenez \u00e0 comprimer un r\u00e9seau neuronal form\u00e9 en une fraction de sa taille d'origine, vous devrez toujours payer les co\u00fbts complets de son entra\u00eenement.</p> <p>La question est de savoir si vous pouvez trouver le sous-r\u00e9seau optimal sans former le r\u00e9seau neuronal complet.</p> <p>En 2018, Jonathan Frankle et Michael Carbin, deux chercheurs en IA au MIT CSAIL et coauteurs, ont publi\u00e9 un article intitul\u00e9 \"The Lottery Ticket Hypothesis\", qui prouve que pour de nombreux mod\u00e8les d'apprentissage profond, il existe de petits sous-ensembles qui peuvent \u00eatre form\u00e9s avec une pr\u00e9cision totale.</p> <p>Un autre article To prune, or not to prune: exploring the efficacy of pruning for model compression montre alors qu'\u00e9laguer les r\u00e9seaux de neurones est g\u00e9n\u00e9ralement pertinent.</p> <p>L'\u00e9fficacit\u00e9 de l'\u00e9lagage sugg\u00e8re que la plupart des mod\u00e8les sont sur-param\u00e9tr\u00e9s et que seul un petit nombre de param\u00e8tres poss\u00e8de un impact sur lmes performances du mod\u00e8le. Les autres param\u00e8tres ne faisant que prendre de la place.</p>  <p>Citation</p> <p>we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.</p>  <p>Les mod\u00e8les de deep learning sont de plus en plus gros et gourmands en ressources. Si cela ne pose pas de probl\u00e8mes lorsque que le mod\u00e8le est h\u00e9berg\u00e9 dans des datacenters, cela peut poser un probl\u00e8me lorsque que l'on souhaite le d\u00e9ployer sur des environnements contraints en ressource : IoT, smartphone, MCU.</p>  <p>Citation</p> <p>Within the realm of model  compression techniques, pruning away (forcing to zero) the less salient connections (parameters) in the neural network has been shown to reduce the number of nonzero parameters in the model with little to no loss in the final model quality.</p>"},{"location":"deep_learning/module9/module9/#idee","title":"Id\u00e9e","text":"<p>Transformer les matrices utilis\u00e9es dans les op\u00e9rations de produit matriciel ou de convolution en matrice creuse.</p> <p>Une matrice creuse est une matrice qui poss\u00e8de beaucoup de z\u00e9ros.</p> \\[     \\begin{pmatrix}0 &amp; 2 &amp; 3 &amp; 0 &amp; 0 &amp; 6\\\\ 0 &amp; -1 &amp; 3 &amp; 0 &amp; 0 &amp; 6 \\\\ 0 &amp; 4 &amp; 3 &amp; 0 &amp; 0 &amp; 8 \\\\ 0 &amp; 2 &amp; 6 &amp; 9 &amp; 0 &amp; 0\\end{pmatrix} \\] <p>L'article cherche \u00e0 r\u00e9pondre \u00e0 la qestion suivante :</p> <ul> <li>Du point de vue de l'inf\u00e9rence, \u00e9tant donn\u00e9e une borne maximale pour l'empreinte m\u00e9moire du mod\u00e8le, comment obtenir le plus pr\u00e9cis ?</li> </ul> <p>Deux m\u00e9thodes sont test\u00e9es :</p> <ul> <li>large-sparse : commencer avec un mod\u00e8le large classique (Inception, ResNet...), mais \u00e9lagu\u00e9 de fa\u00e7on \u00e0 obtenir un mod\u00e8le creux (sparse model) avec un petit nombre de param\u00e8tres non-nuls.</li> <li>small-dense : entra\u00eener de fa\u00e7on classique un petit mod\u00e8le avec une taille similaire au mod\u00e8le large-sparse.</li> </ul>  <p>Citation</p> <p>While pruning focuses on reducing the number of non-zero parameters, in principle, model pruning can be used in conjunction with other techniques to further reduce model size. Quantization techniques aim to reduce the number of bits required to represent each parameter from 32-bit floats to 8 bits or fewer</p>"},{"location":"deep_learning/module9/module9/#methode","title":"M\u00e9thode","text":"\\[ \\begin{align} W &amp;= \\begin{bmatrix}     W_{1,1} &amp; W_{1,2} &amp; \\cdots &amp; W_{1,n}\\\\     W_{2,1} &amp; W_{2,2} &amp; \\cdots &amp; W_{2,n}\\\\     \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\     W_{m,1} &amp; W_{m,2} &amp; \\cdots &amp; W_{m,n}\\\\ \\end{bmatrix} \\nonumber\\\\ \\end{align} \\] <p>Pour chaque couche choisie pour \u00eatre \u00e9lagu\u00e9e, un masque binaire est construit de la m\u00eame dimension que le tenseur de poids de la couche et il d\u00e9termine quels poids participent \u00e0 l'\u00e9tape de feedforward.</p> <p>Les poids sont ordonn\u00e9s suivant leur valeurs absolues et l'on masque les poids de plus petite valeur absolue jusqu'\u00e0 ce qu'un certain seuil \\(0 &lt; s &lt;1\\) de valeurs masqu\u00e9es soit atteint.</p> <p>i.e, au lieu d'avoir l'\u00e9quation suivante</p> \\[ \\mathbf{x}_{2, i} = \\sum_{j = 1}^{m} \\mathbf{W}_{i, j} \\mathbf{x}_{1, j} + \\mathbf{b}_{i} \\] <p>durant l'\u00e9tape de feeforward, on a l'\u00e9quation suivante</p> \\[ \\mathbf{x}_{2, i} = \\left( \\sum_{j = 1}^{m} (\\mathbf{W}_{i, j} \\mathbf{M}_{i, j}) \\mathbf{x}_{1, j} \\right) + (\\mathbf{b}_{i} \\odot \\mathbf{m}_{i}) \\\\ \\] <p>o\u00f9 \\(\\mathbf{M}\\) et \\(\\mathbf{m}\\) sont des masques binaires :</p> \\[ \\begin{align} \\mathbf{M}_{i, j} =   \\begin{cases}   0 &amp; \\text{if $|\\mathbf{W}_{i, j}| &lt; \\lambda$} \\\\   1 &amp; \\text{sinon} \\\\   \\end{cases} \\end{align} \\] <p>et</p> \\[ \\begin{align} \\mathbf{m}_{i} =   \\begin{cases}   0 &amp; \\text{if $|\\mathbf{b}_{i}| &lt; \\lambda$} \\\\   1 &amp; \\text{sinon.} \\\\   \\end{cases} \\end{align} \\] <p>Lors de l'\u00e9tape de r\u00e9tropropagation, le gradient passant par les masque binaires seuls les poids non masqu\u00e9es sont mis \u00e0 jour.</p>"},{"location":"deep_learning/module9/module9/#remarques","title":"Remarques","text":"<p>Au fur et \u00e0 mesure que le taux d'apprentissage baisse, il a \u00e9t\u00e9 observ\u00e9 que les poids \u00e9lagu\u00e9es alors que ce dernier est tr\u00e8s petit sont difficilement compens\u00e9s par les autres. Il est donc important de choisir le bon LRD et de ne pas \u00e9laguer tout le long de l'entra\u00eenement.</p>  <p>Citation</p> <p>Also note that since the weights are initialized randomly, the sparsity in the weight tensors does not exhibit any specific structure. Furthermore, the pruning method described here does not depend on any specific property of the network or the constituent layers, and can be extended directly to a wide-range of neural network architectures.</p>"},{"location":"deep_learning/module9/module9/#travailler-en-basse-precision","title":"Travailler en basse pr\u00e9cision","text":"<p>Les ordinateurs ne peuvent utiliser qu'un nombre fini de bits pour repr\u00e9senter des nombres r\u00e9els infinis.</p> <p>La pr\u00e9cision avec laquelle nous pouvons les repr\u00e9senter d\u00e9pend du nombre de bits que nous utilisons - la virgule flottante 32 bits \u00e9tant la valeur par d\u00e9faut pour la plupart des applications, y compris le deep learning. Il s'av\u00e8re que les DNN peuvent travailler avec des types de donn\u00e9es plus petits, avec moins de pr\u00e9cision, comme les entiers de 8 bits. En gros, nous essayons de travailler avec une ligne de nombres qui se rapproche de la ligne \u00e9parse du bas. Les nombres sont quantifi\u00e9s, c'est-\u00e0-dire discr\u00e9tis\u00e9s \u00e0 certaines valeurs sp\u00e9cifiques, que nous pouvons ensuite repr\u00e9senter en utilisant des entiers au lieu de nombres \u00e0 virgule flottante.</p>"},{"location":"deep_learning/module9/module9/#pourquoi-ca-nous-interesse","title":"Pourquoi \u00e7a nous int\u00e9resse","text":"<ul> <li>L'arithm\u00e9tique avec une profondeur de bit inf\u00e9rieure est plus rapide, en supposant que le mat\u00e9riel le supporte : sans pousser la r\u00e9duction de pr\u00e9cision trop long, les nouvelles architectures de GPU Nvidia permettent de faire un entra\u00eenement mixte des mod\u00e8les en FP32-FP16. M\u00eame si le calcul en virgule flottante n'est pas \"plus lent\" que le calcul en entier sur les processeurs modernes, les op\u00e9rations en virgule flottante 32 bits seront presque toujours plus lentes que, par exemple, les entiers 8 bits.</li> <li>En passant de 32 bits \u00e0 8 bits, nous obtenons (presque) 4x la r\u00e9duction de la m\u00e9moire imm\u00e9diatement. Des mod\u00e8les de d\u00e9ploiement plus l\u00e9gers signifient qu'ils accaparent moins d'espace de stockage, qu'ils sont plus faciles \u00e0 partager sur des bandes passantes plus petites, plus faciles \u00e0 mettre \u00e0 jour, etc.</li> <li>Des largeurs de bits plus faibles signifient \u00e9galement que nous pouvons condenser plus de donn\u00e9es dans les m\u00eames caches/registres. Cela signifie que nous pouvons r\u00e9duire la fr\u00e9quence \u00e0 laquelle nous acc\u00e9dons aux choses \u00e0 partir de la RAM, qui consomme g\u00e9n\u00e9ralement beaucoup de temps et d'\u00e9nergie.</li> <li>L'arithm\u00e9tique \u00e0 virgule flottante est difficile - c'est pourquoi elle n'est pas toujours prise en charge sur les microcontr\u00f4leurs de certains appareils embarqu\u00e9s \u00e0 tr\u00e8s faible puissance, comme les drones, les montres ou les appareils IoT. Le support des nombres entiers, en revanche, est facilement disponible.</li> </ul>"},{"location":"deep_learning/module9/module9/#pourquoi-ca-marche","title":"Pourquoi \u00e7a marche ?","text":"<ol> <li>Les r\u00e9seaux neuronaux sont connus pour \u00eatre assez robustes au bruit et aux autres petites perturbations une fois form\u00e9s. Cela signifie que m\u00eame si nous arrondissons subtilement les chiffres, nous pouvons nous attendre \u00e0 une r\u00e9ponse raisonnablement pr\u00e9cise.</li> <li>Les poids et les activations d'une couche particuli\u00e8re ont souvent tendance \u00e0 se situer dans une petite fourchette, qui peut \u00eatre estim\u00e9e \u00e0 l'avance. Cela signifie que nous n'avons pas besoin de la capacit\u00e9 de stocker des poids allant de \\(10^6\\) \u00e0 \\(10^{-6}\\) dans le m\u00eame type de donn\u00e9es, ce qui nous permet de concentrer nos pr\u00e9cautions sur moins de bits dans une plage plus petite, disons -3 \u00e0 +3. Par exemple, une des fonctions d'activations utilis\u00e9es dans les architecture d\u00e9di\u00e9s est la fonction</li> </ol> \\[     \\mathrm{ReLU}6(x) := \\min(\\max(0,x),6). \\]  <p>L'arithm\u00e9tique en virgule flottante</p>  <p>Source : Training deep neural networks with low precision multiplications.</p>"},{"location":"deep_learning/module9/module9/#la-quantification","title":"La quantification","text":"<p>Quantification</p>  <p>Source :Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</p> <p>Lorsque l'on utilise les m\u00e9thodes de \"Quantization aware training\" avec TensorFlow, c'est ce sch\u00e9ma l\u00e0 qui est utilis\u00e9.</p>  <p>Contrairement aux nombres \u00e0 virgule flottante, il n'y a pas de standards pour repr\u00e9senter des nombres \u00e0 virgules fixes. Le sch\u00e9ma de quantification standard utilis\u00e9 en deep learning requiert les conditions suivantes.</p> <ol> <li>La transformation doit \u00eatre affine, ainsi on a une bijection et on peut retrouver directment les nombres r\u00e9els en faisant la transformation inverse.</li> <li>On doit pouvoir repr\u00e9senter \\(0\\), <code>0.f</code>,  pr\u00e9cis\u00e9ment. Si l'on fait une quantification, puis la transformation inverse, et que l'on travaille en 8-bits, alors \\(2^8 = 256\\) nombres retrouveront leur valeur de fa\u00e7on exacte. Si l'on s'arrange pour que <code>0.f</code> soit une de ces 256 valeurs, alors les auteurs de Quantization and Training of Neural Networks for EfficientInteger-Arithmetic-Only Inference montrent que les DNN ont une pr\u00e9cision am\u00e9lior\u00e9e par rapport aux autres sch\u00e9mas de quantification. Le <code>0.f</code> ayant une signification pr\u00e9cise en Deep Learning : par exemple pour l'enjambement (\"padding\").</li> </ol> <p>La quantification en entier 8 bits n'est qu'un des sch\u00e9mas d'optimisation, TensorFlow permet de convertir par exemple en :</p> <ul> <li>float16</li> <li>int16</li> <li>int8</li> <li>int16 activations et int8 pour les poids.</li> </ul>    Technique Data requirements Size reduction Accuracy Supported hardware     Post-training float16 quantization No data Up to 50% Insignificant accuracy loss CPU, GPU   Post-training dynamic range quantization No data Up to 75% Accuracy loss CPU, GPU (Android)   Post-training integer quantization Unlabelled representative sample Up to 75% Smaller accuracy loss CPU, GPU (Android), EdgeTPU, Hexagon DSP   Quantization-aware training Labelled training data Up to 75% Smallest accuracy loss CPU, GPU (Android), EdgeTPU, Hexagon DSP    <p>Source</p>"},{"location":"deep_learning/module9/module9/#onnx","title":"ONNX","text":"<p>Il existe de nombreux frameworks pour entra\u00eener un mod\u00e8le d'apprentissage profond. Les plus populaires sont Tensorflow et PyTorch. Cependant, un mod\u00e8le entra\u00een\u00e9 par Tensorflow ne peut pas \u00eatre utilis\u00e9 avec PyTorch et vice-versa.</p> <p>Fruit d\u2019une collaboration entre AWS, Facebook et Microsoft, ONNX permet le transfert des mod\u00e8les de deep learning entre diff\u00e9rents frameworks.</p> <p>ONNX est l'abr\u00e9viation de Open Neural Network Exchange.</p>   <p>Citation</p> <p>ONNX est un format ouvert con\u00e7u pour repr\u00e9senter les mod\u00e8les d'apprentissage automatique. ONNX d\u00e9finit un ensemble commun d'op\u00e9rateurs - les \u00e9l\u00e9ments constitutifs des mod\u00e8les d'apprentissage automatique et d'apprentissage profond - et un format de fichier commun pour permettre aux d\u00e9veloppeurs d'IA d'utiliser les mod\u00e8les avec une vari\u00e9t\u00e9 de cadres, d'outils, de moteurs d'ex\u00e9cution et de compilateurs.</p>  <p>Vous pouvez entra\u00eener votre mod\u00e8le dans le framework de votre choix, puis le convertir au format ONNX.</p> <p>L'\u00e9norme avantage d'avoir un format commun est que le logiciel ou le mat\u00e9riel qui charge votre mod\u00e8le au moment de l'ex\u00e9cution n'a besoin que d'\u00eatre compatible avec ONNX.</p> <p>L'int\u00e9r\u00eat d'ONNX est l'inter-op\u00e9rabilit\u00e9 : ONNX supporte un nombre impressionnant de frameworks. L'ensemble des frameworks list\u00e9s ci-dessous peuvent \u00eatre utilis\u00e9s pour entrainer un mod\u00e8le de machine learning de fa\u00e7on transparente, et ONNX se chargera de convertir ce mod\u00e8le au format <code>.onnx</code>, qui lui permettra d'\u00eatre utilis\u00e9 sur un grand nombre de plate-forme.</p>   <p>Ce qu'il faut retenir</p> <p>ONNX est aux mod\u00e8les d'apprentissage automatique ce que JPEG est aux images ou MPEG aux vid\u00e9os.</p>"},{"location":"deep_learning/module9/module9/#tf2onnx","title":"tf2ONNX","text":"<p>Repo Github de tf2onnx</p>"},{"location":"deep_learning/module9/module9/#tensorrt","title":"TensorRT","text":"<p>TensorRT de NVIDIA est un SDK pour l'inf\u00e9rence d'apprentissage profond de haute performance.</p> <p>Il fournit des API pour effectuer l'inf\u00e9rence de mod\u00e8les pr\u00e9-entra\u00een\u00e9s et g\u00e9n\u00e8re des moteurs d'ex\u00e9cution optimis\u00e9s pour votre plateforme.</p> <p>Cette optimisation s'effectue de diff\u00e9rentes mani\u00e8res. Par exemple, TensorRT nous permet d'utiliser l'arithm\u00e9tique INT8 (entier de 8 bits) ou FP16 (virgule flottante de 16 bits) au lieu de l'habituelle FP32. Cette diminution de la pr\u00e9cision arithm\u00e9tique peut acc\u00e9l\u00e9rer de mani\u00e8re significative l'inf\u00e9rence avec une diminution minime de la pr\u00e9cision du mod\u00e8le.</p>"},{"location":"deep_learning/module9/module9/#les-architectures-dediees","title":"Les architectures d\u00e9di\u00e9es","text":""},{"location":"deep_learning/module9/module9/#mobilenet","title":"MobileNet","text":"<p>MobileNet est une s\u00e9rie d'architectures de CNN, d\u00e9marr\u00e9e en 2017 :</p> <ul> <li>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</li> <li>MobileNetV2: Inverted Residuals and Linear Bottlenecks</li> <li>Searching for MobileNetV3</li> </ul> <p>Alors que MobileNetV3 utilise les m\u00e9thodes de \"recherche architecturale neuronale\" (NAS : Neural Architecture Search), ie un algorithme de deep learning cherche des architectures de deep learning optimale (votre serviteur ici pr\u00e9sent n'en connait pas encore assez pour expliquer \u00e7a de fa\u00e7on plus claire); les mod\u00e8les MobileNetV1 et MobileNetV2 reposent principalement sur les couches que l'on appelle les Separable DepthWise Convolutions. Ce sont ces nouvelles couches l\u00e0 qui nous int\u00e9ressent.</p> <p>Pour comprendre l'int\u00e9r\u00eat de ces derni\u00e8res, il faut se replonger d'abord dans les convolutions classiques.</p>"},{"location":"deep_learning/module9/module9/#separable-depthwise-convolutions","title":"Separable DepthWise Convolutions","text":"<p>Lorsque l'on fait de l'analyse num\u00e9rique, ou de l'algorithmie, une notion tr\u00e8s importante est la \"compl\u00e9xit\u00e9 algorithmique\" qui d\u00e9termine un ordre de grandeur du nombre d'op\u00e9rations (additions et multiplications) n\u00e9c\u00e9ssaires pour arriver au r\u00e9sultat voulu. De fa\u00e7on plus g\u00e9n\u00e9ral, le co\u00fbt calculatoire peut \u00eatre important \u00e0 prendre en compte.</p> <p>Supposons que l'on a en entr\u00e9e une feature map de dimensions \\((h_{\\mathrm{in}}, w_{\\mathrm{in}},c_{\\mathrm{in}})\\), selon la convention (Height, Width, Channels). Appliquer alors un noyau de convolution de dimensions \\((k,k,c_{\\mathrm{in}},c_{\\mathrm{out}})\\) pour produire une feature map en sortie de dimensions \\((h_{\\mathrm{in}}, w_{\\mathrm{in}},c_{\\mathrm{out}})\\) demande le nombre d'op\u00e9rations suivant.</p> \\[ h_{\\mathrm{in}} \\cdot w_{\\mathrm{in}} \\cdot c_{\\mathrm{in}} \\cdot k^{2} \\cdot c_{\\mathrm{out}} \\] <p>Chaque pixel de la feature map d'entr\u00e9e est le centre d'un filtre de convolution de taille \\((k,k)\\), comme on a \\(h_{\\mathrm{in}} \\cdot w_{\\mathrm{in}} \\cdot c_{\\mathrm{in}}\\) pixels en tout en entr\u00e9e on a \\(h_{\\mathrm{in}} \\cdot w_{\\mathrm{in}} \\cdot c_{\\mathrm{in}} \\cdot k^{2}\\) op\u00e9rations pour une feature map en sortie. On souhaite \\(c_{\\mathrm{out}}\\) feature maps en sortie, donc le co\u00fbt total est bien le dernier cit\u00e9.</p>   <p>Remarque</p> <p>Comme vous le voyez ici, les dimensions spatiales des feature maps (hauteur, largeur) n'ont pas chang\u00e9es. On est donc dans le cas o\u00f9 la convolution ne r\u00e9duit pas les dimensions spatiales, par exemple dans TensorFlow on a fix\u00e9 le param\u00eatre <code>padding=\"same\"</code>.</p>  <p>La diff\u00e9rence majeure avec une convolution 2d classique, est qu'une separable depthWise convolution est divis\u00e9e en deux op\u00e9rations.</p> <ol> <li>Une premi\u00e8re op\u00e9ration, qui produit une nombre de fature map identique au nombre de feature maps d'entr\u00e9es.</li> <li>Une convolution \\(1\\times1\\), qui elle est responsable de g\u00e9n\u00e9rer le nombre de feature maps n\u00e9c\u00e9ssaires en sortie.</li> </ol>   <p>Application d'une convolution \\(1\\times1\\)</p>  <p>Notons \\(p_{i,j}(F_{k})\\) le pixel \u00e0 la coordon\u00e9e \\((i,j)\\) dans la feature map \\(F_{k}\\).</p> <p>Chacun des pixels obtenus dans la feature map en sortie est alors une combinaison lin\u00e9aire des pixels aux m\u00eames coordonn\u00e9es dans les features maps d'entr\u00e9e. Les coefficients de la combinaison lin\u00e9aire \u00e9tant appris par le r\u00e9seau et les m\u00eames pour tous les pixels de la feature map de sorite, ce sont les coefficients \\((w_{1}^{1}, w_{2}^{1}, w_{3}^{1})\\) du filtre de la convolution \\(1\\times1\\).</p>   <p>On se retrouve donc avec le nombre d'op\u00e9rations suivant.</p> \\[ h_{\\mathrm{in}} \\cdot w_{\\mathrm{in}} \\cdot c_{\\mathrm{in}} (k^{2} + c_{\\mathrm{out}}). \\] <p>Si l'on fait le rapport du nombre d'op\u00e9rations n\u00e9cessaires pour ces deux couches, on obtient :</p> \\[ \\frac{h_{\\mathrm{in}} \\cdot w_{\\mathrm{in}} \\cdot c_{\\mathrm{in}} (k^{2} + c_{\\mathrm{out}})}{h_{\\mathrm{in}} \\cdot w_{\\mathrm{in}} \\cdot c_{\\mathrm{in}} \\cdot k^{2} \\cdot c_{\\mathrm{out}}} = \\frac{1}{k^{2}}+\\frac{1}{c_{\\mathrm{out}}} \\] <p>G\u00e9n\u00e9ralement, on a \\(k \\geq 3\\) et \\(c_{\\mathrm{out}} &gt;&gt; 1\\), ce qui nous donne \\(\\frac{1}{k^{2}}+\\frac{1}{c_{\\mathrm{out}}} &lt; 1\\). Ce qui veut dire que pour produire le m\u00eame nombre de feature maps en sortie, une separable depthwise convolution est beaucoup plus efficace noveau nombre d'op\u00e9rations.</p>"},{"location":"deep_learning/module9/module9/#repvgg","title":"RepVGG","text":"<p>RepVGG: Making VGG-style ConvNets Great Again est un article sorti en Mars 2021 arguant que si les connexions r\u00e9siduelles des architectures de type ResNet sont tr\u00e8s efficaces pour l'entra\u00eenement des mod\u00e8les en leur permettant d'atteindre de tr\u00e8s hautes performances, c'est en frein lorsque l'on doit d\u00e9ployer de telles architectures sur de l'embarqu\u00e9, car les connexions r\u00e9siduelles consomment de la m\u00e9moire inutile lors de l'inf\u00e9rence, et r\u00e9duisent donc la vitesse d'inf\u00e9rence.</p> <p>L'id\u00e9e de l'article est alors la suivante : avoir deux topologies de mod\u00e8les diff\u00e9rentes.</p> <ol> <li>Une topologie avec des connexions r\u00e9siduelles et des architectures de types ResNet.</li> <li>Une topologie lin\u00e9aire avec une architecture du style VGG de lors du d\u00e9ploiement du mod\u00e8le.</li> </ol>  <p>Question</p> <p>Comment passer d'une architetcure \u00e0 une autre ?</p>  <p>L'article s'appuie sur une des techniques classiques d'optimisation des mod\u00e8les : la fusion Convolution-Batchnormalization.</p>  <p>Abstract</p> <p>We present a simple but powerful architecture of convolutional neural network, which has a VGG-like inference-time body composed of nothing but a stack of \\(3\\times3\\) convolution and ReLU, while the training-time model has a multi-branch topology. Such decoupling of the training-time and inference-time architecture is realized by a structural re-parameterization technique so that the model is named RepVGG. On ImageNet, RepVGG reaches over \\(80\\%\\) top-1 accuracy, which is the first time for a plain model, to the best of our knowledge. On NVIDIA 1080Ti GPU, RepVGG models run \\(83\\%\\) faster than ResNet-\\(50\\) or \\(101\\%\\) faster than ResNet-\\(101\\) with higher accuracy and show favorable accuracy-speed trade-off compared to the state-of-the-art models like EfficientNet and RegNet.</p>"},{"location":"deep_learning/module9/module9/#la-fusion-convolution-batchnorm","title":"La fusion Convolution-Batchnorm","text":"<p>La fusion d'une couche de convolution avec une couche de batchnorm ressort les poids et biais d'une nouvelle couche de convolution avec les noyaux de convolutions de m\u00eame dimension.</p> <p>Etant donn\u00e9 le tenseur \\(W\\) de poids des noyaux de convolution d'une couche convolutive et le tenseur de \\(4\\) param\u00e8tres \\(B=(\\gamma, \\beta, \\mu, \\sigma)\\) d'une couche de batchnormalization, on obtient les nouveaux poids et poids de la nouvelle couche convolutive via les formules suivantes.</p> \\[ \\widehat{W}_{:,:,:,j} := \\frac{\\gamma_{j} \\cdot W_{:,:,:,j}}{\\sqrt{\\sigma_{j} + \\epsilon}} \\] \\[ b_{j} = \\beta_{j} - \\frac{\\mu_{j}\\cdot\\gamma_{j}}{\\sqrt{\\sigma_{j} + \\epsilon}} \\] <p>On remarque ici que le biais de la nouvelle couche de convolution ne d\u00e9pend que des param\u00e8tres de la couche de batchnorm. Ce qui est coh\u00e9rent avec la pratique de ne jamais mettre de biais dans une couche de convolution lorsqu'elle est suivie par une couche de batchnorm.</p>  <p>Remarque</p> <p>Le \\(\\epsilon\\) pr\u00e9sent ici est pour s'assurer que l'on ne divise jamais pas z\u00e9ro, dans la pratique il est fix\u00e9 \u00e0 \\(0,001\\).</p>  <p>Ce qui nous donne, dans la pratique la fonction suivante.</p>  <p>Fusion Conv-BN</p> <pre><code># https://scortex.io/batch-norm-folding-an-easy-way-to-improve-your-network-speed/\n# https://github.com/DingXiaoH/RepVGG/blob/4da799e33c890c624bfb484b2c35abafd327ba40/repvgg.py#L68\n\ndef fuse_bn_conv(weights_conv, weights_bn, eps=0.001):\n    gamma = np.reshape(weights_bn[0], (1,1,1,weights_bn[0].shape[0]))\n    beta = weights_bn[1]\n    mean = weights_bn[2]\n    variance = np.reshape(weights_bn[3], (1,1,1,weights_bn[3].shape[0]))\n\n    new_weights = (weights_conv[0]*gamma) / np.sqrt(variance + eps)\n    new_bias = beta - mean*gamma/np.sqrt(variance+eps)\n\n    new_bias = np.reshape(new_bias, weights_bn[3].shape[0])\n\n    return new_weights, new_bias\n# In the code above, the reshaping is necessary to prevent a mistake if the dimension of the output O was the same as the dimension of the input I.\n</code></pre>  <p>D\u00e9taillons.</p> <p>Pour cela, d\u00e9finissons un mod\u00e8le dummy qui nous servira pour nos explications.</p>  <p>Mod\u00e8le dummy</p> <pre><code>img_shape = 32, 32, 3\ninput = Input(img_shape)\nx= Conv2D(filters = 16,\n          kernel_size=3,\n          padding='same',\n          use_bias=True,\n          kernel_initializer='he_uniform',\n          name='testing_conv_init')(input)\nx= BatchNormalization(name=f'testing_bn_init')(x)\nmodel = Model(input, x)\nmodel.summary()\n</code></pre>"},{"location":"deep_learning/module9/module9/#etude-de-la-couche-convolutive","title":"Etude de la couche convolutive","text":"<p>Mod\u00e8le dummy</p> <p><pre><code>weights_conv = model.get_layer(\"testing_conv_init\").get_weights()\n</code></pre> <pre><code>type(weights_conv)\n</code></pre> list</p> <p><pre><code>len(weights_conv)\n</code></pre> 2</p>  <p>Les poids dans une couche convolutive sont une liste de deux \u00e9l\u00e9ments :</p> <ul> <li><code>weights[0]</code> correspond aux poids des noyaux de convolution,</li> <li><code>weights[1]</code> correspond aux biais.</li> </ul> <p><pre><code>type(weights_conv[0])\n</code></pre> numpy.ndarray</p> <p><pre><code>weights_conv[0].shape\n</code></pre> (3, 3, 3, 16)</p> <p>Les axes du tenseur de poids suivent les dimensions suivantes :</p> <ul> <li>kernel_size1 : hauteur du kernel,</li> <li>kernel_size2 : largeur du kernel,</li> <li>channels_in : nombre des feature maps en entr\u00e9e,</li> <li>channels_out : nombres de features maps (filters) en sortie.</li> </ul> <p><code>channels_out</code> est d\u00e9finie dans la couche convolutive via le param\u00e8tres <code>filters</code>, alors que la valeur <code>channels_in</code> est elle directement d\u00e9termin\u00e9e par le tenseur en entr\u00e9e. C'est une diff\u00e9rence de TensorFlow par rapport \u00e0 Pytorch o\u00f9 <code>channels_in</code> et <code>channels_out</code> sont tous les deux des param\u00e8tres des couches convolutives.</p> <p>Ainsi, si l'on veut voir les poids du noyau de convolution par rapport au canal \\(0\\) en la feature map de sortie \\(5\\), on les obtient en regardant :</p> <pre><code>weights_conv[0][:,:,0,5]\n</code></pre> <p>Par d\u00e9faut, les biais des couches de convolutions sont tous initialis\u00e9s \u00e0 z\u00e9ro.</p>"},{"location":"deep_learning/module9/module9/#etude-de-la-batchnorm","title":"Etude de la batchnorm","text":"<p>Mod\u00e8le dummy</p> <p><pre><code>weights_bn = model.get_layer('testing_bn_init').get_weights()\n</code></pre> <pre><code>type(weights_bn)\n</code></pre> list</p> <p><pre><code>len(weights_bn)\n</code></pre> 4</p>  <p>Dans une couche de BatchNormalization, on a 4 types de poids.</p> <ul> <li>Les deux param\u00e8tres de scaling \\(\\gamma\\) et de biais \\(\\beta\\).</li> <li>Les deux param\u00e8tres correspondant \u00e0 la moyenne \\(\\mu\\) et la variance \\(\\sigma\\).</li> </ul> <p>Tous ces param\u00e8tres ne sont pas entra\u00eenables, comme on peut le voir dans la liste suivante.</p> <pre><code>[(var.name, var.trainable) for var in model.get_layer('testing_bn_init').variables]\n</code></pre> <pre><code>[('testing_bn_init/gamma:0', True),\n ('testing_bn_init/beta:0', True),\n ('testing_bn_init/moving_mean:0', False),\n ('testing_bn_init/moving_variance:0', False)]\n</code></pre> <p><pre><code>backend.shape(model.get_layer('testing_bn_init').get_weights())\n</code></pre> Les \\(4\\) param\u00e8tres sont tous des vecteurs de dimension \\(16\\), ce qui correspond au nombre de feature maps en sortie de la couche convolutive.</p>"},{"location":"deep_learning/module9/module9/#nouveau-tenseur-de-poids","title":"Nouveau tenseur de poids","text":"<p>Discutons premi\u00e8rement de la formulation du nouveau tenseur de poids, et voyons pourquoi on modifie la forme de vecteurs \\(\\gamma\\) et \\(\\sigma\\).</p> <p>\\(W_{:,:,:,j}\\) correspond dans la formule au noyau de convolution complet de la \\(j\\)-i\u00e8me feature map de sortie.</p> <pre><code>weights_conv = model.get_layer(\"testing_conv_init\").get_weights()\nweights_conv[0].shape\n</code></pre> <pre><code>(3, 3, 3, 16)\n</code></pre> <p>On a \\(16\\) noyaux de convolution, chacun de dimensions \\((3,3,3)\\). Par exemple, pour \\(j=1\\).</p> <pre><code>weights_conv[0][:,:,:,1].shape\n</code></pre> <p><pre><code>(3, 3, 3)\n</code></pre> Les vecteur \\(\\gamma\\) et \\(\\sigma\\) \u00e9tant des vecteurs de dimension \\(16\\), on va les \"transformer en tenseur\" de dimensions \\((1,1,1,16)\\) pour bien faire correspondre le produit suivant chaque axe.</p> <pre><code>variance = np.reshape(weights_bn[3], (1,1,1,weights_bn[3].shape[0]))\nvariance.shape\n</code></pre> <p><pre><code>(1, 1, 1, 16)\n</code></pre> <pre><code>gamma = np.reshape(weights_bn[0], (1,1,1,weights_bn[0].shape[0]))\ngamma.shape\n</code></pre></p> <pre><code>(1, 1, 1, 16)\n</code></pre>  <p>Au final, la formule</p> <pre><code>new_weights = (weights_conv[0]*gamma) / np.sqrt(variance + eps)\n</code></pre> <p>r\u00e9sume tout cela, tous les tenseurs ayant le m\u00eambre nombre d'axes, les op\u00e9rations sont vectoris\u00e9es et se font axe par axe.</p>"},{"location":"deep_learning/module9/module9/#nouveau-tenseur-de-biais","title":"Nouveau tenseur de biais","text":"<p>Le op\u00e9rations de <code>reshape</code> n'ont pas ajouter de nouveaux scalaires, juste des axes, le calcul du biais se fait alors \u00e9l\u00e9ment par \u00e9l\u00e9ment pour tout \\(j\\).</p>"},{"location":"deep_learning/module9/module9/#verification-via-les-developpements-limites","title":"V\u00e9rification via les d\u00e9veloppements limit\u00e9s","text":"<p>Cr\u00e9ons un tenseur de poids \\(W\\) rep\u00e9resentatif du noyau d'une convolution et un tenseur de poids \\(B=(\\gamma, \\beta, \\mu, \\sigma)\\) repr\u00e9sentatif des coefficients d'une batchnormalization.</p> <p>Pour v\u00e9rifier si tout marche bien, fixons volontairement le tenseur poids comme un tenseur de dimensions \\((3,3,4,5)\\), la dimension du noyau est toujours fix\u00e9 \u00e0 \\((3,3)\\) dans RepVGG, seules les dimensions <code>channels_in</code> et <code>channels_out</code> peuvent changer.</p> <p>Tous les coefficients du tenseur de poids seront fix\u00e9s \u00e0 \\(1\\).</p> <pre><code>conv_weights = np.ones(3*3*4*5).reshape((3,3,4,5))\nconv_weights.shape\n</code></pre> <p><pre><code>(3,3,4,5)\n</code></pre> La dimension <code>channels_out</code> ayant \u00e9t\u00e9 fix\u00e9e \u00e0 \\(5\\), les vecteurs de la batchnormalization seront tous des vecteurs de dimension \\(5\\). Fixons les coefficients suivants.</p> <pre><code>def batchnorm_variables(gamma_coef: float, beta_coef: float, mu_coef: float, sigma_coef: float, channels: int):\n    gamma = gamma_coef*np.ones(channels)\n    beta = beta_coef*np.ones(channels)\n    mu = mu_coef*np.ones(channels)\n    sigma = sigma_coef*np.ones(channels)\n\n    return [gamma, beta, mu, sigma]\n</code></pre> <pre><code>conv, bn = fuse_bn_conv([conv_weights], batchnorm_variables(1,2,1,4,5))\n</code></pre> <p>Par d\u00e9finition, le nouveau tenseur de poids \\(\\widehat{W}\\) de la convolution r\u00e9sultant de la fusion de l'ancienne convolution et de la batchnorm est donn\u00e9 par formule suivante.</p> \\[ \\widehat{W}_{:,:,:,j} := \\frac{\\gamma_{j} \\cdot W_{:,:,:,j}}{\\sqrt{\\sigma_{j} + \\epsilon}} \\] <p>De fa\u00e7on g\u00e9n\u00e9rale, pour \\(\\gamma_{j}, \\sigma_{j}\\), on a le d\u00e9veloppement limit\u00e9 suivant.</p> \\[ \\widehat{W}_{:,:,:,j} := \\frac{\\gamma_{j} \\cdot W_{:,:,:,j}}{\\sqrt{\\sigma_{j} + \\epsilon}} = \\frac{\\gamma_{j}}{\\sqrt{\\sigma_{j}}}\\left[1- \\frac{1}{2\\sigma_{j}}\\epsilon + o(\\epsilon^{2})\\right]W_{:,:,:,j} \\] <p>Dans notre cas, \\(\\forall j, \\gamma_{j} = 1, \\sigma_{j} = 4\\) d'o\u00f9</p> \\[ \\widehat{W}_{:,:,:,j} := \\frac{W_{:,:,:,j}}{\\sqrt{4 + \\epsilon}} = \\left[\\frac{1}{2}- \\frac{1}{16}\\epsilon + o(\\epsilon^{2})\\right]W_{:,:,:,j} \\simeq \\left[\\frac{1}{2}- \\frac{1}{16}\\epsilon\\right]W_{:,:,:,j} \\] <pre><code>def compute_scaling_weight_factor(gamma, sigma):\n    return gamma/np.sqrt(sigma)*(1-0.001/(2*sigma))\n\nscale = compute_scaling_weight_factor(1,4)\nscale\n</code></pre> <pre><code>conv[:,:,:,4]\n</code></pre> <pre><code>array([[[0.49993751, 0.49993751, 0.49993751, 0.49993751],\n        [0.49993751, 0.49993751, 0.49993751, 0.49993751],\n        [0.49993751, 0.49993751, 0.49993751, 0.49993751]],\n\n       [[0.49993751, 0.49993751, 0.49993751, 0.49993751],\n        [0.49993751, 0.49993751, 0.49993751, 0.49993751],\n        [0.49993751, 0.49993751, 0.49993751, 0.49993751]],\n\n       [[0.49993751, 0.49993751, 0.49993751, 0.49993751],\n        [0.49993751, 0.49993751, 0.49993751, 0.49993751],\n        [0.49993751, 0.49993751, 0.49993751, 0.49993751]]])\n</code></pre> <p>Ce qui correspond bien \u00e0 l'approximation obtenue par d\u00e9veloppement limit\u00e9. On peut par exemple v\u00e9rifier si \\(\\widehat{W}\\) est approximativement \u00e9gal \u00e0 <code>conv</code> \u00e0 \\(10^{-3}\\) avec la commande <code>np.isclose</code>. Si <code>np.mean(...)</code> \\(&lt; 1\\) alors le calcul est faux.</p> <pre><code>conv_weights_real = scale*np.ones(3*3*4*5).reshape((3,3,4,5))\nnp.mean(np.isclose(conv, conv_weights_real, rtol=1e-3))\n</code></pre> <pre><code>1.0\n</code></pre> <p>Pour le biais, on a la formule suivante.</p> \\[ b_{j} = \\beta_{j} - \\frac{\\mu_{j}\\cdot\\gamma_{j}}{\\sqrt{\\sigma_{j} + \\epsilon}} = \\beta_{j} - \\frac{\\mu_{j}\\cdot\\gamma_{j}}{\\sqrt{\\sigma_{j}}}\\left[1- \\frac{1}{2\\sigma_{j}}\\epsilon + o(\\epsilon^{2})\\right] \\] <p>dans notre cas, on a :</p> <ul> <li>\\(\\beta_{j} = 2\\),</li> <li>\\(\\gamma_{j} = 1\\),</li> <li>\\(\\mu_{j} = 1\\),</li> <li>\\(\\sigma_{j} = 4\\).</li> </ul> \\[ b_{j} = 2 - \\frac{1}{2}\\left[1- \\frac{1}{8}\\epsilon + o(\\epsilon^{2})\\right] \\simeq 2 - \\frac{1}{2} - \\frac{1}{16}\\epsilon \\] <pre><code>def compute_scaling_bias_factor(gamma, beta, mu, sigma):\n    a = (mu*gamma)/np.sqrt(sigma)\n    b = 1 - 0.001/(2*sigma)\n\n    return beta-a*b\n</code></pre> <pre><code>bias_scale = compute_scaling_bias_factor(1,2,1,4)\nbn_real = bias_scale*np.ones(5)\nnp.mean(np.isclose(bn_real, bn, rtol=1e-3))\n</code></pre> <ul> <li>https://bdtechtalks.com/2020/10/12/deep-learning-neural-network-pruning/</li> <li>https://github.com/BenWhetton/keras-surgeon</li> <li>https://jacobgil.github.io/deeplearning/pruning-deep-learning</li> <li>https://heartbeat.fritz.ai/research-guide-pruning-techniques-for-neural-networks-d9b8440ab10d</li> <li>https://blog.dataiku.com/making-neural-networks-smaller-for-better-deployment-solving-the-size-problem-of-cnns-using-network-pruning-with-keras</li> </ul>"},{"location":"deep_learning/module9/tp9_elaguage/","title":"TP Module 9 : Optimisation des mod\u00e8les, \u00e9lagage","text":""},{"location":"deep_learning/module9/tp9_elaguage/#import-des-librairies-creation-du-dataset","title":"Import des librairies, cr\u00e9ation du dataset","text":"<p>Importons d'abord les librairies qui nous serons n\u00e9cessaires.</p>  <p>TensorFlow</p> <pre><code>import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import load_model\n\nfrom typing import List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom loguru import logger\nimport os\nimport random\nimport datetime\nimport time\n\n# freeze de l'al\u00e9atoire, pour avoir des exp\u00e9riences reproductibles.\nRANDOM_SEED = 42\n\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\ntf.random.set_seed(RANDOM_SEED)\n</code></pre>  <p>Comme nous aurons besoin de cr\u00e9er de nouveaux des datasets pour l'entrainement, d\u00e9crivons une classe qui nous permettra de le faire.</p>  <p>Tensorize</p> <pre><code>class Tensorize(object):\n    \"\"\"Class used to create tensor datasets for TensorFlow.\n\n    Args:\n        object (object): The base class of the class hierarchy, used only to enforce\n            WPS306. See https://wemake-python-stylegui.de/en/latest/pages/usage/\n            violations/consistency.html#consistency.\n    \"\"\"\n\n    def __init__(\n        self, n_classes: int, img_shape: Tuple[int, int, int], random_seed: int\n    ) -&gt; None:\n        \"\"\"Initialization of the class Featurize.\n\n        Initialize the class the number of classes in the datasets, the shape of the\n        images and the random seed.\n\n        Args:\n            n_classes (int): Number of classes in the dataset.\n            img_shape (Tuple[int, int, int]): Dimension of the image, format is (H,W,C).\n            random_seed (int): Fixed random seed for reproducibility.\n        \"\"\"\n        self.n_classes = n_classes\n        self.img_shape = img_shape\n        self.random_seed = random_seed\n        self.AUTOTUNE = tf.data.experimental.AUTOTUNE\n\n    def load_images(self, data_frame: pd.DataFrame, column_name: str) -&gt; List[str]:\n        \"\"\"Load the images as a list.\n\n        Take the dataframe containing the observations and the labels and the return the\n        column containing the observations as a list.\n\n        Args:\n            data_frame (pd.DataFrame): Dataframe containing the dataset.\n            column_name (str): The name of the column containing the observations.\n\n        Returns:\n            The list of observations deduced from the dataframe.\n        \"\"\"\n        return data_frame[column_name].tolist()\n\n    def load_labels(self, data_frame: pd.DataFrame, column_name: str) -&gt; List[int]:\n        \"\"\"Load the labels as a list and encode them.\n\n        Take the dataframe containing the observations and the labels and the return the\n        column containing the labels as an encoded list.\n\n        The encoding is done by taking the set of labels, alphabetically sorted, and\n        then transforming them as integers starting from 0.\n\n        `from sklearn.preprocessing import LabelEncoder` works well to encode labels,\n        but if the dataset is huge, the time it takes to encode all the labels is\n        growing fast. We use anumpy and vectorization to speed up the time.\n\n        See the StackOverflow question :\n        [Question](https://stackoverflow.com/questions/45321999/\n        how-can-i-optimize-label-encoding-for-large-data-sets-sci-kit-learn)\n\n        Args:\n            data_frame (pd.DataFrame): Dataframe containing the dataset.\n            column_name (str): The name of the column containing the labels.\n\n        Returns:\n            The list of encoded labels deduced from the dataframe.\n        \"\"\"\n        label_list = data_frame[column_name].tolist()\n        classes = sorted(set(label_list))\n        logger.info(f\"Found following labels {classes}\")\n\n        labels = np.unique(label_list, return_inverse=True)[1]\n        dic = dict(zip(label_list, labels))\n        logger.info(f\"Dictionnary creation {dic}\")\n        vectorized_get = np.vectorize(dic.get)\n\n        return vectorized_get(label_list)\n\n    def parse_image_and_label(\n        self, filename: str, label: int\n    ) -&gt; Tuple[np.ndarray, int]:\n        \"\"\"Transform image and label.\n\n        Parse image to go from path to a resized np.ndarray, and parse the labels to\n        one-hot encode them.\n\n        Args:\n            filename (str): The path of the image to parse.\n            label (int): The label of the image, as an int, to one-hot encode.\n\n        Returns:\n            A np.ndarray corresponding to the image and the corresponding one-hot label.\n        \"\"\"\n        resized_dims = [self.img_shape[0], self.img_shape[1]]\n        # convert the label to one-hot encoding\n        label = tf.one_hot(label, self.n_classes)\n        # decode image\n        image = tf.io.read_file(filename)\n        # Don't use tf.image.decode_image,\n        # or the output shape will be undefined\n        image = tf.image.decode_jpeg(image)\n        # This will convert to float values in [0, 1]\n        image = tf.image.convert_image_dtype(image, tf.float32)\n        image = tf.image.resize(image, resized_dims)\n\n        return image, label\n\n    def train_preprocess(\n        self, image: np.ndarray, label: List[int]\n    ) -&gt; Tuple[np.ndarray, List[int]]:\n        \"\"\"Augmentation preprocess, if needed.\n\n        Args:\n            image (np.ndarray): The image to augment.\n            label (List[int]): The corresponding label.\n\n        Returns:\n            The augmented pair.\n        \"\"\"\n        image = tf.image.random_flip_left_right(image)\n        image = tf.image.random_flip_up_down(image)\n\n        return image, label\n\n    def create_dataset(\n        self,\n        data_path: str,\n        batch: int,\n        repet: int,\n        prefetch: int,\n        augment: bool,\n    ) -&gt; tf.data.Dataset:\n        \"\"\"Creation of a tensor dataset for TensorFlow.\n\n        Args:\n            data_path (str): Path where the csv file containing the dataframe is\n                located.\n            batch (int): Batch size, usually 32.\n            repet (int): How many times the dataset has to be repeated.\n            prefetch (int): How many batch the CPU has to prepare in advance for the\n                GPU.\n            augment (bool): Does the dataset has to be augmented or no.\n\n        Returns:\n            A batch of observations and labels.\n        \"\"\"\n        df = pd.read_csv(data_path)\n        features = self.load_images(data_frame=df, column_name=\"filename\")\n        labels = self.load_labels(data_frame=df, column_name=\"label\")\n\n        dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n        dataset = dataset.shuffle(len(features), seed=self.random_seed)\n        dataset = dataset.repeat(repet)\n        dataset = dataset.map(\n            self.parse_image_and_label, num_parallel_calls=self.AUTOTUNE\n        )\n        if augment:\n            dataset = dataset.map(\n                self.train_preprocess, num_parallel_calls=self.AUTOTUNE\n            )\n        dataset = dataset.batch(batch)\n        dataset = dataset.cache()\n        return dataset.prefetch(prefetch)\n</code></pre>  <p>et cr\u00e9ons nos 3 datasets classiques.</p>  <p>Attention</p> <p>Bien v\u00e9rifier sur o\u00f9 pointent les adresses dans les csv.</p>   <p>ds, ds_val, ds_train</p> <pre><code>ts = Tensorize(\n            n_classes=2,\n            img_shape=[224,224,3],\n            random_seed=42,\n        )\n\nds = ts.create_dataset(\n    \"prepared_dataset/train.csv\",\n    32,\n    1,\n    1,\n    True,\n)\n\nds_val = ts.create_dataset(\n    \"prepared_dataset/val.csv\",\n    32,\n    1,\n    1,\n    True,\n)\n\nds_test = ts.create_dataset(\n    \"prepared_dataset/test.csv\",\n    32,\n    1,\n    1,\n    True,\n)\n</code></pre>  <p>Une fois que l'on est l\u00e0, on peut charger notre mod\u00e8le de base, entra\u00een\u00e9 sans optimisations particuli\u00e8res.</p> <pre><code>model_vanilla = load_model(\"../facemask_detector.h5\")\nmodel_vanilla.evaluate(ds_test)\n</code></pre>  <p>Remarque</p> <p>Si l'on voit voir la taille du mod\u00e8le, on peut utiliser la commande shell suivante :</p> <pre><code># Check size\ndu --all -h ../facemask_detector.h5\n</code></pre> <p>Ne pas oublier le \"!\" avant si vous le faites dans un notebbok jupyter.</p> <pre><code># Check size\n!du --all -h ../facemask_detector.h5\n</code></pre>"},{"location":"deep_learning/module9/tp9_elaguage/#pruningelagage","title":"Pruning/Elagage","text":"<p>Comme dit pr\u00e9c\u00e9demment, pour pouvoir appliquer l'\u00e9lagage pour optimiser le mod\u00e8le, on va avoir besoin de la librairie tensorflow-model-optimization.</p>  <p>TFMOT</p> <pre><code>!pip install -q tensorflow-model-optimization\nimport tensorflow_model_optimization as tfmot\n</code></pre>  <p>Pour chaque couche choisie pour \u00eatre \u00e9lagu\u00e9e, un masque binaire est construit de la m\u00eame dimension que le tenseur de poids de la couche et il d\u00e9termine quels poids participent \u00e0 l'\u00e9tape de feedforward.</p> <p>Les poids sont ordonn\u00e9s suivant leur valeurs absolues et l'on masque les poids de plus petite valeur absolue jusqu'\u00e0 ce qu'un certain seuil \\(s \\in ]0,1[\\) de valeurs masqu\u00e9es soit atteint.</p> <p>Lors de l'\u00e9tape de r\u00e9tropropagation, le gradient passant par le masque binaire seuls les poids non masqu\u00e9es sont mis \u00e0 jour.</p> <p>Au fur et \u00e0 mesure que le taux d'apprentissage baisse, il a \u00e9t\u00e9 observ\u00e9 que les poids \u00e9lagu\u00e9s alors que ce dernier est tr\u00e8s petit sont difficilement compens\u00e9s par les autres. Il est donc important de choisir le bon LRD et de ne pas \u00e9laguer tout le long de l'entra\u00eenement.</p> <p>Dans TFMOT, on a en particuliers deux m\u00e9thodes pour \u00e9laguer les mod\u00e8les, l'\u00e9lagage constant, et l'\u00e9lagage avec d\u00e9croissance polynomiale.</p>"},{"location":"deep_learning/module9/tp9_elaguage/#elagage-constant","title":"Elagage constant","text":"<p>ConstantSparsity</p> <pre><code>    target_sparsity = 0.5 # A scalar float representing the target sparsity value.\n    begin_step = 0 # Step at which to begin pruning.\n    end_step =  -1 # Step at which to end pruning. -1 by default. -1 implies continuing to prune till the end of training.\n    frequency = 1 # Only apply pruning every frequency steps.\n    epochs = 4 # Number of epochs we'll fine tune with pruning\n\n    # Define pruning schedule\n    pruning_params = {\n        'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(\n            target_sparsity=target_sparsity,\n            begin_step=begin_step,\n            end_step=end_step,\n            frequency=frequency\n        )\n    }\n</code></pre>  <p>D\u00e9taillons.</p> <ul> <li> <p>Elaguer signifie que nous souhaitons avoir uniquement un certain nombre/pourcentage de poids non nuls, ce pourcentage est d\u00e9fini par notre variable <code>target_sparsity</code>, ici fix\u00e9e \u00e0 \\(0.5\\). A la fin de l'entra\u00eenement, nous aurons donc \\(50\\%\\) des poids des couches \u00e9lagu\u00e9es (couches denses, convolutives) qui seront nuls.</p> </li> <li> <p>De par l'observation donn\u00e9e au dessus, nous n'allons pas \u00e9laguer sur toute la dur\u00e9e de l'entra\u00eenement, mais seulement sur les <code>epochs = 4</code> premi\u00e8res \u00e9poques de l'entra\u00eenement. Notre dataset \u00e9tant relativement petit, nous allons \u00e9laguer \u00e0 chaque \u00e9tape (<code>frequency</code>) des ces \u00e9poques, de la premi\u00e8re \u00e9tape (<code>begin_step</code>), \u00e0 la derni\u00e8re de l'\u00e9poque \\(4\\) (<code>end_step</code>).</p> </li> <li> <p>Chacune de ces variables sert de param\u00e8tre \u00e0 la fonction <code>tfmot.sparsity.keras.ConstantSparsity</code>, qui d\u00e9finit comment va se faire l'\u00e9lagage. Ici on a choisi un \u00e9lagage constant (<code>ConstantSparsity</code>) sur les \u00e9tapes et \u00e9poques, ce qui signifie qu'\u00e0 chaque \u00e9tape on choisira la moiti\u00e9 haute des poids : ceux de plus haute magnitude, et que les autres seront mis \u00e0 z\u00e9ro.</p> </li> </ul>"},{"location":"deep_learning/module9/tp9_elaguage/#decroissance-polynomiale","title":"D\u00e9croissance polynomiale","text":"<p>Quote</p> <p>We introduce a new automated gradual pruning algorithm in which the sparsity is increased from an initial sparsity value \\(s_i\\) (usually \\(0\\)) to a final sparsity value \\(s_f\\) over a span of \\(n\\) pruning steps, starting at training step \\(t_0\\) and with pruning frequency \\(\\Delta t\\), Source.</p>  \\[ s_t  = s_f  + (s_i - s_f) \\cdot \\left(1-\\frac{t - t_0}{n\\Delta t} \\right) ^{3} \\quad t \\in \\{ t_{0}, t_{0}+\\Delta t, \\dots, t_{0}+n\\Delta t\\} \\] <p>Le code source de la fonction sur le github de TFMOT d\u00e9finit la fonction d'\u00e9lagague de la fa\u00e7on suivante :</p> <pre><code>Initializes a Pruning schedule with a PolynomialDecay function.\n\nPruning rate grows rapidly in the beginning from initial_sparsity,\nbut then plateaus slowly to the target sparsity. The function applied is\n\n    current_sparsity = final_sparsity + (initial_sparsity - final_sparsity) *\n    (1 - (step - begin_step)/(end_step - begin_step)) ^ exponent\n</code></pre>  <p>Note</p> <p>Pour avoir le nombre total d'\u00e9tapes durant l'entra\u00eenement, on peut utiliser la fonction suivante :</p> <pre><code>end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n</code></pre>   <p>PolynomialDecay</p> <pre><code>initial_sparsity = 0.5 # Sparsity (%) at which pruning begins.\nfinal_sparsity = 0.8 # Sparsity (%) at which pruning ends.\nbegin_step = 0 # Step at which to begin pruning.\nend_step =  10 # Step at which to end pruning.\nfrequency = 1 # Only apply pruning every frequency steps.\n\n# Define pruning schedule\npruning_params = {\n    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n        initial_sparsity = initial_sparsity,\n        final_sparsity = final_sparsity,\n        begin_step = begin_step,\n        end_step = end_step,\n        power = 3,\n        frequency = frequency,\n    )\n}\n</code></pre>  <p>Tout concorde avec la d\u00e9finition de l'article, sauf \\(n\\Delta t\\) et <code>end_step - begin_step</code>. On a</p> <ul> <li>initial_sparsity = \\(s_i\\),</li> <li>final_sparsity = \\(s_f\\),</li> <li>begin_step = \\(t_0\\),</li> <li>power = \\(3\\),</li> </ul> <p>Reste \u00e0 d\u00e9terminer</p> <ul> <li>end_step = \\(t_f\\),</li> <li>frequency = \\(\\Delta t\\),</li> </ul> <p>Regardons le code source, les lignes int\u00e9ressantes sont les lignes \\(247-248\\), o\u00f9 l'on a le code :</p> <p><pre><code>self._should_prune_in_step(step, self.begin_step, self.end_step, self.frequency),\n</code></pre> La fonction PolynomialDecay demande en fait \u00e0 une autre fonction de contr\u00f4le si elle doit pratiquer un \u00e9lagage ou non. Cette fonction, <code>self._should_prune_in_step</code>, est d\u00e9finie ici.</p> <p>Les lignes int\u00e9ressantes sont les lignes \\(62-65\\) :</p> <p><pre><code>is_pruning_turn = tf.math.equal(\n    tf.math.floormod(tf.math.subtract(step, begin_step), frequency), 0)\n\nreturn tf.math.logical_and(is_in_pruning_range, is_pruning_turn)\n</code></pre> Les deux premi\u00e8res lignes demandent si \\(t-t_{0} \\equiv 0 \\, \\mathrm{mod} \\Delta t\\), en d'autres termes si \\(t-t_{0}\\) est un multiple du param\u00e8tre \\(\\Delta t\\), <code>frequency</code>, et retroune un bool\u00e9en. De plus la fonction v\u00e9rifie que l'on est bien dans l'intervalle \\([\\)<code>begin_step</code>, <code>end_step</code>\\(]\\) pour \u00e9laguer.</p> <p>Ce qui veut dire que durant la p\u00e9riode d'entra\u00eenement \\([\\)<code>begin_step</code>, <code>end_step</code>\\(]\\), on ne peut \u00e9laguer que si \\(t-t_{0}\\) est un multiple de la fr\u00e9quence, ce qui concorde avec la formule de l'article.</p>  <p>Comportement de l'\u00e9lagage</p>  <p>Le comportement de l'\u00e9lagage polynomial au cours de l'entra\u00eenement. Source.</p>"},{"location":"deep_learning/module9/tp9_elaguage/#mise-en-place","title":"Mise en place","text":"<p>Compilation du mod\u00e8le</p> <pre><code>model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model_vanilla,\n                                                         **pruning_params)\n\n# prune_low_magnitude requires a recompile.\nmodel_for_pruning.compile(optimizer='sgd',\n                        loss='categorical_crossentropy',\n                        metrics=['accuracy'])\n\n# Train the model.\nlogdir = tempfile.mkdtemp()\ncallbacks = [tfmot.sparsity.keras.UpdatePruningStep(),\n            tfmot.sparsity.keras.PruningSummaries(log_dir=logdir)]\n</code></pre>  <p>Une fois que la m\u00e9thode d'\u00e9lagage a \u00e9t\u00e9 correctement d\u00e9finie, le mod\u00e8le est alors envoy\u00e9 dans la fonction <code>tfmot.sparsity.keras.prune_low_magnitude(original_model, **pruning_params)</code> afin de cr\u00e9er les masques binaires qui serviront \u00e0 l'\u00e9lagage.</p> <p>Des couches ayant \u00e9t\u00e9 rajout\u00e9es (les masques binaires), il est n\u00e9c\u00e9ssaire de recompiler le mod\u00e8le</p> <pre><code># prune_low_magnitude requires a recompile.\nmodel_for_pruning.compile(optimizer='sgd',\n                          loss='categorical_crossentropy',\n                          metrics=['accuracy'])\n</code></pre> <p>On peut alors maintenant lancer un nouvel entra\u00eenement pour \u00e9laguer notre mod\u00e8le.</p>  <p>Entra\u00eenement</p> <pre><code>start = time.time()\nhistory = model_for_pruning.fit(ds,\n                                validation_data=ds_val,\n                                epochs=15,\n                                callbacks=callbacks)\nprint(\"Total training time: \",time.time()-start)\n</code></pre>  <p>Une fois l'entra\u00eenement termin\u00e9, on peut v\u00e9rifier que toutes les couches pouvant \u00eatre \u00e9lagu\u00e9es ont bien \\(50\\%\\) des leur poids qui sont nuls avec l'aide de la fonction suivante.</p>  <p>Sanity Check</p> <pre><code>    #Sanity check to see if the sparsity threshold is reached\nfrom tensorflow_model_optimization.python.core.sparsity.keras import pruning_wrapper\n\ndef get_sparsity(weights):\n    return 1.0 - np.count_nonzero(weights) / float(weights.size)\n\ndef test_sparsity(model, target_sparsity):\n    for layer in model.layers:\n        if isinstance(layer, pruning_wrapper.PruneLowMagnitude):\n            for weight in layer.layer.get_prunable_weights():\n                print(np.allclose(\n                    target_sparsity, get_sparsity(tf.keras.backend.get_value(weight)),\n                    rtol=1e-3, atol=1e-6))\n</code></pre> <pre><code>test_sparsity(model_for_pruning, target_sparsity=0.5)\n</code></pre>   <p>Attention</p> <p>On a rajout\u00e9 des couches (les masques binaires) lors du lancement de l'entra\u00eenement, il est n\u00e9c\u00e9ssaire des les retirer pour pouvoir profiter du mod\u00e8le par la suite.</p>   <p>Stripping</p> <pre><code># Once a model has been pruned to required sparsity, this method can be used to restore the original model with the sparse weights.\n\npruned_model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n</code></pre>  <p>Une fois que le mod\u00e8le a \u00e9t\u00e9 \u00e9lagu\u00e9 avec les crit\u00e8res voulus, il est n\u00e9c\u00e9ssaire de le recompiler pour pouvoir l'utiliser.</p>  <p>Derni\u00e8re compilation</p> <pre><code>pruned_model_for_export.compile(optimizer='adam',\n                      loss='categorical_crossentropy',\n                      metrics=['accuracy'])\n\npruned_model_for_export.evaluate(ds_test)\n</code></pre>  <pre><code>pruned_model_for_export.save('pruned_model.h5')\nmodel_pruned = load_model(\"pruned_model.h5\")\n</code></pre> <p>On peut alors voir que le mod\u00e8le a bien \\(50\\%\\) de ses poids qui sont nuls, sur les couches qui sont les plus impactantes : les noyaux de convolution et les couches denses.</p> <pre><code>for i, w in enumerate(model_pruned.get_weights()):\n    print(\n        f\"{model_pruned.weights[i].name} -- Total: {w.size}, Zeros: {np.sum(w == 0) / w.size * 100:.2f}%\"\n        )\n</code></pre> <p>Pour voir la taille finale du mod\u00e8le optimis\u00e9, on peut alors utiliser la fonction suivante.</p> <pre><code>def get_gzipped_model_size(file):\n    # Returns size of gzipped model, in bytes.\n    import os\n    import zipfile\n\n    _, zipped_file = tempfile.mkstemp('.zip')\n    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n        f.write(file)\n\n    return os.path.getsize(zipped_file)\n\nprint(f\"Size of gzipped pruned model trained from scratch: {get_gzipped_model_size('pruned_model.h5'):.2f} octets\")\n</code></pre>"},{"location":"deep_learning/module9/tp9_tflite/","title":"TP Module 9 : Optimisation des mod\u00e8les, TensorFlow Lite","text":""},{"location":"deep_learning/module9/tp9_tflite/#import-des-librairies-creation-du-dataset","title":"Import des librairies, cr\u00e9ation du dataset","text":"<p>Importons d'abord les librairies qui nous serons n\u00e9cessaires.</p>  <p>TensorFlow</p> <pre><code>from tensorflow import keras\nfrom tensorflow.keras.models import load_model\n\nfrom typing import List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom loguru import logger\nimport os\nimport random\nimport datetime\nimport time\nfrom PIL import Image\n\n# freeze de l'al\u00e9atoire, pour avoir des exp\u00e9riences reproductibles.\nRANDOM_SEED = 42\n\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\ntf.random.set_seed(RANDOM_SEED)\n</code></pre>  <p>Comme nous aurons besoin de cr\u00e9er de nouveaux des datasets pour l'entrainement, d\u00e9crivons une classe qui nous permettra de le faire.</p>  <p>Tensorize</p> <pre><code>class Tensorize(object):\n    \"\"\"Class used to create tensor datasets for TensorFlow.\n\n    Args:\n        object (object): The base class of the class hierarchy, used only to enforce\n            WPS306. See https://wemake-python-stylegui.de/en/latest/pages/usage/\n            violations/consistency.html#consistency.\n    \"\"\"\n\n    def __init__(\n        self, n_classes: int, img_shape: Tuple[int, int, int], random_seed: int\n    ) -&gt; None:\n        \"\"\"Initialization of the class Featurize.\n\n        Initialize the class the number of classes in the datasets, the shape of the\n        images and the random seed.\n\n        Args:\n            n_classes (int): Number of classes in the dataset.\n            img_shape (Tuple[int, int, int]): Dimension of the image, format is (H,W,C).\n            random_seed (int): Fixed random seed for reproducibility.\n        \"\"\"\n        self.n_classes = n_classes\n        self.img_shape = img_shape\n        self.random_seed = random_seed\n        self.AUTOTUNE = tf.data.experimental.AUTOTUNE\n\n    def load_images(self, data_frame: pd.DataFrame, column_name: str) -&gt; List[str]:\n        \"\"\"Load the images as a list.\n\n        Take the dataframe containing the observations and the labels and the return the\n        column containing the observations as a list.\n\n        Args:\n            data_frame (pd.DataFrame): Dataframe containing the dataset.\n            column_name (str): The name of the column containing the observations.\n\n        Returns:\n            The list of observations deduced from the dataframe.\n        \"\"\"\n        return data_frame[column_name].tolist()\n\n    def load_labels(self, data_frame: pd.DataFrame, column_name: str) -&gt; List[int]:\n        \"\"\"Load the labels as a list and encode them.\n\n        Take the dataframe containing the observations and the labels and the return the\n        column containing the labels as an encoded list.\n\n        The encoding is done by taking the set of labels, alphabetically sorted, and\n        then transforming them as integers starting from 0.\n\n        `from sklearn.preprocessing import LabelEncoder` works well to encode labels,\n        but if the dataset is huge, the time it takes to encode all the labels is\n        growing fast. We use anumpy and vectorization to speed up the time.\n\n        See the StackOverflow question :\n        [Question](https://stackoverflow.com/questions/45321999/\n        how-can-i-optimize-label-encoding-for-large-data-sets-sci-kit-learn)\n\n        Args:\n            data_frame (pd.DataFrame): Dataframe containing the dataset.\n            column_name (str): The name of the column containing the labels.\n\n        Returns:\n            The list of encoded labels deduced from the dataframe.\n        \"\"\"\n        label_list = data_frame[column_name].tolist()\n        classes = sorted(set(label_list))\n        logger.info(f\"Found following labels {classes}\")\n\n        labels = np.unique(label_list, return_inverse=True)[1]\n        dic = dict(zip(label_list, labels))\n        logger.info(f\"Dictionnary creation {dic}\")\n        vectorized_get = np.vectorize(dic.get)\n\n        return vectorized_get(label_list)\n\n    def parse_image_and_label(\n        self, filename: str, label: int\n    ) -&gt; Tuple[np.ndarray, int]:\n        \"\"\"Transform image and label.\n\n        Parse image to go from path to a resized np.ndarray, and parse the labels to\n        one-hot encode them.\n\n        Args:\n            filename (str): The path of the image to parse.\n            label (int): The label of the image, as an int, to one-hot encode.\n\n        Returns:\n            A np.ndarray corresponding to the image and the corresponding one-hot label.\n        \"\"\"\n        resized_dims = [self.img_shape[0], self.img_shape[1]]\n        # convert the label to one-hot encoding\n        label = tf.one_hot(label, self.n_classes)\n        # decode image\n        image = tf.io.read_file(filename)\n        # Don't use tf.image.decode_image,\n        # or the output shape will be undefined\n        image = tf.image.decode_jpeg(image)\n        # This will convert to float values in [0, 1]\n        image = tf.image.convert_image_dtype(image, tf.float32)\n        image = tf.image.resize(image, resized_dims)\n\n        return image, label\n\n    def train_preprocess(\n        self, image: np.ndarray, label: List[int]\n    ) -&gt; Tuple[np.ndarray, List[int]]:\n        \"\"\"Augmentation preprocess, if needed.\n\n        Args:\n            image (np.ndarray): The image to augment.\n            label (List[int]): The corresponding label.\n\n        Returns:\n            The augmented pair.\n        \"\"\"\n        image = tf.image.random_flip_left_right(image)\n        image = tf.image.random_flip_up_down(image)\n\n        return image, label\n\n    def create_dataset(\n        self,\n        data_path: str,\n        batch: int,\n        repet: int,\n        prefetch: int,\n        augment: bool,\n    ) -&gt; tf.data.Dataset:\n        \"\"\"Creation of a tensor dataset for TensorFlow.\n\n        Args:\n            data_path (str): Path where the csv file containing the dataframe is\n                located.\n            batch (int): Batch size, usually 32.\n            repet (int): How many times the dataset has to be repeated.\n            prefetch (int): How many batch the CPU has to prepare in advance for the\n                GPU.\n            augment (bool): Does the dataset has to be augmented or no.\n\n        Returns:\n            A batch of observations and labels.\n        \"\"\"\n        df = pd.read_csv(data_path)\n        features = self.load_images(data_frame=df, column_name=\"filename\")\n        labels = self.load_labels(data_frame=df, column_name=\"label\")\n\n        dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n        dataset = dataset.shuffle(len(features), seed=self.random_seed)\n        dataset = dataset.repeat(repet)\n        dataset = dataset.map(\n            self.parse_image_and_label, num_parallel_calls=self.AUTOTUNE\n        )\n        if augment:\n            dataset = dataset.map(\n                self.train_preprocess, num_parallel_calls=self.AUTOTUNE\n            )\n        dataset = dataset.batch(batch)\n        dataset = dataset.cache()\n        return dataset.prefetch(prefetch)\n</code></pre>  <p>et cr\u00e9ons nos 3 datasets classiques.</p>  <p>Attention</p> <p>Bien v\u00e9rifier sur o\u00f9 pointent les adresses dans les csv.</p>   <p>ds, ds_val, ds_train</p> <pre><code>ts = Tensorize(\n            n_classes=2,\n            img_shape=[224,224,3],\n            random_seed=42,\n        )\n\nds = ts.create_dataset(\n    \"prepared_dataset/train.csv\",\n    32,\n    1,\n    1,\n    True,\n)\n\nds_val = ts.create_dataset(\n    \"prepared_dataset/val.csv\",\n    32,\n    1,\n    1,\n    True,\n)\n\nds_test = ts.create_dataset(\n    \"prepared_dataset/test.csv\",\n    32,\n    1,\n    1,\n    True,\n)\n</code></pre>  <p>Chargeons maintenant notre mod\u00e8le que nous allons quantifier.</p>  <p>Chargement du mod\u00e8le</p> <pre><code>model_pruned = load_model(\"../pruned_model_polydecay.h5\")\nmodel_pruned.evaluate(ds_test)\n</code></pre>"},{"location":"deep_learning/module9/tp9_tflite/#tensorflow-lite-quantification","title":"TensorFlow Lite, quantification","text":"<p>TensorFlow Lite est un ensemble d'outils permettant de faire tourner des mod\u00e8les TensorFlow sur de \"l'embarqu\u00e9\", ie du smartphone au microcontr\u00f4leur.</p> <p>TensorFlow Lite poss\u00e8de deux composantes principales :</p> <ul> <li>TensorFlow Lite Converter, qui convertit les mod\u00e8les en un format sp\u00e9cifique, un <code>FlatBuffer</code>, optimis\u00e9 pour les d\u00e9ploiements dans les enrionnements contraints en terme de m\u00e9moire. Il applique aussi des techniques d'optimisations pour r\u00e9duire encore la taille du mod\u00e8le et acc\u00e9l\u00e9rer sa vitesse d'inf\u00e9rence.</li> <li>TensorFlow Lite Interpreter, qui permet de faire tourner les mod\u00e8les convertis.</li> </ul> <p>Une des optimisations l\u00e0 plus utilis\u00e9e est la quantification.</p> <p>En g\u00e9n\u00e9ral, nos mod\u00e8les fonctionnent en format de pr\u00e9cision float32. Tous les param\u00e8tres du mod\u00e8le sont stock\u00e9s dans ce format de pr\u00e9cision, ce qui conduit souvent \u00e0 des mod\u00e8les plus lourds. La lourdeur d'un mod\u00e8le est en corr\u00e9lation directe avec la vitesse \u00e0 laquelle le mod\u00e8le fait des pr\u00e9dictions. Ainsi, il pourrait vous venir naturellement \u00e0 l'esprit que si nous pouvions r\u00e9duire la pr\u00e9cision dans laquelle nos mod\u00e8les fonctionnent, nous pourrions r\u00e9duire les temps de pr\u00e9diction. C'est ce que fait la quantification - elle r\u00e9duit la pr\u00e9cision \u00e0 des formes plus basses comme float16, int8, etc. pour repr\u00e9senter les param\u00e8tres d'un mod\u00e8le.</p> <p>La quantification peut \u00eatre appliqu\u00e9e \u00e0 un mod\u00e8le sous deux formes</p> <ul> <li> <p>La quantification post-entra\u00eenement est appliqu\u00e9e \u00e0 un mod\u00e8le apr\u00e8s sa formation.</p> </li> <li> <p>Entra\u00eenement conscient de la quantification (Quantization Aware Training): un mod\u00e8le est g\u00e9n\u00e9ralement entra\u00een\u00e9 pour compenser la perte de pr\u00e9cision qui pourrait \u00eatre introduite en raison de la quantification. Lorsque vous r\u00e9duisez la pr\u00e9cision des param\u00e8tres de votre mod\u00e8le, il peut en r\u00e9sulter une perte d'informations et vous pourriez constater une certaine r\u00e9duction de la pr\u00e9cision de votre mod\u00e8le. Dans ces situations, une formation tenant compte de la quantification peut \u00eatre tr\u00e8s utile.</p> </li> </ul> <p>Pour installer uniquement l'interpr\u00e9teur, ce que l'on fait en pratique sur la carte d\u00e9di\u00e9e \u00e0 l'inf\u00e9rence, soyez s\u00fbr de choisir le bon interpr\u00e9teur en v\u00e9rifiant sur la page suivante.</p> <pre><code>!pip3 install https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp36-cp36m-linux_x86_64.whl\n\n# Load the model into interpreters\nimport tflite_runtime.interpreter as tflite\n</code></pre>"},{"location":"deep_learning/module9/tp9_tflite/#definition-de-fonction-utiles","title":"D\u00e9finition de fonction utiles","text":"<p>Les mod\u00e8les convertis et optimis\u00e9s <code>.tflite</code> ne lisent pas les datasets au format <code>tf.data.Dataset</code>, qui sont sp\u00e9calis\u00e9s pour l'entra\u00eenement. On va donc faire un dataset de validation de fa\u00e7on classique sous la forme d'un tableau.</p> <p>Les mod\u00e8les <code>.tflite</code> sont optimis\u00e9s pour des utilisation sur CPU unique, Colab ayant \u00e9t\u00e9 mis en mode GPU/CPU en serveur pour les besoins de l'entra\u00eenement du mod\u00e8le vous pourriez \u00eatre surpris de voir que le temps de validation d'un mod\u00e8le <code>.tflite</code> sera ici long, environ 1 image par seconde. Cela est d\u00fb \u00e0 l'optimisation pour CPU, sur un Rapsberry pi, l'inf\u00e9rence est de l'ordre de la milliseconde.</p>  <p>Cr\u00e9ation d'un dataset normal</p> <pre><code>def images(data_frame: pd.DataFrame, column_name: str) -&gt; List[str]:\n\"\"\"Load the images as a list.\n\nTake the dataframe containing the observations and the labels and the return the\ncolumn containing the observations as a list.\n\nArgs:\n    data_frame (pd.DataFrame): Dataframe containing the dataset.\n    column_name (str): The name of the column containing the observations.\n\nReturns:\n    The list of observations deduced from the dataframe.\n\"\"\"\nreturn data_frame[column_name].tolist()\n\ndef labels(data_frame: pd.DataFrame, column_name: str) -&gt; List[int]:\n    \"\"\"Load the labels as a list and encode them.\n\n    Take the dataframe containing the observations and the labels and the return the\n    column containing the labels as an encoded list.\n\n    The encoding is done by taking the set of labels, alphabetically sorted, and\n    then transforming them as integers starting from 0.\n\n    `from sklearn.preprocessing import LabelEncoder` works well to encode labels,\n    but if the dataset is huge, the time it takes to encode all the labels is\n    growing fast. We use anumpy and vectorization to speed up the time.\n\n    See the StackOverflow question :\n    [Question](https://stackoverflow.com/questions/45321999/\n    how-can-i-optimize-label-encoding-for-large-data-sets-sci-kit-learn)\n\n    Args:\n        data_frame (pd.DataFrame): Dataframe containing the dataset.\n        column_name (str): The name of the column containing the labels.\n\n    Returns:\n        The list of encoded labels deduced from the dataframe.\n    \"\"\"\n    label_list = data_frame[column_name].tolist()\n    classes = sorted(set(label_list))\n    logger.info(f\"Found following labels {classes}\")\n\n    labels = np.unique(label_list, return_inverse=True)[1]\n    dic = dict(zip(label_list, labels))\n    logger.info(f\"Dictionnary creation {dic}\")\n    vectorized_get = np.vectorize(dic.get)\n\n    return vectorized_get(label_list)\n\ndf = pd.read_csv(\"prepared_dataset/val.csv\")\nfeatures = images(data_frame=df, column_name=\"filename\")\nval_labels = labels(data_frame=df, column_name=\"label\")\n\n# Empty labels for storing images and labels\nval_images = []\n\nfor image in features:\n    # load the image\n    image = Image.open(image)\n    image = image.resize((224,224))\n    # convert image to numpy array\n    image = np.asarray(image).astype(np.float32)\n    image = np.expand_dims(image, 0)\n    image = image / 255.\n\n    # Append to the list\n    val_images.append(image)\n\n# Create NumPy array\nval_images = np.array(val_images)\n</code></pre>  <pre><code>print(f'val_images : {len(val_images)}, val_labels : {len(val_labels)}')\n</code></pre>  <p>Fonction d'\u00e9valuation</p> <pre><code># A helper function to evaluate the TF Lite model using \"test\" dataset.\n# Comes from: https://www.tensorflow.org/lite/performance/post_training_integer_quant\ndef evaluate_model(interpreter):\n\n    input_index = interpreter.get_input_details()[0][\"index\"]\n    output_index = interpreter.get_output_details()[0][\"index\"]\n\n    # Run predictions on every image in the \"test\" dataset.\n    predictions = []\n    start_time = time.time()\n    for val_image, val_label in zip(val_images, val_labels):\n        interpreter.set_tensor(input_index, val_image)\n\n        # Run inference.\n        interpreter.invoke()\n\n        # Post-processing: remove batch dimension and find the digit with highest\n        # probability.\n        probability = interpreter.get_tensor(output_index)\n        mask_prob = np.argmax(probability[0])\n        predictions.append(mask_prob)\n\n    print(f'{len(predictions)}, {len(val_labels)}')\n    print(f'took {time.time()-start_time} seconds, ie {len(val_images)/(time.time()-start_time)} img/s')\n    accuracy = (predictions == val_labels).mean()\n\n    return accuracy\n</code></pre>"},{"location":"deep_learning/module9/tp9_tflite/#quantification-post-entrainement-ptq","title":"Quantification post-entra\u00eenement (PTQ)","text":"<p>Vous commencez par charger votre mod\u00e8le dans une classe de convertisseur TFLiteConverter, puis vous sp\u00e9cifiez une politique d'optimisation, et enfin, vous demandez \u00e0 TFLite de convertir votre mod\u00e8le avec la politique d'optimisation.</p> <p>Cette forme de quantification est \u00e9galement appel\u00e9e quantification post-entra\u00eenement. Elle quantifie les poids de votre mod\u00e8le avec une pr\u00e9cision flottante de 8 bits.</p>  <p>Quantization</p> <pre><code>converter = tf.lite.TFLiteConverter.from_keras_model(model_pruned)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n#converter.representative_dataset = representative_dataset\nquantized_tflite_model = converter.convert()\n\nf = open(\"tflite_model/quantize_model_pruned.tflite\", \"wb\")\nf.write(quantized_tflite_model)\nf.close()\n</code></pre>   <p>Evaluation</p> <pre><code># Load the model into interpreters\ninterpreter_nor = tf.lite.Interpreter(model_path=\"tflite_model/quantize_model_pruned.tflite\")\ninterpreter_nor.allocate_tensors()\n\n# Evaluate the performance\naccuracy = evaluate_model(interpreter_nor)\nprint(accuracy)\n</code></pre>"},{"location":"deep_learning/module9/tp9_tflite/#quantization-aware-training-qat","title":"Quantization Aware training (QAT)","text":"<p>Une bonne premi\u00e8re approche consiste \u00e0 entra\u00eener votre mod\u00e8le de mani\u00e8re \u00e0 ce qu'il apprenne \u00e0 compenser la perte d'informations qui pourrait \u00eatre induite par la quantification. C'est pr\u00e9cis\u00e9ment ce que nous pouvons faire avec un entra\u00eenement conscient de la quantification. Pour former notre r\u00e9seau \u00e0 la quantification, il suffit d'ajouter les lignes de code suivantes</p>  <p>Attention</p> <p>Les techniques d'entra\u00eenements conscients de la quantification sont encore relativement r\u00e9centes, et si cela ne pose pas trop de probl\u00e8mes pour la plupart des mod\u00e8les, il est important de v\u00e9rifier que les couches que l'on met le supporte</p> <p>Par exemple : BatchNormalization when it follows Conv2D and DepthwiseConv2D layers.</p> <p>Ce qui veut dire le mod\u00e8le ResNet50 propos\u00e9 par <code>tf.keras</code> supporte cet entra\u00eenement, car la brique de base est \\(\\text{Conv-BN-ReLU}\\), mais ResNet50V2, qui lui utilise l'architecture \\(\\text{BN-ReLU-Conv}\\) ne le supporte pas de fa\u00e7on native pour l'instant.</p>  <p>Pour pouvoir utiliser les m\u00e9thodes de quantification durant l'entra\u00eenement, on va installer la librairie TensorFlow Model Optimization.</p>  <p>TFMOT</p> <pre><code>!pip install -q tensorflow-model-optimization\nimport tensorflow_model_optimization as tfmot\n</code></pre>   <p>QAT</p> <pre><code># Let's reload the model and allow the model to be trained in a quantization-aware manner\nqat_model_pruned = tfmot.quantization.keras.quantize_model(model_pruned)\nqat_model_pruned.summary()\n\n# Train the model\nqat_model_pruned.compile(loss=\"categorical_crossentropy\",\n                    optimizer=tf.keras.optimizers.Adam(),\n                    metrics=[\"accuracy\"])\nstart = time.time()\nhistory = qat_model_pruned.fit(ds_train,\n                            validation_data=ds_val,\n                            epochs=10)\nprint(\"Total training time: \",time.time()-start)\n</code></pre>   <p>Sauvergarde du mod\u00e8le</p> <pre><code># Serializing the model and seeing the size of it\nqat_model_pruned.save(\"qat_model_pruned.h5\")\n!ls -lh .\n</code></pre>   <p>Attention</p> <p>La QAT ajoute des couches au mod\u00e8le, donc si l'on essaye de le charger na\u00efvement, on va avoir une erreur. Il faut utilisation la m\u00e9thode contextuelle avec <code>with</code> suivante.</p> <pre><code>with tfmot.quantization.keras.quantize_scope():\n    qat_model_pruned = tf.keras.models.load_model(\"qat_model_pruned.h5\")\n</code></pre>"},{"location":"deep_learning/module9/tp9_tflite/#quantification","title":"Quantification","text":"<p>Une fois que notre mod\u00e8le a \u00e9t\u00e9 entra\u00een\u00e9e, on peut maintenant le quantifier.</p>  <p>Quantification</p> <pre><code># Quantize `q_flower_model` (this one was trained with QAT)\nconverter = tf.lite.TFLiteConverter.from_keras_model(qat_model_pruned)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\nquantized_tflite_model = converter.convert()\nf = open(\"tflite_model/qat_model_pruned.tflite\", \"wb\")\nf.write(quantized_tflite_model)\nf.close()\n\n!ls -lh tflite_model/qat_model_pruned.tflite\n</code></pre>  <p>On peut maintenant \u00e9valuer notre mod\u00e8le pour voir sa pr\u00e9cision.</p>  <p>Evaluation</p> <pre><code>    # Load the model into interpreters\ninterpreter_qat = tf.lite.Interpreter(model_path=\"tflite_model/qat_model_pruned.tflite\")\ninterpreter_qat.allocate_tensors()\n\n# Check\naccuracy = evaluate_model(interpreter_qat)\nprint(accuracy)\n</code></pre>  <p>On peut aussi tr\u00e8s bien essayer la quantification avec les options <code>tf.lite.Optimize.OPTIMIZE_FOR_LATENCY</code> ou <code>tf.lite.Optimize.OPTIMIZE_FOR_SIZE</code>.</p> <p>Lorsque le device sur lequel on va d\u00e9ployer notre mod\u00e8le poss\u00e8de un gpu, on peut aussi utiliser une quantification en float16.</p>  <p>Float16 Quantization</p> <pre><code># Quantize with fp16 policy (float)\nconverter = tf.lite.TFLiteConverter.from_keras_model(qat_model_pruned)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_types = [tf.float16]\n\nquantized_tflite_model = converter.convert()\nf = open(\"tflite_model/qat_model_pruned_fp16.tflite\", \"wb\")\nf.write(quantized_tflite_model)\nf.close()\n\n!ls -lh tflite_model/qat_model_pruned_fp16.tflite\n</code></pre>"},{"location":"devops/applications/","title":"Just enough Applications","text":""},{"location":"devops/applications/#java","title":"Java","text":"<p>ssh app01 to login to app01 server and run below command to download</p> <p>sudo curl https://download.java.net/java/GA/jdk13.0.2/d4173c853231432d94f001e99d882ca7/8/GPL/openjdk-13.0.2_linux-x64_bin.tar.gz --output /opt/openjdk-13.0.2_linux-x64_bin.tar.gz</p> <p>To uncompress run sudo tar -xf /opt/openjdk-13.0.2_linux-x64_bin.tar.gz -C /opt/</p> <p>To verify run /opt/jdk-13.0.2/bin/java -version on app01 and confirm correct version is installed</p> <p>We need to set java binary path in environment PATH variable to use java binaries. So that you can simply run java instead of the full path.</p> <p>Once done verify that you can invoke java simply by running java command.</p> <p>Set path variable with the command export PATH=$PATH:/opt/jdk-13.0.2/bin</p>"},{"location":"devops/applications/#java-build-packaging","title":"Java Build &amp; Packaging","text":"<ol> <li>Develop source code in <code>MyFile.java</code> file.</li> <li>Compile with <code>javac MyFile.java</code> command.</li> <li>Run with <code>java MyFile</code></li> </ol> <p><code>jar cf MyApp.jar MyClass.class Service1.class Service2.class ...</code></p> <p><code>java -jar MyApp.jar</code></p> <p><code>javadoc -d doc MyClass.jar</code></p> <p>Build tools : Maven, Gradle, ANT</p>"},{"location":"devops/applications/#nodejs","title":"NodeJS","text":"<p><code>node -v</code></p> <p><code>node add.js</code> pour lancer le script.</p> <p>package manager : <code>npm</code> : Node Package Manager, npmjs.com.</p> <p><code>npm -v</code></p> <p><code>npm search file</code></p> <p><code>npm install file</code> local install, <code>npm install file -g</code> for global install.</p> <p><code>package.json</code> metadatas/dependencies of the package</p> <p><code>node -s \"console.log(module.paths)\"</code></p> <p><code>ls /usr/lib/node_modules/npm/node_modules</code> builtin modules</p> <p><code>ls /usr/lib/node_modules</code></p>"},{"location":"devops/az_devops/","title":"Some notions on Azure DevOps","text":""},{"location":"devops/az_devops/#structure-des-pipelines-azure-devops","title":"Structure des pipelines Azure DevOps","text":"<p>Les pipelines Azure DevOps jouent le m\u00eame que les github actions et que les fichiers gitlab-ci. Ils permettent d'automatiser les t\u00e2ches les plus r\u00e9currentes et de mettre en place la CI-CD.</p> <p>Un pipeline :</p> <ul> <li>est compos\u00e9 d'un ou plusieurs <code>stages</code>,</li> <li>un <code>stage</code> est une mani\u00e8re d'organiser des <code>jobs</code> de fa\u00e7on coh\u00e9rente,</li> <li>un <code>job</code> tourne sur un agent (runner),</li> <li>un <code>job</code> est compos\u00e9 d'une ou plusieurs <code>steps</code>,</li> <li>Une <code>step</code> est alors soit une <code>task</code>, soit <code>script</code>,</li> <li><code>task</code> et <code>script</code> sont les blocs de bases d'un pipeline azure.</li> </ul>"},{"location":"devops/az_devops/#scripts-tasks","title":"Scripts, Tasks","text":"<p>Un <code>script</code> est simplement un suite de commande \u00e9xecut\u00e9e les unes \u00e0 la suite des autres. Suivant le runner (vm) utilis\u00e9, c'est commandes peuvent soit \u00eatre des commandes shell, avec un runner ubuntu, soit des commandes powershelle (avec un runner windows).</p>  <p>Exemple</p> <pre><code>- script: |\n      python -m pip install --upgrade pip\n      python -m pip install -r requirements-tests.txt\n  displayName: Install testing dependencies\n</code></pre>  <p>Un script poss\u00e8de un <code>displayName</code> qui permet de l'identifier dans la suite des instructions donn\u00e9es.</p> <p>Si un script devient assez gros et complexe, il est alors possible de l'encapsuler dans une <code>task</code>. une <code>task</code> est un <code>script</code> ou une proc\u00e9dure packag\u00e9e qui a \u00e9t\u00e9 abstraite et poss\u00e8de un ensemble d'<code>inputs</code>. Chaque <code>task</code> poss\u00e8de un ensemble d'<code>inputs</code> sp\u00e9cifiques, que l'on peut consulter dans la documentation suivante.</p>  <p>Exemple : UsePythonVersion@0</p> <pre><code>- task: UsePythonVersion@0\ninputs:\n  versionSpec: \"$(python.version)\"\ndisplayName: \"Use Python $(python.version)\"\n</code></pre>"},{"location":"devops/az_devops/#steps","title":"Steps","text":"<p>Une fois que l'on souhaite d\u00e9finir une liste de <code>script</code> ou <code>task</code> que l'on veut \u00e9xecuter les unes \u00e0 la suite des autres, on utilise alors le mot cl\u00e9 <code>steps</code>.</p> <p>Une <code>step</code> est le plus petit bloc de contruction d'un pipeline azure, il peut contenir une ou plusieurs <code>script</code>/<code>task</code></p>  <p>Exemple</p> <pre><code>steps:\n  - task: UsePythonVersion@0\n    inputs:\n      versionSpec: \"$(python.version)\"\n    displayName: \"Use Python $(python.version)\"\n\n  - script: |\n      python -m pip install --upgrade pip\n      python -m pip install -r requirements-tests.txt\n    displayName: Install testing dependencies\n</code></pre>"},{"location":"devops/az_devops/#jobs","title":"Jobs","text":"<p><code>Jobs</code> est une liste de <code>job</code>.</p> <p>Un <code>job</code> est une liste de <code>steps</code> lanc\u00e9es les unes \u00e0 la suites des autres par un agent commun. A chque fois que l'on lance un pipeline, l'agent provisionne une nouvelle machine virutelle pour chaque <code>job</code>, cette machine virtuelle est supprim\u00e9e une fois que la liste des <code>steps</code> pr\u00e9sentes dans le <code>job</code> a \u00e9t\u00e9 enti\u00e8rement \u00e9xecut\u00e9e.</p> <p>Ce qui veut dire que pour chaque <code>job</code> dans une liste de <code>jobs</code>, on a une machine virtuelle diff\u00e9rente.</p> <p>La plupart du temps il est n\u00e9cessaire de d\u00e9finir quel agent on souhaite utiliser via le param\u00e8tre <code>pool.vmImage</code>.</p>  <p>Exemple avec ubuntu-latest</p> <pre><code>jobs:\n  - job: unit_tests\n    displayName: Setup and launch unit tests\n    pool:\n      vmImage: $(imageName)\n    strategy:\n      matrix:\n        Python38:\n          python.version: \"3.8\"\n        Python39:\n          python.version: \"3.9\"\n        Python310:\n          python.version: \"3.10\"\n\n    steps:\n      - task: UsePythonVersion@0\n        inputs:\n          versionSpec: \"$(python.version)\"\n        displayName: \"Use Python $(python.version)\"\n\n      - script: |\n          python -m pip install --upgrade pip\n          python -m pip install -r requirements-tests.txt\n        displayName: Install testing dependencies\n</code></pre>  <p>Pour configurer les <code>jobs</code>, les <code>steps</code>, et <code>stages</code>, il est possbile de d\u00e9finir des strat\u00e9gies via la commande <code>strategy.matrix</code>, pour pouvoir d\u00e9finir une matrice OS/python version.</p>"},{"location":"devops/az_devops/#stages","title":"Stages","text":""},{"location":"devops/az_devops/#template-azure-devops","title":"Template Azure DevOps","text":"<p>Source</p> <p>Azure pipelines permet de cr\u00e9er des templates pour les t\u00e2ches que l'on souhaite r\u00e9utiliser, et de les partager entre nos diff\u00e9rents pipelines.</p> <p>Les templates permettent de partager une logique (t\u00e2che) commune \u00e0 plusieurs pipelines de mani\u00e8re centralis\u00e9e.</p> <p>Il existe 4 types diff\u00e9rents de templates :</p> <ul> <li>Stage Template : permet de d\u00e9finir une suite de <code>stages</code> et de <code>jobs</code> correspondants.</li> <li>Job Template : pour d\u00e9finir une s\u00e9rie de <code>steps</code> \u00e9xecut\u00e9e par un agent.</li> <li>Step Template : pour d\u00e9finir une s\u00e9rie de <code>task</code>, <code>script</code> \u00e9xecut\u00e9e par un agent.</li> <li>Variable Template : pour d\u00e9finir une famille de variables (environnement, etc.)</li> </ul> <p>Pour cr\u00e9er un template, on ne passe pas par la case <code>Pipelines</code> de AZure DevOps, mais on les cr\u00e9e directement en tant que fichier yaml dans un repo.</p>"},{"location":"devops/az_devops/#step-template","title":"Step template","text":"steps-template.yaml<pre><code>---\nsteps:\n    - script: echo This the first step of the template !\n      displayName: Script from Template\n\n    - script: dir\n      displayName: Dir from Template\n\n    - task: UsePythonVersion@0\n      inputs:\n          versionSpec: '3.8'\n      displayName: Use Python\n</code></pre> <p>Le step template est le template de plus bas niveau que vous puissiez cr\u00e9er. Pour l'utiliser, on peut l'appeler avec la commande <code>template</code>.</p> <pre><code>jobs:\n  - job: unit_tests\n    displayName: Setup and launch unit tests\n    pool:\n      vmImage: \"ubuntu-20.04\"\n\n    steps:\n      - template: steps-template.yaml\n\n      - script: |\n          python -m pip install --upgrade pip\n          python -m pip install -r requirements-tests.txt\n        displayName: Install testing dependencies\n</code></pre> <p>Un template (step, job, stage) peut \u00eatre entour\u00e9 de commandes du m\u00eame genre avant ou apr\u00e8s lui, cela ne pose pas de probl\u00e8me, les instructions \u00e9tant lues de fa\u00e7on lin\u00e9aire.</p>"},{"location":"devops/az_devops/#job-template","title":"Job template","text":"jobs-template.yaml<pre><code>---\njobs:\n    - job: Linux\n      pool:\n          vmImage: ubuntu-20.04\n      steps:\n          - bash: echo \"Hello Ubuntu\"\n\n          - template: steps-template.yaml\n\n    - job: Windows\n      pool:\n          vmImage: windows-latest\n      steps:\n          - bash: echo \"Hello Windows\"\n</code></pre> <p>Un job template peut faire appel \u00e0 un ou plusieurs step template en son sein.</p>"},{"location":"devops/az_devops/#stage-template","title":"Stage template","text":"stages-template.yaml<pre><code>---\nstages:\n    - stage: StageTemplate\n      displyName: Stage from template\n      jobs:\n          - job: angularinstall\n            steps:\n                - script: npm install angular\n</code></pre> <p>Le stage template est le seul qui poss\u00e8de toutes les \u00e9tapes d'un pipeline, du <code>stage</code> au <code>script</code>.</p> <pre><code>trigger:\n  - master\n\npool:\n  vmImage: \"ubuntu-latest\"\n\nstages:\n  - stage: StagePipeline\n    displayName: Stage from Pipeline\n    jobs:\n      - job: npminstall\n        steps:\n          - task: Npm@1\n            inputs:\n              command: \"install\"\n  - template: stages-template.yaml\n</code></pre>"},{"location":"devops/az_devops/#passer-des-parametres-a-des-temmplates","title":"Passer des param\u00e8tres \u00e0 des temmplates","text":"<p>Pour l'instant ces templates ont des param\u00e8tres qui sont fixes, si l'on souhaite que ces templates soient plus modulaires, il faut les modifier l\u00e9g\u00e8rement.</p> <p>Voyons comment faire cela sur un job template.</p> modular-jobs-template.yaml<pre><code>---\nparameters:\n    - name: job_name\n      default: job\n    - name: vmImage\n      default: ubuntu-latest\n\njobs:\n    - job: ${{ parameters.job_name }}\n      pool:\n          vmImage: ${{ parameters.vmImage }}\n\n      steps:\n          - script: echo \"This is from the job ${{ parameters.job_name }} in VM ${{ parameters.vmImage }}\"\n</code></pre> <p>Tout d'abord il est n\u00e9cessaire de d\u00e9finir les variables que l'on souhaite pouvoir \u00eatre modifiable dans le pipeline. Cela se fait via l'ajout de la section <code>parameters</code>. Appeler une variable dans ce template se fait via l'usage des <code>${{...}}</code>.</p> <p>Une fois le template modifi\u00e9e, on peut alors l'appeler dans un autre pipeline de la fa\u00e7on suivante.</p> <pre><code>trigger: none\n\njobs:\n  - template: modular-jobs-template.yaml\n    parameters:\n      job_name: Linux\n      vmImage: \"ubuntu-latest\"\n\n  - template: modular-jobs-template.yaml\n    parameters:\n      job_name: Windows\n      vmImage: \"vs2017-win2016\"\n</code></pre>  <p>Question</p> <p>Comment passer \u00e0 un template un param\u00e8tre qui est une variable de sortie d'une autre t\u00e2che ?</p>  <p>Pour cela, il est n\u00e9cessaire de l'\u00e9lever au rang de <code>variables</code>.</p> <pre><code>parameters:\n  STORAGE_ACCOUNT_NAME: \"\"\n  name: \"test\"\n\njobs:\n  - job: ${{ parameters.name }}\n    pool:\n      vmImage: \"ubuntu-20.04\"\n    variables:\n      STORAGE_ACCOUNT_NAME: ${{ parameters.STORAGE_ACCOUNT_NAME }}\n\n    steps:\n      - task: AzureCLI@1\n        inputs:\n          azureSubscription: ${{ parameters.azureSubscription }}\n          scriptLocation: inlineScript\n          arguments: $(STORAGE_ACCOUNT_NAME)\n          inlineScript: |\n            account_name=$1\n            key=$(az storage account key list --account-name $account_name | jq '.[0].value')\n            # more script\n</code></pre>"},{"location":"devops/az_devops/#le-stockage-des-templates-et-la-notion-de-ressources","title":"Le stockage des templates et la notion de ressources","text":""},{"location":"devops/docker/","title":"Docker for the absolute beginner","text":""},{"location":"devops/docker/#survol","title":"Survol","text":"<p>Que sont les conteneurs Docker ? Un conteneur est un environnement informatique compl\u00e8tement isol\u00e9 :</p> <ul> <li>il a son propre r\u00e9seau,</li> <li>il a ses propres processus,</li> <li>il a ses propres volumes mont\u00e9s,</li> </ul> <p>tout comme des machines virtuelles. Vraiment ? Pas vraiment, la diff\u00e9rence majeure avec une machine virtuelle est que tous les conteneurs d'un m\u00eame syst\u00e8me partagent le m\u00eame kernel.</p> <p>Les OS Linux (Ubuntu, Fedora, CentOS, etc) sont tous bas\u00e9s sur le m\u00eame socle : ils poss\u00e8de un noyau (le kernel Linux) qui est responsables de l'interaction avec le hardware, et un ensemble de softwares.</p> <p>Le kernel Linux reste le m\u00eame peu importe la distribution, c'est la collection de softwares pr\u00e9sents qui rend Ubuntu diff\u00e9rent de Fedora, CentOS, etc.</p> <p>Donc lorsque l'on dit que l'ensemble des conteneurs partagent le m\u00eame kernel, on veut dire par l\u00e0 que Docker utilise directement le noyau de l'h\u00f4te sur lequel il tourne en faisant abstraction de la couche software. Les conteneurs peuvent donc faire tourner n'importe quel software ou OS tant qu'il est bas\u00e9 sur le m\u00eame noyau Linux que l'h\u00f4te.</p>  <p>Attention</p> <p>Quel OS n'est pas bas\u00e9 sur un noyau Linux ? Windows.</p> <p>Cela veut donc dire que Docker ne pourra pas faire tourner des conteneurs avec un OS/software Windows sur un h\u00f4te Linux, et inversement. Il faudra un serveur avec Windows Server pour faire tourner des conteneurs windows.</p>   <p>Attention</p> <p>Si vous connaissez Docker, vous allez surement surement dire que c'est faux. Vous pouvez tr\u00e8s bien installer Docker sur Windows, lancer un conteneur Ubuntu et travailler dedans sans soucis.</p> <p>La diff\u00e9rence est que le conteneur Ubuntu ne tourne pas directement sur windows, comme sur un OS Linux classique, windows lance d'abord une VM Linux et ce conteneur va alors tourner \u00e0 l'int\u00e9rieur de la VM Linux, ce qui rajoute une couche suppl\u00e9mentaire par rapport \u00e0 Docker sur un OS Linux classique.</p>"},{"location":"devops/docker/#les-environnements-conteneurises","title":"Les environnements \"conteneuris\u00e9s\"","text":"<p>Un conteneur Docker a la m\u00eame id\u00e9e qu'un conteneur physique : pensez-y comme \u00e0 une bo\u00eete contenant une application.</p> <p>\u00c0 l'int\u00e9rieur de la bo\u00eete, l'application semble avoir un ordinateur \u00e0 elle toute seule : elle a son propre nom de machine et sa propre adresse IP, et elle a aussi son propre disque (les conteneurs Windows ont aussi leur propre registre Windows).</p> <p>Ces \u00e9l\u00e9ments sont tous des ressources virtuelles :</p> <ul> <li>le nom d'h\u00f4te,</li> <li>l'adresse IP,</li> <li>le syst\u00e8me de fichiers sont cr\u00e9\u00e9s par Docker.</li> </ul> <p>Ce sont des objets logiques qui sont g\u00e9r\u00e9s et cr\u00e9\u00e9s par Docker, et ils sont tous r\u00e9unis pour cr\u00e9er un environnement dans lequel une application peut s'ex\u00e9cuter. C'est la \"bo\u00eete\" du conteneur.</p> <p>L'application \u00e0 l'int\u00e9rieur du conteneur ne peut rien voir \u00e0 l'ext\u00e9rieur du conteneur, mais le conteneur est ex\u00e9cut\u00e9e sur un ordinateur, et cet ordinateur peut \u00e9galement ex\u00e9cuter de nombreux autres conteneurs. Les applications dans ces conteneurs ont leurs propres environnements distincts (g\u00e9r\u00e9s par Docker), mais elles partagent toutes le CPU, GPU, et la m\u00e9moire de l'ordinateur, et elles partagent toutes le syst\u00e8me d'exploitation de l'ordinateur.</p>  <p>Remarque</p> <p>Notez ici que les conteneurs sont diff\u00e9rents des machines virtuelles.</p> <ul> <li> <p>Les machines virtuelles n\u00e9cessitent que l'hyperviseur virtualise une pile mat\u00e9rielle compl\u00e8te. Il y a \u00e9galement plusieurs syst\u00e8mes d'exploitation invit\u00e9s, ce qui les rend plus grands et plus \u00e9tendus \u00e0 d\u00e9marrer. C'est ce que sont les instances de cloud AWS/GCP/Azure.</p> </li> <li> <p>Les conteneurs, quant \u00e0 eux, ne n\u00e9cessitent aucune virtualisation de l'hyperviseur ou du mat\u00e9riel. Tous les conteneurs partagent le m\u00eame noyau h\u00f4te. Ils existent comme des environnements d'espace utilisateur isol\u00e9s et d\u00e9di\u00e9s, ce qui les rend beaucoup plus petits en taille et plus rapides \u00e0 d\u00e9marrer.</p> </li> </ul>   <p>Les conteneurs font tourner des instances \"d'images\" d\u00e9finies chacunes par un <code>Dockerfile</code>.</p>  <p>TLDR</p> <ol> <li>Un Dockerfile d\u00e9finit comment construire une image.</li> <li>Une image est un environnement packag\u00e9 contruit par un Dockerfile.</li> <li>Le conteneur Docker est l'endroit o\u00f9 l'image est lanc\u00e9e.</li> </ol>  <p>Docker poss\u00e8de une api qui lui est propre et qui permet de lancer, g\u00e9rer, stopper des conteneurs, avant de voir la r\u00e9daction d'un Dockerfile, voyons les commandes de base de cette api.</p>"},{"location":"devops/docker/#les-commandes-de-base","title":"Les commandes de base","text":"Commande R\u00e9sultat Exemple     <code>docker run image_name</code> lance un conteneur portant le nom <code>image_name</code> <code>docker run ubuntu</code>   <code>docker ps</code> liste l'ensemble des conteneurs lanc\u00e9s    <code>docker ps -a</code> liste l'ensemble des conteneurs pr\u00e9sents sur l'h\u00f4te, qu'ils soient lanc\u00e9s ou non.    <code>docker stop container_id</code> ou <code>docker stop container_name</code> stoppe le conteneur avec l'id ou le nom associ\u00e9. <code>docker stop 57ff613a495d</code> <code>docker stop blissful_leakey</code>   <code>docker rm container_name</code> supprime un conteneur stopp\u00e9 <code>docker rm blissful_leakey</code>   <code>docker images</code> ou <code>docker image ls</code> liste l'ensemble des images pr\u00e9sentes sur l'h\u00f4te    <code>docker rmi image_name</code> supprime localement l'image <code>docker rmi ubuntu</code>   <code>docker pull image_name</code> t\u00e9l\u00e9charge l'image depuis le registre associ\u00e9 sans la lancer <code>docker pull ubuntu</code>   <code>docker exec container_name command</code> ex\u00e9cute une commande \u00e0 l'int\u00e9rieur d'un conteneur lanc\u00e9 <code>docker exec pedantic_boyd cat /etc/hosts</code>   <code>docker run image_name -d</code> lance un conteneur portant le nom <code>image_name</code> en mode d\u00e9tach\u00e9 (ou d\u00e9mon), permet de ne pas bloquer le terminal <code>docker run ubuntu -d</code>   <code>docker attach container_id</code> ou <code>docker attach container_name</code> permet de se rattacher \u00e0 un conteneur lanc\u00e9 en mode d\u00e9tach\u00e9    <code>docker pull image_name</code> t\u00e9l\u00e9charge l'image depuis le registre associ\u00e9 sans la lancer <code>docker pull ubuntu</code>    <ul> <li> <p><code>docker run</code> : si l'image permettant de lancer le conteneur n'est pas disponible localement, Docker se chargera de la t\u00e9l\u00e9charger via le registre de conteneur (container registry) auquel l'image est assign\u00e9 (la plupart du temps ce registre est Docker hub, mais \u00e7a peut aussi \u00eatre nvcr (NVidia Container Registry), ghcr (GitHub Container Registry), etc.). Le t\u00e9l\u00e9chargement n'est fait qu'une seule fois, tant que l'image n'est pas supprim\u00e9e localement.</p> </li> <li> <p><code>docker ps</code> : chaque conteneur se voit attribuer une id et un nom uniques par Docker au moment o\u00f9 ils sont lanc\u00e9s.</p> </li> </ul> <p>La commande Linux <code>ps</code> affiche des informations sur une s\u00e9lection de processus actifs.</p> <p>Par d\u00e9faut, ps s\u00e9lectionne tous les processus ayant le m\u00eame ID utilisateur effectif (euid=EUID) que l'utilisateur actuel et associ\u00e9s au m\u00eame terminal que l'invocateur.</p> <p>Il affiche l'ID du processus (pid=PID), le terminal associ\u00e9 au processus (tname=TTY), le temps CPU cumul\u00e9 au format [DD-]hhss (time=TIME), et le nom de l'ex\u00e9cutable (ucmd=CMD).</p> <p>La commande <code>docker ps</code> fait alors la m\u00eame chose mais pour les conteneurs qui sont lanc\u00e9s, la commande affiche :</p> <ol> <li>l'id du conteneur,</li> <li>le nom de l'image associ\u00e9e (s'il y en a une),</li> <li>la derni\u00e8re commande lanc\u00e9e au d\u00e9marrage du conteneur,</li> <li>quand il a \u00e9t\u00e9 cr\u00e9\u00e9,</li> <li>depuis quand il est lanc\u00e9,</li> <li>les ports sur lesquels il est ouvert</li> <li>son nom, qui est d\u00e9finit de fa\u00e7on al\u00e9atoire par Docker si on ne le sp\u00e9cifie pas.</li> </ol> <pre><code>\u276f docker ps\n\nCONTAINER ID   IMAGE                                                           COMMAND                  CREATED       STATUS       PORTS                                         NAMES\n57ff613a495d   vsc-formation-deep-mlops-9214107b363d7791a4e04f86ccc94a41-uid   \"/bin/sh -c 'echo Co\u2026\"   2 hours ago   Up 2 hours   0.0.0.0:49153-&gt;8000/tcp, :::49153-&gt;8000/tcp   funny_volhard\n</code></pre> <p>Il est alors aussi possible de voir l'ensemble des conteneurs pr\u00e9sents sur l'h\u00f4te, qu'ils soient lanc\u00e9s ou non en rajoutant le param\u00e8tre <code>-a</code>.</p> <pre><code>\u276f docker ps -a\nCONTAINER ID   IMAGE                                                           COMMAND                  CREATED       STATUS                  PORTS                                         NAMES\n57ff613a495d   vsc-formation-deep-mlops-9214107b363d7791a4e04f86ccc94a41-uid   \"/bin/sh -c 'echo Co\u2026\"   2 hours ago   Up 2 hours              0.0.0.0:49153-&gt;8000/tcp, :::49153-&gt;8000/tcp   funny_volhard\n4c65fccd22d6   swaggerapi/swagger-ui                                           \"/docker-entrypoint.\u2026\"   7 days ago    Exited (0) 7 days ago                                                 beautiful_faraday\n7a3bbac61d2a   swaggerapi/swagger-ui                                           \"/docker-entrypoint.\u2026\"   7 days ago    Exited (0) 7 days ago                                                 elated_lamarr\n016e85e65d32   swaggerapi/swagger-ui                                           \"/docker-entrypoint.\u2026\"   7 days ago    Exited (0) 7 days ago                                                 distracted_perlman\na504962076cd   swaggerapi/swagger-ui                                           \"/docker-entrypoint.\u2026\"   7 days ago    Exited (0) 7 days ago                                                 blissful_leakey\n</code></pre> <ul> <li><code>docker rmi</code> : l'ensemble des conteneurs d\u00e9pendants de cette image doivent \u00eatre stopp\u00e9s avant de pouvoir supprimer l'image.</li> </ul>  <p>Attention</p> <p>Un conteneur ne \"vit\" que tant que le processus qui est cens\u00e9 \u00eatre lanc\u00e9 \u00e0 l'int\u00e9rieur tourne. Lancer <code>docker run ubuntu</code>, ne lancera pas un conteneur avec ubuntu, car on a d\u00e9finit aucun processus que ce conteneur devrait h\u00e9berger ! Comme on peut le voir dans les lignes suivantes, notre conteneur ubuntu s'est stopp\u00e9 tout de suite.</p> <pre><code>\u276f docker run ubuntu\n\nUnable to find image 'ubuntu:latest' locally\nlatest: Pulling from library/ubuntu\n125a6e411906: Pull complete\nDigest: sha256:26c68657ccce2cb0a31b330cb0be2b5e108d467f641c62e13ab40cbec258c68d\nStatus: Downloaded newer image for ubuntu:latest\n\n\u276f docker ps\n\nCONTAINER ID   IMAGE                                                           COMMAND                  CREATED       STATUS       PORTS                                         NAMES\n\n\n\u276f docker ps -a\n\nCONTAINER ID   IMAGE                                                           COMMAND                  CREATED         STATUS                     PORTS                                         NAMES\n9a74f900a9ef   ubuntu                                                          \"bash\"                   7 seconds ago   Exited (0) 6 seconds ago                                                 pedantic_boyd\n</code></pre> <p>Si le processus lanc\u00e9 \u00e0 l'int\u00e9rieur du conteneur crash ou s'arr\u00eate, le conteneur s'arr\u00eatera aussi.</p>  <ul> <li><code>docker exec</code></li> </ul> <pre><code>\u276f docker ps\n\nCONTAINER ID   IMAGE                                                           COMMAND                  CREATED          STATUS          PORTS                                         NAMES\n5c1204184739   ubuntu                                                          \"sleep 50\"               11 seconds ago   Up 10 seconds                                                 suspicious_almeida\n\n\u276f docker exec suspicious_almeida cat /etc/hosts\n\n127.0.0.1   localhost\n::1 localhost ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nff00::0 ip6-mcastprefix\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\n172.17.0.3  5c1204184739\n</code></pre> <p>Un conteneur lanc\u00e9 comme <code>ubuntu</code> s'arr\u00eatera tout de suite car il n'y a aucun processus de lancer dedans, cependant on peut quand m\u00eame lancer ce conteneur et travailler dedans en le rendant interactif via la commande suivante.</p> it pour interactif<pre><code>\u276f docker run -it ubuntu\n\nroot@767920134d27:/#  cat /etc/os-release\n\nPRETTY_NAME=\"Ubuntu 22.04 LTS\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"22.04\"\nVERSION=\"22.04 LTS (Jammy Jellyfish)\"\nVERSION_CODENAME=jammy\nID=ubuntu\nID_LIKE=debian\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nUBUNTU_CODENAME=jammy\n\nroot@767920134d27:/# exit\nexit\n</code></pre> <ul> <li> <p><code>docker rmi $(docker images -aq)</code> : supprime toutes les images pr\u00e9sentes sur l'h\u00f4te.</p> </li> <li> <p><code>docker container rm $(docker ps -aq)</code> : supprime tous les conteneurs pr\u00e9sents sur l'h\u00f4te.</p> </li> </ul> <p>le param\u00e8tre <code>-aq</code> pr\u00e9sent ici est l'abbr\u00e9viation de <code>-a -q</code>, <code>-q</code> \u00e9tant le param\u00e8tre pour le mode \"quiet\", qui ne renvoie que l'id des conteneurs, par exemple dans la commande <code>docker ps -q</code>.</p>"},{"location":"devops/docker/#docker-run","title":"<code>docker run</code>","text":"<p>Voyons maintenant d'autres commandes de la famille <code>docker run</code>.</p>"},{"location":"devops/docker/#-it","title":"<code>-it</code>","text":"<p>Par d\u00e9faut, le conteneur docker n'\u00e9coute pas <code>stdin</code>, il n'a pas de terminal depuis lequel le lire. Il faut le lancer en mode interactif avec le param\u00e8tre <code>-i</code>. Pour faire apparaitre le prompteur du terminal il faut rajouter le param\u00e8tre <code>-t</code>, comme dans la commande <code>-it</code> vue un peu plus haut.</p>  <p>Remarque</p> <p>La plupart du temps on ne fait pas la diff\u00e9rence entre <code>-i</code> pour interactif et <code>-it</code> pour interactif avec prompteur du terminal, on dit juste <code>-it</code> pour interactif par abus de langage.</p>"},{"location":"devops/docker/#mapping-des-ports","title":"Mapping des ports","text":"<p>Supposons que j'ai pull <code>vorphus/helloworld-api:1.0</code> en local, qui est une API tr\u00e8s simple que j'ai cod\u00e9 et push sur dockerhub pour l'exemple, voir dockerhub et github pour le code.</p> <p><pre><code>\u276f docker pull vorphus/helloworld-api:1.0\n\n1.0: Pulling from vorphus/helloworld-api\nDigest: sha256:e8d49d5c9fc1924f1702d7b4bc3a28bb42c639fc9f87c6c2031a45859ca2d463\nStatus: Image is up to date for vorphus/helloworld-api:1.0\ndocker.io/vorphus/helloworld-api:1.0\n\n\u276f docker run vorphus/helloworld-api:1.0\n\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:80 (Press CTRL+C to quit)\n</code></pre> Si j'essaye de me connecter \u00e0 <code>http://0.0.0.0:80</code> pour essayer d'aller voir par exemple la documentation de l'api, la seule chose sur laquelle je vais tomber est la chose suivante.</p>  <p>Pourquoi ? Comme dis plus haut, un conteneur docker est par d\u00e9finition totalement isol\u00e9 du reste de l'environnement dans lequel il tourne. Si on veut qu'un conteneur communique avec l'ext\u00e9rieur, il faut le dire explicitement.</p> <pre><code>graph LR\n\n    subgraph \"Docker Host\"\n    A[helloworld-api &lt;br/&gt; 172.17.0.3 &lt;br/&gt; Conteneur]\n    end\n\n    B[R\u00e9seau ext\u00e9rieur]\n\n    A-.-|X|B</code></pre> <p>Une combinaison de <code>docker inspect</code> et de <code>jq</code> permet de trouver l'adresse IP du conteneur.</p> <pre><code>\u276f docker inspect helloworld | jq \".[0].NetworkSettings.Networks.bridge.IPAddress\"\n\n\"172.17.0.3:80\"\n</code></pre> <p>Sauf que cette adresse IP est une adresse IP interne au docker host, et que donc on ne peut pas l'utiliser de l'ext\u00e9rieur. De m\u00eame pour le port 80 d\u00e9fini plus haut lorsque l'on a lanc\u00e9 le conteneur : on ne peut pas acc\u00e9der \u00e0 <code>172.17.0.3:80</code> depuis l'ext\u00e9rieur du docker host.</p> <p>La seule fa\u00e7on (pour l'instant) pour communiquer avec l'api serait alors d'utiliser la commande <code>docker exec</code> pour lancer une requ\u00eate via <code>curl</code> depuis l'int\u00e9rieur du conteneur.</p> <p><pre><code>\u276f docker exec helloworld-api curl 172.17.0.3:80/hello/\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100    17  100    17    0     0  17000      0 --:--:-- --:--:-- --:--:-- 17000\n{\"Hello\":\"World\"}%\n</code></pre> Pas \u00e9vident hein ? La solution est alors de d\u00e9finir un point de passage en joignant un port du r\u00e9seau ext\u00e9rieur au port du docker host concern\u00e9, cela se fait via la commande suivante.</p> <p><code>-p oustide_port:inside_docker_host_port</code></p> <p>En lan\u00e7ant la commande suivante.</p> <p><pre><code>docker run -d --name helloworld -p 80:80 vorphus/helloworld-api:1.0\n</code></pre> Il est alors possible d'acc\u00e9der au conteneur docker depuis le r\u00e9seau externe au docker host. La passerelle faisant le routage n\u00e9cessaire.</p> <pre><code>graph LR\n\n    subgraph \"Docker Host\"\n    A[helloworld-api &lt;br/&gt; 172.17.0.3:80 &lt;br/&gt; Conteneur]\n    end\n\n    B[R\u00e9seau ext\u00e9rieur &lt;br/&gt; 172.17.0.3:80]\n\n    A-.-B</code></pre> <p>On peut alors acc\u00e9der \u00e0 l'api depuis l'ext\u00e9rieur du conteneur.</p> <p><pre><code>\u276f http 172.17.0.3:80/hello/\n\nHTTP/1.1 200 OK\ncontent-length: 17\ncontent-type: application/json\ndate: Fri, 13 May 2022 18:32:55 GMT\nserver: uvicorn\n\n{\n    \"Hello\": \"World\"\n}\n\n\n\u276f curl 172.17.0.3:80/hello/\n\n{\"Hello\":\"World\"}%\n</code></pre> De la m\u00eame fa\u00e7on, en se rendant \u00e0 l'adresse <code>0.0.0.0:80</code> depuis notre navigateur internet, on arrive sur la doc de l'api.</p>"},{"location":"devops/docker/#volumes","title":"Volumes","text":"<p>Pour faire persister de la donn\u00e9e, ou pour en r\u00e9cup\u00e9rer depuis un conteneur, on peut utiliser des volumes, il en existe de deux types : * les r\u00e9pertoires mont\u00e9s, * les volumes nomm\u00e9s.</p> <p>Les r\u00e9pertoires mont\u00e9s ont une fonctionnalit\u00e9 limit\u00e9e par rapport aux volumes. Lorsque vous utilisez un r\u00e9pertoire mont\u00e9, un fichier ou un r\u00e9pertoire sur la machine h\u00f4te est mont\u00e9 dans un conteneur. Le fichier ou le r\u00e9pertoire est r\u00e9f\u00e9renc\u00e9 par son chemin absolu sur la machine h\u00f4te. En revanche, lorsque vous utilisez un volume nomm\u00e9, un nouveau r\u00e9pertoire est cr\u00e9\u00e9 dans le r\u00e9pertoire de stockage de Docker sur la machine h\u00f4te, et Docker g\u00e8re le contenu de ce r\u00e9pertoire.</p> <p>Il n'est pas n\u00e9cessaire que le fichier ou le r\u00e9pertoire existe d\u00e9j\u00e0 sur l'h\u00f4te Docker. Il est cr\u00e9\u00e9 \u00e0 la demande s'il n'existe pas encore. Les r\u00e9pertoires mont\u00e9s sont tr\u00e8s performants, mais ils d\u00e9pendent du fait que le syst\u00e8me de fichiers de la machine h\u00f4te dispose d'une structure de r\u00e9pertoire sp\u00e9cifique. Si vous d\u00e9veloppez de nouvelles applications Docker, envisagez plut\u00f4t d'utiliser des volumes nomm\u00e9s. Vous ne pouvez pas utiliser les commandes Docker CLI pour g\u00e9rer directement les r\u00e9pertoires mont\u00e9s.</p> <p>Les volumes nomm\u00e9s sont le m\u00e9canisme privil\u00e9gi\u00e9 pour la persistance des donn\u00e9es g\u00e9n\u00e9r\u00e9es et utilis\u00e9es par les conteneurs Docker. Alors que les r\u00e9pertoires mont\u00e9s d\u00e9pendent de la structure des r\u00e9pertoires et du syst\u00e8me d'exploitation de la machine h\u00f4te, les volumes sont enti\u00e8rement g\u00e9r\u00e9s par Docker. Les volumes pr\u00e9sentent plusieurs avantages par rapport aux r\u00e9pertoires mont\u00e9s :</p> <ul> <li>Les volumes nomm\u00e9s sont plus faciles \u00e0 sauvegarder ou \u00e0 migrer que les r\u00e9pertoires mont\u00e9s.</li> <li>Vous pouvez g\u00e9rer les volumes nomm\u00e9s \u00e0 l'aide des commandes Docker CLI ou de l'API Docker.</li> <li>Les volumes nomm\u00e9s fonctionnent sur les conteneurs Linux et Windows.</li> <li>Les volumes nomm\u00e9s peuvent \u00eatre partag\u00e9s de mani\u00e8re plus s\u00fbre entre plusieurs conteneurs.</li> <li>Les pilotes de volume vous permettent de stocker des volumes nomm\u00e9s sur des h\u00f4tes ou des fournisseurs de clouds distants, de chiffrer le contenu des volumes ou d'ajouter d'autres fonctionnalit\u00e9s.</li> <li>Les nouveaux volumes nomm\u00e9s peuvent avoir leur contenu pr\u00e9-rempli par un conteneur.</li> <li>Les volumes nomm\u00e9s sur Docker Desktop sont beaucoup plus performants que les r\u00e9pertoires mont\u00e9s des h\u00f4tes Mac et Windows.</li> </ul> <p>En outre, les volumes nomm\u00e9s sont souvent un meilleur choix que la persistance des donn\u00e9es dans la couche inscriptible d'un conteneur, car un volume nomm\u00e9s n'augmente pas la taille des conteneurs qui l'utilisent et le contenu du volume existe en dehors du cycle de vie d'un conteneur donn\u00e9.</p> <p>l'api docker permet : 1. de cr\u00e9er des volumes : <code>docker volume create my-volume</code>, 2. de lister ceux pr\u00e9sent : <code>docker volume ls</code>, 3. d'inspecter un volume : <code>docker volume inspect my-volume</code>, 4. de supprimer un volume : <code>docker volume rm my-volume</code>,</p> <p>Pour utiliser un volume avec un conteneur on ajoute alors la commande <code>-v my-volume:inside_container_path</code> \u00e0 la commande docker run <code>inside_container_path</code> est le chemin vers le r\u00e9pertoire dans le conteneur correspondant aux volume, s'il n'existe pas, il sera cr\u00e9\u00e9. Certaines images poss\u00e8dent des chemins sp\u00e9cifiques, par exemple le conteneur pour la db postgres d\u00e9finit <code>/var/lib/postgresql/data</code> comme chemin par d\u00e9faut vers lequel le volume qui contiendra les tables de la db doit pointer.</p> <p>Voyons un exemple.</p>  <p>Volume nomm\u00e9</p> <p>Cr\u00e9ons un volume.</p> Cr\u00e9ation du volume<pre><code>\u276f docker volume create my-volume\nmy-volume\n</code></pre> <p>Nous allons monter ce volume sur un conteneur ubuntu:20.04.</p> <p>Association du volume<pre><code>\u276f docker run --rm -it --name ubuntu -v my-volume:/opt/data ubuntu:20.04 bash\nUnable to find image 'ubuntu:20.04' locally\n20.04: Pulling from library/ubuntu\nd7bfe07ed847: Pull complete\nDigest: sha256:fd92c36d3cb9b1d027c4d2a72c6bf0125da82425fc2ca37c414d4f010180dc19\nStatus: Downloaded newer image for ubuntu:20.04\nroot@79fbf58313b5:/# cd /opt/data/\n</code></pre> Une fois dans <code>/opt/data/</code>, cr\u00e9ons un fichier <code>test.txt</code>.</p> Cr\u00e9ation du fichier<pre><code>root@79fbf58313b5:/opt/data# touch test.txt\n</code></pre> <p>Installons <code>vim</code>.</p> <p>Installation de Vim<pre><code>root@79fbf58313b5:/opt/data# apt update\n...\nroot@79fbf58313b5:/opt/data# apt install vim\nReading package lists... Done\nBuilding dependency tree\nReading state information... Done\nThe following additional packages will be installed:\n  alsa-topology-conf alsa-ucm-conf file libasound2 libasound2-data libcanberra0 libexpat1 libgpm2 libltdl7 libmagic-mgc libmagic1 libmpdec2 libogg0 libpython3.8 libpython3.8-minimal libpython3.8-stdlib libreadline8 libsqlite3-0\n  libssl1.1 libtdb1 libvorbis0a libvorbisfile3 mime-support readline-common sound-theme-freedesktop vim-common vim-runtime xxd xz-utils\nSuggested packages:\n  libasound2-plugins alsa-utils libcanberra-gtk0 libcanberra-pulse gpm readline-doc ctags vim-doc vim-scripts\nThe following NEW packages will be installed:\n  alsa-topology-conf alsa-ucm-conf file libasound2 libasound2-data libcanberra0 libexpat1 libgpm2 libltdl7 libmagic-mgc libmagic1 libmpdec2 libogg0 libpython3.8 libpython3.8-minimal libpython3.8-stdlib libreadline8 libsqlite3-0\n  libssl1.1 libtdb1 libvorbis0a libvorbisfile3 mime-support readline-common sound-theme-freedesktop vim vim-common vim-runtime xxd xz-utils\n0 upgraded, 30 newly installed, 0 to remove and 7 not upgraded.\nNeed to get 14.9 MB of archives.\nAfter this operation, 70.6 MB of additional disk space will be used.\nDo you want to continue? [Y/n]\n...\n</code></pre> Et \u00e9crivons dedans <code>Bonjour, comment \u00e7a va ?</code>.</p> Ecriture du prochain prix Nobel de litt\u00e9rature<pre><code>root@79fbf58313b5:/opt/data# vim test.txt\nroot@79fbf58313b5:/opt/data# exit\nexit\n</code></pre> <p>Quittons le conteneur, de par la commande <code>--rm</code> ajout\u00e9e dans le <code>docker run ...</code>, le conteneur est automatiquement supprim\u00e9 une fois qu'il est stopp\u00e9. Pour montrer la persistence des donn\u00e9es, remontons ce volume mais cette fois ci sur un conteneur ubuntu:18.04</p> <pre><code>\u276f docker run --rm -it --name ubuntu2 -v my-volume:/opt/data ubuntu:18.04 bash\nUnable to find image 'ubuntu:18.04' locally\n18.04: Pulling from library/ubuntu\n09db6f815738: Pull complete\nDigest: sha256:478caf1bec1afd54a58435ec681c8755883b7eb843a8630091890130b15a79af\nStatus: Downloaded newer image for ubuntu:18.04\n\nroot@830c7585cb75:/# cd /opt/data/\nroot@830c7585cb75:/opt/data# ls\ntest.txt\nroot@830c7585cb75:/opt/data# cat test.txt\nBonjour, comment a va ?\n</code></pre> <p>On retrouve bien notre texte alors que notre conteneur n'existe plus.</p>"},{"location":"devops/docker/#inspection","title":"Inspection","text":"<p>Il est possible d'inspecter un conteneur, inspecter veut ici dire avoir acc\u00e8s aux caract\u00e9ristiques de ce conteneur : son adresse IP, son id, ses variables d'environnements, son \u00e9tat.</p> <ul> <li><code>docker inspect container_id</code>,</li> <li><code>docker inspect container_name</code>.</li> </ul> <pre><code>\u276f docker inspect priceless_johnson | jq .\n[\n  {\n    \"Id\": \"8e9ec6183d98510cd7735b5ea9fd3ee818a1a6c2c83b20f2aad4fc7427f4f56d\",\n    \"Created\": \"2022-06-22T20:00:33.826646629Z\",\n    \"Path\": \"/bin/sh\",\n    \"Args\": [\n      \"-c\",\n      \"echo Container started\\ntrap \\\"exit 0\\\" 15\\n\\nexec \\\"$@\\\"\\nwhile sleep 1 &amp; wait $!; do :; done\",\n      \"-\"\n    ],\n    \"State\": {\n      \"Status\": \"running\",\n      \"Running\": true,\n      \"Paused\": false,\n      \"Restarting\": false,\n      \"OOMKilled\": false,\n      \"Dead\": false,\n      \"Pid\": 57036,\n      \"ExitCode\": 0,\n      \"Error\": \"\",\n      \"StartedAt\": \"2022-06-22T20:00:34.09197443Z\",\n      \"FinishedAt\": \"0001-01-01T00:00:00Z\"\n    },\n    \"Image\": \"sha256:f31c209a4471e0244fedefc92cd8257fbf5d67b6b8bb5c38a1e6a24e257ba12e\",\n    \"ResolvConfPath\": \"/var/lib/docker/containers/8e9ec6183d98510cd7735b5ea9fd3ee818a1a6c2c83b20f2aad4fc7427f4f56d/resolv.conf\",\n    \"HostnamePath\": \"/var/lib/docker/containers/8e9ec6183d98510cd7735b5ea9fd3ee818a1a6c2c83b20f2aad4fc7427f4f56d/hostname\",\n    \"HostsPath\": \"/var/lib/docker/containers/8e9ec6183d98510cd7735b5ea9fd3ee818a1a6c2c83b20f2aad4fc7427f4f56d/hosts\",\n    \"LogPath\": \"/var/lib/docker/containers/8e9ec6183d98510cd7735b5ea9fd3ee818a1a6c2c83b20f2aad4fc7427f4f56d/8e9ec6183d98510cd7735b5ea9fd3ee818a1a6c2c83b20f2aad4fc7427f4f56d-json.log\",\n    \"Name\": \"/priceless_johnson\",\n    \"RestartCount\": 0,\n    \"Driver\": \"overlay2\",\n    \"Platform\": \"linux\",\n    \"MountLabel\": \"\",\n    \"ProcessLabel\": \"\",\n    \"AppArmorProfile\": \"docker-default\",\n    \"ExecIDs\": [\n      \"f0a524885bb5176b7cc0562688a3eddd3cc1ef08839670faef2d15d7c2f6ecce\",\n      \"953d55745a72ef924ecc72a3854e30704a1c49406997546c7e4eb5cb5fba754f\",\n      \"917aa2253c9d114f100aee1757c6f9f14e05e63c4d1c78318021280a93d9a739\",\n      \"ffdac6da8b352c2e3761e23ecc455809e5b741a0374f2980367cd1559d360cfc\",\n      \"866fd97e494db76f65ac041f55d1ee99192fdcee2a0950483cd48bd95ba2f2a6\",\n      \"3ac3c930d2530f7a600684963ff98d0271b01f98eea63d85a3ff0359dd40e3a7\",\n      \"b328fdaf15d0faa92b5473264836fb4d206119a147611e65b0114946490a7ce4\"\n    ],\n    \"HostConfig\": {\n      \"Binds\": null,\n      \"ContainerIDFile\": \"\",\n      \"LogConfig\": {\n        \"Type\": \"json-file\",\n        \"Config\": {}\n      },\n      \"NetworkMode\": \"default\",\n      \"PortBindings\": {},\n      \"RestartPolicy\": {\n        \"Name\": \"no\",\n        \"MaximumRetryCount\": 0\n      },\n      \"AutoRemove\": true,\n      \"VolumeDriver\": \"\",\n      \"VolumesFrom\": null,\n      \"CapAdd\": null,\n      \"CapDrop\": null,\n      \"CgroupnsMode\": \"host\",\n      \"Dns\": [],\n      \"DnsOptions\": [],\n      \"DnsSearch\": [],\n      \"ExtraHosts\": null,\n      \"GroupAdd\": null,\n      \"IpcMode\": \"private\",\n      \"Cgroup\": \"\",\n      \"Links\": null,\n      \"OomScoreAdj\": 0,\n      \"PidMode\": \"\",\n      \"Privileged\": false,\n      \"PublishAllPorts\": true,\n      \"ReadonlyRootfs\": false,\n      \"SecurityOpt\": null,\n      \"UTSMode\": \"\",\n      \"UsernsMode\": \"\",\n      \"ShmSize\": 67108864,\n      \"Runtime\": \"runc\",\n      \"ConsoleSize\": [\n        0,\n        0\n      ],\n      \"Isolation\": \"\",\n      \"CpuShares\": 0,\n      \"Memory\": 0,\n      \"NanoCpus\": 0,\n      \"CgroupParent\": \"\",\n      \"BlkioWeight\": 0,\n      \"BlkioWeightDevice\": [],\n      \"BlkioDeviceReadBps\": null,\n      \"BlkioDeviceWriteBps\": null,\n      \"BlkioDeviceReadIOps\": null,\n      \"BlkioDeviceWriteIOps\": null,\n      \"CpuPeriod\": 0,\n      \"CpuQuota\": 0,\n      \"CpuRealtimePeriod\": 0,\n      \"CpuRealtimeRuntime\": 0,\n      \"CpusetCpus\": \"\",\n      \"CpusetMems\": \"\",\n      \"Devices\": [],\n      \"DeviceCgroupRules\": null,\n      \"DeviceRequests\": null,\n      \"KernelMemory\": 0,\n      \"KernelMemoryTCP\": 0,\n      \"MemoryReservation\": 0,\n      \"MemorySwap\": 0,\n      \"MemorySwappiness\": null,\n      \"OomKillDisable\": false,\n      \"PidsLimit\": null,\n      \"Ulimits\": null,\n      \"CpuCount\": 0,\n      \"CpuPercent\": 0,\n      \"IOMaximumIOps\": 0,\n      \"IOMaximumBandwidth\": 0,\n      \"Mounts\": [\n        {\n          \"Type\": \"bind\",\n          \"Source\": \"/media/vorph/datas/perso/formation-Deep-MLOps\",\n          \"Target\": \"/workspaces/formation-Deep-MLOps\"\n        },\n        {\n          \"Type\": \"bind\",\n          \"Source\": \"/media/vorph/datas/perso/formation-Deep-MLOps\",\n          \"Target\": \"/home/vorph/ArcticVault\"\n        },\n        {\n          \"Type\": \"volume\",\n          \"Source\": \"vscode\",\n          \"Target\": \"/vscode\"\n        }\n      ],\n      \"MaskedPaths\": [\n        \"/proc/asound\",\n        \"/proc/acpi\",\n        \"/proc/kcore\",\n        \"/proc/keys\",\n        \"/proc/latency_stats\",\n        \"/proc/timer_list\",\n        \"/proc/timer_stats\",\n        \"/proc/sched_debug\",\n        \"/proc/scsi\",\n        \"/sys/firmware\"\n      ],\n      \"ReadonlyPaths\": [\n        \"/proc/bus\",\n        \"/proc/fs\",\n        \"/proc/irq\",\n        \"/proc/sys\",\n        \"/proc/sysrq-trigger\"\n      ]\n    },\n    \"GraphDriver\": {\n      \"Data\": {\n        \"LowerDir\": \"/var/lib/docker/overlay2/42cf1cd0e9e587b734a7dff5625e645142c665f383b87493b89555e94aaa7af1-init/diff:/var/lib/docker/overlay2/hnzq7cm6a2z2ohte6hho1h78t/diff:/var/lib/docker/overlay2/9kh5w9j7eyh85l5ipfhafow9o/diff:/var/lib/docker/overlay2/qfigtsio5azazz9e9lxj169fb/diff:/var/lib/docker/overlay2/87ksokwg6u08fi92v1moc014w/diff:/var/lib/docker/overlay2/6oqivtmk6c9txf77ak4z8rc97/diff:/var/lib/docker/overlay2/rk1z51dk5y3x08bpfs3wmgvz5/diff:/var/lib/docker/overlay2/ad56072d4bd05786d0cb51ccbae18dabb5301a0aab8a69ff45dae1eda5416650/diff:/var/lib/docker/overlay2/1d1ac677fca430ebd79ea8026d2e6f9e9361102694ac7c5bd45e483c68e058d1/diff:/var/lib/docker/overlay2/977ef22a55ad57077a5589b66376f4a1ce31de4bea199c33fa3b23d93adfd7a7/diff:/var/lib/docker/overlay2/c4c194d65ca3d69f354910d8ef452332137b93290dd8fb5fe6be23ec56dae577/diff:/var/lib/docker/overlay2/9278cccac45602c698dbb875772c52194474e53dfb4c88e6f56405997c51e116/diff\",\n        \"MergedDir\": \"/var/lib/docker/overlay2/42cf1cd0e9e587b734a7dff5625e645142c665f383b87493b89555e94aaa7af1/merged\",\n        \"UpperDir\": \"/var/lib/docker/overlay2/42cf1cd0e9e587b734a7dff5625e645142c665f383b87493b89555e94aaa7af1/diff\",\n        \"WorkDir\": \"/var/lib/docker/overlay2/42cf1cd0e9e587b734a7dff5625e645142c665f383b87493b89555e94aaa7af1/work\"\n      },\n      \"Name\": \"overlay2\"\n    },\n    \"Mounts\": [\n      {\n        \"Type\": \"bind\",\n        \"Source\": \"/media/vorph/datas/perso/formation-Deep-MLOps\",\n        \"Destination\": \"/workspaces/formation-Deep-MLOps\",\n        \"Mode\": \"\",\n        \"RW\": true,\n        \"Propagation\": \"rprivate\"\n      },\n      {\n        \"Type\": \"bind\",\n        \"Source\": \"/media/vorph/datas/perso/formation-Deep-MLOps\",\n        \"Destination\": \"/home/vorph/ArcticVault\",\n        \"Mode\": \"\",\n        \"RW\": true,\n        \"Propagation\": \"rprivate\"\n      },\n      {\n        \"Type\": \"volume\",\n        \"Name\": \"vscode\",\n        \"Source\": \"/var/lib/docker/volumes/vscode/_data\",\n        \"Destination\": \"/vscode\",\n        \"Driver\": \"local\",\n        \"Mode\": \"z\",\n        \"RW\": true,\n        \"Propagation\": \"\"\n      }\n    ],\n    \"Config\": {\n      \"Hostname\": \"8e9ec6183d98\",\n      \"Domainname\": \"\",\n      \"User\": \"vorph\",\n      \"AttachStdin\": true,\n      \"AttachStdout\": true,\n      \"AttachStderr\": true,\n      \"ExposedPorts\": {\n        \"8000/tcp\": {}\n      },\n      \"Tty\": true,\n      \"OpenStdin\": true,\n      \"StdinOnce\": true,\n      \"Env\": [\n        \"PATH=/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/vorph/.local/bin:/usr/lib/python3.9/dist-packages\",\n        \"LANG=C.UTF-8\",\n        \"GPG_KEY=E3FF2839C048B25C084DEBE9B26995E310250568\",\n        \"PYTHON_VERSION=3.9.10\",\n        \"PYTHON_PIP_VERSION=21.2.4\",\n        \"PYTHON_SETUPTOOLS_VERSION=58.1.0\",\n        \"PYTHON_GET_PIP_URL=https://github.com/pypa/get-pip/raw/2caf84b14febcda8077e59e9b8a6ef9a680aa392/public/get-pip.py\",\n        \"PYTHON_GET_PIP_SHA256=7c5239cea323cadae36083079a5ee6b2b3d56f25762a0c060d2867b89e5e06c5\"\n      ],\n      \"Cmd\": [\n        \"-c\",\n        \"echo Container started\\ntrap \\\"exit 0\\\" 15\\n\\nexec \\\"$@\\\"\\nwhile sleep 1 &amp; wait $!; do :; done\",\n        \"-\"\n      ],\n      \"Image\": \"vsc-formation-deep-mlops-13dc5d37553ea9bead783e4d89aa65ee-uid\",\n      \"Volumes\": null,\n      \"WorkingDir\": \"/home/vorph\",\n      \"Entrypoint\": [\n        \"/bin/sh\"\n      ],\n      \"OnBuild\": null,\n      \"Labels\": {\n        \"devcontainer.local_folder\": \"/media/vorph/datas/perso/formation-Deep-MLOps\"\n      }\n    },\n    \"NetworkSettings\": {\n      \"Bridge\": \"\",\n      \"SandboxID\": \"44c6218be441ba34c2169970c77d6982a5143b466dec06cc909fb3c376cd987d\",\n      \"HairpinMode\": false,\n      \"LinkLocalIPv6Address\": \"\",\n      \"LinkLocalIPv6PrefixLen\": 0,\n      \"Ports\": {\n        \"8000/tcp\": [\n          {\n            \"HostIp\": \"0.0.0.0\",\n            \"HostPort\": \"49153\"\n          },\n          {\n            \"HostIp\": \"::\",\n            \"HostPort\": \"49153\"\n          }\n        ]\n      },\n      \"SandboxKey\": \"/var/run/docker/netns/44c6218be441\",\n      \"SecondaryIPAddresses\": null,\n      \"SecondaryIPv6Addresses\": null,\n      \"EndpointID\": \"80f81b68db4b033695a0b2e08799f61c4c80571f53a52fcb65b53f1a68c3bc75\",\n      \"Gateway\": \"172.17.0.1\",\n      \"GlobalIPv6Address\": \"\",\n      \"GlobalIPv6PrefixLen\": 0,\n      \"IPAddress\": \"172.17.0.2\",\n      \"IPPrefixLen\": 16,\n      \"IPv6Gateway\": \"\",\n      \"MacAddress\": \"02:42:ac:11:00:02\",\n      \"Networks\": {\n        \"bridge\": {\n          \"IPAMConfig\": null,\n          \"Links\": null,\n          \"Aliases\": null,\n          \"NetworkID\": \"d494ba95927d5944c6a7bab44aa41fc10842997700429378d0f8b982499a5dcf\",\n          \"EndpointID\": \"80f81b68db4b033695a0b2e08799f61c4c80571f53a52fcb65b53f1a68c3bc75\",\n          \"Gateway\": \"172.17.0.1\",\n          \"IPAddress\": \"172.17.0.2\",\n          \"IPPrefixLen\": 16,\n          \"IPv6Gateway\": \"\",\n          \"GlobalIPv6Address\": \"\",\n          \"GlobalIPv6PrefixLen\": 0,\n          \"MacAddress\": \"02:42:ac:11:00:02\",\n          \"DriverOpts\": null\n        }\n      }\n    }\n  }\n]\n</code></pre>"},{"location":"devops/docker/#logs","title":"logs","text":"<p>Pour voir les logs qu'un conteneur sort sur <code>stdout</code>, on peut utiliser l'un des deux commandes suivantes.</p> <ul> <li><code>docker logs container_id</code>,</li> <li><code>docker logs container_name</code>.</li> </ul> <p>Ces commandes ne fournirons les logs qu'\u00e0 l'instant t, si l'on souhaite voir les logs en continu, il faut rajouter l'argument <code>-f</code></p> <ul> <li><code>docker logs -f container_id</code>,</li> <li><code>docker logs -f container_name</code>.</li> </ul>"},{"location":"devops/docker/#les-images-docker","title":"Les images Docker","text":""},{"location":"devops/docker/#pourquoi-conteneuriser","title":"Pourquoi conteneuriser ?","text":"<p>Une \u00e9tape pour atteindre la reproductibilit\u00e9 consiste \u00e0 d\u00e9ployer le code et les artefacts versionn\u00e9s dans un environnement reproductible. Cela va bien au-del\u00e0 de l'environnement virtuel que l'on peut configurer pour les applications Python, car il existe des sp\u00e9cifications au niveau du syst\u00e8me (syst\u00e8me d'exploitation, paquets requis, etc.) qui ne sont pas prises en compte par un simple environnement virtuel. Nous voulons \u00eatre en mesure d'encapsuler toutes les exigences dont nous avons besoin afin qu'il n'y ait pas de d\u00e9pendances externes qui emp\u00eacheraient quelqu'un d'autre de reproduire l'application de fa\u00e7on exacte.</p>"},{"location":"devops/docker/#comment-creer-sa-propre-image","title":"Comment cr\u00e9er sa propre image ?","text":"<p>Dans docker, tout commence par la r\u00e9daction d'un <code>Dockerfile</code>, c'est un simple script que vous \u00e9crivez pour dire comment vous allez monter et faire fonctionner votre conteneur docker.</p> <p>Le langage docker est simple \u00e0 comprendre, les t\u00e2ches les plus communes ont leur propres commandes, et pour tout le reste vous pouvez utiliser les commandes shell standards (Bash sur Linux, ou PowerShell sur Windows par exemple).</p> <p>Pour voir comment s'\u00e9crit un <code>Dockerfile</code>, comment construire l'image et lancer le conteneur, prenons l'exemple suivant. C'est un <code>Dockerfile</code> standard que l'on peut utiliser pour entra\u00eener des mod\u00e8les de deep learning.</p>  <p>Dockerfile</p> <pre><code>FROM nvcr.io/nvidia/tensorflow:21.02-tf2-py3\n\nCOPY requirements.txt .\nCOPY requirements-dev.txt .\n\nARG USERNAME=vorph\nARG USER_UID=1000\nARG USER_GID=1000\n\nRUN groupadd -g $USER_GID -o $USERNAME\nRUN useradd -m -u $USER_UID -g $USER_GID -o -s /bin/bash $USERNAME\n\nUSER $USERNAME\n\nENV PATH \"$PATH:/home/vorph/.local/bin\"\n\nRUN /bin/bash -c \"pip install -r requirements.txt\"\n\nRUN /bin/bash -c \"pip install -r requirements-dev.txt\"\n\nEXPOSE 5000\nEXPOSE 8001\n</code></pre>  <p>Un <code>Dockerfile</code> est une suite d'instructions suivies de l'argument correspondant, chaque instruction \u00e9tant une couche (layer) du <code>Dockerfile</code>.</p> <p>La toute premi\u00e8re instruction est toujours la m\u00eame, elle d\u00e9termine quelle sera l'image de base de votre conteneur, est-ce que votre conteneur sera construit sur une base d'OS Ubuntu 18.02, 20.04, sur une base Python 3.8, etc. Ici l'image en question est <code>nvcr.io/nvidia/tensorflow:21.02-tf2-py3</code> une image de TensorFlow 2.4 faite par NVidia, ce qui permet de ne pas avoir \u00e0 se soucier des probl\u00e8mes d'installation ou de d\u00e9pendances (cuda, cudnn, etc).</p>  <p>La premi\u00e8re couche</p> <p>Cette premi\u00e8re couche commence toujours par un <code>FROM</code>, pour dire \u00e0 partir de quelle image de base vous allez construire votre Dockerfile.</p> <p>Il existe ce que l'on appelle des \"registres dockers\" (docker registry), o\u00f9 de mani\u00e8re similaire \u00e0 github, gitlab, etc sont recens\u00e9s les images docker de fa\u00e7on la plupart du temps open source, le plus connu \u00e9tant docker hub. Pour r\u00e9cup\u00e9rer une image, la commande similaire au \"git clone adresse\" est \"docker pull adresse\", et donc d'o\u00f9 vient votre image de base pour votre Dockerfile ?</p>  <p><code>COPY</code> est la commande permettant de copier des dossiers depuis votre machine locale vers votre conteneur Docker. Ici <code>COPY requirements.txt .</code> copie le fichier <code>requirements.txt</code> vers <code>.</code>, ie \u00e0 la racine d\u00e9finie dans l'image <code>nvcr.io/nvidia/tensorflow:21.02-tf2-py3</code>.</p>  <p>Syntaxe</p> <p>La syntaxe est <code>COPY dossier_source dossier_cible</code>.</p>  <p>Par d\u00e9faut, toutes les instructions lanc\u00e9es dans un conteneur docker se font en mode super-admin. Certaines application ayant besoin d'un r\u00e9pertoire <code>/home/</code>, il est souvent n\u00e9cessaire de cr\u00e9er un utilisateur, ce qui est fait dans les lignes suivantes.</p>  <p>Cr\u00e9ation d'un utilisateur</p> <pre><code>ARG USERNAME=vorph\nARG USER_UID=1000\nARG USER_GID=1000\n\nRUN groupadd -g $USER_GID -o $USERNAME\nRUN useradd -m -u $USER_UID -g $USER_GID -o -s /bin/bash $USERNAME\n\nUSER $USERNAME\n</code></pre>"},{"location":"devops/docker/#construction-de-limage","title":"Construction de l'image","text":"<p>Maintenant que votre Dockerfile est r\u00e9dig\u00e9, vous pouvez construire votre image. Le d\u00e9part est toujours le m\u00eame : <code>sudo docker build</code>, suivi d'argument.</p>  <p>Remarque</p> <p>les commandes docker dans le terminal ont besoin d'\u00eatre pass\u00e9 en super-admin, si vous souhaitez ne plus avoir \u00e0 taper <code>sudo docker build</code> mais simplement <code>docker build</code>, <code>docker run</code>, etc vous devez cr\u00e9er un groupe docker et vous ajouter en tant qu'utilisateur dedans. Pour cela, suivez les instructions de la doc officielle. Manage Docker as a non-root user</p>   <p>docker build</p> <pre><code>docker build \\\n--build-arg USER_UID=$(id -u) \\\n--build-arg USER_GID=$(id -g) \\\n--rm \\\n-f Dockerfile \\\n-t project_ai .\n</code></pre>  <p>Les deux arguments <code>--build-arg</code> correspondent aux m\u00eames arguments <code>ARG</code> dans le Dockerfile, <code>ARG USER_UID=1000</code> signifiant que la valeur par d\u00e9faut de USER_ID est 1000, <code>--build-arg</code> permet de r\u00e9\u00e9crire au dessus pour \u00eatre sur d'avoir les bonnes valeurs correspondant au couple <code>uid:gid</code> de votre machine locale.</p> <p><code>--rm</code> permet de supprimer les conteneurs interm\u00e9diaires utilis\u00e9s uniquement durant la construction.</p> <p><code>-f Dockerfile</code> sp\u00e9cifie quel Dockerfile doit \u00eatre utilis\u00e9 pour la construction, ici celui nomm\u00e9 simplement <code>Dockerfile</code>. Le nommage des Dockerfile se fait de la fa\u00e7on suivante : <code>Dockerfile.suffixe</code>, par exemple vous pourriez avoir deux Dockerfiles diff\u00e9rents</p> <ul> <li><code>Dockerfile.cpu</code>,</li> <li><code>Dockerfile.gpu</code>,</li> </ul> <p>o\u00f9 les instructions de construction \u00e0 l'int\u00e9rieur du Dockerfile seraient diff\u00e9rentes que vous utilisiez le gpu ou non. Dans ce cas vous pourriez avoir les commandes suivantes.</p>  <p>docker build</p> <p><pre><code>docker build \\\n--build-arg USER_UID=$(id -u) \\\n--build-arg USER_GID=$(id -g) \\\n--rm \\\n-f Dockerfile.cpu \\\n-t project_ai .\n</code></pre> ou</p> <pre><code>docker build \\\n--build-arg USER_UID=$(id -u) \\\n--build-arg USER_GID=$(id -g) \\\n--rm \\\n-f Dockerfile.gpu \\\n-t project_ai .\n</code></pre>  <p>Enfin, <code>-t project_ai</code> d\u00e9finit le nom que prendra l'image, ici \"project_ai\".</p> <p>Le point <code>.</code> \u00e0 la fin de la commande docker build signifie \u00e0 docker qu'il doit chercher le Dockerfile dans le r\u00e9pertoire actuel.</p>  <p>Remarque</p> <p>Beaucoup d'autres options sont disponibles, n'h\u00e9sitez pas \u00e0 regarder la documentation sur les options de construction.</p>"},{"location":"devops/docker/#une-architecture-en-couches","title":"Une architecture en couches","text":"<p>Comme on l'a vu pr\u00e9c\u00e9dement, un Dockerfile est une suite d'instructions, pour r\u00e9duire l'acc\u00e8s m\u00e9moire et am\u00e9liorer la vitesse de construction des conteneurs, chaque instruction est gard\u00e9e en m\u00e9moire cache. Une m\u00eame instruction peut alors \u00eatre utilis\u00e9e plusieurs fois par plusieurs conteneurs, de plus docker ne relance une instruction que s'il d\u00e9tecte une modification dans cette deni\u00e8re. Si docker d\u00e9tecte une modification dans une couche, il relancera alors aussi toutes les couches suivantes.</p> <p>Prenons pour exemple l'api suivante, tr\u00e8s simple.</p> <pre><code>from fastapi import FastAPI, status\nfrom fastapi.middleware.cors import CORSMiddleware\nimport uvicorn\nfrom typing import Optional\n\napp = FastAPI(\n    title=\"Mathieu's API\",\n    description=\"Simple API to be used as a docker tutorial\",\n    version=\"0.1.0\",\n)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n@app.get(\n    \"/hello/\",\n    tags=[\"hello\"],\n    status_code=status.HTTP_200_OK,\n    response_description=\"Hello !\",\n    summary=\"resume\",\n)\nasync def get_hello():\n    return {\"Hello\": \"World\"}\n</code></pre> <p>Pour la d\u00e9ployer facilement o\u00f9 l'on veux, on r\u00e9dige le Dockerfile suivant.</p> <pre><code>FROM python:3.9\n\nWORKDIR /code\n\nCOPY ./requirements.txt /code/requirements.txt\n\nRUN pip install --no-cache-dir --upgrade -r /code/requirements.txt\n\nCOPY ./app /code/app\n\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n</code></pre> <p>Si par exemple suite \u00e0 une pull request de dependabot, je mets \u00e0 jour le fichier <code>requirements.txt</code>, lorsque je relancerai la construction de l'image, les deux premi\u00e8res instructions ne seront pas ex\u00e9cut\u00e9es (tant que l'on garde l'image <code>python:3.9</code> en local), par contre l'instruction <code>COPY</code> ainsi que toutes celles en dessous seront automatiquement relanc\u00e9es pour prendre en compte les modifications.</p>"},{"location":"devops/docker/#les-variables-denvironnement","title":"Les variables d'environnement","text":"<p>La commande <code>ARG</code> permet de d\u00e9finir des variables d'environnement qui ne seront disponibles que durant la construction de l'image, ici les identifiants d'un utilisateur. On a ensuite besoin d'ajouter cet utilisateur et ce groupe dans les utilisateurs du conteneur, ce qui ce fait via la commande <code>RUN</code> qui permet de lancer des commandes shell.</p> <p>Enfin on sp\u00e9cifie qui sera l'utilisateur de ce conteneur, que sera l'utilisateur que l'on vient de cr\u00e9er. Cela se fait via la commande <code>USER</code>.</p> <p>A la diff\u00e9rence de <code>ARG</code>, <code>ENV</code> d\u00e9finit lui des variables d'environnements qui seront toujours disponibles apr\u00e8s la construction de l'image, et donc lorsque le conteneur sera lanc\u00e9. On peut comme pr\u00e9c\u00e9demment d\u00e9finir un chemin <code>PATH \"$PATH:/home/vorph/.local/bin\"</code> qui est n\u00e9cessaire pour certaines librairies python dans les fichiers <code>requirements.txt</code> et <code>requirements-dev.txt</code>.</p> <p>si l'on souhaite d\u00e9finir une variable d'environnement via l'interface en ligne de commande, cela se fait via l'argument <code>-e</code>.</p> <p><code>docker run -p 38282:8080 --name blue-app -e APP_COLOR=blue -d kodekloud/simple-webapp</code></p> <p>Dans un OS Linux, pour savoir quelles sont les variables d'environnement qui \u00e9t\u00e9 d\u00e9finies, on utilise la commande <code>env</code>. Dans un conteneur c'est identique, pour pouvoir passer cette commande au conteneur il faut toutefois utiliser la commande <code>docker exec</code>.</p>  <p>Exemple</p> env<pre><code>\u276f docker exec -it wizardly_swartz env\nPATH=/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/vorph/.local/bin:/usr/lib/python3.9/dist-packages\nHOSTNAME=a4e4accc5032\nTERM=xterm\nLANG=C.UTF-8\nGPG_KEY=E3FF2839C048B25C084DEBE9B26995E310250568\nPYTHON_VERSION=3.9.10\nPYTHON_PIP_VERSION=21.2.4\nPYTHON_SETUPTOOLS_VERSION=58.1.0\nPYTHON_GET_PIP_URL=https://github.com/pypa/get-pip/raw/2caf84b14febcda8077e59e9b8a6ef9a680aa392/public/get-pip.py\nPYTHON_GET_PIP_SHA256=7c5239cea323cadae36083079a5ee6b2b3d56f25762a0c060d2867b89e5e06c5\nHOME=/home/vorph\n</code></pre>"},{"location":"devops/docker/#cmd-versus-entrypoint","title":"CMD versus ENTRYPOINT","text":"<p><code>ENTRYPOINT</code> et <code>CMD</code> permettent de sp\u00e9cifier une commande qui sera \u00e9xecut\u00e9e lorsque l'image sera lanc\u00e9e en tant que conteneur.</p> <p>La diff\u00e9rence provient dans comment les arguments dans les commandes peuvent \u00eatre surcharg\u00e9es.</p> <pre><code>FROM Ubuntu\n\nCMD [\"sleep\", \"5\"]\n</code></pre> <p><code>docker run ubuntu-sleeper sleep 10</code></p> <p><pre><code>FROM Ubuntu\n\nENTRYPOINT [\"sleep\"]\n</code></pre> <code>docker run ubuntu-sleeper sleep 5</code></p> <p><pre><code>FROM Ubuntu\n\nENTRYPOINT [\"sleep\"]\n\nCMD [\"5\"] #argument par d\u00e9faut pour l'entrypoint\n</code></pre> Les commandes \u00e9xecut\u00e9es par <code>CMD</code> peuvent \u00eatre compl\u00e8tement surcharg\u00e9es lors de la commande <code>docker run</code>, ce qui n'est pas le cas avec <code>ENTRYPOINT</code>.</p>"},{"location":"devops/docker/#docker-compose","title":"docker compose","text":"<p>Compose est un outil permettant de d\u00e9finir et d'ex\u00e9cuter des applications Docker multi-conteneurs.</p> <p>Avec Compose, vous utilisez un fichier YAML pour configurer les services de votre application. Ensuite, avec une seule commande, vous cr\u00e9ez et d\u00e9marrez tous les services \u00e0 partir de votre configuration. Pour en savoir plus sur toutes les fonctionnalit\u00e9s de Compose, consultez la liste des fonctionnalit\u00e9s.</p> <p>Compose fonctionne dans tous les environnements : production, staging, d\u00e9veloppement, test, ainsi que dans les flux de travail CI. Vous pouvez en savoir plus sur chaque cas dans Cas d'utilisation communs.</p> <p>L'utilisation de Compose se r\u00e9sume \u00e0 un processus en trois \u00e9tapes :</p> <ol> <li>D\u00e9finissez l'environnement de votre application \u00e0 l'aide d'un <code>Dockerfile</code> afin qu'il puisse \u00eatre reproduit partout.</li> <li>D\u00e9finissez les services qui composent votre application dans <code>docker-compose.yml</code> afin qu'ils puissent \u00eatre ex\u00e9cut\u00e9s ensemble dans un environnement isol\u00e9.</li> <li>Ex\u00e9cutez docker compose up et la commande Docker compose d\u00e9marre et ex\u00e9cute votre application enti\u00e8re. Vous pouvez \u00e9galement ex\u00e9cuter docker-compose up en utilisant le binaire docker-compose.</li> </ol> <p>Un <code>docker-compose.yml</code> ressemble \u00e0 ceci :</p> <pre><code>version: \"3.9\"  # optional since v1.27.0\n\nservices:\n  web:\n    build: .\n    ports:\n      - \"8000:5000\"\n    volumes:\n      - .:/code\n      - logvolume01:/var/log\n    links:\n      - redis\n  redis:\n    image: redis\n\nvolumes:\n  logvolume01: {}\n</code></pre>"},{"location":"devops/docker/#surcharger-le-docker-composeyaml-pour-gerer-plusieur-environnements","title":"Surcharger le <code>docker-compose.yaml</code> pour g\u00e9rer plusieur environnements","text":""},{"location":"devops/docker/#docker-engine-stockage","title":"Docker engine, stockage","text":"<p>Le Docker Engine est compos\u00e9 de 3 parties :</p> <ul> <li>le docker daemon, qui g\u00e8re le service Docker, les images, conteneurs, volumes etc,</li> <li>l'API REST, qui fait la liaison entre le docker daemon et les autres programmes,</li> <li>la cli Dcoker (<code>docker build</code>, <code>docker run</code>, etc).</li> </ul> <p>Comment Docker g\u00e8re l'isolation d'un conteneur ?</p> <p>Chaque conteneur se voit fournir son propre namespace (espace de nom). Un namespace est un contexte qui permet d'identifier et grouper un ensemble logique d'\u00e9l\u00e9ments utilis\u00e9s par un programme. Dans un m\u00eame contexte et une m\u00eame port\u00e9e (scope), un identifiant doit identifier une entit\u00e9 de mani\u00e8re unique.</p> <p>Ainsi chaque conteneur, au sein de son namespace, se voit attribuer les caract\u00e9ristiques suivantes uniques :</p> <ul> <li>Process ID, Identifiant de processus,</li> <li>R\u00e9seau,</li> <li>InterProcess, communication inter-processus,</li> <li>Syst\u00e8me de montage,</li> <li>UNIX timesharing.</li> </ul> <p>D\u00e9veloppons le PID. Lorsqu'un syst\u00e8me Linux d\u00e9marre, il commence avec un seul processus, avec un identifiant de processus PID : 1, c'est le processus racine qui lance tous les autres processus.</p> <p><pre><code>graph LR\n\n    subgraph \"Linux System\"\n    A[PID : 1]\n\n    B[PID : 2]\n    C[PID : 3]\n    D[PID : 4]\n\n    subgraph \"Sous-syst\u00e8me (Conteneur)\"\n    A1[PID : 1]\n\n    B1[PID : 2]\n    end\n    end\n\n    A-.-&gt; B &amp; C &amp; D\n    A1-.-&gt; B1</code></pre> Une fois que le syst\u00e8me est complp\u00e8tement d\u00e9marr\u00e9, on a tout une famille de processus qui ont chacun un PID unique. On peut obtenir la liste des processus tournant \u00e0 l'instant t sur une machine Linux via la commande <code>ps</code>.</p> <p><pre><code>\u276f ps\n\n    PID TTY          TIME CMD\n  58674 pts/1    00:00:08 zsh\n  58678 pts/1    00:00:00 zsh\n  58724 pts/1    00:00:00 zsh\n  58726 pts/1    00:00:00 zsh\n  58727 pts/1    00:00:00 gitstatusd-linu\n  64791 pts/1    00:00:00 bash\n  64958 pts/1    00:00:00 make\n  64959 pts/1    00:04:51 mkdocs\n 113746 pts/1    00:00:00 ps\n</code></pre> Maintenant, lorsque l'on lance un conteneur, que l'on peut consid\u00e9rer comme un sous-syst\u00e8me, le sous-syst\u00e8me a besoin de se consid\u00e9rer comme un syst\u00e8me ind\u00e9pendant isol\u00e9 du reste, et qu'il a ses propres processus, provenant d'un processus root de PID 1.</p> <p>Docker partageant l'OS de l'h\u00f4te, une isolation compl\u00e8te comme cela n'est pas possible, et les processus du sous-syst\u00e8me ne sont que des autres processus du processus root du syst\u00e8me Linux.</p> <p>Mais \u00e0 l'int\u00e9rieur du conteneur, le fait d'avoir son propre namespace permet de faire en sorte que les processus sont vu comme compl\u00e8tement isol\u00e9s, il y a donc \"virtuellement\" un processus de PID 1 dans chaque conteneur.</p> <pre><code>graph LR\n\n    subgraph \"Linux System\"\n    A[PID : 1]\n\n    B[PID : 2]\n    C[PID : 3]\n    D[PID : 4]\n    E[PID : 5]\n    F[PID : 6]\n\n\n    subgraph \"Sous-syst\u00e8me (Conteneur) &lt;br/&gt; namespace\"\n    A1[PID : 1]\n\n    B1[PID : 2]\n    end\n    end\n\n    A-.-&gt; B &amp; C &amp; D &amp; E &amp; F\n    A1-.-&gt; B1\n    E-.-&gt;A1\n    F-.-&gt;B1</code></pre> <p>Les <code>cgroups</code> (control groups) permettent de limiter l'utilisation cpu et m\u00e9moire des conteneurs, qui par d\u00e9faut est illimit\u00e9e.</p> <ul> <li><code>docker run --cpus=0.5 ubuntu</code></li> <li><code>docker run --memory=100m ubuntu</code></li> </ul>"},{"location":"devops/docker/#reseau","title":"R\u00e9seau","text":"<p>Lorsque Docker est install\u00e9, il cr\u00e9e automatiquement 3 r\u00e9seaux :</p> <ul> <li><code>bridge</code>,</li> <li><code>none</code>,</li> <li><code>host</code>.</li> </ul> <p><code>bridge</code> est le r\u00e9seau par d\u00e9faut auquel est reli\u00e9 un conteneur. Pour l'associer \u00e0 l'un des deux autres, on utilise le param\u00e8tre <code>--network</code>.</p> <ul> <li><code>docker run ubuntu --network=none</code></li> <li><code>docker run ubuntu --network=host</code></li> </ul> <p><code>bridge</code> est un r\u00e9seau priv\u00e9 isol\u00e9 dans le docker host, chaque conteneur tournant se voit alors attribu\u00e9 une adresse ip sur ce r\u00e9seau.</p> <p><pre><code>graph LR\n\n    subgraph \"Docker Host\"\n    A[172.17.0.2 &lt;br/&gt; Conteneur 1]\n    B[172.17.0.3 &lt;br/&gt; Conteneur 2]\n    C[172.17.0.4 &lt;br/&gt; Conteneur 3]\n    D[172.17.0.5 &lt;br/&gt; Conteneur 4]\n\n    E{{ docker0  &lt;br/&gt; 172.17.0.1}}\n    end\n\n    E -.- A &amp; B &amp; C &amp; D</code></pre> Les conteneurs peuvent alors communiquer entre eux via leur ip interne. Pour acc\u00e9der \u00e0 un conteneur depuis l'ext\u00e9rieur, il faut alors associer les ports correspondant correctement, comme vu dans la partie Mapping des ports.</p> <p>Une autre fa\u00e7on d'avoir acc\u00e8s \u00e0 un conteneur depuis l'ext\u00e9rieur est de l'associer au r\u00e9seau <code>host</code>, toute isolation r\u00e9seau est alors supprim\u00e9e et le conteneur est directemnt accessible.</p> <p>Comme son nom l'indique <code>none</code> isole compl\u00e8tement un conteneur de tout r\u00e9seau, et n'est pas accessible de l'ext\u00e9rieur ou par un autre conteneur.</p> <p>Pour cr\u00e9er un second r\u00e9seau dans le docker host, pour par exemple isoler les conteneurs 3 et 4, on peut utiliser la commande <code>docker network create</code>.</p> <pre><code>docker network create \\\n    --driver bridge \\\n    --subnet 182.18.0.1/16 \\\n    --gateway 182.18.0.1 \\\n    custom-isolated-network\n</code></pre>  <p>Exemple</p> <p><code>docker run -d -e MYSQL_ROOT_PASSWORD=db_pass123 --name mysql-db --network custom-isolated-network mysql:5.6</code></p>  <pre><code>graph LR\n\n    subgraph \"Docker Host\"\n    A[172.17.0.2 &lt;br/&gt; Conteneur 1]\n    B[172.17.0.3 &lt;br/&gt; Conteneur 2]\n    C[182.18.0.2 &lt;br/&gt; Conteneur 3]\n    D[182.18.0.3 &lt;br/&gt; Conteneur 4]\n\n    E{{ docker0  &lt;br/&gt; 172.17.0.1}}\n    F{{ docker0  &lt;br/&gt; 182.18.0.1}}\n    end\n\n    E -.- A &amp; B\n    F -.- C &amp; D</code></pre> <p>La configuration r\u00e9seau de chaque conteneur peut \u00eatre vue via la commande <code>docker inspect</code>.</p> <pre><code>\u276f docker network ls\n\nNETWORK ID     NAME      DRIVER    SCOPE\nff42c61ad8f6   bridge    bridge    local\neb710fdaaadf   host      host      local\n1fc8a049c70c   none      null      local\n</code></pre> <pre><code>\u276f docker pull vorphus/helloworld-api:1.0-slim\n\n1.0-slim: Pulling from vorphus/helloworld-api\nDigest: sha256:7688dd17f1287bba526902186e7677d2ec872ca01dead95801b454b76ed49258\nStatus: Image is up to date for vorphus/helloworld-api:1.0-slim\ndocker.io/vorphus/helloworld-api:1.0-slim\n\n\u276f docker run -it --rm -d --name helloworld vorphus/helloworld-api:1.0-slim\ne321acdf171ff17d731e789120d9f2c366aa75a08b7305b11adbdffc6c653cdf\n\n\u276f docker inspect helloworld | jq \".[0].NetworkSettings\"\n\n{\n  \"Bridge\": \"\",\n  \"SandboxID\": \"9df6d42959f98cfa17790f69bc8b30e0cbec2fe2c54b9289f538e7dc98f1003a\",\n  \"HairpinMode\": false,\n  \"LinkLocalIPv6Address\": \"\",\n  \"LinkLocalIPv6PrefixLen\": 0,\n  \"Ports\": {},\n  \"SandboxKey\": \"/var/run/docker/netns/9df6d42959f9\",\n  \"SecondaryIPAddresses\": null,\n  \"SecondaryIPv6Addresses\": null,\n  \"EndpointID\": \"f6ca6b0b04d44343627d938ba99ebec0598cfce606fcf8eb887ba5424ff526d3\",\n  \"Gateway\": \"172.17.0.1\",\n  \"GlobalIPv6Address\": \"\",\n  \"GlobalIPv6PrefixLen\": 0,\n  \"IPAddress\": \"172.17.0.3\",\n  \"IPPrefixLen\": 16,\n  \"IPv6Gateway\": \"\",\n  \"MacAddress\": \"02:42:ac:11:00:03\",\n  \"Networks\": {\n    \"bridge\": {\n      \"IPAMConfig\": null,\n      \"Links\": null,\n      \"Aliases\": null,\n      \"NetworkID\": \"ff42c61ad8f68c356bdfea4c7f0c04dc1d134c8e543080ec7dd9515eb5b0bb1a\",\n      \"EndpointID\": \"f6ca6b0b04d44343627d938ba99ebec0598cfce606fcf8eb887ba5424ff526d3\",\n      \"Gateway\": \"172.17.0.1\",\n      \"IPAddress\": \"172.17.0.3\",\n      \"IPPrefixLen\": 16,\n      \"IPv6Gateway\": \"\",\n      \"GlobalIPv6Address\": \"\",\n      \"GlobalIPv6PrefixLen\": 0,\n      \"MacAddress\": \"02:42:ac:11:00:03\",\n      \"DriverOpts\": null\n    }\n  }\n}\n</code></pre> <p>Remarquons que la commande <code>docker inspect</code> ne marche pas uniquement sur les conteneurs, mais aussi sur les r\u00e9seaux.</p> <pre><code>\u276f docker inspect bridge | jq .\n\n[\n  {\n    \"Name\": \"bridge\",\n    \"Id\": \"ff42c61ad8f68c356bdfea4c7f0c04dc1d134c8e543080ec7dd9515eb5b0bb1a\",\n    \"Created\": \"2022-05-14T10:50:27.795717043+02:00\",\n    \"Scope\": \"local\",\n    \"Driver\": \"bridge\",\n    \"EnableIPv6\": false,\n    \"IPAM\": {\n      \"Driver\": \"default\",\n      \"Options\": null,\n      \"Config\": [\n        {\n          \"Subnet\": \"172.17.0.0/16\",\n          \"Gateway\": \"172.17.0.1\"\n        }\n      ]\n    },\n    \"Internal\": false,\n    \"Attachable\": false,\n    \"Ingress\": false,\n    \"ConfigFrom\": {\n      \"Network\": \"\"\n    },\n    \"ConfigOnly\": false,\n    \"Containers\": {\n      \"2cfe62c9e0c3a57a234f52774333af00155e11a67c04f255e3bb6b2e53614649\": {\n        \"Name\": \"boring_wiles\",\n        \"EndpointID\": \"528b6c7e4a71a71455218a980dcaa3a1b627b25c1f638cad8b58984e71454b21\",\n        \"MacAddress\": \"02:42:ac:11:00:02\",\n        \"IPv4Address\": \"172.17.0.2/16\",\n        \"IPv6Address\": \"\"\n      }\n    },\n    \"Options\": {\n      \"com.docker.network.bridge.default_bridge\": \"true\",\n      \"com.docker.network.bridge.enable_icc\": \"true\",\n      \"com.docker.network.bridge.enable_ip_masquerade\": \"true\",\n      \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\",\n      \"com.docker.network.bridge.name\": \"docker0\",\n      \"com.docker.network.driver.mtu\": \"1500\"\n    },\n    \"Labels\": {}\n  }\n]\n</code></pre> <p>Le docker host poss\u00e8de aussi un serveur DNS embarqu\u00e9 pour la r\u00e9solution des noms. Il est donc possible de se connecter \u00e0 un conteneur uniquement via son nom.</p>"},{"location":"devops/docker/#docker-registry","title":"Docker registry","text":"<p>Un registre Docker est l'\u00e9quivalent de github, gitlab, etc pour les conteneurs docker. C'est un endroit o\u00f9 sont stock\u00e9es et centralis\u00e9es les diff\u00e9rentes images disponibles sur le web.</p> <p>il est possible d'avoir un registre docker personnel sur par exemple docker hub.</p>  <p>D\u00e9ployer un registre priv\u00e9</p> <pre><code>docker run -d -p 5000:5000 --restart=always --name my-registry registry:2\n</code></pre> <p><pre><code>$ docker pull nginx:latest\nlatest: Pulling from library/nginx\n214ca5fb9032: Pull complete\nf0156b83954c: Pull complete\n5c4340f87b72: Pull complete\n9de84a6a72f5: Pull complete\n63f91b232fe3: Pull complete\n860d24db679a: Pull complete\nDigest: sha256:2c72b42c3679c1c819d46296c4e79e69b2616fa28bea92e61d358980e18c9751\nStatus: Downloaded newer image for nginx:latest\ndocker.io/library/nginx:latest\n\n$ docker image tag nginx:latest localhost:5000/nginx:latest\n\n$ docker push localhost:5000/nginx:latest\n</code></pre> The push refers to repository <code>localhost:5000/nginx</code></p> <pre><code>curl -X GET localhost:5000/v2/_catalog\n</code></pre>"},{"location":"devops/docker/#orchestration","title":"Orchestration","text":"<p>Avec une commande <code>docker run</code>, on est capable de d\u00e9ployer une instance d'un conteneur, par exemple une api.</p> <p>Mais que se passe-t-il si le nombre de requ\u00eates envoy\u00e9es \u00e0 cette api est trop important ? On aimerait d\u00e9ployer une seconde instance de cette api pour g\u00e9rer le flux suppl\u00e9mentaire.</p> <p>On pourrait le faire en relan\u00e7ant une commande <code>docker run</code>, mais cela demande de le faire de fa\u00e7on manuelle et de surveiller \u00e0 chaque fois comment cela se passe.</p> <p>Si on conteneur crashe, on devrait pouvoir le d\u00e9tecter et relancer une image automatiquement.</p> <p>Si le docker host crashe, tous les conteneurs lanc\u00e9s s'arr\u00eaterons aussi.</p> <p>Pour g\u00e9rer ces probl\u00e8mes dans un environnement de production, on fait alors appel \u00e0 des syst\u00e8mes d'orchestrations des conteneurs :</p> <ul> <li>docker-swarm,</li> <li>kubernetes,</li> <li>MESOS.</li> </ul>"},{"location":"devops/docker/#docker-et-vs-code-le-fichier-devcontainer","title":"Docker et VS Code : le fichier <code>.devcontainer</code>","text":""},{"location":"devops/docker/#softwares","title":"Softwares","text":"<ul> <li>Container Management and Kubernetes on the Desktop</li> <li>The super duper Podman Desktop Companion</li> <li>Kompose</li> <li> <p>nerdctl: Docker-compatible CLI for containerd</p> </li> <li> <p>Namespaces : La brique de base des conteneurs</p> </li> <li>Securing containers using Docker isolation</li> </ul>"},{"location":"devops/git/","title":"GIT Gud","text":""},{"location":"devops/git/#introduction","title":"Introduction","text":"<p>Git est un syst\u00e8me de contr\u00f4le de version distribu\u00e9 et open source qui permet aux d\u00e9veloppeurs et aux \u00e9quipes d'exploitation de collaborer et de suivre les modifications apport\u00e9es \u00e0 un projet.</p> <p>En tant qu'outil DevOps, Git favorise la collaboration et l'acc\u00e9l\u00e9ration des cycles de publication. Toute personne d\u00e9sireuse de d\u00e9marrer sa carri\u00e8re DevOps ou de passer \u00e0 un niveau sup\u00e9rieur doit commencer par les bases, et Git est l'exigence la plus fondamentale de toutes.</p> <p>Bon nombre des projets open source les plus populaires aujourd'hui sont d\u00e9velopp\u00e9s sur Github - Kubernetes, Ansible, TensorFlow, Rust, Node.js, Go, Terraform, Helm Charts \u00e9tant quelques-uns des plus importants parmi les 100 millions de d\u00e9p\u00f4ts.</p>"},{"location":"devops/git/#repos-locaux-et-distants","title":"Repos locaux et distants","text":"<p>Git a 2 type de repos :</p> <ul> <li>Le repo local, qui se trouve sur votre machine, et auquel vous avez un acc\u00e8s direct,</li> <li>Le repo distant, qui se trouve g\u00e9n\u00e9ralement sur un serveur centralis\u00e9, et qui est optionnel.</li> </ul> <p>Le repo distant est pens\u00e9 comme un back-up de votre repo local.</p> <p>Le repo local est divis\u00e9 en 3 sections.</p> <ol> <li>La zone de travail o\u00f9 sont les fichiers sur lesquels vous travaillez, git ne fait rien avec ces fichiers, il sait juste que ces fichiers sont en train d'\u00eatre modifi\u00e9s.</li> <li>La zone de transit (staging area), contient les nouveaux changements qui seront bient\u00f4t versionn\u00e9s.</li> <li>Les fichiers versionn\u00e9s (committed files).</li> </ol>"},{"location":"devops/git/#installer-git-sur-ubuntu","title":"Installer git sur ubuntu","text":"<pre><code>sudo apt update\nsudo apt install git -y\n\ngit config user.email \"max@example.com\"\ngit config user.name \"max\"\n</code></pre>"},{"location":"devops/git/#initialiser-un-repo-git","title":"Initialiser un repo git","text":"<p>Pour initialiser un repo, placez vous, dans le terminal, dans le dossier dans lequel vous voulez versionner les changements et taper simplement <code>git init</code> dans le terminal.</p> <pre><code>\u276f cd helloworld-git\n\n\u276f git init\n\nD\u00e9p\u00f4t Git vide initialis\u00e9 dans /media/vorph/datas/perso/helloworld/helloworld-git/.git/\n</code></pre> <p>Maintenant que le repo est initialis\u00e9, Git surveille ce dossier et vois les changements qui y sont fait. On peut utiliser la commande <code>git status</code> pour les voir.</p> <pre><code>\u276f touch story.txt\n\u276f echo \"ceci n'est pas un texte\" &gt;&gt; story.txt\n\u276f git status\nSur la branche master\n\nAucun commit\n\nFichiers non suivis:\n  (utilisez \"git add &lt;fichier&gt;...\" pour inclure dans ce qui sera valid\u00e9)\n    story.txt\n\naucune modification ajout\u00e9e \u00e0 la validation mais des fichiers non suivis sont pr\u00e9sents (utilisez \"git add\" pour les suivre)\n</code></pre> <p>Aucun fichiers n'est dans la zone des fichiers versionn\u00e9s, pour versionner notre travail, on le fait en deux \u00e9tapes.</p> <ol> <li>On envoie <code>story.txt</code> en zone de transit via <code>git add</code>,</li> <li>On versionne <code>story.txt</code> avec <code>git commit</code>.</li> </ol> <pre><code>sequenceDiagram\n    participant \"Working Area\"\n    participant \"Staging Area\"\n    participant \"Committed files\"\n\n    Note over \"Working Area\": travail sur story.txt\n    activate \"Working Area\"\n    \"Working Area\"-&gt;&gt;+\"Staging Area\": git add story.txt\n    deactivate \"Working Area\"\n\n    activate \"Staging Area\"\n    \"Staging Area\"-&gt;&gt;\"Committed files\": git commit -m \"commit story.txt\"\n    deactivate \"Staging Area\"</code></pre> <p>Le <code>-m</code> dans <code>git commit -m \"commit story.txt\"</code> est l'argument pour ajouter un message au commit.</p>"},{"location":"devops/git/#revenir-en-arriere","title":"Revenir en arri\u00e8re","text":"<p>Il est possible, lorsque l'on code, de vouloir revenir en arri\u00e8re, soit parce que ce que l'on a \u00e9crit ne veut rien dire, ou alors parce que l'on a fait une erreur, ou autre.</p> <p>Pour revenir en arri\u00e8re il est possible d'utiliser la commande <code>git restore</code>.</p> <p>Il est possible de restaurer \u00e0 deux moments :</p> <ul> <li>soit en zone de travail,</li> <li>soit en zone de transit.</li> </ul>"},{"location":"devops/git/#preliminaires","title":"Pr\u00e9liminaires","text":"<p>D\u00e9finissons un r\u00e9pertoire suivi par git avec au moins un fichier dedans.</p> Initialisation<pre><code>\u276f mkdir helloworld-git\n\u276f cd helloworld-git\n\u276f git init\nInitialized empty Git repository in /home/vorph/work/perso/helloworld-git/.git/\n\u276f touch poeme.txt\n\u276f git add .\n\u276f git status\nOn branch master\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n        new file:   poeme.txt\n\n\u276f git commit -m \"feat: ajout poeme.txt\"\n[master (root-commit) 34ac1ba] feat: ajout poeme.txt\n 1 file changed, 0 insertions(+), 0 deletions(-)\n create mode 100644 poeme.txt\n</code></pre>  <p>Question</p> <p>Pourquoi au moins un fichier dedans ? Tout simplement par ce que git ne peut pas supprimer des fichiers d'un r\u00e9pertoire, la seule chose qu'il puisse faire est restaurer des fichiers \u00e0 un \u00e9tat pr\u00e9cedent, pourvu que ce fichier existe \u00e0 l'\u00e9tat pr\u00e9c\u00e9dent.</p>  <p>Maintenant que l'on a un dossier suivi, voyons comment et ce que l'on peut restaurer.</p>"},{"location":"devops/git/#en-zone-de-travail","title":"En zone de travail","text":"<p>Ecrivons un po\u00e8me.</p> <pre><code>\u276f echo \"c'est un tr\u00e8s joli po\u00e8me n'est-ce pas ?\"&gt;&gt; poeme.txt\n\u276f git status\nOn branch master\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   poeme.txt\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n</code></pre> <p>Lorsque l'on tape la commande <code>git status</code>, on peut voir alors la phrase <code>(use \"git restore &lt;file&gt;...\" to discard changes in working directory)</code>.</p> <p>Lorsque les fichiers modifi\u00e9s sont encore en zone de travail, il est alors possible d'utiliser la commande <code>git restore nom_du_fichier</code> pour annuler toutes les modifications faites \u00e0 ce fichier depuis le dernier commit (ou git add ?).</p> Usage du restore sans commit<pre><code>\u276f cat poeme.txt\n\"c'est un tr\u00e8s joli po\u00e8me n'est-ca pas ?\"\n\u276f git restore poeme.txt\n\u276f cat poeme.txt\n\u276f\n</code></pre> S'il y a d\u00e9j\u00e0 un commit, restore nous y fait revenir<pre><code>\u276f echo \"c'est un tr\u00e8s joli po\u00e8me n'est-ce pas ? C'est le premier ajout\"&gt;&gt; poeme.txt\n\u276f git add .\n\u276f git commit -m \"feat: ajout du premier commit\"\n[master eff1c35] feat: ajout du premier commit\n 1 file changed, 1 insertion(+)\n\n\n\u276f echo \"c'est un tr\u00e8s joli po\u00eame n'est-ce pas ? C'est le deuxi\u00e8me ajout\"&gt;&gt; poeme.txt\n\u276f git status\nOn branch master\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   poeme.txt\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\n\u276f git restore poeme.txt\n\u276f cat poeme.txt\n\"c'est un tr\u00e8s joli po\u00e8me n'est-ce pas ? C'est le premier ajout\"\n</code></pre>"},{"location":"devops/git/#en-zone-de-transit","title":"En zone de transit","text":"<p>Si les modifications sont d\u00e9j\u00e0 en \"staging area\", pr\u00eates \u00e0 \u00eatre versionn\u00e9es. Il est encore possible de restaurer les fichiers mais en ajoutant cette fois ci l'argument <code>--staged</code>.</p> Premier ajout<pre><code>\u276f echo \"c'est un tr\u00e8s joli po\u00e8me n'est-ce pas ? C'est le premier ajout\"&gt;&gt; poeme.txt\n\u276f git add .\n\u276f git commit -m \"feat: ajout du premier commit\"\n[master 791eea7] feat: ajout du premier commit\n 1 file changed, 1 insertion(+), 1 deletion(-)\n</code></pre> Deuxi\u00e8me ajout<pre><code>\u276f echo \"c'est un tr\u00e8s joli po\u00eame n'est-ce pas ? C'est le deuxi\u00e8me ajout\"&gt;&gt; poeme.txt\n\u276f git add .\n\u276f git status\nSur la branche master\nModifications qui seront valid\u00e9es :\n  (utilisez \"git restore --staged &lt;fichier&gt;...\" pour d\u00e9sindexer)\n    modifi\u00e9\u00a0:         poeme.txt\n</code></pre> Retour en arri\u00e8re vers la zone de travail<pre><code>\u276f git restore --staged poeme.txt\n\u276f cat poeme.txt\n\"c'est un tr\u00e8s joli po\u00e8me n'est-ce pas ? C'est le premier ajout\"\n\"c'est un tr\u00e8s joli po\u00eame n'est-ce pas ? C'est le deuxi\u00e8me ajout\"\n\n\u276f git status\nSur la branche master\nModifications qui ne seront pas valid\u00e9es :\n  (utilisez \"git add &lt;fichier&gt;...\" pour mettre \u00e0 jour ce qui sera valid\u00e9)\n  (utilisez \"git restore &lt;fichier&gt;...\" pour annuler les modifications dans le r\u00e9pertoire de travail)\n    modifi\u00e9\u00a0:         poeme.txt\n\naucune modification n'a \u00e9t\u00e9 ajout\u00e9e \u00e0 la validation (utilisez \"git add\" ou \"git commit -a\")\n</code></pre>  <p>Attention</p> <p>Comme vous le voyez ici avec <code>cat poeme.txt</code>, la commande <code>git restore --staged poeme.txt</code> n'a pas supprim\u00e9e la deuxi\u00e8me ligne de <code>poeme.txt</code>. La commande <code>git restore</code>, ne fait que restaurer l'\u00e9tat des fichiers avant la derni\u00e8re commande git \u00e9xecut\u00e9e. Ici dans notre cas, la derni\u00e8re commande git \u00e9xecut\u00e9e \u00e9tait <code>git add .</code>, on est donc revenu \u00e0 l'\u00e9tat d'avant cette commande, ie :</p> <ul> <li>le fichier <code>poeme.txt</code> avait 2 lignes,</li> <li>il \u00e9tait pr\u00eat \u00e0 \u00eatre transf\u00e9r\u00e9 en \"staging area\".</li> </ul> <p>Pour supprimer la deuxi\u00e8me ligne, il faudrait alors de nouveau appliquer une commande <code>git restore</code>.</p> <pre><code>\u276f git restore poeme.txt\n\u276f cat poeme.txt\n\"c'est un tr\u00e8s joli po\u00e8me n'est-ce pas ? C'est le premier ajout\"\n</code></pre>  <ul> <li><code>git rm --cached</code></li> </ul>"},{"location":"devops/git/#ignorer","title":"Ignorer","text":"<p>Il est possible de forcer git \u00e0 ignorer des fichiers ou des r\u00e9pertoires en les ajoutant au fichier <code>.gitignore</code>.</p>"},{"location":"devops/git/#les-logs","title":"Les logs","text":"<p>Pour voir l'ensemble des commits d'un repo git, vous pouvez taper la commande suivante.</p> <pre><code>git log\n</code></pre> Structure d'un log git<pre><code>git log\n\ncommit 83153b74fbedfde6c7bb6d6845ed56149829f86a (HEAD -&gt; master)\nAuthor: vorph &lt;klimczak.mathieu@pm.me&gt;\nDate:   Mon May 16 15:35:19 2022 +0200\n\n    feat: add story.txt\n</code></pre> <p>Un \u00e9l\u00e9ment de log dans Git est constitu\u00e9 des \u00e9l\u00e9ments suivants :</p> <ul> <li>le hash du commit, qui est un identifiant alphanum\u00e9rique unique au commit,</li> <li>l'auteur du commit,</li> <li>la date du commit,</li> <li>le texte \u00e9crit apr\u00e8s l'argument <code>-m</code>.</li> </ul> <p><code>git log --oneline</code> permet de voir l'ensemble de ces infos au format une seule ligne.</p>  <p>Remarque</p> <p>Les fichiers versionn\u00e9s n'apparaissent pas lorsque l'on utilise la commande <code>git log</code>.</p>  <p>Pour les faire appara\u00eetre on peut utiliser la commande <code>git log --name-only</code></p> <pre><code>git log --name-only\n\ncommit 83153b74fbedfde6c7bb6d6845ed56149829f86a (HEAD -&gt; master)\nAuthor: vorph &lt;klimczak.mathieu@pm.me&gt;\nDate:   Mon May 16 15:35:19 2022 +0200\n\n    feat: add story.txt\n\nstory.txt\n</code></pre> <p>Pour limiter l'affichage des logs aux k derniers commits, on utilise la commande <code>git log -n k</code>.</p>"},{"location":"devops/git/#les-branches-git","title":"Les branches Git","text":"<p>De fa\u00e7on basique, une branche est un pointeur vers un certain commit.</p> Cr\u00e9e une branche nomm\u00e9e dev<pre><code>git branch dev\n</code></pre> Switcher sur la branche nomm\u00e9e dev<pre><code>git checkout dev\n</code></pre> Cr\u00e9e une branche nomm\u00e9e dev et switch dessus directement<pre><code>git checkout -b dev\n</code></pre> Supprime une branche nomm\u00e9e dev<pre><code>git branch -d dev\n</code></pre> Liste toutes les branches<pre><code>git branch\n</code></pre> <pre><code>%%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'showBranches': true, 'showCommitLabel':true,'mainBranchName': 'main'}} }%%\ngitGraph\n  commit\n  commit\n  branch dev\n  branch dev-mathieu\n  checkout dev\n  commit\n  commit id:\"3-e2cf2c8\"\n  checkout dev-mathieu\n  commit\n  commit\n  checkout dev\n  merge dev-mathieu\n  checkout main\n  merge dev\n  commit id:\"mise en prod\"</code></pre>"},{"location":"devops/git/#quest-ce-que-head","title":"Qu'est ce que HEAD ?","text":"<p>Ce terme appara\u00eet dans chaque commit que vous faites, par exemple dans l'exemple du dessus avec <code>git log</code>, on a <code>commit 83153b74fbedfde6c7bb6d6845ed56149829f86a (HEAD -&gt; master)</code>.</p> <p><code>HEAD</code> d\u00e9signe votre position actuelle dans le graphe git. En prenant le graphe ci-dessus, si vous \u00eates positionn\u00e9 au dernier commit de la branche <code>dev</code>, alors <code>HEAD</code> sera <code>3-e2cf2c8</code>.</p> <p>Pour voir sur quel commit est positionn\u00e9 <code>HEAD</code>, vous pouvez taper la commande suivante.</p> Montre le commit o\u00f9 est HEAD<pre><code>git rev-parse HEAD\n</code></pre> <p><code>HEAD -&gt; master</code> d\u00e9signe donc la branche (ici <code>master</code>) dans le repo sur laquelle vous faites un <code>git commit</code>. <code>HEAD</code> pointe toujours vers le dernier commit sur la branche actuellement extraite.</p> <p><code>git log --graph --decorate</code> permet d'avoir un graphe des diff\u00e9rents commits, branch et merge de l'historique.</p>"},{"location":"devops/git/#git-merge","title":"<code>git merge</code>","text":"<p>Pour fusionner deux branches, la marche \u00e0 suivre est la suivante :</p> <ol> <li>se positionner sur la branche vers laquelle on veut fusionner,</li> <li>taper la commande <code>git merge branche_a_fusionner</code>.</li> </ol> <p>Par exemple, supposons que je suis sur la branche <code>dev-mathieu</code>, et que je souhaite fusionner mon travail vers la branche <code>dev</code>.</p> <p><pre><code>%%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'showBranches': true, 'showCommitLabel':true,'mainBranchName': 'main'}} }%%\ngitGraph\n  commit\n  commit\n  branch dev\n  branch dev-mathieu\n  checkout dev\n  commit\n  commit\n  checkout dev-mathieu\n  commit\n  commit tag: \"HEAD\"</code></pre> La marche \u00e0 suivre est alors :</p> <ol> <li><code>git checkout dev</code>,</li> <li><code>git merge dev-mathieu</code></li> </ol> <pre><code>%%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'showBranches': true, 'showCommitLabel':true,'mainBranchName': 'main'}} }%%\ngitGraph\n  commit\n  commit\n  branch dev\n  branch dev-mathieu\n  checkout dev\n  commit\n  commit\n  checkout dev-mathieu\n  commit\n  commit\n  checkout dev\n  merge dev-mathieu</code></pre> <p>On a deux type de <code>merge</code> diff\u00e9rents :</p> <ol> <li>fast-forward,</li> <li>no-fast-forward</li> </ol> <p>Un <code>fast-forward merge</code> se fait lorsque la branche vers laquelle on souhaite fusionner n'a pas de commits en dehors de celui cr\u00e9ant la branche. Dans ce cas l\u00e0, la fusion ne cr\u00e9e pas de nouveau commit mais fusionne les commits des deux branches.</p>  <p>fast-forward merge de dev-mathieu vers dev</p> <pre><code>%%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'showBranches': true, 'showCommitLabel':true,'mainBranchName': 'main'}} }%%\ngitGraph\n  commit\n  commit\n  branch dev\n  branch dev-mathieu\n  commit id:\"2-6923bcf\"\n  commit id:\"3-9038f6c\"</code></pre> <pre><code>%%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'showBranches': true, 'showCommitLabel':true,'mainBranchName': 'main'}} }%%\ngitGraph\n  commit\n  commit\n  branch dev\n  commit id:\"2-6923bcf\"\n  commit id:\"3-9038f6c\"</code></pre>  <p>Un <code>no-fast-forward merge</code> se fait lorsque la branche vers laquelle on souhaite fusionner a au moins un commit en dehors de ceux pr\u00e9sents sur la branche \u00e0 fusionner. Dans ce cas l\u00e0, la fusion cr\u00e9era un nouveau commit (un merge commit) en fusionnant les commits des deux branches.</p>  <p>no-fast-forward merge de dev-mathieu vers dev</p> <pre><code>%%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'showBranches': true, 'showCommitLabel':true,'mainBranchName': 'main'}} }%%\ngitGraph\n  commit\n  commit\n  branch dev\n  commit id:\"2-aeb8393\"\n  branch dev-mathieu\n  checkout dev\n  commit id:\"3-e1bb5ee\"\n  checkout dev-mathieu\n  commit id:\"2-6923bcf\"\n  commit id:\"3-9038f6c\"</code></pre> <pre><code>%%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'showBranches': true, 'showCommitLabel':true,'mainBranchName': 'main'}} }%%\ngitGraph\n  commit\n  commit\n  branch dev\n  commit id:\"2-aeb8393\"\n  commit id:\"3-e1bb5ee\"\n  commit id:\"2-6923bcf\"\n  commit id:\"3-9038f6c\"\n  commit id:\"Merge\"</code></pre>"},{"location":"devops/git/#initialiser-et-travailler-avec-un-repo-distant","title":"Initialiser et travailler avec un repo distant","text":"<p>Les solutions les plus connues pour h\u00e9berger des repos distants sont les suivantes :</p> <ul> <li>GitHub,</li> <li>GitLab,</li> <li>Bitbucket,</li> </ul> <p>Chacune des solutions ci-dessus, une fois un repo distant initialis\u00e9 sur leur plateforme, founit une url de la forme :</p> <ul> <li><code>https://github.com/my_organization/my_repo.git</code>,</li> <li><code>https://gitlab.com/my_organization/my_repo.git</code>,</li> <li><code>https://bitbucket.org/my_organization/my_repo.git</code>.</li> </ul> <p>Pour ajouter ce repo distant \u00e0 un projet local, on utilise la commande suivante.</p> Connexion repo local vers repo distant (github par exemple)<pre><code>git remote add origin https://github.com/my_organization/my_repo.git\n</code></pre> <p>Il est possibble de connecter plusieurs repos distants au m\u00eame repo local.</p> <p>Pour voir l'ensemble des repos distants connect\u00e9s au repo local, on utlise <code>git remote -v</code>.</p> <p>Envoyer les commits vers le repo distant se fait via la commande <code>git push origin main</code>.</p> <pre><code>sequenceDiagram\n    participant Working Area\n    participant Staging Area\n    participant Committed files\n    participant Repo distant\n\n    Note over Working Area: travail sur story.txt\n    activate Working Area\n    Working Area-&gt;&gt;Staging Area: git add story.txt\n    deactivate Working Area\n\n    activate Staging Area\n    Staging Area-&gt;&gt;Committed files: git commit -m \"commit story.txt\"\n    deactivate Staging Area\n\n    activate Committed files\n    Committed files-&gt;&gt;Repo distant: git push origin main\n    deactivate Committed files</code></pre> <ul> <li><code>git clone</code></li> <li><code>git fetch origin main</code></li> <li><code>git pull origin main</code></li> </ul> <p><code>git pull = git fetch + git merge</code></p> <ul> <li><code>git fork</code></li> </ul>"},{"location":"devops/git/#difference-git-push-et-git-push-origin-main","title":"Diff\u00e9rence <code>git push</code> et <code>git push origin main</code>","text":"<pre><code>$ git push\n\nfatal: The current branch master has no upstream branch.\nTo push the current branch and set the remote as upstream, use\n\n    git push --set-upstream origin master\n\n$ git push origin master\n\nUsername for 'http://git.example.com': vorphus\nPassword for 'http://vorphus@git.example.com':\nEnumerating objects: 14, done.\nCounting objects: 100% (14/14), done.\nDelta compression using up to 36 threads\nCompressing objects: 100% (12/12), done.\nWriting objects: 100% (14/14), 2.05 KiB | 2.05 MiB/s, done.\nTotal 14 (delta 4), reused 0 (delta 0), pack-reused 0\nremote: . Processing 1 references\nremote: Processed 1 references in total\nTo http://git.example.com/vorphus/story-blog.git\n * [new branch]      master -&gt; master\n</code></pre>"},{"location":"devops/git/#rebasing","title":"Rebasing","text":"<p>Lorsque l'on travaille sur une branche distincte de la branche <code>main</code> et que l'on souhaite mettre \u00e0 jour sa branche avec les modifications apport\u00e9es sur la branche <code>main</code>, on a deux solutions.</p> <ol> <li>Faire un <code>merge</code> de la branche <code>main</code> vers notre branche.</li> </ol> <pre><code>%%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'showBranches': true, 'showCommitLabel':true,'mainBranchName': 'main'}} }%%\ngitGraph\n  commit id:\"3-e1bb5ee\"\n  branch dev\n  checkout main\n  commit id:\"2-aeb8393\"\n  checkout dev\n  commit id:\"2-6923bcf\"\n  commit id:\"3-9038f6c\"</code></pre> <ul> <li><code>git checkout dev</code></li> <li><code>git merge main</code></li> </ul> <p><pre><code>%%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'showBranches': true, 'showCommitLabel':true,'mainBranchName': 'main'}} }%%\ngitGraph\n  commit id:\"3-e1bb5ee\"\n  branch dev\n  checkout main\n  commit id:\"2-aeb8393\"\n  checkout dev\n  commit id:\"2-6923bcf\"\n  commit id:\"3-9038f6c\"\n  merge main</code></pre> 2. Utiliser la commande <code>rebase</code>.</p> <pre><code>%%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'showBranches': true, 'showCommitLabel':true,'mainBranchName': 'main'}} }%%\ngitGraph\n  commit id:\"3-e1bb5ee\"\n  branch dev\n  checkout main\n  commit id:\"2-aeb8393\"\n  checkout dev\n  commit id:\"2-6923bcf\"\n  commit id:\"3-9038f6c\"</code></pre> <ul> <li><code>git checkout dev</code></li> <li><code>git rebase main</code></li> </ul> <p><pre><code>%%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'showBranches': true, 'showCommitLabel':true,'mainBranchName': 'main'}} }%%\ngitGraph\n  commit id:\"3-e1bb5ee\"\n  commit id:\"2-aeb8393\"\n  branch dev\n  checkout dev\n  commit id:\"2-6893bcf\"\n  commit id:\"3-908526c\"</code></pre> La commande <code>rebase</code> va modifier la base de la branche <code>dev</code> pour la positionner au dernier commit de la branche <code>main</code>, et donc mettre \u00e0 jour les fichiers, sans pour autant modifier la position de la branche <code>main</code>.</p> <p>Une diff\u00e9rence principale est que lors d'un <code>merge</code>, les hash des commits ne changent pas. Lors d'un rebase, comme l'on copie les fichiers \u00e0 une nouvelle place, cela compte comme une op\u00e9ration et les hash se mettent \u00e0 jour.</p> <p>En d'autres termes, faire un <code>rebase</code> modifie l'historique Git, alors que ce n'est pas le cas pour un <code>merge</code>.</p> <p><code>git rebase -i HEAD~k</code>, permet de faire un rebase interactif des k derniers commits, pour par exemple faire du \"squash commit\".</p> <p><code>git cherry-pick hash</code></p>"},{"location":"devops/git/#reset-et-revert","title":"Reset et revert","text":"<p>Les commandes <code>git reset</code> et <code>git revert</code> sont toutes les deux des commandes permettant de revenir en arri\u00e8re dans le graphe git. Mais leurs utilisations changent.</p>"},{"location":"devops/git/#git-revert-hash","title":"<code>git revert hash</code>","text":"<p>La commande <code>git revert</code> permet de supprimer toutes les modifications faites lors du commit d\u00e9sign\u00e9 par son hash (g\u00e9n\u00e9ralement le commit pr\u00e9c\u00e9dent).</p> <p>En d'autres termes, si un fichier \u00e0 \u00e9tait ajout\u00e9 lors du commit pr\u00e9c\u00e9dent, appliquer <code>git revert</code> sur ce commit supprimera ce fichier du graphe git.</p> <p>La commande <code>git revert</code> cr\u00e9e un nouveau commit, ce qui permet de suivre les modifications.</p> git revert<pre><code>\u276f cd helloworld-git\n\n\u276f git status\nSur la branche master\nFichiers non suivis:\n  (utilisez \"git add &lt;fichier&gt;...\" pour inclure dans ce qui sera valid\u00e9)\n    note.txt\n\naucune modification ajout\u00e9e \u00e0 la validation mais des fichiers non suivis sont pr\u00e9sents (utilisez \"git add\" pour les suivre)\n\n\u276f git add .\n\n\u276f git commit -m \"feat: add note.txt\"\n[master 4f10f7d] feat: add note.txt\n 1 file changed, 1 insertion(+)\n create mode 100644 note.txt\n\n\u276f git revert 4f10f7d\nSuppression de note.txt\n[master 22dc824] Revert \"feat: add note.txt\"\n 1 file changed, 1 deletion(-)\n delete mode 100644 note.txt\n\n\u276f git status\nSur la branche master\nrien \u00e0 valider, la copie de travail est propre\n\n\u276f git log\n\ncommit 22dc824b7693ac8fc053bc0e1c8d2c23d8e8a981 (HEAD -&gt; master)\nAuthor: vorph &lt;klimczak.mathieu@pm.me&gt;\nDate:   Tue May 17 11:03:44 2022 +0200\n\n    Revert \"feat: add note.txt\"\n\n    This reverts commit 4f10f7d660aa7ce4c54afb2634062733053616c6.\n\n    Test de la commande git revert\n\ncommit 4f10f7d660aa7ce4c54afb2634062733053616c6\nAuthor: vorph &lt;klimczak.mathieu@pm.me&gt;\nDate:   Tue May 17 11:03:25 2022 +0200\n\n    feat: add note.txt\n</code></pre>  <p>Remarque</p> <p>Pour faire un revert sur le commit le plus r\u00e9cent, on peut aussi utliser la commande <code>git revert HEAD~0</code>.</p>"},{"location":"devops/git/#git-reset","title":"<code>git reset</code>","text":"<p>La commande <code>git reset</code> permet elle de supprimer directemnts des commit du graphe git. Selon l'argument qu'on lui assignera <code>--soft</code> ou <code>--hard</code> les modifications qui \u00e9taient pr\u00e9sentes dans ces commits :</p> <ul> <li>retourneront en zone de transit (staging area), avec l'argument <code>--soft</code>,</li> <li>seront purement et simplement supprim\u00e9es, avec l'argument <code>--hard</code>.</li> </ul> <p><code>HEAD~k</code> permet de d\u00e9terminer sur combien de commits on doit revenir en arri\u00e8re, \u00e0 partir du pointeur <code>HEAD</code>, ici on revient en arri\u00e8re sur k commits.</p> <ul> <li><code>git reset --soft HEAD~k</code></li> </ul> <pre><code>\u276f cd /media/vorph/datas/perso/helloworld/helloworld-git\n\n\u276f echo \"test\" &gt; note.txt\n\n\u276f git status\nSur la branche master\nFichiers non suivis:\n  (utilisez \"git add &lt;fichier&gt;...\" pour inclure dans ce qui sera valid\u00e9)\n    note.txt\n\naucune modification ajout\u00e9e \u00e0 la validation mais des fichiers non suivis sont pr\u00e9sents (utilisez \"git add\" pour les suivre)\n\n\u276f git add note.txt\n\n\u276f git commit -m \"feat: add note.txt\"\n[master be87880] feat: add note.txt\n 1 file changed, 1 insertion(+)\n create mode 100644 note.txt\n\n\u276f git reset --soft HEAD~1\n\n\u276f git log\ncommit 22dc824b7693ac8fc053bc0e1c8d2c23d8e8a981 (HEAD -&gt; master)\nAuthor: vorph &lt;klimczak.mathieu@pm.me&gt;\nDate:   Tue May 17 11:03:44 2022 +0200\n\n    Revert \"feat: add note.txt\"\n\n    This reverts commit 4f10f7d660aa7ce4c54afb2634062733053616c6.\n\n    Test de la commande git revert\n\ncommit 4f10f7d660aa7ce4c54afb2634062733053616c6\nAuthor: vorph &lt;klimczak.mathieu@pm.me&gt;\nDate:   Tue May 17 11:03:25 2022 +0200\n\n    feat: add note.txt\n\ncommit 83153b74fbedfde6c7bb6d6845ed56149829f86a\nAuthor: vorph &lt;klimczak.mathieu@pm.me&gt;\nDate:   Mon May 16 15:35:19 2022 +0200\n\n    feat: add story.txt\n</code></pre> <p>Remarquez que le commit <code>be87880</code>, avant <code>git reset --soft HEAD~1</code>, n'appara\u00eet pas dans les logs.</p> note.txt est retourn\u00e9 en staging area<pre><code>\u276f git status\n\nSur la branche master\nModifications qui seront valid\u00e9es :\n  (utilisez \"git restore --staged &lt;fichier&gt;...\" pour d\u00e9sindexer)\n    nouveau fichier\u00a0: note.txt\n</code></pre> <ul> <li><code>git reset --hard HEAD~k</code></li> </ul> note.txt est supprim\u00e9 avec --hard<pre><code>\u276f git status\n\nSur la branche master\nModifications qui seront valid\u00e9es :\n  (utilisez \"git restore --staged &lt;fichier&gt;...\" pour d\u00e9sindexer)\n    nouveau fichier\u00a0: note.txt\n\n\u276f git add note.txt\n\n\u276f git commit -m \"feat: add note.txt\"\n[master c92b48a] feat: add note.txt\n 1 file changed, 1 insertion(+)\n create mode 100644 note.txt\n\n\u276f git reset --hard HEAD~1\nHEAD est maintenant \u00e0 22dc824 Revert \"feat: add note.txt\"\n\n\u276f git status\nSur la branche master\nrien \u00e0 valider, la copie de travail est propre\n</code></pre>"},{"location":"devops/git/#git-stash","title":"<code>git stash</code>","text":"<p>Permet de mettre en cache des documents qui sont dans la staging area pour par exemple eviter de les commit de fa\u00e7on accidentelle.</p> <ul> <li><code>git stash</code></li> <li><code>git stash pop</code></li> <li><code>git stash pop stash_id</code></li> <li><code>git stash show stash_id</code></li> </ul> <pre><code>sequenceDiagram\n    participant Working Area\n    participant Staging Area\n    participant Stash\n\n    Note over Working Area: travail sur story.txt\n    activate Working Area\n    Working Area-&gt;&gt;Staging Area: git add story.txt\n    deactivate Working Area\n\n    activate Staging Area\n    Staging Area-&gt;&gt;Stash: git stash\n    deactivate Staging Area</code></pre> stash met en cache les fichiers de la staging area<pre><code>\u276f cd helloworld-git\n\n\u276f git status\nSur la branche master\nFichiers non suivis:\n  (utilisez \"git add &lt;fichier&gt;...\" pour inclure dans ce qui sera valid\u00e9)\n    note.txt\n    note2.txt\n\naucune modification ajout\u00e9e \u00e0 la validation mais des fichiers non suivis sont pr\u00e9sents (utilisez \"git add\" pour les suivre)\n\n\u276f git add note.txt\n\n\u276f git stash\nArbre de travail et \u00e9tat de l index sauvegard\u00e9s dans WIP on master: 22dc824 Revert \"feat: add note.txt\"\n\n\u276f git add note2.txt\n\n\u276f git stash\nArbre de travail et \u00e9tat de l index sauvegard\u00e9s dans WIP on master: 22dc824 Revert \"feat: add note.txt\"\n\n\u276f git status\nSur la branche master\nrien \u00e0 valider, la copie de travail est propre\n</code></pre> <p>Les deux fichiers sans visibles dans stash<pre><code>git stash list\n\nstash@{0}: WIP on master: 22dc824 Revert \"feat: add note.txt\"\nstash@{1}: WIP on master: 22dc824 Revert \"feat: add note.txt\"\n</code></pre> Pour voir quels \u00e9l\u00e9ments sont stock\u00e9s dans un des \u00e9l\u00e9ments du stash, on peut utiliser la commande <code>git stash show</code>.</p> <pre><code>\u276f git stash show stash@{0}\nnote.txt | 1 +\n1 file changed, 1 insertion(+)\n</code></pre> <p>On peut lancer plusieurs commandes \u00e0 ma suite :</p> <p><code>git stash show stash@{0} ; git stash show stash@{1}; git stash show stash@{2}</code></p> <p><code>stash</code> marche comme une pile, on est en mode LIFO : Last In First Out.</p> LIFO<pre><code>\u276f git stash pop\nSur la branche master\nModifications qui seront valid\u00e9es :\n  (utilisez \"git restore --staged &lt;fichier&gt;...\" pour d\u00e9sindexer)\n    nouveau fichier\u00a0: note2.txt\n\nrefs/stash@{0} supprim\u00e9 (f3642afe7ca5869fbc6498e693bb6745a5087db3)\n\n\n\u276f git stash pop\nSur la branche master\nModifications qui seront valid\u00e9es :\n  (utilisez \"git restore --staged &lt;fichier&gt;...\" pour d\u00e9sindexer)\n    nouveau fichier\u00a0: note.txt\n    nouveau fichier\u00a0: note2.txt\n\nrefs/stash@{0} supprim\u00e9 (1f490abc174ac6b5089f8329ac116b6db285b0eb)\n</code></pre>"},{"location":"devops/git/#git-reflog","title":"<code>git reflog</code>","text":"<pre><code>\u276f git status\nSur la branche master\nModifications qui seront valid\u00e9es :\n  (utilisez \"git restore --staged &lt;fichier&gt;...\" pour d\u00e9sindexer)\n    nouveau fichier\u00a0: note.txt\n\nFichiers non suivis:\n  (utilisez \"git add &lt;fichier&gt;...\" pour inclure dans ce qui sera valid\u00e9)\n    note2.txt\n\n\u276f git commit -m \"feat: add note.txt\"\n[master d77fcf6] feat: add note.txt\n 1 file changed, 1 insertion(+)\n create mode 100644 note.txt\n\n\u276f git add note2.txt\n\n\u276f git commit -m \"feat: add note2.txt\"\n[master 0f502d5] feat: add note2.txt\n 1 file changed, 1 insertion(+)\n create mode 100644 note2.txt\n\n\u276f git reflog\n\n0f502d5 (HEAD -&gt; master) HEAD@{0}: commit: feat: add note2.txt\nd77fcf6 HEAD@{1}: commit: feat: add note.txt\n22dc824 HEAD@{2}: reset: moving to HEAD\n22dc824 HEAD@{3}: reset: moving to HEAD\n22dc824 HEAD@{4}: reset: moving to HEAD\n22dc824 HEAD@{5}: reset: moving to HEAD\n22dc824 HEAD@{6}: reset: moving to HEAD\n22dc824 HEAD@{7}: reset: moving to HEAD\n22dc824 HEAD@{8}: reset: moving to HEAD\n22dc824 HEAD@{9}: reset: moving to HEAD~1\nc92b48a HEAD@{10}: commit: feat: add note.txt\n22dc824 HEAD@{11}: reset: moving to HEAD~1\nbe87880 HEAD@{12}: commit: feat: add note.txt\n22dc824 HEAD@{13}: revert: Revert \"feat: add note.txt\"\n4f10f7d HEAD@{14}: commit: feat: add note.txt\n83153b7 HEAD@{15}: commit (initial): feat: add story.txt\n\n\u276f git reset --hard HEAD~1\nHEAD est maintenant \u00e0 d77fcf6 feat: add note.txt\n\n\u276f git reflog\nd77fcf6 HEAD@{1}: reset: moving to HEAD~1\n0f502d5 (HEAD -&gt; master) HEAD@{2}: commit: feat: add note2.txt\nd77fcf6 HEAD@{3}: commit: feat: add note.txt\n22dc824 HEAD@{4}: reset: moving to HEAD\n22dc824 HEAD@{5}: reset: moving to HEAD\n22dc824 HEAD@{6}: reset: moving to HEAD\n22dc824 HEAD@{7}: reset: moving to HEAD\n22dc824 HEAD@{8}: reset: moving to HEAD\n22dc824 HEAD@{9}: reset: moving to HEAD\n22dc824 HEAD@{10}: reset: moving to HEAD\n22dc824 HEAD@{11}: reset: moving to HEAD~1\nc92b48a HEAD@{12}: commit: feat: add note.txt\n22dc824 HEAD@{13}: reset: moving to HEAD~1\nbe87880 HEAD@{14}: commit: feat: add note.txt\n22dc824 HEAD@{15}: revert: Revert \"feat: add note.txt\"\n4f10f7d HEAD@{16}: commit: feat: add note.txt\n83153b7 HEAD@{17}: commit (initial): feat: add story.txt\n\n\u276f git reset --hard 0f502d5\nHEAD est maintenant \u00e0 0f502d5 feat: add note2.txt\n\n\u276f git reflog\n0f502d5 (HEAD -&gt; master) HEAD@{0}: reset: moving to 0f502d5\nd77fcf6 HEAD@{1}: reset: moving to HEAD~1\n0f502d5 (HEAD -&gt; master) HEAD@{2}: commit: feat: add note2.txt\nd77fcf6 HEAD@{3}: commit: feat: add note.txt\n22dc824 HEAD@{4}: reset: moving to HEAD\n22dc824 HEAD@{5}: reset: moving to HEAD\n22dc824 HEAD@{6}: reset: moving to HEAD\n22dc824 HEAD@{7}: reset: moving to HEAD\n22dc824 HEAD@{8}: reset: moving to HEAD\n22dc824 HEAD@{9}: reset: moving to HEAD\n22dc824 HEAD@{10}: reset: moving to HEAD\n22dc824 HEAD@{11}: reset: moving to HEAD~1\nc92b48a HEAD@{12}: commit: feat: add note.txt\n22dc824 HEAD@{13}: reset: moving to HEAD~1\nbe87880 HEAD@{14}: commit: feat: add note.txt\n22dc824 HEAD@{15}: revert: Revert \"feat: add note.txt\"\n4f10f7d HEAD@{16}: commit: feat: add note.txt\n83153b7 HEAD@{17}: commit (initial): feat: add story.txt\n\n\u276f ls\n\uf15c note.txt  \uf15c note2.txt  \uf15c story.txt\n</code></pre> <ul> <li>What's the difference between git reflog and log?</li> </ul>"},{"location":"devops/git/#tags-et-release","title":"tags et release","text":""},{"location":"devops/git/#commits-conventionnels","title":"Commits Conventionnels","text":"<p>Voir la documentation</p> <p>Le commit contient les \u00e9l\u00e9ments structurels suivants, permettant de communiquer \u00e0 l\u2019intention des consommateurs de votre biblioth\u00e8que:</p> <ol> <li><code>fix</code>: un commit de type <code>fix</code> corrige un bogue dans le code (cela est en corr\u00e9lation avec <code>PATCH</code> en versioning s\u00e9mantique).</li> <li><code>feat</code>: un commit de type <code>feat</code> introduit une nouvelle fonctionnalit\u00e9 dans le code (cela est en corr\u00e9lation avec <code>MINOR</code> en versioning s\u00e9mantique).</li> <li><code>BREAKING CHANGE</code>: un commit qui a dans le pied de page le mot clef <code>BREAKING CHANGE</code>:, ou ajoute un <code>!</code> apr\u00e8s le type/scope, introduit un changement cassant l\u2019API (cela est en corr\u00e9lation avec <code>MAJOR</code> en versioning s\u00e9mantique). Un <code>BREAKING CHANGE</code> peut faire partie des commits de n\u2019importe quel type.</li> <li>Les types autre que <code>fix:</code> et <code>feat:</code> sont autoris\u00e9s, par exemple @commitlint/config-conventional (bas\u00e9 sur the Angular convention) recommande <code>build:</code>, <code>chore:</code>, <code>ci:</code>, <code>docs:</code>, <code>style:</code>, <code>refactor:</code>, <code>perf:</code>, <code>test:</code>, etc.</li> <li>Les pieds de pages autre que <code>BREAKING CHANGE: &lt;description&gt;</code> peuvent \u00eatre fourni et suivre une convention similaire \u00e0 git trailer format.</li> </ol>"},{"location":"devops/kubernetes/","title":"Kubernetes for the beginner","text":"<p>Aussi d\u00e9nomm\u00e9 k8s, kubernetes a \u00e9t\u00e9 d\u00e9velopp\u00e9 par Google, capitalisant sur leur exp\u00e9rience des conteneurs en production.</p> <p>k8s est un syst\u00e8me d'orchestration de conteneurs. Pour les conteneurs on se r\u00e9f\u00e8re \u00e0 la section pr\u00e9c\u00e9dente, parlons de l'orchestration.</p>"},{"location":"devops/kubernetes/#orchestration","title":"Orchestration","text":"<p>Avec une commande <code>docker run</code>, on est capable de d\u00e9ployer une instance d'un conteneur, par exemple une api.</p> <ul> <li>Mais que se passe-t-il si le nombre de requ\u00eates envoy\u00e9es \u00e0 cette api est trop important ? On pourrait d\u00e9ployer une seconde instance de cette api pour g\u00e9rer le flux suppl\u00e9mentaire.</li> <li>On pourrait le faire en relan\u00e7ant une commande <code>docker run</code>, mais cela demande de le faire de fa\u00e7on manuelle et de surveiller \u00e0 chaque fois comment cela se passe.</li> <li>De la m\u00eame fa\u00e7on, un fois le pic de requ\u00eate pass\u00e9, on pourrait alors vouloir supprimer certaines instances, pour r\u00e9cup\u00e9rer des ressources.</li> <li>Si le conteneur crashe, on devrait pouvoir le d\u00e9tecter et relancer une image automatiquement.</li> <li>Si le docker host crashe, tous les conteneurs lanc\u00e9s s'arr\u00eateront aussi.</li> </ul> <p>Pour g\u00e9rer ces probl\u00e8mes dans un environnement de production, on fait alors appel \u00e0 des syst\u00e8mes d'orchestrations des conteneurs :</p> <ul> <li>docker-swarm,</li> <li>kubernetes,</li> <li>MESOS.</li> </ul> <p>On se concentre ici sur kubernetes.</p> <p>Les avantages d'un orchestrateur sont alors que vos applications sont facilement et toujours disponibles, un crash \u00e9tant alors automatiquement d\u00e9tect\u00e9 et une nouvelle instance lanc\u00e9e. Plusieurs instance d'une m\u00eame application peuvent \u00eatre lanc\u00e9es en m\u00eame temps, ce qui permet d'\u00e9quilibrer le traffic.</p>"},{"location":"devops/kubernetes/#architecture-kubernetes","title":"Architecture kubernetes","text":"<p>Quand on parle de k8s, on d'un cluster kubernetes, de la m\u00eame fa\u00e7on qu'on parlerait d'un cluster de cpu travaillant de fa\u00e7on conjointe.</p> <p>L'atome d'un cluster k8s est un node (noeud), un noeud est une machine, qu'elle soit physique ou virtuelle, sur laquelle est install\u00e9 k8s. C'est sur  un noeud que se lance les conteneurs que k8s orchestre. Evidemment, si l'on a qu'un seul noeud et qu'il crashe, k8s ne pourra rien faire et donc vos conteneurs ne seront plus accessibles. C'est l\u00e0 tout l'int\u00e9r\u00eat d'un cluster centralisant plusieurs noeuds.</p> <p>Dans un cluster k8s, il y a toujours un noeud principal, le Master node, c'est sur lui que k8s est install\u00e9, et qui g\u00e8re l'ensemble des noeuds du cluster qui sont des workers nodes. C'est le noeud principal qui est responsable de l'orchestration en tant que tel.</p> <p>Lorsque l'on installe kubernetes, on installe en fait une suite de composants :</p> <ul> <li>un serveur api,</li> <li>un service <code>etcd</code>,</li> <li>un service <code>kubelet</code>,</li> <li>un <code>container runtime</code>,</li> <li>un controlleur,</li> <li>un scheduler.</li> </ul> <p>L'api est l'interface principale avec laquelle l'utilisateur interagit, les lignes de commandes, le management des conteneurs, tout se fait via cette api.</p> <p><code>etcd</code> est le syst\u00e8me de stockage interne de kubernetes, il est par exemple responsable de la configuration des logs entre les diff\u00e9rentes instances d'un m\u00eame conteneur.</p> <p>Le scheduler est celui qui distribue le travail, ou les conteneurs, aux diff\u00e9rents noeuds du cluster. Il assigne les conteneurs nouvellments cr\u00e9\u00e9s aux noeuds.</p> <p>Le controlleur est le cerveau de l'orchestration, il est responsable de la surveillance des noeuds et des contneeurs qui crashent. C'est lui qui d\u00e9cide de recr\u00e9er un nouveau conteneur si n\u00e9cessaire.</p> <p>Le <code>container runtime</code> est le software sous-jacent permettant de faire tourner les conteneurs, dans la plupart du temps, c'est containerd.</p>  <p>Remarque</p> <p>Il existe d'autres container runtime, on peut citer par exemple cri-o.</p>  <p><code>kubelet</code> est l'agent qui tourne sur chaque noeud, et qui surveille que les conteneurs pr\u00e9sents sur le noeud fonctionnent comme pr\u00e9vu.</p>"},{"location":"devops/kubernetes/#kubectl","title":"<code>kubectl</code>","text":"<p><code>kubectl</code> est l'outil de ligne de commande principal pour interagir, d\u00e9ployer et manager avec kubernetes.</p> helloworld de kubernetes<pre><code>kubectl run hello-minikube\n</code></pre> Affiche les informations du cluster<pre><code>kubectl cluster-info\n</code></pre> liste l'ensemble des noeuds du conteneur<pre><code>kubectl get nodes\n</code></pre>"},{"location":"devops/kubernetes/#installer-kubernetes","title":"Installer kubernetes","text":"<p>Pour installer k8s sur sa propre machine, il est possible d'utiliser l'une des solutions suivantes.</p> <ul> <li>Minikube</li> <li>k3s</li> <li>MicroK8s</li> <li>Kubeadm</li> </ul> <p>Dans tous les cas, on ne pourra avoir qu'un seul noeud, et il sera aussi n\u00e9cessaire d'installer un software de machine virtuelle. Sur les syst\u00e8mes Linux, l'outil de vistualisation install\u00e9 de base est KVM.</p> <p>Pour v\u00e9rifier si KVM est correctement install\u00e9 sur votre syst\u00e8me, vous pouvez v\u00e9rifier avec la commande <code>kvm-ok</code>.</p> <pre><code>\u276f kvm-ok\n\nINFO: /dev/kvm exists\nKVM acceleration can be used\n</code></pre>"},{"location":"devops/kubernetes/#references","title":"References","text":"<ul> <li> <p>Install and set up the kubectl tool: https://kubernetes.io/docs/tasks/tools/</p> </li> <li> <p>Install Minikube: https://minikube.sigs.k8s.io/docs/start/ https://kubernetes.io/fr/docs/tasks/tools/install-minikube/</p> </li> <li> <p>Install VirtualBox: https://www.virtualbox.org/wiki/Downloads https://www.virtualbox.org/wiki/Linux_Downloads</p> </li> <li> <p>Minikube Tutorial: https://kubernetes.io/docs/tutorials/hello-minikube/</p> </li> </ul>"},{"location":"devops/kubernetes/#concept-de-base-les-pods","title":"Concept de base : les Pods","text":"<p>Avec k8s, le but ultime est de d\u00e9ployer une application sous la forme de conteneurs sur un ensemble de machines configur\u00e9es comme des noeuds de travail sur un cluster.</p> <pre><code>graph LR\n\n    subgraph cluster\n        A[node]\n\n        B[node]\n\n        C[node]\n    end</code></pre> <p>Cependant, k8s ne d\u00e9ploie pas directement les conteneurs sur les noeuds de travail, les conteneurs sont encapsul\u00e9s dans un objet k8s connu sous le nom de pod.</p> <pre><code>graph LR\n\n    subgraph cluster\n        subgraph node\n            A[pod &lt;br/&gt; conteneur]\n        end\n\n        subgraph node\n            B[pod &lt;br/&gt; conteneur]\n        end\n\n        subgraph node\n            C[pod &lt;br/&gt; conteneur]\n        end\n    end</code></pre> <p>Un pod est une instance d'une application. C'est le plus petit objet que l'on puisse cr\u00e9er dans k8s.</p>  <p>Exemple</p> <p>Exemple de configuration la plus simple, un cluter avec un noeud unique, contenant un unique pod, faisant tourner un conteneur avec python 3.8 dessus.</p> <pre><code>graph LR\n\n    subgraph cluster\n        subgraph node\n            A[pod &lt;br/&gt; conteneur python:3.8]\n        end\n    end</code></pre>  <pre><code>graph LR\n\n    subgraph cluster\n        subgraph node\n            A[pod &lt;br/&gt; conteneur python:3.8]\n        end\n    end\n\nB[User1] &amp; C[User2] &amp; D[User3] &amp; E[User4] -.-&gt; A</code></pre> <p>Si beaucoup d'utilisateurs se connectent au pod et que l'on souhaite \u00e9quilibrer la charge, la solution retenue par k8s est alors de lancer un nouveau pod avec le m\u00eame conteneur sur le m\u00eame noeud.</p> <pre><code>graph LR\n\n    subgraph cluster\n        subgraph node\n            A1[pod 1&lt;br/&gt; conteneur python:3.8]\n            A2[pod 2&lt;br/&gt; conteneur python:3.8]\n        end\n    end\n\nB[User1] &amp; C[User2] -.-&gt; A1\nD[User3] &amp; E[User4] -.-&gt; A2</code></pre> <p>Et si le nombre d'utilisateur augmente encore et que le noeud n'a plus la capacit\u00e9 d'acceuillir un pod suppl\u00e9mentaire ? Il est toujours possible de rajouter un nouveau noeud avec un nouveau pod contenant le conteneur.</p> <pre><code>graph LR\n\n    subgraph cluster\n        subgraph node 1\n            A1[pod 1&lt;br/&gt; conteneur python:3.8]\n            A2[pod 2&lt;br/&gt; conteneur python:3.8]\n        end\n        subgraph node 2\n            A3[pod 3&lt;br/&gt; conteneur python:3.8]\n        end\n    end\n\nB[User1] &amp; C[User2] -.-&gt; A1\nD[User3] &amp; E[User4] -.-&gt; A2\nF[User5] &amp; G[User6] -.-&gt; A3</code></pre> <p>La plupart du temps, les pods sont en correspondance bijective avec les conteneurs faisant tourner les applications. En d'autres termes, un pod = un conteneur. Pour augmenter la charge on augmente le nombre de pods, pour la baisser on en supprime.</p>"},{"location":"devops/kubernetes/#pod-multi-conteneur","title":"POD multi-conteneur","text":"<p>La correspondance pod/conteneur n'est pas une r\u00e8gle stricte et un pod peut tr\u00e8s bien faire tourner plusieurs conteneurs. Dans la plupart des cas ces conteneurs ne sont pas identiques. On peut par exemple imaginer un pod contenant une API REST et son UI correspondante, avec les connexions reliant les deux comme le ferait docker compose. Les conteneurs \u00e0 l'int\u00e9reur d'un m\u00eame pod partage le m\u00eame namespace, storage space, r\u00e9seau, etc.</p>"},{"location":"devops/kubernetes/#les-commandes-de-base","title":"Les commandes de base","text":"<p>l'API utilis\u00e9 pour g\u00e9rer les cluster k8s est <code>kubectl</code>.</p>    Commande R\u00e9sultat Exemple     <code>kubectl run podname --image image_path</code> lance un pod nomm\u00e9 <code>podname</code> contenant l'image se trouvant \u00e0 l'adresse (cr) <code>image_path</code>. <code>kubectl run helloworld-api --image vorphus/helloworld-api:1.0</code>   <code>kubectl get pods</code> Liste l'ensemble des pods.    <code>kubectl get pods -o wide</code> Liste l'ensemble des pods avec plus d'informations.    <code>kubectl describe pod podname</code> Affiche des informations concernant le pod <code>podname</code>. <code>kubectl describe pod helloworld-api</code>   <code>kubectl delete pod podname</code> Supprime le pod <code>podname</code>. <code>kubectl delete pod helloworld-api</code>    kubectl describe pod helloworld-api<pre><code>\u276f kubectl describe pod helloworld-api\n\nName:         helloworld-api\nNamespace:    default\nPriority:     0\nNode:         minikube/192.168.39.66\nStart Time:   Thu, 07 Jul 2022 22:22:47 +0200\nLabels:       run=helloworld-api\nAnnotations:  &lt;none&gt;\nStatus:       Running\nIP:           172.17.0.3\nIPs:\n  IP:  172.17.0.3\nContainers:\n  helloworld-api:\n    Container ID:   docker://1bb8c1064c829172282c0e13fc08ef22b24755077f161352ff5a7cee240b3cf1\n    Image:          vorphus/helloworld-api:1.0\n    Image ID:       docker-pullable://vorphus/helloworld-api@sha256:e8d49d5c9fc1924f1702d7b4bc3a28bb42c639fc9f87c6c2031a45859ca2d463\n    Port:           &lt;none&gt;\n    Host Port:      &lt;none&gt;\n    State:          Running\n      Started:      Thu, 07 Jul 2022 22:22:48 +0200\n    Ready:          True\n    Restart Count:  0\n    Environment:    &lt;none&gt;\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wmjdf (ro)\nConditions:\n  Type              Status\n  Initialized       True\n  Ready             True\n  ContainersReady   True\n  PodScheduled      True\nVolumes:\n  kube-api-access-wmjdf:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       &lt;nil&gt;\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              &lt;none&gt;\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  17m   default-scheduler  Successfully assigned default/helloworld-api to minikube\n  Normal  Pulled     17m   kubelet            Container image \"vorphus/helloworld-api:1.0\" already present on machine\n  Normal  Created    17m   kubelet            Created container helloworld-api\n  Normal  Started    17m   kubelet            Started container helloworld-api\n</code></pre> <p>Pour cr\u00e9er un d\u00e9ploiement utilisant la m\u00e9thode imp\u00e9rative, on utilise la commande <code>kubectl create</code>.</p> <p><code>kubectl create deployment nginx --image=nginx</code></p> <ul> <li> <p>Kubernetes Concepts \u2013 https://kubernetes.io/docs/concepts/</p> </li> <li> <p>Pod Overview- https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/</p> </li> </ul>"},{"location":"devops/kubernetes/#concepts-pods-replicasets-deployments","title":"Concepts : PODs, ReplicaSets, Deployments","text":""},{"location":"devops/kubernetes/#creer-un-pod-via-yaml","title":"Cr\u00e9er un pod via YAML","text":"<p>k8s utilise la synthaxe YAML pour cr\u00e9er ses objets. Tous les fichiers utilis\u00e9s par k8s suivent la m\u00eame structure.</p> <p>Il y a 4 entr\u00e9e de niveau maximal (root), et ces entr\u00e9es sont n\u00e9cessaires dans un fichier yml pour k8s.</p> pod-definition.yml<pre><code>---\napiVersion: v1\nkind: Pod\nmetadata:\n    name: myapp-pod\n    labels:\n        app: my-app\n        type: front-end\n\nspec:\n    containers:\n        - name: nginx-container\n          image: nginx\n</code></pre> <ul> <li> <p><code>apiVersion</code> correspond \u00e0 la version de l'api k8s utilis\u00e9e pour cr\u00e9er les objets. En fonction de ce que l'on souhaite cr\u00e9er, il faut utiliser la bonne version. Pour cr\u00e9er des pods, on utilise la version <code>v1</code>.</p> </li> <li> <p><code>kind</code> correspond au type d'objet que l'on essaye de cr\u00e9er. On a la correspondance suivante entre <code>apiVersion</code> et <code>kind</code>.</p> </li> </ul>    kind apiVersion     POD v1   Service v1   ReplicaSet apps/v1   Deployment apps/v1    <ul> <li><code>metadata</code> correspond aux donn\u00e9es qui sont rattach\u00e9es \u00e0 l'objet lui m\u00eame, comme son nom, ses labels, etc. Par d\u00e9finition, <code>metadata</code> attend comme valeur un dictionnaire <code>yml</code>, alors que <code>apiVersion</code> et <code>kind</code> attendent un <code>string</code> comme valeur.</li> </ul>  <p>Remarque</p> <p>Les jeux cl\u00e9/valeur sous la cl\u00e9 <code>labels</code> sont compl\u00e8tement libre et \u00e0 la discr\u00e9tion de l'administrateur, ces labels servent \u00e0 cr\u00e9er des tags pour filtrer les pods.</p>  <ul> <li><code>spec</code> contient les sp\u00e9cifications techniques de l'objet que l'on cr\u00e9e, ici comme on cr\u00e9e un pod, on s'attend aux sp\u00e9cifications du ou des conteneurs adjoints au pod. Comme un pod peut contenir plusieurs conteneurs, on s'attend \u00e0 une liste sous la cl\u00e9 <code>containers</code>.</li> </ul> <p>Pour cr\u00e9er le pod \u00e0 partir de ce fichier de configuration, on utilise a commande suivante.</p> <pre><code>kubectl create -f pod-definition.yml\n</code></pre> <pre><code>kubectl get pods\n</code></pre> <pre><code>kubectl describe pod my-app-pod\n</code></pre> <pre><code>kubectl run redis --image=redis --dry-run=client -o yaml &gt; redis-definition.yaml\n</code></pre> redis-definition.yml<pre><code>---\napiVersion: v1\nkind: Pod\nmetadata:\n    creationTimestamp:\n    labels:\n        run: redis\n    name: redis\nspec:\n    containers:\n        - image: redis\n          name: redis\n          resources: {}\n    dnsPolicy: ClusterFirst\n    restartPolicy: Always\nstatus: {}\n</code></pre> <pre><code>kubectl create -f redis-definition.yaml\n</code></pre> <p>Pour mettre \u00e0 jour l'image d'un pod d\u00e9finie via la m\u00e9thode imp\u00e9rative, on peut utiliser la commande <code>kubectl edit</code>.</p> <p><code>kubectl edit pod redis</code></p> <p>Si l'on a utilis\u00e9 un fichier yaml, puis qu'on l'a \u00e9dit\u00e9 via Vim, Nano, ou autre, pour mettre \u00e0 jour le pod on utilise la commande <code>kubectl apply</code>.</p> <p><code>kubectl apply -f redis-definition.yaml</code></p>"},{"location":"devops/kubernetes/#replications","title":"R\u00e9plications","text":"<p>Les contr\u00f4leurs sont les syst\u00e8mes sous jacents qui g\u00e8rent les objets de kubernetes. En particulier les r\u00e9plications.</p> <p>Si l'on reprend notre cas d'usage tr\u00e8s basique d'un unique conteneur dans un unique pod, alors si le pod crashe l'utilisateur n'a plus aucun acc\u00e8s  et perd toutes les informations.</p> <pre><code>graph LR\n\n    subgraph cluster\n        subgraph node\n            A[pod &lt;br/&gt; conteneur python:3.8]\n        end\n    end</code></pre> <p>Le contr\u00f4leur de r\u00e9plication permet de g\u00e9rer cela, c'est lui qui se charge de faire tourner plusieurs instances d'un m\u00eame pod pour \u00e9viter les coup\u00fbres de service.</p> <pre><code>graph LR\n    subgraph node\n        subgraph \"replication controller\"\n                A1[pod &lt;br/&gt; conteneur python:3.8]\n\n                A2[pod &lt;br/&gt; conteneur python:3.8]\n        end\n    end</code></pre> <p>Notez que le contr\u00f4leur de r\u00e9plications se place au niveau du noeud, et pas du cluster. Le contr\u00f4leur s'assure que le nombre de pods sp\u00e9cifi\u00e9s est pr\u00e9sent tout le temps, m\u00eame si ce nombre est 1.</p> <p>Le contr\u00f4leur s'occupe aussi du load balancing et du scaling des pods via les diff\u00e9rents noeuds du cluster.</p>"},{"location":"devops/kubernetes/#replicaset-controleur-de-replications","title":"ReplicaSet, contr\u00f4leur de r\u00e9plications","text":"<p>Pour cr\u00e9er et g\u00e9rer des r\u00e9plications, il y a deux m\u00e9thodes dans k8s :</p> <ul> <li>D\u00e9finir un <code>ReplicationController</code>.</li> <li>D\u00e9finir un <code>ReplicaSet</code>.</li> </ul> <p><code>ReplicaSet</code> est la fa\u00e7on la plus moderne de d\u00e9finir des r\u00e9plications dans k8s, mais les deux sont valides et sont sensiblement similaires dans leur \u00e9criture.</p>"},{"location":"devops/kubernetes/#replicationcontroller","title":"<code>ReplicationController</code>","text":"<p>Pour d\u00e9finir un <code>ReplicationController</code>, on utilise la m\u00e9thode suivante.</p> rc-definition.yml<pre><code>---\napiVersion: v1\nkind: ReplicationController\nmetadata:\n    name: myapp-rc\n    labels:\n        app: myapp\n        type: front-end\n\nspec:\n    template:\n        metadata:\n            name: myapp-pod\n            labels:\n                app: myapp\n                type: front-end\n        spec:\n            containers:\n                - name: nginx-container\n                  image: nginx\n\n    replicas: 3\n</code></pre> <p>La premi\u00e8re partie :</p> <p><pre><code>apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: myapp-rc\n  labels:\n    app: myapp\n    type: front-end\n</code></pre> est identique \u00e0 celle d\u00e9finie dans la r\u00e9daction d'un pod : <code>Pod</code> et <code>ReplicationController</code> utilise la m\u00eame version de l'api et les <code>metadata</code> ont la m\u00eame signification dans les deux cas.</p> <p>La deuxi\u00e8me partie elle est sp\u00e9cifique au choix <code>ReplicationController</code>. La partie <code>spec</code> d\u00e9finit quel type d'objet le <code>ReplicationController</code> doit g\u00e9n\u00e9rer. Pour cela il a besoin :</p> <ol> <li>du <code>template</code> de l'objet qu'il va g\u00e9n\u00e9rer</li> <li>du nombre de r\u00e9pliques qu'il doit g\u00e9n\u00e9rer (<code>replicas</code>).</li> </ol> <pre><code>spec:\n  template:\n\n  replicas: n\n</code></pre> <p>Dans notre cas, l'objet que l'on souhaite g\u00e9rer avec le <code>ReplicationController</code> \u00e9tant un <code>Pod</code>, la partie <code>template</code> devra donc contenir les <code>spec</code> caract\u00e9ristiques d'un pod.</p> <pre><code>spec:\n  template:\n    metadata:\n      name: myapp-pod\n      labels:\n        app: myapp\n        type: front-end\n    spec:\n      containers:\n        - name: nginx-container\n          image: nginx\n\n  replicas: 3\n</code></pre> <p>Remarquez que la partie</p> <pre><code>    metadata:\n      name: myapp-pod\n      labels:\n        app: myapp\n        type: front-end\n    spec:\n      containers:\n        - name: nginx-container\n          image: nginx\n</code></pre> <p>est exactement celle qui est d\u00e9finie, en dehors de <code>apiVersion</code> et <code>kind</code>, dans un fichier yaml pour cr\u00e9er un pod.</p> <p>Si l'on veut cr\u00e9er 3 r\u00e9pliques, on sp\u00e9cifit dans la partie <code>replicas</code> le nombre 3.</p> <p>On peut alors le cr\u00e9er avec la commande suivante.</p> <pre><code>kubectl create -f rc-definition.yml\n</code></pre> <p>Pour voir l'objet <code>ReplicationController</code> on tape la commande suivante.</p> <pre><code>kubectl get replicationcontroller\n</code></pre> <p>On peut bien sur voir les pods cr\u00e9\u00e9s avec la commande suivante.</p> <pre><code>kubectl get pods\n</code></pre> <p>Enfin, pour le d\u00e9truire, on utilise la commande suivante.</p> <pre><code>kubectl delete replicationcontroller myapp-rc\n</code></pre>"},{"location":"devops/kubernetes/#replicaset","title":"<code>ReplicaSet</code>","text":"<p>Pour d\u00e9finir un <code>ReplicaSet</code>, on utilise la m\u00e9thode suivante.</p> replicaset-definition.yml<pre><code>---\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n    name: myapp-replicaset\n    labels:\n        app: myapp\n        type: front-end\n\nspec:\n    template:\n        metadata:\n            name: myapp-pod\n            labels:\n                app: myapp\n                type: front-end\n        spec:\n            containers:\n                - name: nginx-container\n                  image: nginx\n\n    replicas: 3\n    selector:\n        matchLabels:\n            type: front-end\n</code></pre> <p>Peu de choses changent par rapport au fichier d'un <code>ReplicationController</code>.</p> <ol> <li>On doit changer la version : <code>apps/v1</code>.</li> <li>Le <code>kind</code> change : <code>ReplicaSet</code></li> <li>La partie <code>template</code> a exactement la m\u00eame fonction que pour <code>ReplicationController</code>.</li> </ol> <p>La partie que change vraiment dans <code>spec</code> est la cl\u00e9 <code>selector</code>. C'est cette partie qui permet au <code>ReplicaSet</code> d'identifier quels sont les pods dont il doit se charger.</p>  <p>Question</p> <p>Pourquoi d\u00e9finir quels sont les pods dont il doit se charger si l'on a d\u00e9j\u00e0 d\u00e9fini un template de pods dans le <code>ReplicaSet</code> ?</p>  <p>C'est parce que les <code>ReplicaSet</code> sont aussi capables de g\u00e9rer des pods qui n'ont pas \u00e9t\u00e9 cr\u00e9\u00e9s en m\u00eame temps que le <code>ReplicaSet</code>, m\u00eame si ces pods ne sont pas dans le m\u00eame noeud.</p> <p>Une des fa\u00e7ons d'identifier les pods que le <code>ReplicaSet</code> doit prendre en compte est de filtrer sur les labels d\u00e9finis dans les <code>metadata</code> des pods. Pour cela on utilise la cl\u00e9 <code>matchLabels</code>.</p> ReplicaSet va g\u00e9rer tous les pods avec le label type: front-end<pre><code>    selector:\n        matchLabels:\n            type: front-end\n</code></pre> <p>Le <code>selector</code> est la diff\u00e9rence majeure entre <code>ReplicationController</code> et <code>ReplicaSet</code>.</p> <p>On peut alors le cr\u00e9er avec la commande suivante.</p> <pre><code>kubectl create -f replicaset-definition.yml\n</code></pre> <p>Pour voir l'objet <code>ReplicaSet</code> on tape la commande suivante.</p> <p><pre><code>kubectl get replicaset\n</code></pre> <pre><code>kubectl get rs\n</code></pre></p> <pre><code>kubectl describre replicaset replicaset-name\n</code></pre> <p>On peut bien sur voir les pods cr\u00e9\u00e9s avec la commande suivante.</p> <p><pre><code>kubectl get pods\n</code></pre> Enfin, pour le d\u00e9truire, on utilise la commande suivante.</p> <p><pre><code>kubectl delete replicaset myapp-replicaset\n</code></pre> <pre><code>kubectl edit replicaset myapp-replicaset\n</code></pre> Dans l'optique o\u00f9 le <code>ReplicaSet</code> est cr\u00e9\u00e9 apr\u00e8s que les pods avec les bons labels ont \u00e9t\u00e9 cr\u00e9\u00e9s, le <code>ReplicaSet</code> a tout de m\u00eame besoin de la section <code>template</code>. Comme il est cens\u00e9 monitorer les pods et s'assurer que tous sont dispos, si des pods monitor\u00e9s crashe, le <code>ReplicaSet</code> a besoin de conna\u00eetre le <code>template</code> sur lequel il doit se baser pour recr\u00e9er un nouveau pod.</p>"},{"location":"devops/kubernetes/#augmenter-le-nombre-de-repliques","title":"Augmenter le nombre de r\u00e9pliques","text":"<p>Si l'on souhaite augmenter le nombre de r\u00e9pliques d'un pod, dans un <code>ReplicaSet</code> ou un <code>ReplicationController</code>, une des solutions est de modifier en cons\u00e9quence la section <code>replicas</code> de ces fichiers et de lancer une misa \u00e0 jour avec la commande suivante.</p> <pre><code>kubectl replace -f file.yml\n</code></pre> <p>Une autre m\u00e9thode est d'utiliser la commande <code>kubectl scale</code> :</p> <pre><code>kubectl scale --replicas=6 -f file.yml\n</code></pre> <pre><code>kubectl scale --replicas=6 replicaset myapp-replicaset\n</code></pre>"},{"location":"devops/kubernetes/#deployment","title":"<code>Deployment</code>","text":"<p>Comment d\u00e9ployer son application dans un environnement de production ? En particulier on souhaite les caract\u00e9ristiques suivantes.</p> <ol> <li>D\u00e9ployer des instances de pods dans un <code>ReplicaSet</code> de fa\u00e7on simple.</li> <li>Mettre \u00e0 jour les conteneurs depuis le registre de fa\u00e7on automatis\u00e9e.</li> <li>Faire des mises \u00e0 jour glissante pour \u00e9viter de rendre l'application compl\u00e8tement inaccessible au moment du d\u00e9ploiement de la maj.</li> <li>Faire un rollback si n\u00e9cessaire.</li> <li>Mettre en pause les changements.</li> <li>Les relancer.</li> </ol> <p>Tout \u00e7a se fait gr\u00e2ce \u00e0 l'objet <code>Deployment</code> de k8s, qui est un objet de plus haut niveau que <code>ReplicaSet</code>.</p> <pre><code>graph TD\n    subgraph Deployment\n        subgraph ReplicaSet\n                A1[pod &lt;br/&gt; conteneur python:3.8]\n                A2[pod &lt;br/&gt; conteneur python:3.8]\n                A3[pod &lt;br/&gt; conteneur python:3.8]\n                A4[pod &lt;br/&gt; conteneur python:3.8]\n                A5[pod &lt;br/&gt; conteneur python:3.8]\n        end\n    end</code></pre> <p>deployment-definition.yml<pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n    name: myapp-deployment\n    labels:\n        app: myapp\n        type: front-end\n\nspec:\n    template:\n        metadata:\n            name: myapp-pod\n            labels:\n                app: myapp\n                type: front-end\n        spec:\n            containers:\n                - name: nginx-container\n                  image: nginx\n\n    replicas: 3\n    selector:\n        matchLabels:\n            type: front-end\n</code></pre> On peut alors le cr\u00e9er avec la commande suivante.</p> <pre><code>kubectl create -f deployment-definition.yml\n</code></pre> <p><pre><code>kubectl create -f deployment-definition.yml --record\n</code></pre> permet d'enregistrer les causes de changement.</p> <p>Pour voir l'objet <code>Deployment</code> on tape la commande suivante.</p> <pre><code>kubectl get deployment\n</code></pre> <pre><code>kubectl describre deployment deployment-name\n</code></pre> <p>Un <code>Deployment</code> cr\u00e9e un <code>ReplicaSet</code>, que l'on peut voir avec la commande suivante.</p> <pre><code>kubectl get replicasets\n</code></pre> <p>Qui lui m\u00eame cr\u00e9e des pods que l'on peut voir avec la commande suivante.</p> <p><pre><code>kubectl get pods\n</code></pre> Enfin, pour le d\u00e9truire, on utilise la commande suivante.</p> <p><pre><code>kubectl delete deployment myapp-deployment\n</code></pre> <pre><code>kubectl edit deployment myapp-deployment\n</code></pre></p> <p>Pour voir tous les objets k8s d'un m\u00eame namespace, il est possible de d'utiliser la commande suivante.</p> <pre><code>kubectl get all\n</code></pre>"},{"location":"devops/kubernetes/#updates-et-rollback","title":"Updates et Rollback","text":"<p>Un <code>Deployment</code> d\u00e9clenche un <code>rollout</code>. Chaque nouveau <code>rollout</code> cr\u00e9e une \"R\u00e9vision de d\u00e9ploiement\".</p> <pre><code>graph TD\n    subgraph Deployment Revision 1\n        subgraph ReplicaSet1\n                A1[pod &lt;br/&gt; conteneur python:3.8]\n                A2[pod &lt;br/&gt; conteneur python:3.8]\n                A3[pod &lt;br/&gt; conteneur python:3.8]\n                A4[pod &lt;br/&gt; conteneur python:3.8]\n                A5[pod &lt;br/&gt; conteneur python:3.8]\n        end\n    end\n</code></pre> <p>Lorsque que l'application est mise \u00e0 jour, eg le conteneur change de version, un nouveau <code>rollout</code> est d\u00e9clench\u00e9 et cr\u00e9e une nouvelle \"R\u00e9vision de d\u00e9ploiement\".</p> <pre><code>graph TD\n    subgraph Deployment Revision 2\n        subgraph ReplicaSet2\n                B1[pod &lt;br/&gt; conteneur python:3.9]\n                B2[pod &lt;br/&gt; conteneur python:3.9]\n                B3[pod &lt;br/&gt; conteneur python:3.9]\n                B4[pod &lt;br/&gt; conteneur python:3.9]\n                B5[pod &lt;br/&gt; conteneur python:3.9]\n        end\n    end</code></pre> <p>C'est ce qui permet de monitorer les d\u00e9ploiements et de revenir en arri\u00e8re si n\u00e9cessaire.</p> <p>On peut voir le status du rollout avec la commande suivante.</p> <p><pre><code>kubectl rollout status deployment_name\n</code></pre> <pre><code>kubectl rollout history deployment_name\n</code></pre></p> <p>La strat\u00e9gie par d\u00e9faut pour mettre \u00e0 jour le d\u00e9ploiement est de mettre \u00e0 jour les pods un par un au fur et \u00e0 mesure de leur disponibilit\u00e9, plut\u00f4t que de tous les supprimer d'un coup et de rendre l'application incaccessible. C'est ce que k8s appelle un \"rolling update\".</p> <p>Pratiquement, comment on fait la mise \u00e0 jour ? Une fois les modifications faites dans le fichier de configuration <code>.yml</code>, on utilise la commande suivante.</p> <pre><code>kubectl apply -f deployment_file.yml\n</code></pre> <p>Pour annuler la mise \u00e0 jour et faire un rollback, on utilise la commande suivante.</p> <pre><code>kubectl rollout undo deployment/deployment_name\n</code></pre>"},{"location":"devops/kubernetes/#notion-de-reseaux-dans-kubernetes","title":"Notion de r\u00e9seaux dans kubernetes","text":""},{"location":"devops/kubernetes/#cas-dun-noeud-unique","title":"Cas d'un noeud unique","text":"<p>Commen\u00e7ons simplement, un cluster avec un unique noeud contenant un unique pod python3.8.</p> <pre><code>graph LR\n\n    subgraph cluster\n        subgraph node\n            A[pod &lt;br/&gt; conteneur python:3.8]\n        end\n    end</code></pre> <p>Ce noeud poss\u00e8de une adresse ip, <code>192.168.1.2</code>, que l'on utilise pour acc\u00e9der au noeud kubernetes. Par exemple via ssh.</p> <pre><code>graph LR\n\n    subgraph cluster\n        subgraph node\n            A[pod &lt;br/&gt; conteneur python:3.8]\n            B{{192.168.1.2}}\n        end\n    end</code></pre>  <p>Remarque</p> <p>Dans le cas de Minikube, l'adresse ip dont on parle ici est l'adresse ip du noeud Minikube \u00e0 l'int\u00e9rieur de l'hyperviseur. Votre syst\u00e8me (Linux, Windows, etc) peut avoir une autre adresse.</p> <pre><code>graph LR\n    subgraph Syst\u00e8me\n        subgraph noeud Minikube\n            A[pod &lt;br/&gt; conteneur python:3.8]\n            B{{192.168.1.2}}\n        end\n            C{{192.168.1.10}}\n    end</code></pre>  <p>A la diff\u00e9rence de Docker o\u00f9 une adresse ip est associ\u00e9e \u00e0 un conteneur, dans kubernetes une adresse ip est assign\u00e9e \u00e0 un pod. Chaque pod poss\u00e8de donc sa propre adresse ip.</p> <pre><code>graph LR\n\n    subgraph cluster\n        subgraph node\n            A[pod &lt;br/&gt; conteneur python:3.8 &lt;/br&gt; 10.244.0.2]\n            B{{192.168.1.2}}\n        end\n    end</code></pre> <p>Disons qu'ici l'adresse ip du pod est <code>10.224.0.2</code>.</p> <p>Lorsque kubernetes est initialis\u00e9, un r\u00e9seau priv\u00e9 interne (avec l'adresse <code>10.244.0.0</code>) est alors cr\u00e9\u00e9 et tous les pods lui sont rattach\u00e9s.</p> <pre><code>graph LR\n\n    subgraph cluster\n        subgraph node\n            A[pod &lt;br/&gt; conteneur python:3.8 &lt;/br&gt; 10.244.0.2]\n            B{{192.168.1.2}}\n            C[[10.244.0.0]]\n        end\n    end\n    C-.-A</code></pre> <p>Chaque pod d\u00e9ploy\u00e9 se voit alors assigner une adresse ip par ce r\u00e9seau. Il peuvent communiquer entre eux via cette ip.</p> <pre><code>graph LR\n\n    subgraph cluster\n        subgraph node\n            A1[pod &lt;br/&gt; conteneur python:3.8 &lt;/br&gt; 10.244.0.2]\n            A2[pod &lt;br/&gt; conteneur python:3.8 &lt;/br&gt; 10.244.0.3]\n            A3[pod &lt;br/&gt; conteneur python:3.8 &lt;/br&gt; 10.244.0.4]\n            B{{192.168.1.2}}\n            C[[10.244.0.0]]\n        end\n    end\n    C-.-A1 &amp; A2 &amp; A3</code></pre>"},{"location":"devops/kubernetes/#cas-de-plusieurs-noeuds","title":"Cas de plusieurs noeuds","text":"<p>On consid\u00e8re 2 noeuds, chacun de ces noeuds poss\u00e8de un pod et un r\u00e9seau priv\u00e9 fournissant l'adresse ip du pod.</p> <pre><code>graph LR\n\n    subgraph cluster\n        subgraph node1\n            A1[pod &lt;br/&gt; conteneur python:3.8 &lt;/br&gt; 10.244.0.2]\n            C1[[10.244.0.0]]\n            B1{{192.168.1.2}}\n        end\n\n        subgraph node2\n            A2[pod &lt;br/&gt; conteneur python:3.8 &lt;/br&gt; 10.244.0.2]\n            B2{{192.168.1.3}}\n            C2[[10.244.0.0]]\n        end\n    C1 -.- Error -.- C2\n    end\n\n    A1 -.- C1\n    A2 -.- C2</code></pre> <p>Ces pods et r\u00e9seaux internes ayant la m\u00eame adresse ip, la communication entre ces deux noeuds dans un cluster sera impossible.</p> <p>De fa\u00e7on g\u00e9n\u00e9rale dans un cluster, kubenertes ne g\u00e8re pas de fa\u00e7on automatique les r\u00e9seaux. Il s'attend \u00e0 ce soit nous qui g\u00e9rions la configuration r\u00e9seau.</p> <p>Dans la configuration du r\u00e9seau, kubernetes demande que ces deux conditions soient remplies pour pouvoir fonctionner.</p> <ul> <li>Tous les pods, conteneurs, doivent pouvoir communiquer les uns avec les autres sans traduction d'adresse r\u00e9seau (NAT).</li> <li>Tous les noeuds peuvent communiquer avec tous les conteneurs, et inversement, sans traduction d'adresse r\u00e9seau.</li> </ul>  <p>Network address translation</p> <p>En r\u00e9seau informatique, on dit qu'un routeur fait du network address translation (NAT, \"traduction d'adresse r\u00e9seau\" ou parfois \"translation d'adresse r\u00e9seau\") lorsqu'il fait correspondre des adresses IP \u00e0 d'autres adresses IP.</p> <p>En particulier, un cas courant est de permettre \u00e0 des machines disposant d'adresses priv\u00e9es qui font partie d'un intranet et ne sont ni uniques ni routables \u00e0 l'\u00e9chelle d'Internet, de communiquer avec le reste d'Internet en utilisant vers l'ext\u00e9rieur des adresses externes publiques, uniques et routables.</p> <p>Wikipedia</p>  <p>Kubernetes s'attend donc \u00e0 ce que l'on mette en place une solution r\u00e9seau qui satisfait ces 2 crit\u00e8res.</p>  <p>Exemples</p> <p>Pour pouvoir mettre en place une telle configuration r\u00e9seau, des solutions open source existent, telles que Calico, flannel, ou VMWare NSX dans un environnement VMWare.</p>  <pre><code>graph LR\n\n    subgraph cluster\n        subgraph node1\n            A1[pod &lt;br/&gt; conteneur python:3.8 &lt;/br&gt; 10.244.0.2]\n            C1[[10.244.0.0]]\n            B1{{192.168.1.2}}\n        end\n\n        subgraph node2\n            A2[pod &lt;br/&gt; conteneur python:3.8 &lt;/br&gt; 10.244.1.2]\n            C2[[10.244.1.0]]\n            B2{{192.168.1.3}}\n        end\n    D[[Routing]]\n    C1 -.- D -.- C2\n    end\n\n    A1 -.- C1\n    A2 -.- C2</code></pre>"},{"location":"devops/kubernetes/#les-services","title":"Les <code>Services</code>","text":"<p>Les services permettent la communication entre diff\u00e9rents composants \u00e0 l'int\u00e9rieur et \u00e0 l'exterieur de kubernetes.</p> <pre><code>graph TD\n\n    subgraph cluster\n        subgraph node\n            A[Utilisateur]\n            B[pod &lt;br/&gt; conteneur frontend]\n            C[pod &lt;br/&gt; conteneur backend]\n            D[pod &lt;br/&gt; conteneur backend]\n\n            A -.- Service1 -.- B\n            C -.- Service2 -.- B\n            D -.- Service3 -.- B\n        end\n\n    end</code></pre> <p>Les services permettent donc un couplage faible entre les diff\u00e9rents composants n\u00e9cessaires.</p> <p>Un service est simplement un moyen de faire correspondre logiquement des pods avec des politiques pour y acc\u00e9der. N'oubliez pas que les pods sont des ressources \u00e9ph\u00e9m\u00e8res et qu'aucun pod n'est jamais recr\u00e9\u00e9 une fois qu'il a \u00e9t\u00e9 supprim\u00e9. Les services offrent donc un moyen de fournir une connexion \u00e0 ces pods, quelle que soit leur adresse IP. Ceci est r\u00e9alis\u00e9 par l'utilisation du <code>selector</code>. Habituellement, dans le manifeste d'un d\u00e9ploiement, des tags sont fournies dans le mod\u00e8le du pod ce qui sert \u00e0 identifier et \u00e0 regrouper les pod en fonction de nos besoins. Des <code>selector</code> sont ensuite d\u00e9finis dans le manifeste des services qui correspondent aux tags d\u00e9finis dans le manifeste du d\u00e9ploiement ou du pod. Pour chaque pod qui correspond au s\u00e9lecteur, un endpoint est cr\u00e9\u00e9 pour ce pod.</p> <p>Nous allons maintenant discuter des diff\u00e9rents types de services offerts (du moins les plus connus), du plus restreint au plus ouvert.</p>"},{"location":"devops/kubernetes/#clusterip","title":"ClusterIP","text":""},{"location":"devops/kubernetes/#nodeport","title":"NodePort","text":"<p>Prenons l'exemple suivant. On a un noeud d'ont l'adresse IP est <code>192.168.1.2</code> et un pod dont l'ip <code>10.244.0.2</code> est h\u00e9rit\u00e9e du r\u00e9seau <code>10.244.0.0</code>.</p> <p>On a aussi un laptop qui est sur le m\u00eame r\u00e9seau que le noeud.</p> <pre><code>graph LR\n\n    subgraph cluster\n        subgraph node\n            A1[pod &lt;br/&gt; conteneur FastAPI &lt;/br&gt; 10.244.0.2]\n            B{{192.168.1.2}}\n            C[[10.244.0.0]]\n        end\n    end\n    C-.-A1\n\n    D[Laptop &lt;/br&gt; 192.168.1.10]</code></pre>  <p>Question</p> <p>Comment acc\u00e9der au pod depuis le laptop ?</p>  <p>Si l'on se connecte en ssh au noeud, on sera alors capable d'appeler le pod via un <code>curl http://10.244.0.2</code>.</p> <pre><code>graph LR\n\n    subgraph cluster\n        subgraph node\n            A1[pod &lt;br/&gt; conteneur FastAPI &lt;/br&gt; 10.244.0.2]\n            B{{192.168.1.2}}\n            C[[10.244.0.0]]\n        end\n    end\n    C-.-A1\n\n    D[Laptop &lt;/br&gt; 192.168.1.10]\n    D -.- ssh -.- B</code></pre> <p>Mais on souhaite simplement \u00eatre capable d'appeler le noeud via un <code>curl http://192.168.1.2</code>. On a donc besoin d'un int\u00e9rm\u00e9diaire capable de faire le routage entre le laptop et le pod contenu dans le noeud.</p> <p>Une telle solution est fournie par le service <code>NodePort</code>, qui mappe un port du noeud var un port du pod.</p> <pre><code>graph LR\n\n    subgraph cluster\n        subgraph node\n            A1[pod &lt;br/&gt; conteneur FastAPI &lt;/br&gt; 10.244.0.2]\n            B{{192.168.1.2}}\n            C[[10.244.0.0]]\n            E[port &lt;/br&gt; 30008]\n            F[Service]\n        end\n    end\n    C-.-A1\n\n    D[Laptop &lt;/br&gt; 192.168.1.10]\n    D -.- E -.- F -.- A1</code></pre> <p>Dans le d\u00e9tail, on a 3 ports qui sont impliqu\u00e9s :</p> <pre><code>graph LR\n\n    subgraph cluster\n        subgraph node\n            A1[pod &lt;br/&gt; port 80 &lt;/br&gt; 10.244.0.2]\n            E[port &lt;/br&gt; 30008]\n            F[Service &lt;/br&gt; port 80]\n        end\n    end\n\n    E -.- F -.- A1</code></pre> <ul> <li>Le port 80 du pod, sur lequel tourne l'api, c'est celui auquel on veut avoir acc\u00e8s. On le nomme le <code>targetPort</code>.</li> <li>Le port 80 du service, simplement nomm\u00e9 <code>port</code>.</li> <li>Le port 30008 sur le noeud lui m\u00eame qui permet un acc\u00e8s externe, le <code>nodePort</code>.</li> </ul>  <p>Attention</p> <p>Le port sur le noeud a pour valeur 30008 et sur un noeud les ports ne peuvent avoir que des valeurs comprises entre 30000 et 32767.</p>  <p>Les termes donn\u00e9s ici sont du point du service. Le service est comme un serveur virtuel \u00e0 l'int\u00e9rieur du noeud. A l'int\u00e9rieur du cluster, il poss\u00e8de sa propre adresse IP (l'ip cluster du service).</p>  <p>Question</p> <p>Comment sp\u00e9cifier au service que le port 30 correspond au bon pod ? Il peut y avoir plusieurs milliers de pods dans le m\u00eame noeud.</p>  <p>De la m\u00eame fa\u00e7on que pour les <code>ReplicaSet</code>, les services poss\u00e8dent une cl\u00e9 <code>selector</code> dans <code>spec</code> qui permet au service de filtrer les pods sur lesquels il doit s'appliquer par rapport aux tags.</p> <p>Ainsi, pour le pod suivant qui poss\u00e8de les tags <code>app: myapp</code> et <code>type: front-end</code>, il suffit de les renseigner dans le <code>selector</code> du service.</p> pod-definition.yml<pre><code>---\napiVersion: v1\nkind: Pod\nmetadata:\n    name: myapp-pod\n    labels:\n        app: my-app\n        type: front-end\n\nspec:\n    containers:\n        - name: nginx-container\n          image: nginx\n</code></pre> <p>De cette fa\u00e7on l\u00e0.</p> service-definition.yml<pre><code>---\napiVersion: v1\nkind: Service\nmetadata:\n    name: myapp-service\n\nspec:\n    type: NodePort\n    ports:\n        - targetPort: 80\n          port: 80\n          NodePort: 30008\n\n    selector:\n        app: my-app\n        type: front-end\n</code></pre>  <p>Remarque</p> <p>Si <code>targetPort</code> n'est pas d\u00e9fini, k8s supposera qu'il a la m\u00eame valeur que <code>port</code>.</p>  <p>Si on a plusieurs pod avec le m\u00eame tags, ie des pods d\u00e9finis via un <code>ReplicaSet</code> ou un <code>Deployment</code> le service de chargera du routage vers l'ensemble de ces r\u00e9pliques et agira comme load balancer, que les pods soient dans le m\u00eame noeud ou non.</p> <p>Par cons\u00e9quent,</p> <ul> <li>un seul pod dans un seul noeud,</li> <li>plusieurs pods dans un seul noeud,</li> <li>plusieurs pods dans plusieurs noeuds,</li> </ul> <p>cela ne change rien \u00e0 la m\u00e9thode de d\u00e9finition d'un service <code>NodePort</code> car kubernetes s'assure de le d\u00e9finir de mani\u00e8re transverse.</p> <p>Pour lancer le service, on utilise la commande suivante.</p> <p><code>kubectl create -f service-definition.yaml</code></p> <p>Pour voir l'ensemble des services lanc\u00e9s dans le cluster, on peut utiliser la commande <code>kubectl get svc</code>, <code>svc</code> d\u00e9sigant service control.</p> <p>Sur Minikube, pour avoir acc\u00e8s \u00e0 l'adresse qu'il faut utiliser pour contacter le pod, on peut taper la commande suivante.</p> <pre><code>minikube service myapp-service --url\n</code></pre> <p>La commande nous affichera l'adresse pour contacter le pod, par exemple <code>http://192.168.99.101:30004</code>.</p>"},{"location":"devops/kubernetes/#loadblancer","title":"Loadblancer","text":"<p>Ne fonctionne que sur les plateformes Cloud.</p>"},{"location":"devops/kubernetes/#k8s-snippets","title":"k8s snippets","text":"<pre><code>{\n    \"k8s Pod\": {\n        \"prefix\": \"k-pod\",\n        \"body\": [\n            \"apiVersion: v1\",\n            \"kind: Pod\",\n            \"metadata:\",\n            \"\\tcreationTimestamp:\",\n            \"\\tname: pod-name\",\n            \"\\tlabels:\",\n            \"\\t\\tapp: my-app\",\n            \"\\t\\ttype: front-end\",\n            \"\\t\\tkey3: value3\",\n\n            \"spec:\",\n            \"\\tcontainers:\",\n            \"\\t\\t- name: nginx-container\",\n            \"\\t\\t  image: nginx\",\n            \"\\t\\t  resources: {}\",\n            \"\\trestartPolicy: Always\",\n\n            \"status: {}\\n\",\n            \"---\",\n\n        ],\n        \"description\": \"k8s Pod\"\n    },\n\n    \"k8s ReplicationController\": {\n        \"prefix\": \"k-replication-controller\",\n        \"body\": [\n            \"apiVersion: v1\",\n            \"kind: ReplicationController\",\n            \"metadata:\",\n            \"\\tname: rc-name\",\n            \"\\tlabels:\",\n            \"\\t\\tapp: my-app\",\n            \"\\t\\ttype: front-end\",\n            \"\\t\\tkey3: value3\",\n\n            \"spec:\",\n            \"\\ttemplate:\",\n            \"\\t\\tmetadata:\",\n            \"\\t\\t\\tname: rc-name\",\n            \"\\t\\t\\tlabels:\",\n            \"\\t\\t\\t\\tapp: my-app\",\n            \"\\t\\t\\t\\ttype: front-end\",\n            \"\\t\\t\\t\\tkey3: value3\",\n            \"\\t\\tspec:\",\n            \"\\t\\t\\tcontainers:\",\n            \"\\t\\t\\t\\t- name: container-name\",\n            \"\\t\\t\\t\\t  image: image-name\",\n\n            \"\\treplicas: 3\\n\",\n            \"---\",\n        ],\n        \"description\": \"k8s ReplicationController\"\n    },\n\n    \"k8s ReplicaSet\": {\n        \"prefix\": \"k-replicaset\",\n        \"body\": [\n            \"apiVersion: apps/v1\",\n            \"kind: ReplicaSet\",\n            \"metadata:\",\n            \"\\tname: replicaset-name\",\n            \"\\tlabels:\",\n            \"\\t\\tapp: my-app\",\n            \"\\t\\ttype: front-end\",\n            \"\\t\\tkey3: value3\",\n\n            \"spec:\",\n            \"\\ttemplate:\",\n            \"\\t\\tmetadata:\",\n            \"\\t\\t\\tname: rc-name\",\n            \"\\t\\t\\tlabels:\",\n            \"\\t\\t\\t\\tapp: my-app\",\n            \"\\t\\t\\t\\ttype: front-end\",\n            \"\\t\\t\\t\\tkey3: value3\",\n            \"\\t\\tspec:\",\n            \"\\t\\t\\tcontainers:\",\n            \"\\t\\t\\t\\t- name: container-name\",\n            \"\\t\\t\\t\\t  image: image-name\",\n\n            \"\\treplicas: 3\",\n            \"\\tselector:\",\n            \"\\t\\tmatchLabels:\",\n            \"\\t\\t\\ttype: front-end\\n\",\n            \"---\",\n        ],\n        \"description\": \"k8s ReplicaSet\"\n    },\n\n    \"k8s Deployment\": {\n        \"prefix\": \"k-deployment\",\n        \"body\": [\n            \"apiVersion: apps/v1\",\n            \"kind: Deployment\",\n            \"metadata:\",\n            \"\\tname: myapp-deployment\",\n            \"\\tlabels:\",\n            \"\\t\\tapp: my-app\",\n            \"\\t\\ttype: front-end\",\n            \"\\t\\tkey3: value3\",\n\n            \"spec:\",\n            \"\\ttemplate:\",\n            \"\\t\\tmetadata:\",\n            \"\\t\\t\\tname: myapp-pod\",\n            \"\\t\\t\\tlabels:\",\n            \"\\t\\t\\t\\tapp: my-app\",\n            \"\\t\\t\\t\\ttype: front-end\",\n            \"\\t\\t\\t\\tkey3: value3\",\n            \"\\t\\tspec:\",\n            \"\\t\\t\\tcontainers:\",\n            \"\\t\\t\\t\\t- name: container-name\",\n            \"\\t\\t\\t\\t  image: image-name\",\n\n            \"\\treplicas: 3\",\n            \"\\tselector:\",\n            \"\\t\\tmatchLabels:\",\n            \"\\t\\t\\ttype: front-end\\n\",\n            \"---\",\n        ],\n        \"description\": \"k8s Deployment\"\n    },\n\n    \"k8s NodePort\": {\n        \"prefix\": \"k-nodeport\",\n        \"body\": [\n            \"apiVersion: v1\",\n            \"kind: Service\",\n            \"metadata:\",\n            \"\\tname: myapp-service\\n\",\n            \"spec:\",\n            \"\\ttype: NodePort\",\n            \"\\tports:\",\n            \"\\t\\t- targetPort: 80\",\n            \"\\t\\t  port: 80\",\n            \"\\t\\t  NodePort: 30008\\n\",\n            \"\\tselector:\",\n            \"\\t\\tapp: my-app\",\n            \"\\t\\ttype: front-end\\n\",\n            \"---\",\n        ],\n        \"description\": \"k8s NodePort\"\n    },\n\n    \"k8s ClusterIP\": {\n        \"prefix\": \"k-clusterip\",\n        \"body\": [\n            \"apiVersion: v1\",\n            \"kind: Service\",\n            \"metadata:\",\n            \"\\tname: back-end\\n\",\n            \"spec:\",\n            \"\\ttype: ClusterIP\",\n            \"\\tports:\",\n            \"\\t\\t- targetPort: 80\",\n            \"\\t\\t  port: 80\\n\",\n            \"\\tselector:\",\n            \"\\t\\tapp: my-app\",\n            \"\\t\\ttype: back-end\\n\",\n            \"---\",\n        ],\n        \"description\": \"k8s ClusterIP\"\n    },\n\n    \"k8s LoadBalancer\": {\n        \"prefix\": \"k-loadbalancer\",\n        \"body\": [\n            \"apiVersion: v1\",\n            \"kind: Service\",\n            \"metadata:\",\n            \"\\tname: back-end\\n\",\n            \"spec:\",\n            \"\\ttype: LoadBalancer\",\n            \"\\tports:\",\n            \"\\t\\t- targetPort: 80\",\n            \"\\t\\t  port: 80\",\n            \"\\t\\t  NodePort: 30008\\n\",\n            \"\\tselector:\",\n            \"\\t\\tapp: my-app\",\n            \"\\t\\ttype: back-end\\n\",\n            \"---\",\n        ],\n        \"description\": \"k8s LoadBalancer\"\n    },\n\n}\n</code></pre>"},{"location":"devops/linux/","title":"Just enough Linux to shine in society","text":"<p>Quasiment tous les outils utilis\u00e9s pour le DevOps ont d'abord \u00e9t\u00e9 d\u00e9velopp\u00e9s pour Linux puis port\u00e9ssur Windows, souvent avec un d\u00e9calage important :</p> <ul> <li>Docker Linux : 2013,</li> <li>Docker for Windows : 2016.</li> </ul>  <p>Exemple</p> <ul> <li>Ansible ne tourne pas sur Windows de fa\u00e7on native, mais peut tourner sur WSL2.</li> <li>Kubernetes ne tourne pas sur Windows.</li> </ul>"},{"location":"devops/linux/#working-with-shell","title":"Working with Shell","text":""},{"location":"devops/linux/#home-sweet-home","title":"<code>/home/</code> sweet <code>/home/</code>","text":"<p>D\u00e8s que l'on lance un terminal Linux, le premier r\u00e9pertoire dans lequel vous vous trouverez sera le r\u00e9pertoire principal.</p> <p>Si vous avez comme nom d'utilisateur <code>vorphus</code>, votre terminal sera directement ouvert au r\u00e9pertoire <code>/home/vorphus</code>.</p> <p>De fa\u00e7on g\u00e9n\u00e9rique, l'utilisateur se nomme <code>user</code> dans les docs. Pour voir le nom d'utilisateur, on peut soit taper la commande <code>whoami</code> dans le terminal, soit savoir que le nom d'utilisateur est stock\u00e9 dans la variable d'environnement <code>USER</code>, on peut alors taper <code>echo $USER</code> pour afficher son contenu.</p> <p>Le r\u00e9pertoire <code>/home/$USER</code> est unique \u00e0 chaque utilisateur.</p> <p>On peut taper la commande <code>pwd</code> dans le terminal pour voir dans quel r\u00e9pertoire l'on se trouve.</p>"},{"location":"devops/linux/#commandes-basiques","title":"Commandes basiques","text":"<p>On a deux types de commandes dans le Shell :</p> <ul> <li>Les commandes internes : <code>echo</code>, <code>cd</code>, <code>pwd</code>, <code>mkdir</code>, <code>set e.t.c</code>, qui sont fournies avec le Shell.</li> <li>Les commandes externes : <code>mv</code>, <code>date</code>, <code>uptime</code>, <code>cp</code>, qui sont des fichiers binaires ou scripts distincts qui sont appel\u00e9es par le Shell.</li> </ul> <p>Pour d\u00e9terminer si une commande est interne ou externe, on peut taper <code>type</code> suivi de la commande.</p> <pre><code>\u276f type mv\nmv is /usr/bin/mv\n\n\u276f type echo\necho is a shell builtin\n</code></pre>    Commande R\u00e9sultat     <code>echo</code> print a line of text   <code>ls</code> list files and folders   <code>cd</code> change directory   <code>mkdir</code> create a directory   <code>touch</code> create a file   <code>mv new_file.txt sample_file.txt</code> move <code>new_file.txt</code> to <code>sample_file.txt</code>   <code>pwd</code> print the present working directory   <code>cat file.txt</code> show the content of <code>file.txt</code>    <p>Des commandes successives peuvent \u00eatre lanc\u00e9es avec le point virgule.</p> Commandes successives<pre><code>cd new_dir; mkdir www; pwd\n</code></pre> Cr\u00e9ation de plusieurs r\u00e9pertoires en une seule fois<pre><code>mkdir France Angleterre Belgique\n</code></pre> <p>Les commandes du type</p> <p><pre><code>mkdir /tmp/europe\nmkdir /tmp/europe/france\nmkdir /tmp/europe/france/lille\n</code></pre> peuvent se simplifier en une seule ligne via l'argument <code>-p</code> permettant de cr\u00e9er de fa\u00e7on r\u00e9currente les r\u00e9pertoires parents. On a ainsi la commande suivante.</p> Commande simplif\u00e9e, cr\u00e9ation directe des dossiers parents<pre><code>mkdir -p /tmp/europe/france/lille\n</code></pre> <p>De m\u00eame pour la suppression/copie r\u00e9cursive des donn\u00e9es avec <code>-r</code>.</p> suppression r\u00e9cursive<pre><code>rm -r /tmp/europe/france/lille\n</code></pre> copie r\u00e9cursive<pre><code>cp -r my_dir1 /tmp/my_dir1\n</code></pre> <p>La commande <code>tree /home/vorph/test_dir</code> permet de voir toute l'arborescence de <code>test_dir</code>, ie l'ensemble des fichiers et dossiers, sous la forme d'un arbre.</p>"},{"location":"devops/linux/#chemins-absolus-et-relatifs","title":"Chemins absolus et relatifs","text":"<ul> <li> <p><code>/home/vorphus/test</code> est un chemin absolu. Un chemin absolu sp\u00e9cifie le chemin du r\u00e9pertoire, ou du dossier, depuis le dossier root <code>/</code>.</p> </li> <li> <p>Si on est dans le \"home directory\" <code>/home/vorphus</code>, alors <code>test</code> est un chemin relatif au r\u00e9pertoire dans lequel on se trouve d\u00e9j\u00e0.</p> </li> </ul>"},{"location":"devops/linux/#protip-pushd-popd","title":"Protip : <code>pushd</code> &amp; <code>popd</code>","text":"<p>La commande <code>pushd</code> permet de mettre en cache l'adresse du r\u00e9pertoire o\u00f9 l'on se trouve actuellement avant de changer de r\u00e9pertoire. On peut alors y revenir avec <code>popd</code>.</p> <p>Si l'on est dans <code>/home/vorph</code>, <code>pushd /etc</code> nous am\u00e8ne au r\u00e9pertoire <code>/etc</code>, tout en mettant <code>/home/vorph</code> en haut de la pile des r\u00e9pertoires.</p> <p>La commande <code>popd</code> nous permet alors de revenir au r\u00e9pertoire en haut de la pile, eg ici <code>/home/vorph</code>.</p>"},{"location":"devops/linux/#obtenir-de-laide-sur-les-commandes","title":"Obtenir de l'aide sur les commandes","text":"<p>On a plusieurs moyen d'obtenir de l'aide sur les commandes Linux et ce qu'elles sont sens\u00e9es faire.</p> <ul> <li>On peut utiliser la commande <code>whatis</code> pour une d\u00e9cription succinte.</li> </ul> <pre><code>\u276f whatis mv\nmv (1)               - move (rename) files\n\u276f whatis cp\ncp (1)               - copy files and directories\n</code></pre> <ul> <li>On peut lire le manuel de la commande avec <code>man</code>.</li> </ul> <pre><code>\u276f man mv\nMV(1)                     UserCommands                           MV(1)\n\nNAME\n       mv - move (rename) files\n\nSYNOPSIS\n       mv [OPTION]... [-T] SOURCE DEST\n       mv [OPTION]... SOURCE... DIRECTORY\n       mv [OPTION]... -t DIRECTORY SOURCE...\n\nDESCRIPTION\n       Rename SOURCE to DEST, or move SOURCE(s) to DIRECTORY.\n\n       Mandatory arguments to long options are mandatory for short options too.\n\n       --backup[=CONTROL]\n              make a backup of each existing destination file\n\n       -b     like --backup but does not accept an argument\n\n       -f, --force\n              do not prompt before overwriting\n\n       -i, --interactive\n              prompt before overwrite\n...\n</code></pre> <ul> <li>Certaines commandes fournissent une aide via la commande <code>--help</code>.</li> </ul> <pre><code>\u276f mv --help\nUtilisation\u00a0: mv [OPTION]... [-T] SOURCE DEST\n         ou\u00a0: mv [OPTION]... SOURCE... R\u00c9PERTOIRE\n         ou\u00a0: mv [OPTION]... -t R\u00c9PERTOIRE SOURCE...\nRenommer SOURCE en DEST, ou d\u00e9placer le ou les SOURCEs vers R\u00c9PERTOIRE.\n\nLes arguments obligatoires pour les options longues le sont aussi pour les\noptions courtes.\n      --backup[=CONTROL]       archiver chaque fichier de destination existant\n  -b                           identique \u00e0 --backup mais sans argument\n  -f, --force                  ne pas demander de confirmation avant d'\u00e9craser\n  -i, --interactive            demander confirmation avant d'\u00e9craser\n  -n, --no-clobber             ne pas \u00e9craser les fichiers existants\n...\n</code></pre> <ul> <li><code>apropos regex</code> fera une recherche dans toutes les pages de <code>man</code> pour trouver les commandes contenant la regex donn\u00e9e.</li> </ul>"},{"location":"devops/linux/#shell-types","title":"Shell types","text":"<p>D\u00e9finition</p> <p>En informatique, le terme shell d\u00e9signe un logiciel fournissant une interface \u00e0 l'utilisateur pour des composantes d'un ensemble informatique plus grand. Bien qu'il puisse aussi d\u00e9signer une interface graphique, shell est plus g\u00e9n\u00e9ralement employ\u00e9 pour d\u00e9signer un interpr\u00e9teur de lignes de commandes pouvant acc\u00e9der aux services et interagir avec le noyau d'un syst\u00e8me d'exploitation. Dans le cas d'Ubuntu, un shell interagit avec le noyau Linux. Source</p>  <p>Il existe diff\u00e9rents types de shell :</p> <ul> <li>Bourne Shell (Sh Shell)</li> <li>C Shell (csh ou tcsh)</li> <li>Korn Shell (ksh)</li> <li>Z Shell (zsh)</li> <li>Bourne again Shell (bash)</li> </ul> <p>Si l'ensemble des shells cit\u00e9s au dessus ont une base de commandes communes (<code>cp</code>, <code>cd</code>, <code>mv</code>, <code>ls</code>, etc.), il existe des commandes ou des synthaxes sp\u00e9cifiques \u00e0 chacun, comme on le verra avec le shebang dans les scripts shell. On se concentre ici sur le plus utilis\u00e9, Bash.</p>  <p>Info</p> <p>Pour voir quel shell est utilis\u00e9 dans un terminal : <code>echo $SHELL</code>.</p>  <p>La commande <code>alias</code> permet de cr\u00e9er des raccourcis permettant d'appeler d'autres commandes. Par exemple, pour renommer la commande <code>date</code> en <code>dt</code>, on utilise<code>alias dt=date</code>.</p> <p>La commande <code>history</code> permet elle d'avoir un historique des commandes d\u00e9j\u00e0 tap\u00e9es dans le terminal. Commande fort utile pour se rappeler de la synthaxe de certaine commandes, \u00e0 utiliser en conjonction avec <code>grep</code>.</p>"},{"location":"devops/linux/#les-variables-denvironnement","title":"Les variables d'environnement","text":"<p>D\u00e9finition</p> <p>Une variable d'environnement est une valeur dynamique, charg\u00e9e en m\u00e9moire, pouvant \u00eatre utilis\u00e9e par plusieurs processus fonctionnant simultan\u00e9ment. Sur la plupart des syst\u00e8mes d'exploitation, les emplacement de certaines librairies, voire des principaux ex\u00e9cutables du syst\u00e8me peuvent avoir un emplacement diff\u00e9rent selon l'installation.</p> <p>Ainsi, gr\u00e2ce aux variables d'environnement, il est possible, \u00e0 partir d'un programme, de faire r\u00e9f\u00e9rence \u00e0 un emplacement en s'appuyant sur les variables d'environnement d\u00e9finissant ces donn\u00e9es. Source</p>  <p>Beaucoup de softwares, par exemple lorsque l'on d\u00e9ploie des conteneurs, on besoin de variables d'environnement pour fonctionner. On peut citer par exemple :</p> <ul> <li>L'adresse d'une DB,</li> <li>Le mot de passe d'une DB,</li> <li>Un port r\u00e9seau;</li> <li>etc.</li> </ul> <p>La commande <code>env</code> permet de lister l'ensemble des variables d'environnement d\u00e9finies dans votre shell. Lorsque l'on tape cette commande, on peut par exemple avoir les r\u00e9sultats suivants.</p> <pre><code>NAME=Laptop3080\nHOME=/home/vorph\nUSER=vorph\nLOGNAME=vorph\nSHELL=/usr/bin/zsh\nWSL_DISTRO_NAME=Ubuntu-20.04\nSHLVL=1\nPWD=/home/vorph/work/perso/formation-Deep-MLOps\n</code></pre> <p>Une variable d'environnement est par convention toujours d\u00e9finie en majuscule.</p> <p>Pour en d\u00e9fnir une, rien de plus simple, il suffit de la d\u00e9finir dans le terminal, on peut alors y acc\u00e9der pour la voir par exemple en tapant la commande <code>echo</code> suivant de la variable.</p>  <p>Exemple</p> <pre><code>\u276f MON_NOM=mathieu\n\n\u276f echo $MON_NOM\nmathieu\n</code></pre>"},{"location":"devops/linux/#export","title":"<code>export</code>","text":"<p>En fait, non. Ce que nous avons cr\u00e9er l\u00e0 est une variable de shell, ce qui veut dire qu'elle ne sera valable que dans le shell dans lequel vous travailler. Pour vraiment d\u00e9finir une variable d'environnement, il faut utiliser la commande <code>export</code>.</p> Diff\u00e9rence entre variable de shell et d'environnemnt, en passant de zsh \u00e0 bash<pre><code>\u276f MON_NOM=mathieu\n\u276f echo $MON_NOM\nmathieu\n\u276f bash\n(base) vorph@Laptop3080:~/work/perso/formation-Deep-MLOps$ echo $MON_NOM\n\n(base) vorph@Laptop3080:~/work/perso/formation-Deep-MLOps$ zsh\n\u276f export MON_NOM=mathieu\n\u276f bash\n(base) vorph@Laptop3080:~/work/perso/formation-Deep-MLOps$ echo $MON_NOM\nmathieu\n</code></pre> <p>D\u00e9finir une variable de shell en simplement sp\u00e9cifiant son nom dans <code>zsh</code> ne la fera pas persister dans <code>bash</code>, alors que d\u00e9finir une variable d'environnement via la commande <code>export</code> la fera persister peut importe le shell que vous utiliser.</p>  <p>Question</p> <p>Y a-t-il une diff\u00e9rence de lieu de stockage de stockage entre les variables de shell et les variables d'environnement ?</p>   <p>Attention</p> <p>Les variables d'environnement d\u00e9finies par vos soins ne persisteront que jusqu'\u00e0 ce que vous fermiez le terminal dans lequel vous les avez d\u00e9fini.</p>  <p>Pour que les variables d'environnement persistent, il faut les rentrer dans <code>~/.profile</code> ou <code>~/.pam_environment</code>.</p>"},{"location":"devops/linux/#la-variable-path","title":"La variable <code>PATH</code>","text":"<p>Une variable d'environnement particuli\u00e8res est la variable <code>PATH</code>.</p>  <p>D\u00e9finition</p> <p>La variable d'environnement PATH gouverne les chemins d'ex\u00e9cution des logiciels ubuntu. Cette variable PATH permet d'installer et d\u2019utiliser en local un logiciel sans avoir fait appel \u00e0 l'administration syst\u00e8me. Source</p>  <p>Ainsi, lorsque vous tapez par exemple la commande <code>python</code>, pour avoir acc\u00e8s \u00e0 un terminal python, le fichier binaire \u00e9xecut\u00e9 se trouve dans une adresse d\u00e9termin\u00e9e dans l'ensemble des chemins d\u00e9finies dans la variable <code>PATH</code>.</p> <p>Pour avoir acc\u00e8s \u00e0 l'ensemble des chemins d\u00e9finis dans <code>PATH</code>, on fait comme pr\u00e9c\u00e9demment.</p> <p><pre><code>\u276f echo $PATH\n/home/vorph/.cargo/bin:/home/vorph/miniconda3/bin:/home/vorph/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Windows/system32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/Program Files/NVIDIA Corporation/NVIDIA NvDLISR:/mnt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt/c/WINDOWS/System32/WindowsPowerShell/v1.0:/mnt/c/WINDOWS/System32/OpenSSH:/mnt/c/Program Files/Docker/Docker/resources/bin:/mnt/c/ProgramData/DockerDesktop/version-bin:/mnt/c/Users/mathieu/AppData/Local/Microsoft/WindowsApps:/mnt/c/Programmes_tiers/MicrosoftVSCode/bin:/mnt/c/Users/mathieu/AppData/Local/GitHubDesktop/bin\n</code></pre> Chaque chemin est d\u00e9limit\u00e9 par un double point <code>:</code>, ainsi <code>/home/vorph/.cargo/bin:/home/vorph/miniconda3/bin:/home/vorph/miniconda3/condabin</code> constituent 3 chemins diff\u00e9rents :</p> <ul> <li><code>/home/vorph/.cargo/bin</code>,</li> <li><code>/home/vorph/miniconda3/bin</code>,</li> <li><code>/home/vorph/miniconda3/condabin</code>.</li> </ul>"},{"location":"devops/linux/#which","title":"<code>which</code>","text":"<p>Simplement taper <code>echo $PATH</code> ne renseigne pas beaucoup, tant il y a de chemins. Pour voir le chemin d'un logiciel sp\u00e9cifique on peut utiliser la commande <code>which</code>.</p> Le chemin vers python<pre><code>\u276f which python\n/home/vorph/miniconda3/bin/python\n</code></pre> <p>Si l'on regarde attentivement le r\u00e9sultat de la commande <code>echo $PATH</code>, on verra que l'on ne voit pas le chemin vers <code>/home/vorph/miniconda3/bin/python</code>, en revanche on a bien le chemin vers <code>/home/vorph/miniconda3/bin</code>. Les chemins r\u00e9pertori\u00e9s dans la variables PATH sont les chemins racines vers d'autres logiciels.</p>"},{"location":"devops/linux/#export-path","title":"<code>export PATH</code>","text":"<p>Pour ajouter un chemin \u00e0 la variable <code>PATH</code>, on utilise la commande <code>export PATHH=$PATH:chemin</code>.</p> <p>Par exemple, si notre logiciel se trouve dans <code>/opt/...</code> on tape la commande suivante.</p> <pre><code>export PATH=$PATH:/opt/....\n</code></pre>"},{"location":"devops/linux/#linux-core-concepts","title":"Linux Core concepts","text":""},{"location":"devops/linux/#le-noyau-linux","title":"Le noyau Linux","text":"<p>Le noyau Linux (Linux kernel) est la composante principale du syst\u00e8me d'exploitation, le noyau fait l'interface principale entre la partie hardware et software.</p> <pre><code>graph LR\n\n  subgraph Software\n  B[Applications/Processus]\n  end\n\n  A[Linux Kernel]\n\n  subgraph Hardware\n  C1[M\u00e9moire]\n\n  C2[CPU/GPU]\n\n  C3[Devices]\n  end\n\n  A&lt;-..-&gt;B\n\n  A&lt;-..-&gt;C1 &amp; C2 &amp; C3</code></pre> <p>Le noyau Linux est responsable des 4 t\u00e2ches principales suivantes :</p> <ul> <li>gestion de la m\u00e9moire,</li> <li>gestion des processus : quel processus peut utiliser le CPU/GPU, comment, quand, et pour combien de temps,</li> <li>drivers des p\u00e9riph\u00e9riques,</li> <li>s\u00e9curit\u00e9 et gestion des appels syst\u00e8mes.</li> </ul> <p>Le noyau Linux est monolithique, ces t\u00e2ches sont faites par lui m\u00eame et non d\u00e9l\u00e9gu\u00e9es.</p> <p>Le noyau Linux est aussi modulaire, ces comp\u00e9tences peuvent \u00eatre \u00e9tendues par l'ajout de modules.</p>  <p>Info</p> <p>Pour l'exemple, des kernels qui ne sont pas monolithiques sont les suivants :</p> <ul> <li>QNX,</li> <li>Symbian,</li> <li>L4Linux,</li> <li>Singularity,</li> <li>K42,</li> <li>Mac OS X,</li> <li>Integrity,</li> <li>PikeOS,</li> <li>HURD,</li> <li>Minix,</li> <li>Coyotos.</li> </ul> <p>Source : Yanice Karaouzene</p>"},{"location":"devops/linux/#version-du-noyau","title":"Version du noyau","text":"<p>Pour avoir le nom du noyau, on peut taper la commande <code>uname</code>, qui ne produit que peu d'informations.</p> <pre><code>\u276f uname\nLinux\n</code></pre> <p>Pour voir la version du noyau utilis\u00e9, taper <code>uname -r</code> ou <code>uname -a</code>.</p> <pre><code>\u276f uname -r\n5.13.0-40-generic\n\u276f uname -a\nLinux vorph-maison 5.13.0-40-generic #45~20.04.1-Ubuntu SMP Mon Apr 4 09:38:31 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux\n</code></pre> <ul> <li>5 = Version du noyau,</li> <li>13 = Version majeure,</li> <li>0 = Version mineure,</li> <li>40 = patch release,</li> <li>generic = information sp\u00e9cifique \u00e0 la distribution.</li> </ul> <p>Pour avoir toutes les informations sur la version de l'OS qui est utilis\u00e9e, on peut aller voir dans le r\u00e9pertoire <code>/etc</code>. A l'int\u00e9rieur devrait se trouver un fichier nomm\u00e9 <code>os-release</code>. On peut trouver ce fichier via <code>ls /etc/*release*</code>.</p> <pre><code>\u276f ls /etc/*release*\n/etc/lsb-release  \uf481 /etc/os-release\n\n\u276f cat /etc/os-release\nNAME=\"Ubuntu\"\nVERSION=\"20.04.4 LTS (Focal Fossa)\"\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME=\"Ubuntu 20.04.4 LTS\"\nVERSION_ID=\"20.04\"\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nVERSION_CODENAME=focal\nUBUNTU_CODENAME=focal\n</code></pre> <p>Pour plus d'informations, aller voir sur kernel.org qui recense le code source de toutes les versions du noyau Linux disponibles.</p>"},{"location":"devops/linux/#espace-kernel-et-espace-utilisateur","title":"Espace kernel et espace utilisateur","text":"<p>Voyons maintenant comment la m\u00e9moire est g\u00e9r\u00e9e dans un OS Linux.</p> <p>La m\u00e9moire est divis\u00e9e en deux espaces s\u00e9par\u00e9s, l'espace kernel et l'espace utilisateur.</p> <p>L'espace kernel est la partie de la m\u00e9moire dans laquelle le noyau provisionne et \u00e9xecute ses services, un processus tournant dans l'espace kernel \u00e0 un acc\u00e8s illimit\u00e9 au hardware.</p> <p>Tous les processus tournant hors de l'espace kernel tournent dans l'espace utilisateur, qui a un acc\u00e8s restreint au CPU/GPU et \u00e0 la m\u00e9moire.</p> <pre><code>graph LR\n\n  subgraph Kernel space\n  A[Linux Kernel]\n  B[Device drivers]\n  end\n\n\n  subgraph User space\n  C[Applications/Processus]\n  end</code></pre> <p>L'espace kernel contient :</p> <ul> <li>kernel code,</li> <li>kernel extensions,</li> <li>devices drivers.</li> </ul> <p>L'espace utilisateur lui contient entre autre les programmes cod\u00e9s dans le languages suivants :</p> <ul> <li>C,</li> <li>Java,</li> <li>Python,</li> <li>Ruby,</li> <li>Docker containers,</li> <li>etc.</li> </ul> <p>Lorsqu'une application dans l'espace utilisateur tourne et qu'elle a besoin d'acc\u00e9der au hardware pour par exemple :</p> <ul> <li>ouvrir un fichier,</li> <li>\u00e9crire dans un fichier,</li> <li>d\u00e9finir une variable,</li> <li>etc,</li> </ul> <p>l'espace utilisateur produit un \"system call\" \u00e0 l'espace kernel qui lui fournit les ressources n\u00e9cessaires via les drivers.</p> <pre><code>graph LR\n\n  subgraph Kernel space\n  A[Linux Kernel]\n  B[Device drivers]\n  end\n\n  subgraph User space\n  C[Applications/Processus]\n  end\n\n  D[Hardware]\n\n  C-.-&gt;|System Call|B-.-&gt;D</code></pre>"},{"location":"devops/linux/#linux-et-hardware","title":"Linux et hardware","text":"<p>Comment Linux identifie le hardware dans son OS.</p> <p>Prenons l'exemple d'une cl\u00e9 usb branch\u00e9e sur un pc avec un OS Linux.</p> <ol> <li>D\u00e8s que la cl\u00e9 usb est branch\u00e9e, le driver correspondant dans l'espace kernel d\u00e9tecte un changement d'\u00e9tat et g\u00e9n\u00e8re un \u00e9v\u00e8nement, appel\u00e9 <code>uevent</code>.</li> <li>Cet \u00e9v\u00e8nement est envoy\u00e9 au \"user space device manager daemon\", appel\u00e9 <code>udev</code>.</li> <li><code>udev</code> cr\u00e9e alors de fa\u00e7on dynamique un noeud de device correspondant \u00e0 la cl\u00e9 usb se trouvant dans le syst\u00e8me de fichier <code>/dev</code> et lui assigne le nom <code>sdb1</code> (par exemple).</li> <li>Une fois ces \u00e9tapes faites, la cl\u00e9 usb et son contenu seront list\u00e9s comme <code>/dev/sdb1</code>.</li> </ol>  <p>Attention</p> <p><code>/dev/sdb1</code> n'est pas un r\u00e9pertoire, c'est l'adresse que l'OS Linux assigne \u00e0 la cl\u00e9 usb. Tenter de faire un <code>cd</code> vous donnera l'erreur suivante.</p> <pre><code>\u276f cd /dev/sdb1\ncd: n'est pas un dossier: /dev/sdb1\n</code></pre>  <pre><code>graph LR\n  subgraph Ext\u00e9rieur\n  A[Cl\u00e9 USB]\n  B[PC]\n  end\n\n  subgraph Kernel space\n  C[Device driver]\n  end\n\n  subgraph User space\n  D[udev]\n  end\n\n  E[/ /dev/sdb1 /]\n\n  A-.-&gt;B-.-&gt;C\n  C-.-&gt;|uevents|D-.-&gt;E</code></pre> <p>Comment avoir des infos sur les composants hardware ?</p> <ul> <li> <p><code>dmesg</code> (pour l'anglais \"display message\") est une commande sur les syst\u00e8mes d'exploitation de type Unix qui affiche la m\u00e9moire tampon des messages du noyau. Quand un syst\u00e8me Linux boot, il y a de nombreux messages qui peuvent ou non s'afficher (suivant votre OS), ces messages contiennent des logs du hardware</p> </li> <li> <p><code>udevadm info</code> requ\u00e8te la db de <code>udev</code> pour des infos concernant les p\u00e9riph\u00e9riques.</p> </li> <li> <p><code>udevadm monitor</code> est \u00e0 l'\u00e9coute de nouveaux <code>uevent</code>, et les affichera dans le terminal.</p> </li> </ul>  <p>Exemple</p> <p>Voici ce qui se passe avant et apr\u00e8s avoir branch\u00e9 un disque usb avec <code>udevadm monitor</code>.</p> avant<pre><code>\u276f udevadm monitor\nmonitor will print the received events for:\nUDEV - the event which udev sends out after rule processing\nKERNEL - the kernel uevent\n</code></pre> apr\u00e8s<pre><code>\u276f udevadm monitor\nmonitor will print the received events for:\nUDEV - the event which udev sends out after rule processing\nKERNEL - the kernel uevent\n\nKERNEL[13256.354009] add      /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1 (usb)\nKERNEL[13256.354885] add      /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0 (usb)\nKERNEL[13256.355001] bind     /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1 (usb)\nUDEV  [13256.362347] add      /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1 (usb)\nKERNEL[13256.366283] add      /devices/virtual/workqueue/scsi_tmf_6 (workqueue)\nKERNEL[13256.366319] add      /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0/host6 (scsi)\nKERNEL[13256.366327] add      /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0/host6/scsi_host/host6 (scsi_host)\nKERNEL[13256.366342] bind     /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0 (usb)\nKERNEL[13256.366353] add      /bus/usb/drivers/usb-storage (drivers)\nKERNEL[13256.366361] add      /module/usb_storage (module)\nUDEV  [13256.367136] add      /devices/virtual/workqueue/scsi_tmf_6 (workqueue)\nKERNEL[13256.367352] add      /bus/usb/drivers/uas (drivers)\nKERNEL[13256.367364] add      /module/uas (module)\nUDEV  [13256.367427] add      /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0 (usb)\nUDEV  [13256.367778] add      /bus/usb/drivers/usb-storage (drivers)\nUDEV  [13256.367788] add      /module/usb_storage (module)\nUDEV  [13256.367794] add      /bus/usb/drivers/uas (drivers)\nUDEV  [13256.368248] add      /module/uas (module)\nUDEV  [13256.370659] bind     /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1 (usb)\nUDEV  [13256.371298] add      /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0/host6 (scsi)\nUDEV  [13256.371888] add      /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0/host6/scsi_host/host6 (scsi_host)\nUDEV  [13256.372727] bind     /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0 (usb)\nKERNEL[13257.370058] add      /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0/host6/target6:0:0 (scsi)\nKERNEL[13257.370127] add      /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0/host6/target6:0:0/6:0:0:0 (scsi)\nKERNEL[13257.370157] add      /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0/host6/target6:0:0/6:0:0:0/scsi_device/6:0:0:0 (scsi_device)\nKERNEL[13257.370315] add      /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0/host6/target6:0:0/6:0:0:0/scsi_disk/6:0:0:0 (scsi_disk)\nKERNEL[13257.370379] add      /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0/host6/target6:0:0/6:0:0:0/scsi_generic/sg4 (scsi_generic)\nKERNEL[13257.370562] add      /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0/host6/target6:0:0/6:0:0:0/bsg/6:0:0:0 (bsg)\nUDEV  [13257.373300] add      /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0/host6/target6:0:0 (scsi)\nUDEV  [13257.374079] add      /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0/host6/target6:0:0/6:0:0:0 (scsi)\nUDEV  [13257.374986] add      /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0/host6/target6:0:0/6:0:0:0/scsi_device/6:0:0:0 (scsi_device)\nUDEV  [13257.375082] add      /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0/host6/target6:0:0/6:0:0:0/scsi_disk/6:0:0:0 (scsi_disk)\nUDEV  [13257.375095] add      /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0/host6/target6:0:0/6:0:0:0/bsg/6:0:0:0 (bsg)\nUDEV  [13257.375197] add      /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0/host6/target6:0:0/6:0:0:0/scsi_generic/sg4 (scsi_generic)\nKERNEL[13257.392547] add      /devices/virtual/bdi/8:48 (bdi)\nUDEV  [13257.393077] add      /devices/virtual/bdi/8:48 (bdi)\nKERNEL[13257.426707] add      /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0/host6/target6:0:0/6:0:0:0/block/sdd (block)\nKERNEL[13257.426733] add      /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0/host6/target6:0:0/6:0:0:0/block/sdd/sdd1 (block)\nKERNEL[13257.482588] bind     /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0/host6/target6:0:0/6:0:0:0 (scsi)\nUDEV  [13257.602272] add      /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0/host6/target6:0:0/6:0:0:0/block/sdd (block)\nUDEV  [13257.851325] add      /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0/host6/target6:0:0/6:0:0:0/block/sdd/sdd1 (block)\nUDEV  [13257.851983] bind     /devices/pci0000:00/0000:00:14.0/usb1/1-13/1-13.1/1-13.1:1.0/host6/target6:0:0/6:0:0:0 (scsi)\nKERNEL[13266.337010] add      /devices/virtual/bdi/8:49-fuseblk (bdi)\nUDEV  [13266.339098] add      /devices/virtual/bdi/8:49-fuseblk (bdi)\n</code></pre>  <ul> <li><code>lspci</code> liste tous les p\u00e9riph\u00e9riques du \"bus pci\" du pc, ie les fameuses cartes qui s'enfichent dans la carte m\u00e8re (GPU, carte r\u00e9seau, RAM etc.)</li> </ul> <pre><code>\u276f lspci\n\n00:00.0 Host bridge: Intel Corporation Device 9b33 (rev 05)\n00:01.0 PCI bridge: Intel Corporation Xeon E3-1200 v5/E3-1500 v5/6th Gen Core Processor PCIe Controller (x16) (rev 05)\n00:08.0 System peripheral: Intel Corporation Xeon E3-1200 v5/v6 / E3-1500 v5 / 6th/7th/8th Gen Core Processor Gaussian Mixture Model\n00:12.0 Signal processing controller: Intel Corporation Comet Lake PCH Thermal Controller\n00:14.0 USB controller: Intel Corporation Comet Lake USB 3.1 xHCI Host Controller\n00:14.2 RAM memory: Intel Corporation Comet Lake PCH Shared SRAM\n00:14.3 Network controller: Intel Corporation Wi-Fi 6 AX201\n00:16.0 Communication controller: Intel Corporation Comet Lake HECI Controller\n00:17.0 SATA controller: Intel Corporation Device 06d2\n00:1c.0 PCI bridge: Intel Corporation Device 06b8 (rev f0)\n00:1c.4 PCI bridge: Intel Corporation Device 06bc (rev f0)\n00:1f.0 ISA bridge: Intel Corporation Device 0685\n00:1f.3 Audio device: Intel Corporation Comet Lake PCH cAVS\n00:1f.4 SMBus: Intel Corporation Comet Lake PCH SMBus Controller\n00:1f.5 Serial bus controller [0c80]: Intel Corporation Comet Lake PCH SPI Controller\n01:00.0 VGA compatible controller: NVIDIA Corporation Device 2204 (rev a1)\n01:00.1 Audio device: NVIDIA Corporation Device 1aef (rev a1)\n02:00.0 USB controller: ASMedia Technology Inc. Device 3241\n03:00.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL8125 2.5GbE Controller (rev 04)\n</code></pre>"},{"location":"devops/linux/#vi","title":"Vi","text":"<p>Vi est un \u00e9diteur de texte minimaliste install\u00e9 par d\u00e9faut dans la plupart des distributions Linux.</p> <p>Il poss\u00e8de deux modes :</p> <ul> <li>mode commande : copier, coller, suppression, mais pas d'\u00e9criture possible,</li> <li>mode insertion : n\u00e9cessaire pour \u00e9crire du contenu dans un fichier.</li> </ul>  <p>Info</p> <p>Pour activer le mode insertion dans Vi, appuyez sur la touche <code>i</code>. Pour revenir au mode commande, il faut appuyer sur la touche echap <code>esc</code>.</p>  installer Vim plut\u00f4t que Vi, pour un exp\u00e9rience plus user friendly<pre><code>sudo apt update\nsudo apt install vim\n</code></pre>"},{"location":"devops/linux/#commandes-basiques_1","title":"Commandes basiques","text":"Commande R\u00e9sultat     <code>x</code> supprime un caract\u00e8re   <code>dd</code> supprime la ligne enti\u00e8re   <code>yy</code> copie la ligne   <code>p</code> colle le ligne   <code>ctrl + u</code> scrolle ver le haut   <code>ctrl + d</code> scrolle vers le bas   <code>:</code> affiche l'invite de commande   <code>:w</code> sauvegarde le fichier   <code>:w filename</code> sauvegarde le fichier sous le nom filename   <code>:q</code> quitte l'\u00e9diteur <code>vi</code>   <code>:wq</code> sauvegarder et quitter   <code>/string</code> recherche le <code>string</code> dans le texte   <code>n</code> va \u00e0 l'occurence suivante du <code>string</code>"},{"location":"devops/linux/#more-linux-commands","title":"More Linux commands","text":"<ul> <li><code>whoami</code></li> <li><code>id</code></li> <li><code>su</code> : switch user</li> <li> <p><code>ssh user@hostname</code></p> </li> <li> <p><code>sudo ...</code> toujours un utilisateur classique, mais avec des privil\u00e8ges root, liste dans <code>etc/sudoers</code></p> </li> <li><code>wget http://www.url.com/some-file.txt -O some-file.txt</code></li> <li><code>curl http://www.url.com/some-file.txt -O</code></li> </ul>"},{"location":"devops/linux/#package-management","title":"Package Management","text":""},{"location":"devops/linux/#centos","title":"CentOS","text":"<p>CentOS utilise RPM (Red Hat Package Manager), les softwares sont alors packag\u00e9s sous la forme <code>telnet.rpm</code>, pour installer un tel package on tape alors la commande suivante.</p> <pre><code>rpm -i telnet.rpm\n</code></pre> <p>l'argument <code>-i</code> signifiant que l'on veut installer quelque chose. Pour supprimer un package, on tape</p> <pre><code>rpm -e telnet.rpm\n</code></pre> <p>et pour requ\u00eater la db sur un package particuliers, on a la commande suivante.</p> <pre><code>rpm -q telnet.rpm\n</code></pre> <p><code>rpm</code> ne fait qu'installer le package qu'on lui a point\u00e9, il n'installe aucune de ses d\u00e9pendances ! Pour installer un package et l'ensemble de ses d\u00e9pendances, on utilise <code>yum</code> qui est un surcouche de <code>rpm</code>.</p>  <p>Exemple</p> <p><code>yum install ansible</code> installera <code>ansible</code> et l'ensemble de ses d\u00e9pendances, par exemple <code>python</code>, <code>pyYAML</code>, <code>sshpass</code>.</p>  <p>Comment <code>yum</code> sait o\u00f9 sont localis\u00e9es, ie dans quel repo, les d\u00e9pendances d'un package ? L'ensemble des d\u00e9pendances classiques sont list\u00e9es dans <code>/etc/yum.repos.d</code>, c'est l\u00e0 que <code>yum</code> cherche en premier.</p> <p>Pour voir la liste des repos disponibles sur un syst\u00e8me CentOS, on tape la commande <code>yum repolist</code>.</p> <ul> <li><code>ls /etc/yum.repos.d</code></li> <li><code>cat /etc/yum.repos.d/CentOS-Base.repo</code></li> </ul> <p>Pour l'ensemble des packages, par exemple <code>ansible</code>, disponible \u00e0 l'installation, on peut taper la commande suivante.</p> <ul> <li><code>yum list ansible</code>,</li> <li><code>yum remove ansible</code> supprime le package,</li> <li><code>yum --showduplicates list ansible</code> pour voir les diff\u00e9rentes versions disponibles.</li> <li><code>yum install ansible-2.4.2.0</code></li> </ul>"},{"location":"devops/linux/#ubuntu","title":"Ubuntu","text":""},{"location":"devops/linux/#services","title":"Services","text":"<p>D\u00e8s qu'un software comme une DB, un webserveur ou Docker est install\u00e9 sur une machine, il est configur\u00e9 comme un service, pour que ce service marche, il doit \u00eatre lanc\u00e9.</p> <ul> <li><code>service nom_du_service start</code> est la commande permattant de le lancer.</li> </ul>  <p>Exemple</p> <p><code>service httpd start</code> lance un serveur apache.</p>  <p>Une m\u00e9thode plus moderne de lancer un service est d'utiliser la commande <code>systemctl</code>.</p> <ul> <li><code>systemctl start httpd</code></li> </ul> <p><code>systemctl</code> est la commande utilis\u00e9e pour manager les services sur un serveur manag\u00e9 par <code>systemd</code>.</p> <ul> <li> <p><code>systemctl stop httpd</code></p> </li> <li> <p><code>systemctl status httpd</code></p> </li> <li> <p><code>systemctl enable httpd</code> lance le service automatiquement quand le serveur boot.</p> </li> <li> <p><code>systemctl disable httpd</code> d\u00e9sactive le service automatiquement quand le serveur boot.</p> </li> </ul>  <p>Exemple</p> <pre><code>\u276f systemctl status docker.service\n\n\u25cf docker.service - Docker Application Container Engine\n    Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled)\n    Active: active (running) since Wed 2022-04-27 11:50:20 CEST; 3h 23min ago\nTriggeredBy: \u25cf docker.socket\n    Docs: https://docs.docker.com\nMain PID: 1719 (dockerd)\n    Tasks: 87\n    Memory: 167.7M\n    CGroup: /system.slice/docker.service\n            \u251c\u2500 1719 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock\n            \u251c\u2500 2034 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 3001 -container-ip 172.26.0.2 -co&gt;\n            \u251c\u2500 2041 /usr/bin/docker-proxy -proto tcp -host-ip :: -host-port 3001 -container-ip 172.26.0.2 -contain&gt;\n            \u251c\u2500 2055 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 5050 -container-ip 172.25.0.2 -co&gt;\n            \u251c\u2500 2062 /usr/bin/docker-proxy -proto tcp -host-ip :: -host-port 5050 -container-ip 172.25.0.2 -contain&gt;\n            \u251c\u2500 2103 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 5432 -container-ip 172.25.0.3 -co&gt;\n            \u251c\u2500 2111 /usr/bin/docker-proxy -proto tcp -host-ip :: -host-port 5432 -container-ip 172.25.0.3 -contain&gt;\n            \u251c\u250017675 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 49153 -container-ip 172.17.0.2 -c&gt;\n            \u2514\u250017681 /usr/bin/docker-proxy -proto tcp -host-ip :: -host-port 49153 -container-ip 172.17.0.2 -contai&gt;\n\navril 27 11:50:19 vorph-maison dockerd[1719]: time=\"2022-04-27T11:50:19.614727941+02:00\" level=info msg=\"No non-loc&gt;\navril 27 11:50:19 vorph-maison dockerd[1719]: time=\"2022-04-27T11:50:19.614743509+02:00\" level=info msg=\"IPv6 enabl&gt;\navril 27 11:50:19 vorph-maison dockerd[1719]: time=\"2022-04-27T11:50:19.723169676+02:00\" level=info msg=\"No non-loc&gt;\navril 27 11:50:19 vorph-maison dockerd[1719]: time=\"2022-04-27T11:50:19.723184792+02:00\" level=info msg=\"IPv6 enabl&gt;\navril 27 11:50:20 vorph-maison dockerd[1719]: time=\"2022-04-27T11:50:20.260585965+02:00\" level=info msg=\"Loading co&gt;\navril 27 11:50:20 vorph-maison dockerd[1719]: time=\"2022-04-27T11:50:20.310879570+02:00\" level=info msg=\"Docker dae&gt;\navril 27 11:50:20 vorph-maison dockerd[1719]: time=\"2022-04-27T11:50:20.311527346+02:00\" level=info msg=\"metrics AP&gt;\navril 27 11:50:20 vorph-maison dockerd[1719]: time=\"2022-04-27T11:50:20.312283730+02:00\" level=info msg=\"Daemon has&gt;\n</code></pre>"},{"location":"devops/linux/#exemple","title":"Exemple","text":"<p>Supposons que l'on souhaite configurer l'API suivante comme un service.</p> my_app.py<pre><code>from fastapi import FastAPI, status\nfrom fastapi.middleware.cors import CORSMiddleware\nimport uvicorn\n\napp = FastAPI(\n    title=\"Mathieu's API\",\n    description=\"Simple API to be define as a systemd service\",\n    version=\"0.1.0\",\n)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n\n@app.get(\n    \"/hello/\",\n    tags=[\"hello\"],\n    status_code=status.HTTP_200_OK,\n    response_description=\"Hello !\",\n    summary=\"resume\",\n)\nasync def get_hello() -&gt; str:\n    return \"Hello !\"\n\n\nif __name__ == \"__main__\":\n    uvicorn.run(\n        \"my_app:app\",\n        host=\"127.0.0.1\",\n        port=8001,\n        log_level=\"info\",\n        reload=True,\n    )\n</code></pre> <p>si on la lance via la commande suivante.</p> <p><pre><code>/home/vorph/miniconda3/envs/api/bin/python -m my_app\n</code></pre> On peut alors tester qu'elle marche bien en faisant une requ\u00eate <code>GET</code>, par exemple avec httpie.</p> <pre><code>\u276f http 127.0.0.1:8001/hello/\n\nHTTP/1.1 200 OK\ncontent-length: 9\ncontent-type: application/json\ndate: Wed, 04 May 2022 09:00:35 GMT\nserver: uvicorn\n\n\"Hello !\"\n</code></pre>  <p>Remarque</p> <p>Le fais d'utiliser un chemin absolu <code>/home/vorph/miniconda3/envs/api/bin/python</code> pour lancer python est important et sera utilis\u00e9 par la suite, pour conna\u00eetre le chemin complet de votre \u00e9xecutable python, vous pouvez utiliser la commande suivante dans un terminal.</p> <pre><code>\u276f which python\n\n/home/vorph/miniconda3/envs/api/bin/python\n</code></pre> <p>Il sera aussi utile par la suite de conna\u00eetre le chemin absolu de <code>my_app.py</code>, on peut le conna\u00eetre en utilisant la commande <code>readlink</code>.</p> <pre><code>\u276f readlink -f my_app.py\n\n/media/vorph/datas/formation-Deep-MLOps/includes/my_app.py\n</code></pre> <p>Evidemment, la commande <code>readlink</code> n\u00e9cessite que vous soyez dans le r\u00e9pertoire o\u00f9 se trouve <code>my_app.py</code> pour pouvoir lire le chemin absolu.</p>   <p>Remarque</p> <p>Il n'est pas n\u00e9cessaire de laisser les lignes suivantes \u00e0 la fin de <code>my_app.py</code>.</p> <pre><code>if __name__ == \"__main__\":\n    uvicorn.run(\"my_app:app\", host=\"127.0.0.1\", port=8001, log_level=\"info\")\n</code></pre> <p>On pourrait tr\u00e8s bien les enlever, mais dans ce cas l\u00e0, il faudrait activer le bon environnement virtuel puis lancer la commande suivante.</p> <pre><code>uvicorn my_app:app --reload --port 8001\n</code></pre> <p>Ce qui fait deux actions plut\u00f4t qu'une, cela sera plus pratique pour la suite.</p> <p>Autres r\u00e9f\u00e9rences :</p> <ul> <li>Python script in systemd: virtual environment or real environment</li> </ul>  <p>L'id\u00e9e de la configurer comme un service est que l'on pourra alors utiliser <code>systemctl</code> pour la lancer et l'arr\u00eater via les commandes <code>systemctl start my_app</code> et <code>systemctl stop my_app</code>.</p> <p>De cette fa\u00e7on l'administrateur du serveur n'a pas besoin de se soucier du chemin o\u00f9 se trouve l'API, ou m\u00eame du langage dans lequel est cod\u00e9e cette API, il sait que c'est un service qu'il peut lancer et stopper \u00e0 sa guise.</p> <p>Il sera m\u00eame possible de la lancer de fa\u00e7on automatique au d\u00e9but de chaque d\u00e9marrage du serveur, ou de la relancer si le serveur crash.</p> <p>Pour pouvoir lancer un script python comme un service, par exemple avec les commande <code>systemctl start my_app</code>, <code>systemctl stop my_app</code>, <code>my_app</code> faisant r\u00e9f\u00e9rence au nom que l'on souhaite assigner au service, on doit alors configurer ce script comme un service <code>systemd</code> en d\u00e9finissant un \"systemd unit file\" dans <code>/etc/systemd/system</code>.</p> <p>On d\u00e9finit le fichier systemd suivant.</p> /etc/systemd/system/my_app.serivce<pre><code>[Unit]\nDescription=A simple API to test systemd services\n\n[Service]\n#ExecStartPre=some code to run before my_app.py\nExecStart=/home/vorph/miniconda3/envs/api/bin/python /media/vorph/datas/formation-Deep-MLOps/includes/my_app.py\n#ExecStartPre=some code to run afters my_app.py\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Une fois le fichier cr\u00e9\u00e9 dans <code>/etc/systemd/system</code>, il est n\u00e9cessaire de red\u00e9marrer le processus systemd en lan\u00e7ant la commande suivante.</p> <p><code>systemctl daemon-reload</code></p> <p>Seulement de cette fa\u00e7on le nouveau service configur\u00e9 sera pris en compte dans <code>/etc/systemd/system</code>, il suffit alors de lancer le nouveau service lancer via <code>systemctl start my_app</code> pour qu'il d\u00e9marre. On peut alors v\u00e9rifier qu'il fonctionne en faisant la r\u00e9qu\u00eate suivante.</p> <pre><code>\u276f curl http://127.0.0.1:8001/hello/\n\n\"Hello !\"\n</code></pre>  <p>Attention</p> <p>Les chemins utilis\u00e9s dans un ficheier <code>.service</code> doivent toujours \u00eatre des chemins absolus. Les chemins relatifs ne fonctionnent pas.</p>  <pre><code>\u276f systemctl status my_app\n\u25cf my_app.service\n     Loaded: loaded (/etc/systemd/system/my_app.service; static; vendor preset: enabled)\n     Active: active (running) since Wed 2022-05-04 14:22:11 CEST; 1min 39s ago\n   Main PID: 134295 (python)\n      Tasks: 3 (limit: 76995)\n     Memory: 1.7G\n     CGroup: /system.slice/my_app.service\n             \u251c\u2500134295 /home/vorph/miniconda3/envs/api/bin/python /opt/perso/my_app.py\n             \u251c\u2500134780 /home/vorph/miniconda3/envs/api/bin/python -c from multiprocessing.resource_tracker import ma&gt;\n             \u2514\u2500134781 /home/vorph/miniconda3/envs/api/bin/python -c from multiprocessing.spawn import spawn_main; s&gt;\n\nmai 04 14:22:11 vorph-maison systemd[1]: Started my_app.service.\nmai 04 14:22:11 vorph-maison python[134295]: INFO:     Will watch for changes in these directories: ['/']\nmai 04 14:22:11 vorph-maison python[134295]: INFO:     Uvicorn running on http://127.0.0.1:8001 (Press CTRL+C to qu&gt;\nmai 04 14:23:43 vorph-maison python[134295]: error walking file system: OSError [Errno 40] Too many levels of symbo&gt;\nmai 04 14:23:43 vorph-maison python[134295]: INFO:     Started reloader process [134295] using watchgod\nmai 04 14:23:44 vorph-maison python[134781]: INFO:     Started server process [134781]\nmai 04 14:23:44 vorph-maison python[134781]: INFO:     Waiting for application startup.\nmai 04 14:23:44 vorph-maison python[134781]: INFO:     Application startup complete.\nmai 04 14:23:44 vorph-maison python[134781]: INFO:     127.0.0.1:55460 - \"GET /hello/ HTTP/1.1\" 200 OK\n</code></pre> <p>Pour la stopper, il suffit de lancer <code>systemctl stop my_app</code>.</p> <p>Comment configurer le service pour qu'il se lance automatiquement au d\u00e9marrage du serveur ?</p> <p>C'est la partie <code>[Install]</code> du fichier <code>my_app.service</code> qui le d\u00e9finit. La partie <code>WantedBy=multi-user.target</code> d\u00e9signe que se service doit \u00eatre lanc\u00e9 d\u00e8s le d\u00e9marrage. Pour que ce param\u00e8tre soit pris en compte, il est alors n\u00e9cessaire de lancer la commande suivante.</p> <p><code>systemctl enable my_app</code></p> <p>Approfondir :</p> <ul> <li>Why do most systemd examples contain WantedBy=multi-user.target?</li> <li>Systemd service - what is <code>multi-user.target</code></li> <li>Systemd Services 101</li> <li>systemd 101</li> <li>Systemd \u2013 Easy as 1, 2, 3</li> <li>Comment utiliser Systemctl pour g\u00e9rer les services et les unit\u00e9s de Systemd</li> <li>systemd.service \u2014 Service unit configuration</li> </ul>"},{"location":"devops/linux/#placer-votre-code-dans-opt","title":"Placer votre code dans <code>/opt/</code>","text":"<p>Dans la plupart des syst\u00e8mes d'exploitations Linux, il existe un r\u00e9pertoire nomm\u00e9 <code>/opt</code>. Ici <code>/opt</code> peut se comprendre comme \"option\" ou \"optionnal\", pour citer la r\u00e9ponse StackOverflow de What does \"opt\" mean (as in the \"opt\" directory)? Is it an abbreviation? :</p>  <p>Quote</p> <p>In the old days, <code>/opt</code> was used by UNIX vendors like AT&amp;T, Sun, DEC and 3rd-party vendors to hold \"Option\" packages; i.e. packages that you might have paid extra money for. I don't recall seeing <code>/opt</code> on Berkeley BSD UNIX. They used <code>/usr/local</code> for stuff that you installed yourself.</p> <p>But of course, the true \"meaning\" of the different directories has always been somewhat vague. That is arguably a good thing, because if these directories had precise (and rigidly enforced) meanings you'd end up with a proliferation of different directory names.</p> <p>The Filesystem Hierarchy Standard says this about <code>/opt/*</code>:</p> <ul> <li>\"/opt is reserved for the installation of add-on application software packages.\"</li> </ul> <p>By contrast it says this about <code>/usr/local/*</code>:</p> <ul> <li>\"The /usr/local hierarchy is for use by the system administrator when installing software locally.\"</li> </ul> <p>These days, <code>/usr/local/*</code> is typically used for installing software that has been built locally, possibly after tweaking configuration options, etcetera.</p>  <p>En d'autres termes :</p> <ul> <li>Si votre programme est programm\u00e9 dans un langage compil\u00e9, par exemple le C++ ou le Rust, et que vous le compilez, alors vous devriez le placer dans <code>/usr/local/*</code>.</li> <li>Votre application est un binaire unique, alors vous le copierez dans <code>/usr/local</code>.</li> <li>Vous voulez utiliser une alternative d'un programme syst\u00e8me existant construit \u00e0 partir des sources en utilisant <code>make</code>. Dans ce cas, vous l'installerez dans <code>/usr/local</code>.</li> <li>Si vous d\u00e9ployez une application, et que par design, tous ses fichiers sont dans le m\u00eame r\u00e9pertoire, alors on la d\u00e9ploiera dans un r\u00e9pertoire <code>/opt/my_app/</code>.</li> </ul> <p>Cela ne reste que des conventions, mais elles sont largement utilis\u00e9es et cela \u00e9vite de se poser trop de questions sur o\u00f9 est tel application.</p> <p>Dans le cas qui nous interesse ici, d\u00e9ployer notre API basique comme un service, il ne faudrait donc pas mettre</p> <p><code>ExecStart=/home/vorph/miniconda3/envs/api/bin/python /media/vorph/datas/formation-Deep-MLOps/includes/my_app.py</code></p> <p>Dans notre fichier <code>my_app.service</code>, mais copier notre api et toutes ses d\u00e9pendances (eg Dockerfile, docker-compose, etc) dans un r\u00e9pertoire <code>/opt/code/</code> par exemple, et mettre</p> <p><code>ExecStart=/home/vorph/miniconda3/envs/api/bin/python /opt/code/my_app.py</code></p> <p>dans <code>my_app.service</code>.</p>  <p>TLDR</p> <p>Pour cr\u00e9er un service \u00e0 partir d'une application <code>my_app.py</code>:</p> <ol> <li>Mettre l'application dans un r\u00e9pertoire <code>/opt/code/my_app.py</code>.</li> <li>D\u00e9finir un \"systemd unit file\" <code>/etc/systemd/system/my_app.service</code>.</li> <li>Relancer le d\u00e9mon <code>systemd</code> via <code>systemctl daemon-reload</code>.</li> <li>Lancer le service avec <code>systemctl start my_app</code>.</li> <li>Faire la configuration pour le lancement du service de fa\u00e7on automatique, si n\u00e9cessaire.</li> </ol>  <ul> <li>What does /opt mean in Linux?</li> <li>Linux : Directory /opt vs /usr/local</li> <li>What does \"opt\" mean (as in the \"opt\" directory)? Is it an abbreviation?</li> </ul>"},{"location":"devops/network/","title":"Les bases du r\u00e9seau","text":""},{"location":"devops/network/#networking-101","title":"Networking 101","text":""},{"location":"devops/network/#switching","title":"Switching","text":"<p>Qu'est ce qu'un r\u00e9seau ?</p> <p>Supposons que nous ayons deux ordinateurs, VM, Cloud Center, etc A et B.</p> <p>Comment A communique-t-il avec B ?</p> <pre><code>graph LR\n  A[PC A] --&gt;|?| B[PC B];\n  B[PC B] --&gt;|?| A[PC A];</code></pre> <p>On les connecte tous les deux \u00e0 un switch commun, et ce switch cr\u00e9e un r\u00e9seau contenant A et B.</p> <pre><code>graph LR\n  A[PC A] --&gt; C[switch];\n  B[PC B] --&gt; C[switch];\n  C[switch] --&gt; B[PC B];\n  C[switch] --&gt; A[PC A];</code></pre> <p>Pour pouvoir les connecter, on a besoin d'une interface, qu'elle soit physique ou virtuelle.</p> <p>Pour conna\u00eetre cette interface, on peut taper la commande <code>ip link</code>.</p>  <p>Exemple</p> <p>Sur mon pc, cela donne le r\u00e9sulat suivant.</p> <pre><code>\u276f ip link\n2: enp3s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000\n    link/ether 2c:f0:5d:d4:db:e6 brd ff:ff:ff:ff:ff:ff\n3: wlo1: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DORMANT group default qlen 1000\n    link/ether 9c:29:76:75:3f:aa brd ff:ff:ff:ff:ff:ff\n    altname wlp0s20f3\n5: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default\n    link/ether 02:42:3a:62:5a:b9 brd ff:ff:ff:ff:ff:ff\n</code></pre>  <p>Dans la nomenclature Linux, <code>en</code> est le pr\u00e9fixe pour une interface ethernet, et <code>wl</code> pour une interface wifi.</p> <p>Remarquons que Docker \u00e0 sa propre interface <code>docker0</code> pour faire le lien entre les conteneurs et le reste du r\u00e9seau.</p>  <p>Attention</p> <p>Les nomenclatures peuvent quelque peu varier suivant les distributions, il n'est pas rare de voir <code>eth0</code> pour une interface ethernet.</p>  <p>Supposons que le r\u00e9seau cr\u00e9\u00e9 poss\u00e8de l'adresse \\(192.168.1.0\\) et que A et B on deux interface ethernet nomm\u00e9 <code>enp3s0</code> et <code>enp3s1</code>.</p> <pre><code>graph LR\n  A[PC A &lt;br/&gt; enp3s0] --&gt; C[switch &lt;br/&gt; 192.168.1.0];\n  B[PC B &lt;br/&gt; enp3s1] --&gt; C[switch &lt;br/&gt; 192.168.1.0];\n  C[switch &lt;br/&gt; 192.168.1.0] --&gt; B[PC B &lt;br/&gt; enp3s1];\n  C[switch &lt;br/&gt; 192.168.1.0] --&gt; A[PC A &lt;br/&gt; enp3s0];</code></pre> <p>Pour ajouter les deux ordinateurs A et B, on leur assigne alors une adresse ip sur ce r\u00e9seau, via la commande suivante.</p> <ul> <li><code>ip addr add 192.168.1.10/24 dev enp3s0</code> pour A,</li> <li><code>ip addr add 192.168.1.11/24 dev enp3s1</code> pour B.</li> </ul> <pre><code>graph LR\n  A[PC A &lt;br/&gt; 192.168.1.10] --&gt; C[switch &lt;br/&gt; 192.168.1.0];\n  B[PC B &lt;br/&gt; 192.168.1.11] --&gt; C[switch &lt;br/&gt; 192.168.1.0];\n  C[switch &lt;br/&gt; 192.168.1.0] --&gt; B[PC B &lt;br/&gt; 192.168.1.11];\n  C[switch &lt;br/&gt; 192.168.1.0] --&gt; A[PC A &lt;br/&gt; 192.168.1.10];</code></pre> <p>Chacun des deux pc poss\u00e8de alors une adresse sur ce r\u00e9seau et peut communiquer avec l'autre. Il est pas exemple possible depuis A de faire un <code>ping</code> vers B en tapant <code>ping 192.168.1.11</code>.</p> <p>Les communications ici entre A et B se font via le switch. Le switch ne peut recevoir et envoyer des informations que depuis des interfaces faisant parties du r\u00e9seau d\u00e9fini.</p> <p>Mettons maintenant deux r\u00e9seaux d\u00e9finis par deux switchs avec deux adresses ip diff\u00e9rentes. Comment A peut-il communiquer avec C ?</p> <pre><code>graph TD\n  subgraph r\u00e9seau 1\n  A[PC A &lt;br/&gt; 192.168.1.10] --&gt; C[switch &lt;br/&gt; 192.168.1.0];\n  B[PC B &lt;br/&gt; 192.168.1.11] --&gt; C[switch &lt;br/&gt; 192.168.1.0];\n  C[switch &lt;br/&gt; 192.168.1.0] --&gt; B[PC B &lt;br/&gt; 192.168.1.11];\n  C[switch &lt;br/&gt; 192.168.1.0] --&gt; A[PC A &lt;br/&gt; 192.168.1.10];\n  end\n  subgraph r\u00e9seau 2\n  D[PC C &lt;br/&gt; 192.168.2.10] --&gt; E[switch &lt;br/&gt; 192.168.2.0];\n  F[PC D &lt;br/&gt; 192.168.2.11] --&gt; E[switch &lt;br/&gt; 192.168.2.0];\n  E[switch &lt;br/&gt; 192.168.2.0] --&gt; D[PC D &lt;br/&gt; 192.168.2.11];\n  E[switch &lt;br/&gt; 192.168.2.0] --&gt; F[PC C &lt;br/&gt; 192.168.2.10];\n  end\n\n  A[PC A &lt;br/&gt; 192.168.1.10] -.-&gt;|?| D[PC C &lt;br/&gt; 192.168.2.10];</code></pre>"},{"location":"devops/network/#routing","title":"Routing","text":"<p>C'est l\u00e0 qu'intervient le routeur. Un routeur permet de connecter deux r\u00e9seaux ensembles. On peut le penser comme un serveur avec de multiples ports r\u00e9seaux.</p> <p><pre><code>graph LR\n  subgraph r\u00e9seau 1\n  A[PC A &lt;br/&gt; 192.168.1.10] --&gt; C[switch &lt;br/&gt; 192.168.1.0];\n  B[PC B &lt;br/&gt; 192.168.1.11] --&gt; C[switch &lt;br/&gt; 192.168.1.0];\n  C[switch &lt;br/&gt; 192.168.1.0] --&gt; B[PC B &lt;br/&gt; 192.168.1.11];\n  C[switch &lt;br/&gt; 192.168.1.0] --&gt; A[PC A &lt;br/&gt; 192.168.1.10];\n  end\n\n  subgraph r\u00e9seau 2\n  D[PC C &lt;br/&gt; 192.168.2.10] --&gt; E[switch &lt;br/&gt; 192.168.2.0];\n  F[PC D &lt;br/&gt; 192.168.2.11] --&gt; E[switch &lt;br/&gt; 192.168.2.0];\n  E[switch &lt;br/&gt; 192.168.2.0] --&gt; D[PC D &lt;br/&gt; 192.168.2.11];\n  E[switch &lt;br/&gt; 192.168.2.0] --&gt; F[PC C &lt;br/&gt; 192.168.2.10];\n  end\n\n  G[Routeur]-.-C[switch &lt;br/&gt; 192.168.1.0];\n  G[Routeur]-.-E[switch &lt;br/&gt; 192.168.2.0];</code></pre> Puisque le routeur connecte deux r\u00e9seaux, il a deux adresses qui lui sont assign\u00e9es : une pour l'identifier sur chaque r\u00e9seau.</p> <pre><code>graph LR\n  subgraph r\u00e9seau 1\n  A[PC A &lt;br/&gt; 192.168.1.10] --&gt; C[switch &lt;br/&gt; 192.168.1.0];\n  B[PC B &lt;br/&gt; 192.168.1.11] --&gt; C[switch &lt;br/&gt; 192.168.1.0];\n  C[switch &lt;br/&gt; 192.168.1.0] --&gt; B[PC B &lt;br/&gt; 192.168.1.11];\n  C[switch &lt;br/&gt; 192.168.1.0] --&gt; A[PC A &lt;br/&gt; 192.168.1.10];\n  end\n\n  subgraph r\u00e9seau 2\n  D[PC C &lt;br/&gt; 192.168.2.10] --&gt; E[switch &lt;br/&gt; 192.168.2.0];\n  F[PC D &lt;br/&gt; 192.168.2.11] --&gt; E[switch &lt;br/&gt; 192.168.2.0];\n  E[switch &lt;br/&gt; 192.168.2.0] --&gt; D[PC D &lt;br/&gt; 192.168.2.11];\n  E[switch &lt;br/&gt; 192.168.2.0] --&gt; F[PC C &lt;br/&gt; 192.168.2.10];\n  end\n\n  G[Routeur]-.-|192.168.1.1|C[switch &lt;br/&gt; 192.168.1.0];\n  G[Routeur]-.-|192.168.2.1|E[switch &lt;br/&gt; 192.168.2.0];</code></pre> <p>Maintenant que le routeur relie les deux r\u00e9seaux, chacun des 4 pc peut communiquer l'un avec l'autre.</p> <p>Quand le PC A veut envoyer un paquet au PC C, comment sait-il o\u00f9 est le routeur sur le r\u00e9seau, pour envoyer le paquet via lui ? Le routeur n'est qu'un syst\u00e8me suppl\u00e9mentaire sur le r\u00e9seau, il peut y en avoir des centaines.</p>"},{"location":"devops/network/#default-gateway","title":"Default Gateway","text":"<p>C'est l\u00e0 que l'on configure les syst\u00e8mes avec une gateway, ou passerelle. Si un r\u00e9seau est une chambre, alors la gateway est une porte pour communiquer vers l'ext\u00e9rieur.</p> <p>Les syst\u00e8mes ont besoin de savoir o\u00f9 est cette gateway pour pouvoir communiquer entre r\u00e9seaux.</p> <p>Pour voir les diff\u00e9rentes configuration de route, ou table de routage d'un syst\u00e8me, on peut alors taper la commande suivante.</p> <p><code>route</code></p>  <p>Exemple</p> <p>Sur mon pc, cela donne le r\u00e9sultat suivant.</p> <pre><code>\u276f route\n\nTable de routage IP du noyau\nDestination     Passerelle      Genmask         Indic Metric Ref    Use Iface\ndefault         livebox.home    0.0.0.0         UG    100    0        0 enp3s0\nlink-local      0.0.0.0         255.255.0.0     U     1000   0        0 enp3s0\n172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0\n192.168.1.0     0.0.0.0         255.255.255.0   U     100    0        0 enp3s0\n</code></pre>  <p>Dans notre exemple, si l'on tape cette commande, comme aucune passerelle n'a encore \u00e9t\u00e9 d\u00e9finie, on obtiendra une table de routage vide.</p> <pre><code>graph LR\n  subgraph r\u00e9seau 1\n  A[PC A &lt;br/&gt; 192.168.1.10] --&gt; C[switch &lt;br/&gt; 192.168.1.0];\n  B[PC B &lt;br/&gt; 192.168.1.11] --&gt; C[switch &lt;br/&gt; 192.168.1.0];\n  C[switch &lt;br/&gt; 192.168.1.0] --&gt; B[PC B &lt;br/&gt; 192.168.1.11];\n  C[switch &lt;br/&gt; 192.168.1.0] --&gt; A[PC A &lt;br/&gt; 192.168.1.10];\n  end\n\n  subgraph r\u00e9seau 2\n  D[PC C &lt;br/&gt; 192.168.2.10] --&gt; E[switch &lt;br/&gt; 192.168.2.0];\n  F[PC D &lt;br/&gt; 192.168.2.11] --&gt; E[switch &lt;br/&gt; 192.168.2.0];\n  E[switch &lt;br/&gt; 192.168.2.0] --&gt; D[PC D &lt;br/&gt; 192.168.2.11];\n  E[switch &lt;br/&gt; 192.168.2.0] --&gt; F[PC C &lt;br/&gt; 192.168.2.10];\n  end\n\n  G[Routeur]-.-|192.168.1.1|C[switch &lt;br/&gt; 192.168.1.0];\n  G[Routeur]-.-|192.168.2.1|E[switch &lt;br/&gt; 192.168.2.0];</code></pre> <pre><code>\u276f route\n\nTable de routage IP du noyau\nDestination     Passerelle      Genmask         Indic Metric Ref    Use Iface\n</code></pre> <p>Pour configurer une passerelle du PC B vers des syst\u00e8mes du r\u00e9seau 2, on tape alors la commande suivante.</p> <p><code>ip route add 192.168.2.0/24 via 192.168.1.1</code></p> <p><code>via 192.168.1.1</code> d\u00e9finit la passerelle qui sera utilis\u00e9e par le r\u00e9seau 1 pour communiquer vers le r\u00e9seau 2.</p> <p>La table de routage du r\u00e9seau 1 se mettra alors \u00e0 jour.</p> <pre><code>\u276f route\n\nTable de routage IP du noyau\nDestination     Passerelle      Genmask         Indic Metric Ref    Use Iface\n192.168.2.0     192.168.1.1     255.255.255.0   U     100    0        0 eth0\n</code></pre>  <p>Attention</p> <p>Si l'on s'arette l\u00e0, la communication ne va que dans un sens, du r\u00e9seau 1 vers le r\u00e9seau 2. Il faut aussi d\u00e9finir la passerelle pour aller dans l'autre sens.</p>  <p>cela se fait via <code>ip route add 192.168.1.0/24 via 192.168.2.1</code>.</p> <p>Mais si maintenant le r\u00e9seau 2 a besoin d'acc\u00e9der \u00e0 internet ? Par exemple \u00e0 l'ip internet 172.217.194.0, qui est celle de Google (\u00e7a peut \u00eatre utile) ?</p> <p><pre><code>graph LR\n  subgraph r\u00e9seau 1\n  A[PC A 192.168.1.10] --&gt; C[switch];\n  B[PC B 192.168.1.11] --&gt; C[switch];\n  C[switch 192.168.1.0] --&gt; B[PC B 192.168.1.11];\n  C[switch 192.168.1.0] --&gt; A[PC A 192.168.1.10];\n  end\n\n  subgraph r\u00e9seau 2\n  D[PC C 192.168.2.10] --&gt; E[switch];\n  F[PC D 192.168.2.11] --&gt; E[switch];\n  E[switch 192.168.2.0] --&gt; D[PC D 192.168.2.11];\n  E[switch 192.168.2.0] --&gt; F[PC C 192.168.2.10];\n  end\n\n  G[Routeur]-.-|192.168.1.1|C[switch 192.168.1.0];\n  G[Routeur]-.-|192.168.2.1|E[switch 192.168.2.0];\n\n  subgraph Internet\n  H{{Google 172.217.194.0}}\n  end</code></pre> Et bien on rajoute une nouvelle passerelle.</p> <p><code>ip route add 172.217.194.0/24 via 192.168.2.1</code></p> <p>La table de routage aura alors une nouvelle route.</p> <pre><code>\u276f route\n\nTable de routage IP du noyau\nDestination     Passerelle      Genmask         Indic Metric Ref    Use Iface\n192.168.2.0     192.168.1.1     255.255.255.0   U     100    0        0 eth0\n172.217.194.0   192.168.2.1     255.255.255.0   U     0      0        0 eth0\n</code></pre> <pre><code>graph LR\n  subgraph r\u00e9seau 1\n  A[PC A 192.168.1.10] --&gt; C[switch];\n  B[PC B 192.168.1.11] --&gt; C[switch];\n  C[switch 192.168.1.0] --&gt; B[PC B 192.168.1.11];\n  C[switch 192.168.1.0] --&gt; A[PC A 192.168.1.10];\n  end\n\n  subgraph r\u00e9seau 2\n  D[PC C 192.168.2.10] --&gt; E[switch];\n  F[PC D 192.168.2.11] --&gt; E[switch];\n  E[switch 192.168.2.0] --&gt; D[PC D 192.168.2.11];\n  E[switch 192.168.2.0] --&gt; F[PC C 192.168.2.10];\n  end\n\n  subgraph Internet\n  H1{{Google 172.217.194.0}};\n  end\n\n  G[Routeur]-.-|192.168.1.1|C[switch 192.168.1.0];\n  G[Routeur]-.-|192.168.2.1|E[switch 192.168.2.0];\n  G[Routeur]-.-H1{{Google 172.217.194.0}};\n</code></pre> <p>Mais il y a des milliards de sites, on ne va quand m\u00eame pas faire \u00e7a pour tous ?</p> <pre><code>graph LR\n  subgraph r\u00e9seau 1\n  A[PC A 192.168.1.10] --&gt; C[switch];\n  B[PC B 192.168.1.11] --&gt; C[switch];\n  C[switch 192.168.1.0] --&gt; B[PC B 192.168.1.11];\n  C[switch 192.168.1.0] --&gt; A[PC A 192.168.1.10];\n  end\n\n  subgraph r\u00e9seau 2\n  D[PC C 192.168.2.10] --&gt; E[switch];\n  F[PC D 192.168.2.11] --&gt; E[switch];\n  E[switch 192.168.2.0] --&gt; D[PC D 192.168.2.11];\n  E[switch 192.168.2.0] --&gt; F[PC C 192.168.2.10];\n  end\n\n  G[Routeur]-.-|192.168.1.1|C[switch 192.168.1.0];\n  G[Routeur]-.-|192.168.2.1|E[switch 192.168.2.0];\n  G[Routeur]-.-H1{{Google 172.217.194.0}};\n\n  subgraph Internet\n  H1{{Google 172.217.194.0}}\n  H2{{Site 1}}\n  H3{{Site 2}}\n  H4{{...}}\n  end</code></pre> <p>Plut\u00f4t que d'ajouter une nouvelle route pour chaque adresse ip, on peut simplement dire \"pour chaque r\u00e9seau dont je ne connais pas la route, utilise ce routeur comme passerelle par d\u00e9faut\".</p> <p><code>ip route add default via 192.168.2.1</code></p> <p>Ainsi, toute requ\u00eate alors vers un r\u00e9seau inconnu passe par ce routeur particuliers.</p>  <p>Exemple</p> <p>Sur mon pc, la passerelle par d\u00e9faut est <code>livebox.home</code>, qui est ma box internet, ce qui est coh\u00e9rent car c'est elle qui fait la jonction entre mon r\u00e9seau local et internet.</p> <pre><code>\u276f route\n\nTable de routage IP du noyau\nDestination     Passerelle      Genmask         Indic Metric Ref    Use Iface\ndefault         livebox.home    0.0.0.0         UG    100    0        0 enp3s0\nlink-local      0.0.0.0         255.255.0.0     U     1000   0        0 enp3s0\n172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0\n192.168.1.0     0.0.0.0         255.255.255.0   U     100    0        0 enp3s0\n</code></pre> <p>Notons qu'\u00e0 la place de <code>default</code> comme destination, on aurait pu avoir <code>0.0.0.0</code>, ce qui revient au m\u00eame puisque <code>0.0.0.0</code> signifie \"n'importe quelle adresse ip\".</p> <p>Lorsque l'on voit <code>0.0.0.0</code> dans la section passerelle, cela signifie simplement que l'on a pas besoin de passerelle pour acc\u00e9der \u00e0 ce r\u00e9seau.</p>  <p>Comment d\u00e9finir un system Linux comme routeur ? Prenons la situation suivante.</p> <pre><code>graph LR\n  subgraph A\n  A1[eth0 &lt;br/&gt; 192.168.1.5];\n  end\n\n  subgraph B\n  B1[eth0 &lt;br/&gt; 192.168.1.6];\n  B2[eth1 &lt;br/&gt; 192.168.2.6];\n  end\n\n  subgraph C\n  C1[eth0 &lt;br/&gt; 192.168.2.5];\n  end\n\n  A1[eth0 &lt;br/&gt; 192.168.1.5]-.-|192.168.1.0|B1[eth0 &lt;br/&gt; 192.168.1.6]\n  B2[eth1 &lt;br/&gt; 192.168.2.6]-.-|192.168.2.0|C1[eth0 &lt;br/&gt; 192.168.2.5]</code></pre> <p>On a un syst\u00e8me Linux B avec deux interfaces, chacune connectant un autre syst\u00e8me Linux. Donc B est connect\u00e9 aux 2 r\u00e9seaux, et a donc une ip sur chaque r\u00e9seau.</p>  <p>Question</p> <p>Comment A peut-il communiquer avec C ?</p>  <p>Si l'on essaye na\u00efvement de faire un <code>ping 192.168.2.5</code> depuis le r\u00e9seau A, on aura le message suivant <code>Connect: Network is unreachable</code>. Maintenant on sait pourquoi, A n'a aucune id\u00e9e de comment joindre 192.168.2.0 car il n'y a pas de passerelle. On doit lui dire que la passerelle passe par B.</p> <p>On fait \u00e7a via <code>ip route add 192.168.2.0/24 via 192.168.1.6</code>, et la m\u00eame chose dans l'autre sens : <code>ip route add 192.168.1.0/24 via 192.168.2.6</code>.</p> <p>Si l'on r\u00e9esaye <code>ping 192.168.2.5</code> maintenant, nous n'aurons plus le message d'erreur, mais nous pas non plus de r\u00e9ponses. Par d\u00e9faut et pour des raisons de s\u00e9curit\u00e9, dans Linux, les paquets ne sont pas transmis d'une interface \u00e0 l'autre, ie il n'y a aucune communication entre <code>eth0</code> et <code>eth1</code> dans le r\u00e9seau B.</p> <pre><code>graph LR\n  subgraph A\n  A1[eth0 &lt;br/&gt; 192.168.1.5];\n  end\n\n  subgraph B\n  B1[eth0 &lt;br/&gt; 192.168.1.6];\n  B2[eth1 &lt;br/&gt; 192.168.2.6];\n  end\n\n  subgraph C\n  C1[eth0 &lt;br/&gt; 192.168.2.5];\n  end\n\n  A1[eth0 &lt;br/&gt; 192.168.1.5]-.-|192.168.1.0|B1[eth0 &lt;br/&gt; 192.168.1.6]\n  B2[eth1 &lt;br/&gt; 192.168.2.6]-.-|192.168.2.0|C1[eth0 &lt;br/&gt; 192.168.2.5]\n  B1[eth0 &lt;br/&gt; 192.168.1.6] -.-|X| B2[eth1 &lt;br/&gt; 192.168.2.6]</code></pre> <p>Pour autoriser B \u00e0 transmettre les paquets d'une interface \u00e0 l'autre, on doit aller voir dans <code>/proc/sys/net/ipv4/ip_forward</code>.</p>  <p>Exemple</p> <p>Sur mon pc, cela donne :</p> <pre><code>\u276f cat /proc/sys/net/ipv4/ip_forward\n1\n</code></pre> <p>La valeur est sur 1, donc le transfert d'une interface \u00e0 l'autre est autoris\u00e9, sinon elle serait \u00e0 0.</p>   <p>Attention</p> <p>Jusqu'\u00e0 pr\u00e9sent, toutes les commandes que l'on a expliquer ne persisterons pas au red\u00e9marrage du syst\u00e8me ! Il faudrait les refaire \u00e0 chaque fois.</p>  <p>Pour que les changements soient persistents, il faut modifier la valeur du param\u00e8tre <code>net.ipv4.ip_forward</code> dans <code>/etc/sysctl.conf</code> en la mettant \u00e0 1.</p>  <p>Exemple</p> <pre><code>\u276f cat /etc/sysctl.conf\n#\n# /etc/sysctl.conf - Configuration file for setting system variables\n# See /etc/sysctl.d/ for additional system variables.\n# See sysctl.conf (5) for information.\n#\n\n#kernel.domainname = example.com\n\n# Uncomment the following to stop low-level messages on console\n#kernel.printk = 3 4 1 3\n\n##############################################################3\n# Functions previously found in netbase\n#\n\n# Uncomment the next two lines to enable Spoof protection (reverse-path filter)\n# Turn on Source Address Verification in all interfaces to\n# prevent some spoofing attacks\n#net.ipv4.conf.default.rp_filter=1\n#net.ipv4.conf.all.rp_filter=1\n\n# Uncomment the next line to enable TCP/IP SYN cookies\n# See http://lwn.net/Articles/277146/\n# Note: This may impact IPv6 TCP sessions too\n#net.ipv4.tcp_syncookies=1\n\n# Uncomment the next line to enable packet forwarding for IPv4\n#net.ipv4.ip_forward=1\n\n# Uncomment the next line to enable packet forwarding for IPv6\n#  Enabling this option disables Stateless Address Autoconfiguration\n#  based on Router Advertisements for this host\n#net.ipv6.conf.all.forwarding=1\n\n\n###################################################################\n# Additional settings - these settings can improve the network\n# security of the host and prevent against some network attacks\n# including spoofing attacks and man in the middle attacks through\n# redirection. Some network environments, however, require that these\n# settings are disabled so review and enable them as needed.\n#\n# Do not accept ICMP redirects (prevent MITM attacks)\n#net.ipv4.conf.all.accept_redirects = 0\n#net.ipv6.conf.all.accept_redirects = 0\n# _or_\n# Accept ICMP redirects only for gateways listed in our default\n# gateway list (enabled by default)\n# net.ipv4.conf.all.secure_redirects = 1\n#\n# Do not send ICMP redirects (we are not a router)\n#net.ipv4.conf.all.send_redirects = 0\n#\n# Do not accept IP source route packets (we are not a router)\n#net.ipv4.conf.all.accept_source_route = 0\n#net.ipv6.conf.all.accept_source_route = 0\n#\n# Log Martian Packets\n#net.ipv4.conf.all.log_martians = 1\n#\n\n###################################################################\n# Magic system request Key\n# 0=disable, 1=enable all, &gt;1 bitmask of sysrq functions\n# See https://www.kernel.org/doc/html/latest/admin-guide/sysrq.html\n# for what other values do\n#kernel.sysrq=438\n</code></pre>"},{"location":"devops/network/#dns-sous-linux","title":"DNS sous Linux","text":"<p>On consid\u00e8re deux pc A et B reli\u00e9s au m\u00eame r\u00e9seau, avec des adresse ip qui ont \u00e9t\u00e9 assign\u00e9es. Il sont capables de communiquer entre eux, ie <code>ping 192.168.1.11</code> depuis A fonctionne.</p> <pre><code>graph LR\n  subgraph A\n  A1[eth0 &lt;br/&gt; 192.168.1.10];\n  end\n\n  B1{{192.168.1.0}};\n\n  subgraph B\n  C1[eth0 &lt;br/&gt; 192.168.1.11];\n  end\n\n  A1[eth0 &lt;br/&gt; 192.168.1.10]-.-B1{{192.168.1.0}}\n  C1[eth0 &lt;br/&gt; 192.168.1.11]-.-B1{{192.168.1.0}}</code></pre> <p>Le syst\u00e8me B poss\u00e8de des bases de donn\u00e9es, donc plut\u00f4t que de se souvenir de son adresse ip, on souhaite lui associer le nom <code>db</code>. Ainsi on souhaiterait que la commande <code>ping db</code> fonctionne, pour l'instant ce n'est pas le cas.</p> <pre><code>graph LR\n  subgraph A\n  A1[eth0 &lt;br/&gt; 192.168.1.10];\n  end\n\n  B1{{192.168.1.0}};\n\n  subgraph B\n  C1[eth0 &lt;br/&gt; db &lt;br/&gt; 192.168.1.11];\n  end\n\n  A1[eth0 &lt;br/&gt; 192.168.1.10]-.-B1{{192.168.1.0}}\n  C1[db &lt;br/&gt; eth0 &lt;br/&gt; 192.168.1.11]-.-B1{{192.168.1.0}}</code></pre> <p>En d'autres termes, on souhaiterait dire au syst\u00e8me A que lorsque l'on utilise le nom <code>db</code>, on veut en fait parler de l'adresse ip <code>192.168.1.11</code>.</p> <p>En encore d'autres termes, on devrait cr\u00e9er un dictionnaire sur le syst\u00e8me A disant que la cl\u00e9 <code>db</code> a pour valeur l'adresse ip <code>192.168.1.11</code>. C'est exactement ce que fait le fichier <code>/etc/hosts</code> sous Linux. Il suffit alors de rajouter la ligne <code>192.168.1.11          db</code> dans ce fichier pour que le poing depuis A sur B fonctionne.</p>  <p>Exemple</p> <p>Avec mon laptop sous WSL2, j'ai le fichier suivant.</p> <pre><code>\u276f cat /etc/hosts\n\n# This file was automatically generated by WSL. To stop automatic generation of this file, add the following entry to /etc/wsl.conf:\n# [network]\n# generateHosts = false\n127.0.0.1       localhost\n127.0.1.1       Laptop3080.localdomain  Laptop3080\n\ufeff\n192.168.1.21    host.docker.internal\n192.168.1.21    gateway.docker.internal\n127.0.0.1       kubernetes.docker.internal\n\n# The following lines are desirable for IPv6 capable hosts\n::1     ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nff00::0 ip6-mcastprefix\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\n</code></pre>  <p>Un point important \u00e0 noter, nous avons dit au syst\u00e8me A que le syst\u00e8me B s'appelait <code>db</code>, il va prendre \u00e7a pour acquis, peut importe le nom que vous mettrez dans <code>/etc/hosts</code> il sera consid\u00e9r\u00e9 comme vrai par A.</p> <p>On peut m\u00eame assigner deux noms diff\u00e9rents pointant sur la m\u00eame ip cela ne pose aucun soucis, par exemple on peut faire la chose suivante.</p> <pre><code>\u276f cat /etc/hosts\n\n192.168.1.11    db\n192.168.1.11    www.google.com\n</code></pre> <p>et les deux <code>ping</code> fonctionneront !</p>  <p>Exemple</p> <p>En faisant <code>cat /etc/hosts</code> sur mon laptop, on a vu que l'adresse <code>127.0.1.1</code> \u00e9tait r\u00e9pertori\u00e9e sous 3 cl\u00e9es diff\u00e9rentes : <code>localhost</code>, <code>Laptop3080</code> et <code>Laptop3080.localdomain</code>, un <code>ping</code> sur chacun des \" fonctionne sans soucis et appelle la m\u00eame adresse ip.</p>   <p>En d'autres termes, A ne v\u00e9rifie pas si le vrai nom de B est <code>db</code>, pour voir le vrai nom du sys\u00e8me B, il faut taper la commande suivante dans le sys\u00e8me B : <code>hostname</code>.</p> <pre><code>\u276f hostname\n\nLaptop3080\n</code></pre> <p>A s'en fiche, il utilisera le nom qui est dans le fichier <code>/etc/hosts</code>.</p> <p>La traduction du nom d\u00e9fini dans <code>/etc/hosts</code> en une adresse ip valide s'appelle la r\u00e9solution du nom.</p> <pre><code>graph LR\n  subgraph A\n  A1[eth0 &lt;br/&gt; web &lt;br/&gt; 192.168.1.10];\n  end\n\n  B1{{192.168.1.0}};\n\n  subgraph B\n  C1[eth0 &lt;br/&gt; db &lt;br/&gt; 192.168.1.11];\n  end\n\n  subgraph C\n  D1[eth0 &lt;br/&gt; nfs &lt;br/&gt; 192.168.1.12];\n  end\n\n  A1[eth0 &lt;br/&gt; web &lt;br/&gt; 192.168.1.10]-.-B1{{192.168.1.0}}\n  C1[eth0 &lt;br/&gt; db &lt;br/&gt; 192.168.1.11]-.-B1{{192.168.1.0}}\n  D1[eth0 &lt;br/&gt; nfs &lt;br/&gt; 192.168.1.12]-.-B1{{192.168.1.0}}</code></pre> <p>Avec des sys\u00e8mes relativement simples comme celui au dessus, on peut encore s'en sortir en d\u00e9finissant un m\u00eame fichier <code>/etc/hosts</code> pour les 3 syst\u00e8mes A, B et C.</p> <pre><code>\u276f cat /etc/hosts\n\n192.168.1.10    web\n192.168.1.11    db\n192.168.1.12    nfs\n</code></pre> <p>Pour les sys\u00e8mes modernes avec plusieurs milliers d'intefaces connect\u00e9ss entre elles, ce n'est plus faisables. Si une seule ip change, il est alors n\u00e9cessaire de changer l'ensemble des fichier <code>/etc/hosts</code> de toutes les machines, ce qui devient rapidement impossible.</p> <p>L'id\u00e9ee est alors de centraliser ce dictionnaire en un unique serveur, lorsque le syst\u00e8me A aura alors besoin de faire une r\u00e9solution de nom, il enverra une requ\u00eate \u00e0 ce serveur, qui se chargera de faire la r\u00e9solution de nom et de lui renvoyer le r\u00e9sultat.</p> <p>Ce serveur est ce que l'on appelle le serveur DNS.</p> <pre><code>graph LR\n  subgraph A\n  A1[eth0 &lt;br/&gt; web &lt;br/&gt; 192.168.1.10];\n  end\n\n  B1{{192.168.1.0}};\n\n  subgraph B\n  C1[eth0 &lt;br/&gt; db &lt;br/&gt; 192.168.1.11];\n  end\n\n  subgraph C\n  D1[eth0 &lt;br/&gt; nfs &lt;br/&gt; 192.168.1.12];\n  end\n\n  subgraph D\n  E1[eth0];\n  end\n\n  subgraph ...\n  F1[eth0];\n  end\n\n  subgraph DNS\n  G1[eth0 &lt;br/&gt; 192.168.1.100];\n  G2[/ /etc/hosts /]\n  end\n\n\n  A1[eth0 &lt;br/&gt; web &lt;br/&gt; 192.168.1.10]-.-B1{{192.168.1.0}}\n  C1[eth0 &lt;br/&gt; db &lt;br/&gt; 192.168.1.11]-.-B1{{192.168.1.0}}\n  D1[eth0 &lt;br/&gt; nfs &lt;br/&gt; 192.168.1.12]-.-B1{{192.168.1.0}}\n  G1[eth0]-.-B1{{192.168.1.0}}</code></pre> <p>Comment faire en sorte que chaque syst\u00e8me de r\u00e9seau pointe correctement vers le serveur DNS quand il a besoin de faire une r\u00e9solution de nom ?</p> <p>Le serveur DNS est reli\u00e9 au r\u00e9seau, il a donc une adresse ip, ici <code>192.168.1.100</code>. Chaque sysr\u00e8me Linux poss\u00e8de une fichier de configuration DNS <code>/etc/resolv.conf</code>. Il suffit alors de le renseigner comme l'on renseignerait le fichier <code>/etc/hosts</code>.</p> <pre><code>cat /etc/resolv.conf\n\nnameserver    192.168.1.100\n</code></pre>  <p>Exemple</p> <p>Sur mon laptop, l'adresse du serveur DNS auquel je suis reli\u00e9 est la suivante.</p> <pre><code>\u276f cat /etc/resolv.conf\n# This file was automatically generated by WSL. To stop automatic generation of this file, add the following entry to /etc/wsl.conf:\n# [network]\n# generateResolvConf = false\nnameserver 172.27.80.1\n</code></pre>  <p>Si un nom ou une adresse ip change, il suffit alors de mettre \u00e0 jour le registre du serveur DNS. Actuellement, avec les principes de r\u00e9plications des donn\u00e9es, la mise \u00e0 jour d'un nom de domaine ou d'une adresse ip peut prendre entre 30 minutes et plusieurs heures, le temps que le changement soit pris en compte par l'ensemble des serveurs DNS pr\u00e9sents sur internet.</p> <p>Avoir un serveur DNS ne veut pas dire que le fichier <code>/etc/hosts</code> devient inutile. Si l'on souhaite provisioner un serveur de tests, en souhaitant que seuls les syt\u00e8mes de notre r\u00e9seau, voire m\u00eame uniquement notre syst\u00e8me puissent y acc\u00e9der, on peut renseigner unqiement le fichier <code>/etc/hosts</code> afin d'avoir une r\u00e9solution locale.</p> <p>Le fichier <code>/etc/hosts</code> est toujours prioritaire sur le serveur DNS, pour faire une r\u00e9solution, un syst\u00e8me cherchera d'abord dans <code>/etc/hosts</code>, et s'il ne trouve pas la solution, il fera ensuite une requ\u00eate DNS.</p> <p>Pour changer l'ordre de priorit\u00e9, cela se fait dans le fichier de configuration <code>/etc/nsswitch.conf</code> en modifiant l'ordre des valeurs pour le param\u00e8tre <code>hosts</code>.</p>  <p>Exemple</p> <pre><code>\u276f cat /etc/nsswitch.conf\n\n\n# /etc/nsswitch.conf\n#\n# Example configuration of GNU Name Service Switch functionality.\n# If you have the `glibc-doc-reference' and `info' packages installed, try:\n# `info libc \"Name Service Switch\"' for information about this file.\n\npasswd:         files systemd\ngroup:          files systemd\nshadow:         files\ngshadow:        files\n\nhosts:          files dns\nnetworks:       files\n\nprotocols:      db files\nservices:       db files\nethers:         db files\nrpc:            db files\n\nnetgroup:       nis\n</code></pre>  <p>Si votre serveur DNS est sur un r\u00e9seau priv\u00e9, par exemple un serveur DNS sur un r\u00e9seau bancaire, il est possible que votre DNS ne puisse pas r\u00e9soudre tous les noms, par exemple <code>ping wwww.facebook.com</code> peut ne pas fonctionner, et donc dans ce cas vous serez dans l'impossibilit\u00e9 d'y acc\u00e9der.</p> <p>Il est possible de rajouter d'autres serveurs DNS, par exemple le serveru DNS de Google situ\u00e9 \u00e0 l'adresse <code>8.8.8.8</code>. Pour faire cela il suffit de modifier le fichier <code>/etc/resolv.conf</code> en ajoutant la ligne <code>Forward All to 8.8.8.8</code>, les noms non r\u00e9solus par votre serveru DNS, seront alors transmises au serveur DNS de Google.</p> <p>Un nom de la forme <code>wwww.facebook.com</code> avec <code>www.</code> au d\u00e9but et <code>.com</code> \u00e0 la fin est ce que l'on appelle un nom de domaine.</p> <p>Pourquoi cette synthaxe ?</p> <p>L'id\u00e9e est de grouper les choses ensembles. La derni\u00e8re partie est ce que l'on appelle le nom de domaine de plus haut niveau (top level domain name), il en existe peu, on peut par exemple citer <code>.com</code>, <code>.fr</code>, <code>.net</code>, <code>.edu</code>, <code>.org</code>, <code>.io</code>, etc. Ils sont cens\u00e9s repr\u00e9senter le but du site, <code>.com</code> pour les sites commerciaux, <code>.fr</code> pour les sites fran\u00e7ais, <code>.net</code> pour network, <code>.edu</code> pour les \u00e9coles/universit\u00e9s, <code>.org</code> pour les organisations \u00e0 but non lucratif, etc.</p> <p>Pour une adresse comme <code>google.com</code>, on peut alors hi\u00e9rarchiser de la fa\u00e7on suivante. On a la racine le <code>.</code> suivi du nom de domaine de plus haut niveau, <code>google</code> est le nom de domaine assign\u00e9 \u00e0 google, et en desous nous avons les sous-domaines, si l'on veut utiliser google maps on ira sur <code>maps.google.com</code>, pour se connecter \u00e0 son drive on ira sur <code>drive.google.com</code> etc.</p> <pre><code>graph TD\n\n  A1{{ Root &lt;br/&gt; . }}\n\n  B1{{ Top Level Domain Name &lt;br/&gt; .com }}\n\n  C1{{ Google }}\n\n  D1{{Subdomain &lt;br/&gt; mail}}\n  D2{{Subdomain &lt;br/&gt; drive}}\n  D3{{Subdomain &lt;br/&gt; www}}\n  D4{{Subdomain &lt;br/&gt; maps}}\n  D5{{Subdomain &lt;br/&gt; apps}}\n\n  A1-.-&gt;B1\n\n  B1-.-&gt;C1\n\n  C1-.-&gt;D1\n  C1-.-&gt;D2\n  C1-.-&gt;D3\n  C1-.-&gt;D4\n  C1-.-&gt;D5</code></pre> <pre><code>graph LR\n\n  A1[apps.google.com]\n\n  B1{{Org &lt;br/&gt; DNS }}\n\n  C1{{Root &lt;br/&gt; DNS }}\n\n  D1{{.com &lt;br/&gt; DNS }}\n\n  D2{{Google &lt;br/&gt; DNS }}\n\n  D3[216.58.221.78]\n\n  A1-.-&gt;B1\n\n  B1-.-&gt;C1\n\n  C1-.-&gt;D1\n\n  D1-.-&gt;D2\n  D2-.-&gt;D3\n\n  D3-.-&gt;|cache|A1</code></pre> <p><code>nslookup</code></p> <p><code>dig</code></p> <p><code>search domain.com</code> dans <code>/etc/resolv.conf</code></p>"},{"location":"devops/redis/","title":"Redis 101","text":"<p>Redis est une solution de stockage cl\u00e9-valeur pouvant \u00eatre utilis\u00e9 comme un service de queue entre diff\u00e9rents micro-services.</p>"},{"location":"devops/redis/#installation-sous-linux","title":"Installation sous Linux","text":"<p>Pour installer redis, on utilise le serveur redis via le package fourni.</p> <pre><code>sudo apt update\nsudo apt install redis-server\n</code></pre> <p>Ce qui installera redis et toutes ses d\u00e9pendances.</p> <p>Pour avoir un plus de contr\u00f4le sur le serveru redis, on peut le d\u00e9finir comme un service. Pour cela, on a besoin de modifier le fichier de configuration de redis.</p> <p>Si l'on regarde le fichier de configuration via la commande suivante.</p> <pre><code>sudo vim /etc/redis/redis.conf\n</code></pre> <p>On peut alors chercher l'option <code>supervised</code>, qui par d\u00e9faut est sur <code>no</code>, or si l'on travaille sur un syst\u00e8me Linux, les services peuvent \u00eatre manag\u00e9s par <code>systemd</code>. Il suffit alors de changer <code>supervised no</code> par <code>supervised systemd</code>, pour le d\u00e9finir comme un service sous Linux.</p> <p><pre><code># If you run Redis from upstart or systemd, Redis can interact with your\n# supervision tree. Options:\n#   supervised no      - no supervision interaction\n#   supervised upstart - signal upstart by putting Redis into SIGSTOP mode\n#   supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET\n#   supervised auto    - detect upstart or systemd method based on\n#                        UPSTART_JOB or NOTIFY_SOCKET environment variables\n# Note: these supervision methods only signal \"process is ready.\"\n#       They do not enable continuous liveness pings back to your supervisor.\nsupervised systemd\n</code></pre> On le relance alors pour prendre les changements en compte, et on peut aussi v\u00e9rifier son statut.</p> <pre><code>\u276f sudo systemctl restart redis.service\n\n\u276f sudo systemctl status redis.service\n\n\u25cf redis-server.service - Advanced key-value store\n     Loaded: loaded (/lib/systemd/system/redis-server.service; enabled; vendor preset: enabled)\n     Active: active (running) since Wed 2022-06-15 22:06:36 CEST; 5s ago\n       Docs: http://redis.io/documentation,\n             man:redis-server(1)\n    Process: 30903 ExecStart=/usr/bin/redis-server /etc/redis/redis.conf (code=exited, status=0/SUCCESS)\n   Main PID: 30904 (redis-server)\n      Tasks: 4 (limit: 76989)\n     Memory: 2.1M\n     CGroup: /system.slice/redis-server.service\n             \u2514\u250030904 /usr/bin/redis-server 127.0.0.1:6379\n\njuin 15 22:06:36 vorph-maison systemd[1]: Starting Advanced key-value store...\njuin 15 22:06:36 vorph-maison systemd[1]: redis-server.service: Cant open PID file /run/redis/redis-server.pid (yet?) after start: Operation not permitted\njuin 15 22:06:36 vorph-maison systemd[1]: Started Advanced key-value store.\n</code></pre>"},{"location":"devops/redis/#architecture","title":"Architecture","text":"<p>Redis a une architecture client-serveur et utilise un mod\u00e8le de demande-r\u00e9ponse. Cela signifie que vous (le client) vous connectez \u00e0 un serveur Redis via une connexion TCP, sur le port 6379 par d\u00e9faut. Vous demandez une action (comme une forme de lecture, d'\u00e9criture, d'obtention, de r\u00e9glage ou de mise \u00e0 jour), et le serveur vous renvoie une r\u00e9ponse.</p> <p>Il peut y avoir de nombreux clients qui parlent au m\u00eame serveur, ce qui est vraiment la raison d'\u00eatre de Redis ou de toute application client-serveur. Chaque client effectue une lecture (g\u00e9n\u00e9ralement bloquante) sur un socket en attendant la r\u00e9ponse du serveur.</p> <p>Pour communiquer avec ce serveur, on utilise <code>redis-cli</code></p> <p>Le <code>cli</code> de <code>redis-cli</code> signifie interface en ligne de commande, et le <code>server</code> de <code>redis-server</code> sert \u00e0 faire tourner un serveur. De la m\u00eame mani\u00e8re que vous ex\u00e9cuteriez python \u00e0 la ligne de commande, vous pouvez ex\u00e9cuter <code>redis-cli</code> pour acc\u00e9der \u00e0 une boucle interactive REPL (Read Eval Print Loop) o\u00f9 vous pouvez ex\u00e9cuter des commandes client directement depuis le shell.</p> <p>Par d\u00e9faut, redis \u00e9coute sur le port 6379 de l'adresse loopback 127.0.0.1.</p> <p>Pour acc\u00e9der au serveur, tape alors dans un terminal la commande <code>redis-cli</code>.</p> <p>La commande la plus simple pour v\u00e9rifer sur le serveur fonctionne correctement et de tapper <code>PING</code> et voir s'il nous r\u00e9pond.</p> PING<pre><code>\u276f redis-cli\n127.0.0.1:6379&gt; ping\nPONG\n</code></pre> on peut aussi \u00e9crire un message, attention aux quotes<pre><code>127.0.0.1:6379&gt; PING\nPONG\n127.0.0.1:6379&gt; PING hello\n\"hello\"\n127.0.0.1:6379&gt; PING hello how are you\n(error) ERR wrong number of arguments for 'ping' command\n127.0.0.1:6379&gt; PING \"hello how are you\"\n\"hello how are you\"\n</code></pre> <p>On quitte le serveur redis en tapant simplement <code>exit</code>.</p>"},{"location":"devops/redis/#redis-vu-comme-un-dictionnaire-python","title":"Redis vu comme un dictionnaire Python","text":"<p>Redis est l'acronyme de Remote Dictionnary Service.</p> <p>Dans les grandes largeurs, il y a de nombreus parall\u00e8les entre un dictionnaire python, ou de fa\u00e7on plus globale un table de hashage.</p> <ul> <li>Une db Redis stocke des donn\u00e9es sous la forme <code>cl\u00e9:valeur</code> et supportent des commandes telles que <code>GET</code>, <code>SET</code>, <code>DEL</code> et une centaine d'autres.</li> <li>Les cl\u00e9s sont toujours de <code>strings</code>.</li> <li>Les valeurs peuvent avoir diff\u00e9rents types, les plus connus \u00e9tant <code>string</code>, <code>list</code>, <code>hashes</code>, and <code>sets</code>.</li> </ul> <p>Prenons l'exemple tr\u00e8s simple suivant.</p> pays:capitale<pre><code>\u276f redis-cli\n127.0.0.1:6379&gt; set bahamas nassau\nOK\n127.0.0.1:6379&gt; set france paris\nOK\n127.0.0.1:6379&gt; get france\n\"paris\"\n127.0.0.1:6379&gt; get bahamas\n\"nassau\"\n127.0.0.1:6379&gt; get japan\n(nil)\n127.0.0.1:6379&gt; exit\n</code></pre> <p>La commande <code>set</code> permet d'ins\u00e9rer des donn\u00e9es dans la db Redis, pour ins\u00e9rer on suit le format <code>set cl\u00e9 valeur</code>, avec bien un espace entre <code>cl\u00e9</code> et <code>valeur</code>. Etant donn\u00e9e une cl\u00e9, on peut r\u00e9cup\u00e9rer sa valeur associ\u00e9e via la commande <code>get cl\u00e9</code>. Notez que m\u00eame si un jeu <code>cl\u00e9:valeur</code> n'a pas \u00e9t\u00e9 rentr\u00e9, R\u00e9dis ne remontera un message d'erreur mais <code>(nil)</code>, qui correspond au <code>None</code> de python.</p> <p>Il est aussi possible de rentrer plusieurs jeux <code>cl\u00e9:valeur</code> de fa\u00e7on simultan\u00e9e <code>mset cl\u00e91 valeur1 cl\u00e92 valeur2 cl\u00e93 valeur3 ...</code>, on r\u00e9cup\u00e8re les valeurs de fa\u00e7on similaire avec <code>mget cl\u00e91 cl\u00e92 cl\u00e93 ...</code>.</p> mset-mget<pre><code>127.0.0.1:6379&gt; mset belgium bruxelles germany berlin italy rome\nOK\n\n127.0.0.1:6379&gt; mget belgium france italy\n1) \"bruxelles\"\n2) \"paris\"\n3) \"rome\"\n</code></pre> <p>La commande <code>exists</code> permet de v\u00e9rifier si une cl\u00e9 (et uniquement une cl\u00e9, pas une valeur), est pr\u00e9sente en db en renvoyant un bool\u00e9en.</p> exists<pre><code>127.0.0.1:6379&gt; exists france\n(integer) 1\n127.0.0.1:6379&gt; exists paris\n(integer) 0\n127.0.0.1:6379&gt; exists japan\n(integer) 0\n</code></pre>"},{"location":"devops/redis/#hash-datatype","title":"Hash datatype","text":"<p>Un <code>hash</code> (hachage en fran\u00e7ais), pour comparer avec python, correspond \u00e0 un dictionnaire hi\u00e9rarchique avec un niveau de profondeur, ie un dictionnaire dont la valeur de chaque cl\u00e9 \"principale\" est un dictionnaire.</p> <p>Dictionnaire hi\u00e9rarchique avec un niveau de profondeur.<pre><code>data = {\n    \"user\": {\n        \"prenom\": \"mathieu\",\n        \"nom\": \"klimczak\",\n        \"github\": \"https://github.com/Klimorg\",\n    }\n}\n</code></pre> Un hachage est donc une correspondance de cha\u00eenes de caract\u00e8res, appel\u00e9 paires champ-valeur, qui se trouve sous une cl\u00e9 de niveau sup\u00e9rieur.</p> <p>Ins\u00e9rer un hash dans Redis se fait via la commande <code>hset cl\u00e9 champs valeur</code>, ici on a :</p> <ul> <li>une seule cl\u00e9 <code>user</code>,</li> <li>trois champs <code>pr\u00e9nom</code>, <code>nom</code>, <code>github</code>,</li> <li>les trois valeurs correspondantes.</li> </ul> <p>On peut r\u00e9cup\u00e9rer la valeur d'un champs particuliers via la commande <code>hget cl\u00e9 champs</code>.</p> <p>On peut aussi rentrer toutes les valeurs d'un hash en une seule fois avec la commande <code>hmset</code>, et tout r\u00e9cup\u00e9rer avec <code>hgetall cl\u00e9</code>.</p> hset hget hmset hgetall<pre><code>127.0.0.1:6379&gt; hset user prenom mathieu\n(integer) 1\n127.0.0.1:6379&gt; hset user nom klimczak\n(integer) 1\n127.0.0.1:6379&gt; hset user github \"https://github.com/Klimorg\"\n(integer) 1\n127.0.0.1:6379&gt; hget user nom\n\"klimczak\"\n127.0.0.1:6379&gt; hget user github\n\"https://github.com/Klimorg\"\n\n127.0.0.1:6379&gt; hget user mail\n(nil)\n\n127.0.0.1:6379&gt; hget user nom prenom\n(error) ERR wrong number of arguments for 'hget' command\n\n127.0.0.1:6379&gt; hmset user2 prenom paul nom tondelier github aucun\nOK\n127.0.0.1:6379&gt; hgetall user2\n1) \"prenom\"\n2) \"paul\"\n3) \"nom\"\n4) \"tondelier\"\n5) \"github\"\n6) \"aucun\"\n127.0.0.1:6379&gt;\n</code></pre> <p>Deux types de valeurs suppl\u00e9mentaires sont les listes et les ensembles, qui peuvent prendre la place d'un hash ou d'une cha\u00eene de caract\u00e8res comme valeur Redis.</p> <p>Les hachages, les listes et les ensembles ont chacun des commandes qui sont particuli\u00e8res \u00e0 ce type de donn\u00e9es donn\u00e9, qui sont dans certains cas indiqu\u00e9es par leur lettre initiale :</p> <ul> <li> <p>Hash : Les commandes permettant d'op\u00e9rer sur les hachages commencent par un H, comme <code>HSET</code>, <code>HGET</code> ou <code>HMSET</code>.</p> </li> <li> <p>Sets : Les commandes permettant d'op\u00e9rer sur des ensembles commencent par un S, comme <code>SCARD</code>, qui obtient le nombre d'\u00e9l\u00e9ments \u00e0 la valeur de l'ensemble correspondant \u00e0 une cl\u00e9 donn\u00e9e.</p> </li> <li> <p>Listes : Les commandes permettant d'op\u00e9rer sur des listes commencent par un L ou un R. Les exemples incluent <code>LPOP</code> et <code>RPUSH</code>. Le L ou le R fait r\u00e9f\u00e9rence au c\u00f4t\u00e9 de la liste sur lequel on op\u00e8re. Quelques commandes de liste sont \u00e9galement pr\u00e9c\u00e9d\u00e9es d'un B, qui signifie blocage. Une op\u00e9ration de blocage ne permet pas aux autres op\u00e9rations de l'interrompre pendant son ex\u00e9cution. Par exemple, <code>BLPOP</code> ex\u00e9cute un left-pop bloquant sur une structure de liste.</p> </li> </ul>"},{"location":"devops/redis/#sources","title":"Sources","text":"<ul> <li>How to Use Redis With Python</li> </ul>"},{"location":"devops/shell/","title":"Shell scripting","text":"<ul> <li>Google Shell Style Guide</li> </ul>"},{"location":"devops/shell/#introduction","title":"Introduction","text":"<p>Un script shell est une suite d'instruction shell le tout encapsul\u00e9 dans un fichier avec l'extension <code>.sh</code>.</p> <p>helloworld.sh<pre><code>echo \"Hello World shell\"\n</code></pre> On peut alors lancer un tel script via la commande suivante.</p> helloworld.sh<pre><code>bash helloworld.sh\n</code></pre>"},{"location":"devops/shell/#execution","title":"Ex\u00e9cution","text":"<p>Les scripts shell peuvent aussi \u00eatre d\u00e9finis comme des commandes ex\u00e9cutables.</p> <p>La convention veut alors que l'on ne met pas l'extension \u00e0 la fin, le fichier s'appelant alors simplement <code>helloworld</code>.</p> <p>Si on essaye tout de suite de lancer le script comme un ex\u00e9cutable, on aura l'erreur suivante.</p> <p><pre><code>\u276f helloworld\nzsh: command not found: helloworld\n</code></pre> Pour pouvoir le lancer en tant qu'ex\u00e9cutable, Linux a besoin de savoir dans quel chemin chercher ce script.</p> <p>On peut trouver la liste des chemins en tapant <code>echo $PATH</code>.</p> <pre><code>\u276f echo $PATH\n/home/vorph/gems/bin:/home/vorph/miniconda3/bin:/home/vorph/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n</code></pre> <p>Pour rajouter le chemin du script en tant qu'ex\u00e9cutable, on utilise la commande suivante.</p> <pre><code>export PATH=$PATH:chemin\n</code></pre> <pre><code>export PATH=$PATH:$pwd\n</code></pre> <p><pre><code>\u276f echo \"echo 'Hello World Shell'\" &gt; helloworld\n\n\u276f export PATH=$PATH:/home/vorph\n\n\u276f helloworld\nzsh: permission non accord\u00e9e: helloworld\n</code></pre> Reste \u00e0 d\u00e9finir la bonne permission pour qu'il soit disponible comme un ex\u00e9cutable. Cela se fait avec la commande <code>chmod</code>.</p> Autorise l'\u00e9x\u00e9cution<pre><code>chmod +x helloworld\n</code></pre> Cette fois ci \u00e7a va marcher<pre><code>\u276f echo \"echo 'Hello World Shell'\" &gt; helloworld\n\n\u276f export PATH=$PATH:/home/vorph\n\n\u276f chmod +x helloworld\n\n\u276f helloworld\nHello World Shell\n</code></pre>"},{"location":"devops/shell/#les-variables","title":"Les variables","text":""},{"location":"devops/shell/#definition-statique","title":"D\u00e9finition statique","text":"<p>Dans un script schell, les variables sont d\u00e9finies avec le symbole <code>$</code>, comme dans la commande permettant de voir le shell que l'on utilise.</p> <p><pre><code>echo $SHELL\n</code></pre> Il suffit alors de d\u00e9finir la valeur de la variable en d\u00e9but de script.</p> helloworld-variables.sh<pre><code>user=Vorphus\necho \"Hello World shell, it's $user\"\n</code></pre>  <p>Attention</p> <p>Un nom de variable ne peut contenir que des caract\u00e8res alphanum\u00e9riques ou underscore <code>_</code>. Shell est aussi sensible \u00e0 la casse.</p> <p><code>$variable_1</code> fontionnne, mais pas <code>$variable-1</code>.</p>  <p>Une varible peut aussi stocker le r\u00e9sultat d'un autre script, dans ce as l\u00e0, on d\u00e9finit alors la variable avec des parenth\u00e8ses en plus.</p> <pre><code>variable=$(script)\n</code></pre> <pre><code>\u276f variable=$(ls)\n\n\u276f echo $variable\ndemo_deploy\ndocs\nincludes\nsite\nazure_sdk.md\nDockerfile.dev\nLICENSE\nmakefile\nmkdocs.yml\nREADME.md\nrequirements.txt\n</code></pre> <p>Une variable \u00e0 laquelle ont rajouter quelque chose, comme du text, doit \u00eatre encapsul\u00e9 entre accolades <code>${...}</code>.</p> <pre><code>file_name=test\n\ncp $file_name ${file_name}_bkp\n</code></pre> <pre><code>FILE01=\"Japan\"\nFILE02=\"Egypt\"\nFILE03=\"Canada\"\n\ncd /home/vorph\n\necho \"Creating file called $FILE01\"\ntouch $FILE01\n\necho \"Creating file called $FILE02\"\ntouch $FILE02\n\necho \"Creating file called $FILE03\"\ntouch $FILE03\n</code></pre>"},{"location":"devops/shell/#definiion-dynamique","title":"D\u00e9finiion dynamique","text":""},{"location":"devops/shell/#les-arguments-en-ligne-de-commande","title":"Les arguments en ligne de commande","text":"<p>Dans un terminal, les arguments d\u00e9finis pour un script shell sont des variables sp\u00e9cifiques stock\u00e9es sous la variables <code>$k</code> o\u00f9 k correspond au k-i\u00e8me argument d\u00e9fini dans le terminal.</p> <ul> <li><code>$0</code> correspond au nom du script,</li> <li><code>$1</code> correspond au premier argument apr\u00e8s le nom du script,</li> <li>etc.</li> </ul> helloworld-cli.sh<pre><code>user=$1\necho \"Hello World shell, it's $user\"\n</code></pre> <pre><code>\u276f sh helloworld-cli.sh mathieu\n\nHello World shell, its mathieu\n</code></pre>"},{"location":"devops/shell/#inputs","title":"Inputs","text":"<p>Autre autre fa\u00e7on de d\u00e9finir une variable est de la demander comme input dans le terminal, cela se fait avoir la commande <code>read</code>, on peut rajouter l'argument <code>-p</code> (p pour \"prompt\") pour d\u00e9finir un texte \u00e0 afficher dans le terminal avant de donner la variable d'input.</p> helloworld-input.sh<pre><code>read -p \"What's your name ? \" user\necho \"Hello World shell, it's $user\"\n</code></pre>"},{"location":"devops/shell/#arithmetique","title":"Arithm\u00e9tique","text":"<p>Pour faire des calculs, on peut utiliser la commande <code>expr</code>, chacun des \u00e9l\u00e9ments de l'op\u00e9ration doit alors \u00eatre s\u00e9paar\u00e9 par un espace.</p> <pre><code>expr 6 + 3\n</code></pre> Pour une multiplication, le symbole * doit \u00eatre \u00e9chapp\u00e9<pre><code>expr 6 \\* 3\n</code></pre> <p>Au lieu d'utiliser une commande particuli\u00e8re, on peut utiliser <code>echo</code>, il faut alors mettre le calcul entre double parenth\u00e8ses.</p> <pre><code>A=6\nB=3\necho $((A+B))\necho $((A-B))\necho $((A/B))\necho $((A*B))\n</code></pre> bc pour basic computer -l pour les floats<pre><code>echo $A/$B | bc -l\n</code></pre>"},{"location":"devops/shell/#les-controles-logiques","title":"Les contr\u00f4les logiques","text":""},{"location":"devops/shell/#if","title":"<code>if</code>","text":"<pre><code>if [ condition1 ]\nthen\n    statement1\nelif [ condition2 ]\n    statement2\nelse\n    statement3\nfi\n</code></pre> <p>La condition d\u00e9finissant <code>if</code> est toujours comprises entre crochets, et un espace doit \u00eatre pr\u00e9sent entre chaque membre de la condition.</p> helloworld-if.sh<pre><code>read -p \"What's your name ? \" user\n\nif [ $user = mathieu ]\n  then echo \"Hello World shell, it's $user\"\nelse\n  echo \"Hello stranger !\"\nfi\n</code></pre> <ul> <li>=</li> <li>!=</li> <li>-eq</li> <li>-ne</li> <li>-gt</li> <li>-lt</li> </ul> <p>Les doubles crochets pour les conditions <code>[[ condtion ]]</code> sont une am\u00e9lioration des crochets simples, ils ne sont disponibles que pour <code>bash</code> et les shells plus r\u00e9cents. Ils permettents l'\u00e9criture de conditions plus fines.</p> <p>Pour g\u00e9rer plusieurs conditions \u00e0 la fois,</p> <ul> <li>on utilise <code>&amp;&amp;</code> pour l'intersection.</li> </ul> Si condition1 et condition2 sont vraies, alors...<pre><code>if [ condition1 ] &amp;&amp; [ condition2 ]\nthen\n    ...\nfi\n</code></pre> Si condition1 et condition2 sont vraies, alors...<pre><code>if [[ condition1 &amp;&amp; condition2 ]]\nthen\n    ...\nfi\n</code></pre> <ul> <li>on utilise <code>||</code> pour l'union.</li> </ul> Si condition1 ou condition2 est vraie, alors...<pre><code>if [ condition1 ] || [ condition2 ]\nthen\n    ...\nfi\n</code></pre> Si condition1 ou condition2 est vraie, alors...<pre><code>if [[ condition1 || condition2 ]]\nthen\n    ...\nfi\n</code></pre> <p>Pour le syst\u00e8me de fichier, on a les commandes suivantes.</p>    Commande R\u00e9sultat     <code>[ -e FILE ]</code> si le fichier existe   <code>[ -d FILE ]</code> si le fichier existe et est un r\u00e9pertoire   <code>[ -s FILE ]</code> si le fichier existe et a un taille plus grande que 0   <code>[ -x FILE ]</code> si le fichier est \u00e9xecutable   <code>[ -w FILE ]</code> si le fichier est \u00e9crivable    <pre><code>if [ -d \"/home/vorph/caleston\" ]\nthen\n  echo \"Directory exists\"\nelse\n  echo \"Directory not found\"\nfi\n</code></pre>"},{"location":"devops/shell/#for","title":"<code>for</code>","text":"<pre><code>for machin in list_of_machin\ndo\n    commande $machin\ndone\n</code></pre> La liste peut \u00eatre sstock\u00e9e dans un autre fichier<pre><code>for machin in $(cat list.txt)\ndo\n    commande $machin\ndone\n</code></pre> helloworld-for.sh<pre><code>read -p \"What's your name ? \" user\n\nif [ \"$user\" = mathieu ]\n  then\n  echo \"Hello World shell, it's $user\"\n  for item in $(cat list.txt)\n    do\n    echo \"$item\"\n    done\nelse\n  echo \"Hello stranger !\"\nfi\n</code></pre> <p>Pour faire une it\u00e9ration sur des entiers, on peut utiliser la commande <code>{0..k}</code>, comme <code>range(k)</code> en python.</p> helloworld-for2.sh<pre><code>read -p \"What's your name ? \" user\n\nif [ $user = mathieu ]\n  then\n  echo \"Hello World shell, it's $user\"\n  for item in {1..5}\n    do\n    echo $item\n    done\nelse\n  echo \"Hello stranger !\"\nfi\n</code></pre>"},{"location":"devops/shell/#ecriture-de-la-boucle-en-style-c","title":"Ecriture de la boucle en style C","text":"<pre><code>for (( machin = 0; machin &lt;= 5; machin++ ))\ndo\n    commande $machin\ndone\n</code></pre>"},{"location":"devops/shell/#exemples","title":"Exemples","text":"Compte le nombre de lignes dans des fichiers<pre><code>for file in $(ls)\ndo\n    echo \"Line count of $file is $(cat $file | wc -l)\"\ndone\n</code></pre> Installer les packages d'une liste<pre><code>for package in $(cat install-packages.txt)\ndo\n    sudo apt-get -y install $package\ndone\n</code></pre> Compte le nombre de requ\u00eates GET POST DELETE dans des fichiers de log<pre><code>echo -e \" Log name   \\t      GET      \\t      POST    \\t   DELETE \"\necho -e \"------------------------------------------------------------\"\n\nfor app in $(cat /tmp/assets/apps.txt)\ndo\n  get_requests=$(cat /var/log/apps/${app}_app.log | grep \"GET\" | wc -l)\n  post_requests=$(cat /var/log/apps/${app}_app.log | grep \"POST\" | wc -l)\n  delete_requests=$(cat /var/log/apps/${app}_app.log | grep \"DELETE\" | wc -l)\n  echo -e \" ${app}    \\t ${get_requests}    \\t    ${post_requests}   \\t   ${delete_requests}\"\n\ndone\n</code></pre> rename all files within the images folder that has extension jpeg to jpg<pre><code>for file in $(ls images)\ndo\n        if [[ $file = *.jpeg ]]\n                then\n                new_name=$(echo $file| sed 's/jpeg/jpg/g')\n                mv images/$file images/$new_name\n        fi\ndone\n</code></pre>"},{"location":"devops/shell/#while","title":"<code>while</code>","text":"Structure globale<pre><code>while [ condition ]\ndo\n    intruction\n    r\u00e9\u00e9valuation_de_la_condition\ndone\n</code></pre> helloworld-while.sh<pre><code>read -p \"What's your name ? \" user\n\nwhile [ $user = mathieu ]\ndo\n  echo \"Hello World shell, it's $user\"\n  for item in $(cat list.txt)\n    do\n    echo $item\n    done\n  read -p \"What's your name ? \" user\ndone\nexit\n</code></pre> <p>Tant que le nom d'utilisateur donn\u00e9 dans le terminal est <code>mathieu</code>, il \u00e9crira et listera les \u00e9lements qui sont pr\u00e9sents dans <code>list.txt</code>. avant le <code>done</code>, il est n\u00e9cessaire de r\u00e9\u00e9valuer la valeur de <code>user</code> pour savoir si l'on reste dans la boule <code>while</code> o\u00f9 si l'on en sort.</p> Exemple<pre><code>while true\ndo\n    echo \"1. Shutdown\"\n    echo \"2. Restart\"\n    echo \"3. Exit Menu\"\n    read -p \"Enter your choice : \" choice\n\n    if [ $choice -eq 1 ]\n    then\n        shutdown now\n    elif [ $choice -eq 2 ]\n    then\n        shutdown -r now\n    elif [ $choice -eq 3 ]\n    then\n        break\n    else\n        continue\n    fi\ndone\n</code></pre> calculatrice basique<pre><code>while true\ndo\n  echo \"1. Add\"\n  echo \"2. Subtract\"\n  echo \"3. Multiply\"\n  echo \"4. Divide\"\n  echo \"5. Quit\"\n\n  read -p \"Enter your choice: \" choice\n\n  if [ $choice -eq 1 ]\n  then\n        read -p \"Enter Number1: \" number1\n        read -p \"Enter Number2: \" number2\n        echo Answer=$(( $number1 + $number2 ))\n  elif [ $choice -eq 2 ]\n  then\n        read -p \"Enter Number1: \" number1\n        read -p \"Enter Number2: \" number2\n        echo Answer=$(( $number1 - $number2 ))\n  elif [ $choice -eq 3 ]\n  then\n        read -p \"Enter Number1: \" number1\n        read -p \"Enter Number2: \" number2\n        echo Answer=$(( $number1 * $number2 ))\n  elif [ $choice -eq 4 ]\n  then\n        read -p \"Enter Number1: \" number1\n        read -p \"Enter Number2: \" number2\n        echo Answer=$(( $number1 / $number2 ))\n  elif [ $choice -eq 5 ]\n  then\n    break\n  fi\n\ndone\n</code></pre>"},{"location":"devops/shell/#case","title":"<code>case</code>","text":"<p><code>case</code> permet de remplacer les longues listes de <code>if-then-else</code> en d\u00e9finissant les choix diff\u00e9rements.</p> Exemple<pre><code>while true\ndo\n    echo \"1. Shutdown\"\n    echo \"2. Restart\"\n    echo \"3. Exit Menu\"\n    read -p \"Enter your choice : \" choice\n\n    case $choice in\n\n        1) shutdown now\n        ;;\n        2) shutdown -r now\n        ;;\n        3) break\n        ;;\n        *) continue\n        ;;\n    esac\ndone\n</code></pre> calculatrice avec case<pre><code>while true\ndo\n  echo \"1. Add\"\n  echo \"2. Subtract\"\n  echo \"3. Multiply\"\n  echo \"4. Divide\"\n  echo \"5. Quit\"\n\n  read -p \"Enter your choice: \" choice\n\n  case $choice in\n    1)\n        read -p \"Enter Number1: \" number1\n        read -p \"Enter Number2: \" number2\n        echo Answer=$(( $number1 + $number2 ))\n        ;;\n    2)\n        read -p \"Enter Number1: \" number1\n        read -p \"Enter Number2: \" number2\n        echo Answer=$(( $number1 - $number2 ))\n        ;;\n\n    3)\n        read -p \"Enter Number1: \" number1\n        read -p \"Enter Number2: \" number2\n        echo Answer=$(( $number1 * $number2 ))\n        ;;\n    4)\n        read -p \"Enter Number1: \" number1\n        read -p \"Enter Number2: \" number2\n        echo Answer=$(( $number1 / $number2 ))\n        ;;\n    5)\n        break\n        ;;\n  esac\n\ndone\n</code></pre>"},{"location":"devops/shell/#shebang","title":"Shebang","text":"<p>Comme expliqu\u00e9 dans la partie sur Linux, il existe plusieurs types de shells, et chaque shell peut avoir un jeu de commandes sp\u00e9cifiques qui ne marcheront pas dans un autre shell.</p> <p>Par exemple, si l'on reprend le script suivant.</p> helloworld-for2.sh<pre><code>read -p \"What's your name ? \" user\n\nif [ $user = mathieu ]\n  then\n  echo \"Hello World shell, it's $user\"\n  for item in {1..5}\n    do\n    echo $item\n    done\nelse\n  echo \"Hello stranger !\"\nfi\n</code></pre> <p>la commande <code>{1..5}</code> permettant de d\u00e9finir un intervalle n'est valable que pour le shell <code>bash</code>, pour le <code>dash</code> : Debian Almquist SHell, on aura le r\u00e9sultat suivant.</p> erreur<pre><code>sh helloworld-for2.sh\nWhat's your name ? mathieu\nHello World shell, it's mathieu\n{1..5}\n</code></pre> <p>Pour faire comprendre \u00e0 votre terminal quel environnement utiliser pour lancer le script, on utilise un shebang. Un shebang est une ligne rajout\u00e9e au tout d\u00e9but d'un script d\u00e9finissant quel shell doit \u00eatre utiliser pour lancer le script.</p> <p>Un shebang s'\u00e9crit de cette fa\u00e7on : <code>#!/bin/bash</code>, <code>#!</code> suivi de l'adresse du shell \u00e0 utiliser. Cela va automatiquement instruire le terminal qu'il faudra alors lancer le script via le shell bash.</p>  <p>Attention</p> <p>Le shebang n'est utile que si le script doit se lancer en \u00e9xecutable, ie avec <code>chmod +x</code>.</p> <p>Taper <code>sh helloworld-for2.sh</code>, m\u00eame avec le shebang <code>#!/bin/bash</code> ajout\u00e9 r\u00e9sultera en une erreur comme au dessus car l\u00e0 on a explicitement sp\u00e9cifi\u00e9 que l'on voulait utilser <code>sh</code> et non <code>bash</code>.</p> <p>Par contre, lancer <code>./helloworld-for2.sh</code> ne posera aucun soucis, le shebang disant d'utiliser <code>bash</code> pour lancer le script.</p>  <p><code>#!/bin/sh</code> est par exemple le shebang pour utiliser le <code>shell</code> classique.</p>"},{"location":"devops/shell/#exit-codes","title":"Exit codes","text":"<p>Tout script shell qui finit son \u00e9xecution produit un code de sortie (exit code) et un statut de sortie (exit status).</p> <ul> <li>Si <code>exit status = 0</code> alors le script s'est termin\u00e9 sans encombres.</li> <li>Si <code>exit status &gt; 0</code> alors le script s'est termin\u00e9 suite \u00e0 une erreur.</li> </ul> <p>Les codes de sorties ne sont pas affich\u00e9s dans le terminal une fois le script fini. Le code de sortie d'un script est stock\u00e9 dans une variable sp\u00e9cifique : <code>?</code>, pour voir le code de sortie d'une commande, on tape alors <code>echo $?</code>.</p> <p>Pour d\u00e9finir explicitement le code de sortie \u00e0 retourner lors d'un probl\u00e8me ou autre dans votre script, on utilise la commande <code>exit k</code> o\u00f9 k est un chiffre compris en 0 et 255, g\u00e9n\u00e9ralement on utilise 1 pour dire qu'il y a eu un probl\u00e8me.</p>"},{"location":"devops/shell/#les-fonctions","title":"Les fonctions","text":"<pre><code>#!/bin/bash\n\ncreate_dataset(){\n    #creation d'un repertoire\n    mkdir ./datas/raw_dataset\n    #boucle sur tous les \u00e9l\u00e9ments d'un repertoire donn\u00e9\n    for f in ./datas/raw_datas/*;\n    do\n        if [ -d \"$f\" ];\n        then\n            # $f is a directory\n            b=$(basename $f)\n            # on r\u00e9cup\u00e8re le nom du repertoire\n            echo \"Making new directories for\" $b\n            mkdir ./datas/raw_dataset/$b\n            #on cr\u00e9e un dossier avec le m\u00eame nom\n            #ls $f/ | head -$1\n            echo \"Copying the first $1 pictures for folder $b\"\n            for F in $(ls $f/ | sort | head -$1);\n            do\n                cp $f/$F ./datas/raw_dataset/$b/$F\n            done\n        fi\n    done\n    echo \"Done.\"\n}\n\ncreate_dataset $1\n</code></pre> The function add must echo the result so that it can be captured in the result variable.<pre><code>function add(){\n  sum=$(( $1 + $2 ))\n  echo $sum\n}\n\nresult=$(add 3 5)\necho \"The result is $result\"\n</code></pre>"},{"location":"devops/shell/#checking","title":"Checking","text":"<p>ShellCheck</p>"},{"location":"devops/shell/#lire-un-fichier-csv","title":"Lire un fichier CSV","text":""},{"location":"devops/shell/#misc","title":"Misc","text":"<pre><code>cat &gt; setup-db.sql &lt;&lt;-EOF\n  CREATE DATABASE ecomdb;\n  CREATE USER 'ecomuser'@'localhost' IDENTIFIED BY 'ecompassword';\n  GRANT ALL PRIVILEGES ON *.* TO 'ecomuser'@'localhost';\n  FLUSH PRIVILEGES;\nEOF\n</code></pre>"},{"location":"devops/terminal/","title":"Just enough terminal knowledge to shine in society","text":"<ul> <li>The Missing Semester of Your CS Education</li> <li>Great Practical Ideas in CS 2017</li> <li>Great Practical Ideas in CS 2021</li> </ul>"},{"location":"devops/terminal/#curl","title":"curl","text":"<ul> <li>How to start using Curl and why: a hands-on introduction</li> </ul>"},{"location":"devops/terminal/#httpie","title":"httpie","text":"<p>httpie est un client en ligne de commande pour tester les API en http et https. Il poss\u00e8de une syntaxe simple et \u00e9l\u00e9gante, et se combine tr\u00e8s bien avec l'outil suivant pour traiter les donn\u00e9es JSON.</p> <p>Suivant l'adresse sur laquelle on fait la requ\u00eate, <code>httpie</code> d\u00e9tecte automatiquement si la requ\u00eate demand\u00e9e sera une requ\u00eate <code>POST</code> ou <code>GET</code>.</p>  <p>Exemple</p> <p>Une requ\u00eate <code>GET</code> sur mon API perso <code>fastapi.mathieuklimczak.com/inferences/</code> se fait alors simplement comme \u00e7a.</p> <pre><code>https fastapi.mathieuklimczak.com/inferences/\n\nHTTP/1.1 200 OK\nContent-Length: 516\nContent-Type: application/json\nDate: Thu, 12 May 2022 14:37:47 GMT\nServer: uvicorn\n\n[\n    {\n        \"confidence\": 1.0,\n        \"id\": 1,\n        \"inference_date\": \"2022-03-17\",\n        \"inference_time\": \"12:50:02\",\n        \"num_detections\": 0\n    },\n    {\n        \"confidence\": 1.0,\n        \"id\": 2,\n        \"inference_date\": \"2022-03-17\",\n        \"inference_time\": \"14:35:50\",\n        \"num_detections\": 0\n    },\n    {\n        \"confidence\": 1.0,\n        \"id\": 3,\n        \"inference_date\": \"2022-03-17\",\n        \"inference_time\": \"14:35:50\",\n        \"num_detections\": 0\n    },\n    {\n        \"confidence\": 1.0,\n        \"id\": 4,\n        \"inference_date\": \"2022-03-17\",\n        \"inference_time\": \"14:35:50\",\n        \"num_detections\": 0\n    },\n    {\n        \"confidence\": 1.0,\n        \"id\": 5,\n        \"inference_date\": \"2022-03-17\",\n        \"inference_time\": \"14:35:50\",\n        \"num_detections\": 0\n    }\n]\n</code></pre>"},{"location":"devops/terminal/#jq","title":"jq","text":"<p>jq est une outil de parsing des donn\u00e9es json dans le terminal. Il est capable de slicer, filtrer et transformer la donn\u00e9es structur\u00e9es de la m\u00eame fa\u00e7on que <code>sed</code>, <code>awk</code> ou <code>grep</code>.</p>  <p>Exemple</p> <p>Pour ne r\u00e9cup\u00e9rer que les donn\u00e9es, sans le header de la requ\u00eate, on peut alors combiner <code>httpie</code> et <code>jq</code>.</p> <pre><code>\u276f https fastapi.mathieuklimczak.com/inferences/ | jq .\n\n[\n  {\n    \"inference_date\": \"2022-03-17\",\n    \"inference_time\": \"12:50:02\",\n    \"num_detections\": 0,\n    \"confidence\": 1,\n    \"id\": 1\n  },\n  {\n    \"inference_date\": \"2022-03-17\",\n    \"inference_time\": \"14:35:50\",\n    \"num_detections\": 0,\n    \"confidence\": 1,\n    \"id\": 2\n  },\n  {\n    \"inference_date\": \"2022-03-17\",\n    \"inference_time\": \"14:35:50\",\n    \"num_detections\": 0,\n    \"confidence\": 1,\n    \"id\": 3\n  },\n  {\n    \"inference_date\": \"2022-03-17\",\n    \"inference_time\": \"14:35:50\",\n    \"num_detections\": 0,\n    \"confidence\": 1,\n    \"id\": 4\n  },\n  {\n    \"inference_date\": \"2022-03-17\",\n    \"inference_time\": \"14:35:50\",\n    \"num_detections\": 0,\n    \"confidence\": 1,\n    \"id\": 5\n  }\n]\n</code></pre> <p>Pour r\u00e9cup\u00e9rer le premier \u00e9l\u00e9ment de la liste, on tape la commande suivante.</p> <pre><code>\u276f https fastapi.mathieuklimczak.com/inferences/ | jq \".[0]\"\n\n{\n  \"inference_date\": \"2022-03-17\",\n  \"inference_time\": \"12:50:02\",\n  \"num_detections\": 0,\n  \"confidence\": 1,\n  \"id\": 1\n}\n</code></pre> <p>Pour avoir uniquement la date de l'inf\u00e9rence.</p> <pre><code>\u276f https fastapi.mathieuklimczak.com/inferences/ | jq \".[0].inference_date\"\n\n\"2022-03-17\"\n</code></pre> <pre><code>\u276f https fastapi.mathieuklimczak.com/inferences/ | jq  '.[] | {id : .id, date : .inference_date}'\n\n{\n  \"id\": 1,\n  \"date\": \"2022-03-17\"\n}\n{\n  \"id\": 2,\n  \"date\": \"2022-03-17\"\n}\n{\n  \"id\": 3,\n  \"date\": \"2022-03-17\"\n}\n{\n  \"id\": 4,\n  \"date\": \"2022-03-17\"\n}\n{\n  \"id\": 5,\n  \"date\": \"2022-03-17\"\n}\n\n\u276f https fastapi.mathieuklimczak.com/inferences/ | jq  '[.[] | {id : .id, date : .inference_date}]'\n\n[\n  {\n    \"id\": 1,\n    \"date\": \"2022-03-17\"\n  },\n  {\n    \"id\": 2,\n    \"date\": \"2022-03-17\"\n  },\n  {\n    \"id\": 3,\n    \"date\": \"2022-03-17\"\n  },\n  {\n    \"id\": 4,\n    \"date\": \"2022-03-17\"\n  },\n  {\n    \"id\": 5,\n    \"date\": \"2022-03-17\"\n  }\n]\n</code></pre>"},{"location":"devops/terraform/","title":"Terraform 101","text":""},{"location":"devops/terraform/#infrastructure-as-code-iac","title":"Infrastructure as Code : IaC","text":"<p>Pour provisionner des ressources chez AWS, GCP, Azure, etc il est possible de passer par l'interface d'administration. Lorsque l'on a beaucoup de ressources \u00e0 cr\u00e9er cela est loin d'\u00eatre optimal.</p> <p>L'id\u00e9e est alors de coder tout cela, par exemple en passantt par du shell et l'interface en ligne de commande fournie pour rendre tout cela plus modulaire.</p>  <p>Exemple d'IaC</p> Creation de 2 AZML workspace dans 2 resource groups<pre><code>#!/bin/bash\nBLUE='\\033[1;34m'\nGREEN='\\033[1;32m'\nRED='\\033[1;31m'\nNC='\\033[0m' # No Color\n\ncreate_shared_resources(){\n\n    resourceGroup1=${1:-\"RG-EU-TEST-MLOPS1\"}\n    resourceGroup2=${2:-\"RG-EU-TEST-MLOPS2\"}\n    workspace1=${3:-\"workspace-mlops-1\"}\n    workspace2=${4:-\"workspace-mlops-2\"}\n    location=${5:-\"westeurope\"}\n\n    echo -e \"${BLUE}resourceGroup1${NC}  : ${GREEN}$resourceGroup1${NC}\"\n    echo -e \"${BLUE}resourceGroup2${NC}  : ${GREEN}$resourceGroup2${NC}\"\n    echo -e \"${BLUE}workspace1${NC}      : ${GREEN}$workspace1${NC} \"\n    echo -e \"${BLUE}workspace2${NC}      : ${GREEN}$workspace2${NC}\"\n    echo -e \"${BLUE}location${NC}        : ${GREEN}$location${NC}\"\n\n\n    if ! az --version &gt; /dev/null 2&gt;&amp;1;\n        then\n            echo -e \"${RED}Failure: az cli is not installed. Please install it first.${NC}\" &gt;&amp;2\n            exit 1\n        else\n            echo -e \"${GREEN}Success: az cli is installed, processing.${NC}\"\n    fi\n\n    echo -e \"${BLUE}Logging into your azure.${NC}\"\n    az login --tenant XXXXXXX.onmicrosoft.com\n\n    echo -e \"${BLUE}Create first resource group${NC}\"\n    az group create -l \"$location\" -n \"$resourceGroup1\"\n\n    echo -e \"${BLUE}Create second resource group${NC}\"\n    az group create -l \"$location\" -n \"$resourceGroup2\"\n\n    echo -e \"${BLUE}Create third resource group containing storage account${NC}\"\n    az group create -l \"$location\" -n \"XXXXXXX\"\n\n    echo -e \"${BLUE}Create storage account${NC}\"\n    az storage account create --name \"storagemlops\" --resource-group \"XXXXXXX\"\n\n    echo -e \"${BLUE}Get storage account resrouce id storage account${NC}\"\n    storage_id=$(az storage account show \\\n    --name storagemlops \\\n    --resource-group XXXXXXX \\\n    --query id \\\n    --output tsv)\n\n    echo -e \"${BLUE}Create workspace 1${NC}\"\n    az ml workspace create --file workspace1.yaml --resource-group \"$resourceGroup1\"\n\n    echo -e \"${BLUE}Create workspace 2${NC}\"\n    az ml workspace create --file workspace2.yaml --resource-group \"$resourceGroup2\"\n}\n\n\ncreate_shared_resources \"$2\" \"$2\" \"$3\" \"$4\" \"$5\"\n</code></pre>  <p>Le code ci dessus, bien que fonctionnel, demande de savoir programmer en shell, et n'est pas r\u00e9utilisable pour autre chose que ce pour quoi il est pr\u00e9vu. D'o\u00f9 le d\u00e9veloppement de solutions comme Terraform et Ansible.</p> <p>En solution d'IaC, on peut par exemple citer :</p> <ul> <li>Docker,</li> <li>Ansible,</li> <li>Terraform,</li> <li>CloudFormation,</li> <li>Vagrant,</li> <li>Packer,</li> <li>Saltstack,</li> <li>Puppet.</li> </ul> <p>Les solutions d'IaC se rangent g\u00e9n\u00e9ralement en 3 grandes sous-familles.</p>    Configuration Management Server Templating Provisionnement     Ansible Docker Terraform   Puppet Packer CloudFormation   Saltstack Vagrant    Chef"},{"location":"devops/terraform/#configuration-management","title":"Configuration Management","text":"<p>Fait pour:</p> <ul> <li>Installer et g\u00e9rer les softwares</li> <li>Maintenir une structure standard</li> <li>Le contr\u00f4le de version</li> <li>Etre idempotent</li> </ul>"},{"location":"devops/terraform/#server-templating","title":"Server Templating","text":"<ul> <li>Fait pour d\u00e9ployer des softwares et d\u00e9pendances pr\u00e9-install\u00e9es</li> <li>Les exemples les plus courants de tels templates sont des VM ou des images Docker.</li> <li>Ce genre d'infrastructure est immutable. Si on veut la changer, il faut changer le template.</li> </ul>"},{"location":"devops/terraform/#provisionnement","title":"Provisionnement","text":"<p>Premet de d\u00e9ployer des ressources d'infrastructure immutable, comme des serveurs, des bases de donn\u00e9es, des composantes r\u00e9seaux, etc.</p> <p>La plupart de ses solutions sont multi-fournisseurs (AWS, GCP, Azure, etc)</p>"},{"location":"devops/terraform/#terraform","title":"Terraform","text":"<p>Installation</p>"},{"location":"devops/terraform/#langage-hcl","title":"Langage HCL","text":"<p>Les fichier terraform sont des fichiers avec l'extension <code>.tf</code> et sont \u00e9crits en HashiCorp Configuration Language (HCL).</p> <p>Un fichier terraform \u00e9crit en HCL est compos\u00e9 d'une suite d'instructions ayant la syntaxe suivante.</p> <pre><code>&lt;resource&gt; &lt;param\u00eatres&gt; {\n  cl\u00e91 = valeur1\n  cl\u00e92 = valeur2\n}\n</code></pre> <p>Par exemple, si l'on souhaite \u00e9crire un fichier local gr\u00e2ce \u00e0 terraform, on le fait de la mani\u00e8re suivante.</p> <pre><code>resource \"local_file\" \"hello\" {\n  filename = \"hello.txt\"\n  content  = \"hello from tf !\"\n}\n</code></pre> <pre><code>block_name \"resource_type\" \"resource_name\" {\n    # arguments specifics to the resource type\n    filename = \"hello.txt\"\n    content  = \"hello from tf !\"\n}\n</code></pre>"},{"location":"devops/terraform/#tf-workflow","title":"TF Workflow","text":"<ol> <li>R\u00e9daction du fichier <code>.tf</code>.</li> <li>Initialisation via <code>terraform init</code>.</li> <li>Review via <code>terraform plan</code>.</li> <li>Application via <code>terraform apply</code>.</li> </ol> <pre><code>\u276f terraform init\n\nInitializing the backend...\n\nInitializing provider plugins...\n- Finding latest version of hashicorp/local...\n- Installing hashicorp/local v2.2.3...\n- Installed hashicorp/local v2.2.3 (signed by HashiCorp)\n\nTerraform has created a lock file .terraform.lock.hcl to record the provider\nselections it made above. Include this file in your version control repository\nso that Terraform can guarantee to make the same selections by default when\nyou run \"terraform init\" in the future.\n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running \"terraform plan\" to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.\n\u276f terraform plan\n\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # local_file.hello will be created\n  + resource \"local_file\" \"hello\" {\n      + content              = \"hello from tf !\"\n      + directory_permission = \"0777\"\n      + file_permission      = \"0777\"\n      + filename             = \"hello.txt\"\n      + id                   = (known after apply)\n    }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nNote: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if you run \"terraform apply\" now.\n\u276f terraform plan -out local.tfplan\n\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # local_file.hello will be created\n  + resource \"local_file\" \"hello\" {\n      + content              = \"hello from tf !\"\n      + directory_permission = \"0777\"\n      + file_permission      = \"0777\"\n      + filename             = \"hello.txt\"\n      + id                   = (known after apply)\n    }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nSaved the plan to: local.tfplan\n\nTo perform exactly these actions, run the following command to apply:\n    terraform apply \"local.tfplan\"\n\u276f terraform apply \"local.tfplan\"\nlocal_file.hello: Creating...\nlocal_file.hello: Creation complete after 0s [id=9e87d6c4005d4f68dee4a2ab989e243fd56f7a5f]\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n\u276f terraform show\n# local_file.hello:\nresource \"local_file\" \"hello\" {\n    content              = \"hello from tf !\"\n    directory_permission = \"0777\"\n    file_permission      = \"0777\"\n    filename             = \"hello.txt\"\n    id                   = \"9e87d6c4005d4f68dee4a2ab989e243fd56f7a5f\"\n}\n</code></pre>"},{"location":"devops/tls_ssl/","title":"Just enough SSL TLS certificates","text":"<p>Un certificat est utilis\u00e9 pour garantir la confiance entre deux devices pendant une transaction.</p> <p>Par exemple, lorsqu'un utilisateur souhaite se connecter \u00e0 un serveur web, les certificats TLS sont l\u00e0 pour assurer que la communication entre les deux est correctement chiffr\u00e9e et que le serveur est l\u00e0 o\u00f9 il pr\u00e9tend \u00eatre.</p> <p>Il existe plusieurs m\u00e9thodes de chiffrements.</p>"},{"location":"devops/tls_ssl/#chiffrement-symetrique","title":"Chiffrement sym\u00e9trique","text":"<p>La m\u00e9thode la plus simple pour chiffrer et acc\u00e9der \u00e0 des donn\u00e9es est via un couple login/password.</p> <p><pre><code>sequenceDiagram\n    participant Client\n    participant WebServer\n\n    Note over Client: G\u00e9n\u00e8re une requ\u00eate Q\n    loop Chiffrement\n        Client-&gt;&gt;Client: Chiffrement via K\n    end\n\n    activate Client\n    Client-&gt;&gt;WebServer: Enc(Q)\n    Client-&gt;&gt;WebServer: cl\u00e9 K\n    deactivate Client\n\n    loop D\u00e9chiffrement\n        WebServer-&gt;&gt;WebServer: D\u00e9chiffrement via K\n    end\n\n    Note over WebServer: Tra\u00eetement de Q\n    Note over WebServer: G\u00e9n\u00e8re une r\u00e9ponse A\n\n    loop Chiffrement\n        WebServer-&gt;&gt;WebServer: Chiffrement via K\n    end\n\n    activate WebServer\n    WebServer-&gt;&gt;Client: Enc(A)\n    deactivate WebServer\n\n    loop D\u00e9chiffrement\n        Client-&gt;&gt;Client: D\u00e9chiffrement via K\n    end</code></pre> Le probl\u00e8me du chiffrement sym\u00e9trique est que le client et le webserver utilise la m\u00eame cl\u00e9 pour chiffrer et d\u00e9chiffrer les messages, de plus comme la cl\u00e9 doit voyager (au moins un fois lors de la connexion initiale), ce type de chiffrement est susceptible \u00e0 des attaques du type \"man in the middle\".</p>"},{"location":"devops/tls_ssl/#chiffrement-asymetrique","title":"Chiffrement asym\u00e9trique","text":"<p>Plut\u00f4t que d'utiliser une seule cl\u00e9, le chiffrement asym\u00e9trique utilise une paire de cl\u00e9s: une cl\u00e9 publique et une cl\u00e9 priv\u00e9e. Pour la suite de l'explication, on peut penser \u00e0 la cl\u00e9 publique comme un cadenas publique.</p> <p>Le chiffrement asym\u00e9trique est \u00e9galement connu sous le nom de cryptographie \u00e0 cl\u00e9 publique.</p> <p>Un cadenas publique est mise gratuitement \u00e0 la disposition de toute personne susceptible de vouloir chiffrer le message.</p> <p>La seconde cl\u00e9 priv\u00e9e est gard\u00e9e secr\u00e8te afin que seul l'utilisateur initial puisse la conna\u00eetre.</p> <p>Un message crypt\u00e9 \u00e0 l'aide d'un cadenas publique ne peut \u00eatre d\u00e9chiffr\u00e9 qu'\u00e0 l'aide d'une cl\u00e9 priv\u00e9e, tandis qu'un message chiffr\u00e9 \u00e0 l'aide d'une cl\u00e9 priv\u00e9e peut \u00eatre d\u00e9chiffr\u00e9 \u00e0 l'aide d'un cadenas publique.</p> <p>Seul le cadenas publique transite vers le serveur, et l'unique cl\u00e9 pouvant d\u00e9bloquer l'acc\u00e8s est alors la cl\u00e9 priv\u00e9e contenue sur le device du Client.</p> <p>Une commande du type <code>ssh -i id_rsa user1@server1</code> permet alors de se connecter en ssh au serveur.</p> <p>L'ensemble des cl\u00e9s</p>  <p>Exemple</p> <p>Dans le cas o\u00f9 l'on souhaite s\u00e9curiser l'acc\u00e8s \u00e0 un WebServer via SSH. On peut alors g\u00e9n\u00e9rer la cl\u00e9 priv\u00e9e et le cadenas via la commande <code>ssh-keygen</code>. On a alors la cl\u00e9 priv\u00e9e <code>id_rsa</code> et le cadenas <code>id_rsa.pub</code>.</p> <p>La liste des entr\u00e9es possibles sur le serveur, ie les cl\u00e9s/cadenas publiques permettant de s'y connecter sont list\u00e9es dans <code>~/.ssh/authorized_keys</code>.</p> <pre><code>sequenceDiagram\n    participant Client\n    participant WebServer\n\n    loop ssh-keygen\n        Client-&gt;&gt;Client: Cl\u00e9 priv\u00e9e g\u00e9n\u00e9r\u00e9 K\n        Client-&gt;&gt;Client: Cadenas publique &lt;br/&gt; g\u00e9n\u00e9r\u00e9 L\n    end\n\n    activate Client\n    Client-&gt;&gt;WebServer: L\n    deactivate Client\n\n    Note over WebServer : R\u00e9ception de L\n    Note over WebServer : S\u00e9curisation</code></pre>  <p>Une seule cl\u00e9/cadenas publique peut \u00eatre utilis\u00e9e pour s\u00e9curiser l'acc\u00e8s \u00e0 plusieurs serveurs, tant que la cl\u00e9 priv\u00e9e ne fuite pas cela ne pose pas de soucis.</p> <p>Dans le cas o\u00f9 un autre Client souhaite acc\u00e9der aux serveurs, il n'a alors qu'a g\u00e9n\u00e9rer une nouvelle paire de cl\u00e9s pour lui via <code>ssh-keygen</code> et vous fournir sa cl\u00e9 publique. Vous ayant acc\u00e8s au serveur pourra alors la rajouter \u00e0 la liste des cl\u00e9s publiques autoris\u00e9es dans <code>~/.ssh/authorized_keys</code>.</p>"},{"location":"devops/tls_ssl/#openssl-et-https","title":"openssl et https","text":"<p>Question</p> <p>Est-il possible de s\u00e9curiser le transfert de la cl\u00e9 K lors d'un chiffrement sym\u00e9trique ?</p> <p>Oui, avec du chiffrement asym\u00e9trique.</p>  <p>Supposons que nous sommes dans le cas d'un chiffrement sym\u00e9trique, comme au dessus, et que l'on souhaite transf\u00e9rer de fa\u00e7on sure la cl\u00e9 de chiffrement.</p> <p>Une solution est alors de cr\u00e9er une paire de cl\u00e9 c\u00f4t\u00e9 serveur via <code>openssl</code>.</p> G\u00e9n\u00e9ration de la cl\u00e9 publique<pre><code>openssl genrsa -out ma_cle.key 1024\n</code></pre> G\u00e9n\u00e9ration de la cl\u00e9/cadenas publique associ\u00e9e<pre><code>openssl rsa -in ma_cle.key -pubout &gt; ma_cle.pem\n</code></pre> <ul> <li>pem file difference - ssh-keygen vs openssl</li> <li>What are the differences between ssh generated keys(ssh-keygen) and OpenSSL keys (PEM)and what is more secure for ssh remote login?</li> <li>How to get a .pem file from ssh key pair?</li> <li>OpenSSL Quick Reference Guide</li> <li>6 OpenSSL command options that every sysadmin should know</li> </ul> <p>La premi\u00e8re fois que le Client se connecte en https au serveur, il devine la cl\u00e9 publique g\u00e9n\u00e9r\u00e9e <code>ma_cle.pem</code> lui ai transmise.</p> <p>Le browser du Client chiffre alors la cl\u00e9 sym\u00e9trique via la <code>ma_cle.pem</code> qui vient de lui \u00eatre transmise.</p> <p>Le couple (cl\u00e9 sym\u00e9trique chiffr\u00e9e, <code>ma_cle.pem</code>) fait alors le chemin inverse vers le serveur, o\u00f9 la cl\u00e9 sym\u00e9trique est d\u00e9chiffr\u00e9e avec <code>ma_cle.key</code>.</p> <pre><code>sequenceDiagram\n    participant Client\n    participant WebServer\n\n    activate Client\n    Client-&gt;&gt;WebServer: Premi\u00e8re connexion https\n    deactivate Client\n\n    Note over Client: cl\u00e9 sym\u00e9trique K\n\n    Note over WebServer: openssl\n    loop G\u00e9n\u00e9ration\n        WebServer-&gt;&gt;WebServer: ma_cle.key\n        WebServer-&gt;&gt;WebServer: ma_cle.pem\n    end\n\n    activate WebServer\n    WebServer-&gt;&gt;Client: ma_cle.pem\n    deactivate WebServer\n\n    loop Chiffrement via ma_cle.pem\n        Client-&gt;&gt;Client: Enc(K)\n    end\n\n    activate Client\n    Client-&gt;&gt;WebServer: Enc(K)\n    deactivate Client\n\n    loop D\u00e9chiffrement via ma_cle.key\n        WebServer-&gt;&gt;WebServer: K\n    end\n\n    activate WebServer\n    WebServer-&gt;&gt;Client: Acc\u00e8s autoris\u00e9\n    deactivate WebServer</code></pre> <p>Afin d'\u00e9viter les probl\u00e8mes, y-a-t'il un moyen de v\u00e9rifier que les cl\u00e9s/cadenas du style <code>ma_cle.pem</code> que l'on re\u00e7oit du serveur sont authentiques, et non pas des cl\u00e9s qu'un hacker aurait pu g\u00e9n\u00e9rer en reroutant votre connexion vers son serveur ?</p> <p>En r\u00e9alit\u00e9, le serveur n'envoie pas que la cl\u00e9 <code>ma_cle.pem</code>, il envoie aussi un certificat, qui ne contient pas uniquement la cl\u00e9, mais aussi l'adresse DNS du serveur ainsi que toutes les adresse DNS alternatives par lesquelles il peut \u00eatre connu.</p> <p>Pour v\u00e9rifier la l\u00e9gitimit\u00e9 d'un certificat, il est n\u00e9cessaire qu'il soit sign\u00e9, cela se fait par des autorit\u00e9s de certifications (CA) publiques tierces. Il est aussi possible d'avoir des autorit\u00e9s des certifications priv\u00e9es, par exemple pour chiffer le r\u00e9seau interne d'une entreprise.</p> <p>PKI : Public Key Infrastructure.</p> <p>Certificats g\u00e9n\u00e9r\u00e9s pour une cl\u00e9 publique :</p> <ul> <li><code>server.crt</code></li> <li><code>server.pem</code></li> <li><code>client.crt</code></li> <li><code>client.pem</code></li> </ul> <p>Certificats g\u00e9n\u00e9r\u00e9s pour une cl\u00e9 priv\u00e9e :</p> <ul> <li><code>server.key</code></li> <li><code>server-key.pem</code></li> <li><code>client.key</code></li> <li><code>client-key.pem</code></li> </ul>"},{"location":"devops/tls_ssl/#exemple","title":"Exemple","text":"<p>Create a CSR (certificate signing request) <code>/etc/httpd/csr/app01.csr</code> (key name should be <code>app01.key</code>). Below are the required details which should be used while creating CSR.</p> <ol> <li>Country Name = SG</li> <li>State or Province Name = Capital Tower</li> <li>Locality Name = CT</li> <li>Organization Name = KodeKloud</li> <li>Organizational Unit Name = Education</li> <li>Common Name = app01.com</li> <li>Email Address = admin@kodekloud.com</li> <li>Keep challenge password blank.</li> <li>Keep optional company name blank.</li> </ol> <p>cd into <code>/etc/httpd/csr</code> directory and run command <code>sudo openssl req -new -newkey rsa:2048 -nodes -keyout app01.key -out app01.csr</code> to generate a CSR file.</p> <p>To verify the entries we used to create a CSR, run the command:</p> <p><code>openssl req -noout -text -in app01.csr</code></p> <pre><code>$ openssl req -noout -text -in app01.csr\n\n\nCertificate Request:\n    Data:\n        Version: 0 (0x0)\n        Subject: C=SG, ST=Capital Tower, L=CT, O=KodeKloud, OU=Education, CN=app01.com/emailAddress=admin@kodekloud.com\n        Subject Public Key Info:\n            Public Key Algorithm: rsaEncryption\n                Public-Key: (2048 bit)\n                Modulus:\n                    00:c8:65:79:1c:48:f8:be:7e:1e:12:ed:ff:12:3f:\n                    67:f1:76:a9:09:6e:95:25:17:bd:3a:95:e9:e0:5b:\n                    2c:e7:82:07:60:80:88:74:93:ea:d5:49:90:28:33:\n                    98:a4:f5:a7:fa:48:d9:89:5c:17:c5:75:73:ee:ea:\n                    9a:c1:1e:47:25:9a:29:a5:de:4f:fe:49:8a:21:19:\n                    8b:bb:a9:d6:93:1e:de:f6:30:d3:fe:d9:db:fb:6f:\n                    83:f7:4e:68:f2:b2:ab:3e:22:8b:03:38:64:9c:32:\n                    53:9f:36:07:4e:12:6a:47:28:fd:35:41:4f:2a:d5:\n                    07:37:89:8a:a6:e7:5e:11:ba:0e:90:6f:3e:53:7b:\n                    fb:ef:0f:88:b2:a7:28:d4:ce:01:5b:79:40:f5:35:\n                    1a:ef:70:94:db:99:94:90:e9:3b:9a:6d:26:2c:b0:\n                    0a:b4:d8:bf:a8:31:4c:14:bd:93:0b:c8:0c:a6:1d:\n                    98:10:45:41:9f:8e:35:b5:bc:90:24:c8:b5:9e:00:\n                    e6:39:58:50:f6:b9:bc:11:02:fd:96:ff:47:f0:da:\n                    eb:d9:91:ec:1a:09:e6:7d:ca:41:ba:a6:b8:71:6a:\n                    e8:33:c6:8b:c3:b9:9a:ff:c7:b6:a4:86:9a:cb:e5:\n                    13:cd:5d:59:31:c4:3f:89:f4:99:cc:f9:37:89:40:\n                    9b:49\n                Exponent: 65537 (0x10001)\n        Attributes:\n            a0:00\n    Signature Algorithm: sha256WithRSAEncryption\n         18:d2:9a:97:66:e3:79:a8:f2:a2:98:0c:2d:66:26:3e:c2:8c:\n         57:8c:1f:7c:6a:e7:e6:df:fe:d6:16:b4:e6:0e:3f:e0:8d:bc:\n         ab:c4:46:73:a7:eb:e2:29:cb:06:da:6f:a9:39:1c:68:8f:86:\n         26:2f:3a:a6:da:40:2c:a1:e8:87:5e:3a:c0:b5:d6:e8:bf:79:\n         a4:62:07:fa:db:b7:dc:0e:45:33:0a:96:be:9b:2d:02:55:f0:\n         2a:ff:fd:5c:b2:45:41:b5:e5:76:b6:6c:35:52:be:a9:4e:d1:\n         c1:46:2a:a4:b0:35:e4:3a:46:69:f1:bf:1e:13:8a:b4:20:22:\n         fe:66:d4:ef:cd:21:52:31:11:e0:43:59:45:4a:ad:2e:47:4c:\n         be:42:58:fa:58:58:6e:e8:30:dc:27:4b:6b:3b:98:3c:d2:ff:\n         4b:4d:d5:d4:96:91:f6:de:74:8d:53:b9:53:7b:38:ce:3b:4f:\n         ee:0f:f6:b8:2b:32:a1:c3:43:a1:a0:29:ec:13:ab:36:f4:16:\n         93:d6:0a:5d:19:40:ab:b7:81:66:07:5e:c1:8f:62:b9:0e:67:\n         07:a5:f1:2b:ec:a1:ab:bb:83:81:f7:d8:aa:a7:e6:78:e5:f3:\n         f6:98:6b:70:e4:75:b1:bf:7f:d3:b2:37:1f:86:dd:30:e9:b1:\n         3b:ab:d7:9e\n</code></pre> <p>Great! We have now generated the CSR. We must now send it to a CA to get it signed. However there is no CA available, so we will create our own self signed certificate.</p> <p>On app01 create a self signed certificate <code>/etc/httpd/certs/app01.crt</code> (key name should <code>app01.key</code>). Below are the required details which should be used while creating the certificate.</p> <ol> <li>Country Name = SG</li> <li>State or Province Name = Capital Tower</li> <li>Locality Name = CT</li> <li>Organization Name = KodeKloud</li> <li>Organizational Unit Name = Education</li> <li>Common Name = app01.com</li> <li>Email Address = admin@kodekloud.com</li> </ol> <p>cd into <code>/etc/httpd/certs</code> directory and run command <code>sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout app01.key -out app01.crt</code> and then pass in the details given above.</p> <p>On app01 server we have an Apache web server already installed and configured and ssl mode is already enabled. In the <code>/etc/httpd/conf.d/ssl.conf</code> file update the SSL certificate and key to use your <code>app01.crt</code> and <code>app01.key</code> files. After making changes remember to restart Apache config.</p> <p>For your reference the certificate you created in the previous steps is at <code>/etc/httpd/certs/app01.crt</code> and the key is at <code>/etc/httpd/certs/app01.key</code>. The properties in the file are SSLCertificateFile and SSLCertificateKeyFile. To test if server is using correct certificate or not run this command and check if it returns your certificate:</p> <p><code>echo | openssl s_client -showcerts -servername app01.com -connect app01:443 2&gt;/dev/null | openssl x509 -inform pem</code></p> <p>Modify the settings in the file <code>/etc/httpd/conf.d/ssl.conf</code> to point to the self signed cert and key you created. Then restart httpd service using the command <code>sudo service httpd restart</code>.</p> <pre><code>Listen 443 https\n\nSSLPassPhraseDialog exec:/usr/libexec/httpd-ssl-pass-dialog\nSSLSessionCache         shmcb:/run/httpd/sslcache(512000)\nSSLSessionCacheTimeout  300\n\nSSLRandomSeed startup file:/dev/urandom  256\nSSLRandomSeed connect builtin\n#SSLRandomSeed startup file:/dev/random  512\n#SSLRandomSeed connect file:/dev/random  512\n#SSLRandomSeed connect file:/dev/urandom 512\n\nSSLCryptoDevice builtin\n#SSLCryptoDevice ubsec\n\n##\n## SSL Virtual Host Context\n##\n\n&lt;VirtualHost _default_:443&gt;\n\nDocumentRoot \"/var/www/html\"\nServerName app01.com:443\n\nErrorLog logs/app01_ssl_error_log\nTransferLog logs/app01_ssl_access_log\nLogLevel warn\n\n#   SSL Engine Switch:\n#   Enable/Disable SSL for this virtual host.\nSSLEngine on\n\n#   SSL Protocol support:\n# List the enable protocol levels with which clients will be able to\n# connect.  Disable SSLv2 access by default:\nSSLProtocol all -SSLv2 -SSLv3\n\n#   SSL Cipher Suite:\n#   List the ciphers that the client is permitted to negotiate.\n#   See the mod_ssl documentation for a complete list.\nSSLCipherSuite HIGH:3DES:!aNULL:!MD5:!SEED:!IDEA\n\n#   Server Certificate:\n# Point SSLCertificateFile at a PEM encoded certificate.  If\n# the certificate is encrypted, then you will be prompted for a\n# pass phrase.  Note that a kill -HUP will prompt again.  A new\n# certificate can be generated using the genkey(1) command.\nSSLCertificateFile /etc/httpd/certs/app01.crt\n\n#   Server Private Key:\n#   If the key is not combined with the certificate, use this\n#   directive to point at the key file.  Keep in mind that if\n#   you've both a RSA and a DSA private key you can configure\n#   both in parallel (to also allow the use of DSA ciphers, etc.)\nSSLCertificateKeyFile /etc/httpd/certs/app01.key\n\n#SSLOptions +FakeBasicAuth +ExportCertData +StrictRequire\n&lt;Files ~ \"\\.(cgi|shtml|phtml|php3?)$\"&gt;\n    SSLOptions +StdEnvVars\n&lt;/Files&gt;\n&lt;Directory \"/var/www/cgi-bin\"&gt;\n    SSLOptions +StdEnvVars\n&lt;/Directory&gt;\n\nBrowserMatch \"MSIE [2-5]\" \\\n         nokeepalive ssl-unclean-shutdown \\\n         downgrade-1.0 force-response-1.0\n\n#   Per-Server Logging:\n#   The home of a custom SSL log file. Use this when you want a\n#   compact non-error SSL logfile on a virtual host basis.\nCustomLog logs/ssl_request_log \\\n          \"%t %h %{SSL_PROTOCOL}x %{SSL_CIPHER}x \\\"%r\\\" %b\"\n\n&lt;/VirtualHost&gt;\n</code></pre> <pre><code>[thor@app01 certs]$ sudo service httpd restart\nRedirecting to /bin/systemctl restart httpd.service\n[thor@app01 certs]$ echo | openssl s_client -showcerts -servername app01.com -connect app01:443 2&gt;/dev/null | openssl x509 -inform pem\n-----BEGIN CERTIFICATE-----\nMIID+TCCAuGgAwIBAgIJAL+q3TOi/zk/MA0GCSqGSIb3DQEBCwUAMIGSMQswCQYD\nVQQGEwJTRzEWMBQGA1UECAwNQ2FwaXRhbCBUb3dlcjELMAkGA1UEBwwCQ1QxEjAQ\nBgNVBAoMCUtvZGVLbG91ZDESMBAGA1UECwwJRWR1Y2F0aW9uMRIwEAYDVQQDDAlh\ncHAwMS5jb20xIjAgBgkqhkiG9w0BCQEWE2FkbWluQGtvZGVrbG91ZC5jb20wHhcN\nMjIwNTEwMTk0NjM5WhcNMjMwNTEwMTk0NjM5WjCBkjELMAkGA1UEBhMCU0cxFjAU\nBgNVBAgMDUNhcGl0YWwgVG93ZXIxCzAJBgNVBAcMAkNUMRIwEAYDVQQKDAlLb2Rl\nS2xvdWQxEjAQBgNVBAsMCUVkdWNhdGlvbjESMBAGA1UEAwwJYXBwMDEuY29tMSIw\nIAYJKoZIhvcNAQkBFhNhZG1pbkBrb2Rla2xvdWQuY29tMIIBIjANBgkqhkiG9w0B\nAQEFAAOCAQ8AMIIBCgKCAQEAyxqRPJBP9sp18fEi4q9CGpHd3idAFheWy6sBoAbY\nI7gKRlpKbMuZC3ezstrnOPVt3AeQJFDDZ5CLZ1FN2fXx3tEdz/U8FElxBK8FbwkY\n+igDJpBfEMkbIF2cNMMA0fw5F7589ddx3aCB4okjuJOSho7L6GtBRdtZUTEJCvgV\ns8JTncXfm6Ad6pQe0+XrNb5ZDNct71RCVwWA+g8jzAYkTBiJ1psl3OJW+YY6kcXK\n/8NWB8SjgCEfRRxfnwx7K49dO7PlNM7CuRj0CwHz5gGkm3eIez6v/1WVZU2D6tSR\nAbI61pXx3yMhLi/+Hywh8pysKMMJkA6gYA4m0PJ7kbnKrQIDAQABo1AwTjAdBgNV\nHQ4EFgQULcWGO0WKuybjbnvP885eJGOMjS0wHwYDVR0jBBgwFoAULcWGO0WKuybj\nbnvP885eJGOMjS0wDAYDVR0TBAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAZJLP\noTQsWKyRNWN+pYOOwHRLe5+fFUH4/uSLFNq1oZ/3F0fE+6hftcJ6Izi9D/UR4n3P\nlhGFtw5/gZGEeQWwf+NvSn+ZjN2nhsS//69giLkQkVv6HGMXoZbwl1RItTYhndp9\n75pRPLrXQjhKrvRR5Q72bujutCUyZsR7K1spUbh7YMHdshwxrqal0Z/WsDZtko6c\n4zNwGxwuhcni/Ld7i7mGDiKsuqNSaA8myv0q6t1jDLOQBU3KZYbyQJvYG5IGeekq\nqtBgSMqPbiy67lTtP5qb8djoe6qI44++4b4LQhXI7x6v/hdKpW9btPisb0g+xSnq\nW0THP/VibgL6TKNFYA==\n-----END CERTIFICATE-----\n</code></pre>"},{"location":"devops/web_server/","title":"Just enough Web Frameworks and Web Servers","text":""},{"location":"devops/web_server/#backend-web-frameworks","title":"Backend web frameworks","text":"<pre><code>sequenceDiagram\n    Client-&gt;&gt;WebServer: Requ\u00eate\n    activate WebServer\n    WebServer-&gt;&gt;Client: R\u00e9ponse\n    deactivate WebServer</code></pre> <p>Deux types de sites web :</p> <ul> <li>statiques : le contenu du site ne change pas. Une fois le contenu servi, il n'y a plus de contact avec le serveur,</li> <li>dynamiques.</li> </ul>"},{"location":"devops/web_server/#apache-web-server","title":"Apache Web Server","text":"<p>Open source HTTP serveur d\u00e9velopp\u00e9 et maintenu par la fondation Apache. Plut\u00f4y utilis\u00e9 comme serveur frontend.</p> <pre><code>yum install httpd\n</code></pre> <pre><code>service httpd start\n</code></pre> <pre><code>service httpd status\n</code></pre> <p>pour acc\u00e9der aux logs, on utilise les commandes suivantes.</p> <p><code>cat /var/log/httpd/acces_log</code></p> <p><code>cat /var/log/httpd/error_log</code></p> <p>Le fichier de configuration de httpd sur trouve dans <code>/etc/httpd/conf/httpd.conf</code></p>"},{"location":"devops/web_server/#apache-tomcat","title":"Apache Tomcat","text":"<p>Backend serveur utilis\u00e9 pour d\u00e9ployer des applications java. Il est n\u00e9cessaire d'avoir java d'install\u00e9 pour pouvoir l'utiliser.</p> <p><code>yum install java-1.8.0-openjdk-devel</code></p> <p>Port par d\u00e9faut de Apache Tomcat : <code>8080</code>.</p> <p><code>jar -cvf app.war *</code></p>"},{"location":"devops/web_server/#python-web-frameworks","title":"Python web frameworks","text":"<p>Django, Flask.</p> <p>Production grade serveurs pour Python :</p> <ul> <li>Gunicorn</li> <li>uWSGI</li> <li>Gevent</li> <li>Twisted Web</li> </ul> <p><code>gunicorn main:app</code>.</p> <p><code>gunicorn main:app -w 2</code> avec 2 workers.</p> <p>Pour voir le nombre total de processus <code>Gunicorn</code> qui tourne sur le serveur, on peut taper la commande suivante.</p> <p><code>ps -ef | grep gunicorn | grep -v grep</code></p> <p>Le nombre de processus <code>Gunicorn</code> retourn\u00e9 correspond au processus principal plus le nombre de worker d\u00e9ploy\u00e9s.</p>"},{"location":"devops/web_server/#nodejs","title":"NodeJS","text":"<p>Express.js is the main web server framework.</p> <p><code>npm install</code> pour installer l'ensemble des d\u00e9pendances du projet.</p> package.json<pre><code>cat package.json\n{\n  \"name\": \"example-contentful-theExampleApp-js\",\n  \"version\": \"0.0.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"start:watch\": \"nodemon ./bin/www --ignore public/\",\n    \"start:dev\": \"node ./bin/www\",\n    \"debug\": \"node debug ./bin/www\",\n    \"start\": \"NODE_ENV=production node ./bin/www\",\n    \"start:production\": \"NODE_ENV=production node ./bin/www\",\n    \"lint\": \"eslint ./app.js routes\",\n    \"format\": \"eslint --fix . bin --ignore public node_modules\",\n    \"pretest\": \"npm run lint\",\n    \"test\": \"npm run test:unit &amp;&amp; npm run test:integration &amp;&amp; npm run test:e2e\",\n    \"test:e2e\": \"node test/run-e2e-test.js\",\n    \"test:e2e:dev\": \"node test/run-e2e-test.js --dev\",\n    \"test:integration\": \"jest test/integration\",\n    \"test:integration:watch\": \"jest test/integration --watch\",\n    \"test:unit\": \"jest test/unit\",\n    \"test:unit:watch\": \"jest test/unit --watch\"\n  },\n  \"engines\": {\n    \"node\": \"&gt;=8.9.3\"\n  },\n  \"dependencies\": {\n    \"body-parser\": \"^1.18.2\",\n    \"contentful\": \"^6.0.0\",\n    \"cookie-parser\": \"~1.4.3\",\n    \"dotenv\": \"^5.0.0\",\n    \"execa\": \"^0.9.0\",\n    \"express\": \"^4.16.2\",\n    \"helmet\": \"^3.11.0\",\n    \"lodash\": \"^4.17.5\",\n    \"marked\": \"^0.3.16\",\n    \"morgan\": \"^1.9.1\",\n    \"pug\": \"~2.0.0-beta6\"\n  },\n  \"devDependencies\": {\n    \"cheerio\": \"^1.0.0-rc.2\",\n    \"cookie\": \"^0.3.1\",\n    \"eslint\": \"^4.18.1\",\n    \"eslint-config-standard\": \"^11.0.0\",\n    \"eslint-plugin-import\": \"^2.8.0\",\n    \"eslint-plugin-node\": \"^6.0.0\",\n    \"eslint-plugin-promise\": \"^3.6.0\",\n    \"eslint-plugin-standard\": \"^3.0.1\",\n    \"jest\": \"^22.4.0\",\n    \"nodemon\": \"^1.18.9\",\n    \"supertest\": \"^3.0.0\",\n    \"yargs\": \"^11.0.0\"\n  }\n}\n</code></pre> <p><code>npm run start</code>, <code>npm run debug</code>, ... pour lancer la valeur correspondante dans la partie <code>scripts</code> de <code>package.json</code>.</p> <p>PM2 : Production Process Manager for Node.js applications with a built-in load balancer.</p> <p><code>pm2 start app.js</code></p> <p><code>pm2 start app.js -i 4</code> avec 4 instances de l'application.</p> <pre><code>thor@host01 /opt/the-example-app.nodejs$ pm2 start app.js\n\n                        -------------\n\n__/\\\\\\\\\\\\\\\\\\\\\\\\\\____/\\\\\\\\____________/\\\\\\\\____/\\\\\\\\\\\\\\\\\\_____\n _\\/\\\\\\/////////\\\\\\_\\/\\\\\\\\\\\\________/\\\\\\\\\\\\__/\\\\\\///////\\\\\\___\n  _\\/\\\\\\_______\\/\\\\\\_\\/\\\\\\//\\\\\\____/\\\\\\//\\\\\\_\\///______\\//\\\\\\__\n   _\\/\\\\\\\\\\\\\\\\\\\\\\\\\\/__\\/\\\\\\\\///\\\\\\/\\\\\\/_\\/\\\\\\___________/\\\\\\/___\n    _\\/\\\\\\/////////____\\/\\\\\\__\\///\\\\\\/___\\/\\\\\\________/\\\\\\//_____\n     _\\/\\\\\\_____________\\/\\\\\\____\\///_____\\/\\\\\\_____/\\\\\\//________\n      _\\/\\\\\\_____________\\/\\\\\\_____________\\/\\\\\\___/\\\\\\/___________\n       _\\/\\\\\\_____________\\/\\\\\\_____________\\/\\\\\\__/\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\_\n        _\\///______________\\///______________\\///__\\///////////////__\n\n\n                          Runtime Edition\n\n        PM2 is a Production Process Manager for Node.js applications\n                     with a built-in Load Balancer.\n\n                Start and Daemonize any application:\n                $ pm2 start app.js\n\n                Load Balance 4 instances of api.js:\n                $ pm2 start api.js -i 4\n\n                Monitor in production:\n                $ pm2 monitor\n\n                Make pm2 auto-boot at server restart:\n                $ pm2 startup\n\n                To go further checkout:\n                http://pm2.io/\n\n\n                        -------------\n\n[PM2] Spawning PM2 daemon with pm2_home=/home/thor/.pm2\n[PM2] PM2 Successfully daemonized\n[PM2] Starting /opt/the-example-app.nodejs/app.js in fork_mode (1 instance)\n[PM2] Done.\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2502 name   \u2502 namespace   \u2502 version \u2502 mode    \u2502 pid      \u2502 uptime \u2502 \u21ba    \u2502 status    \u2502 cpu      \u2502 mem      \u2502 user     \u2502 watching \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 0   \u2502 app    \u2502 default     \u2502 0.0.0   \u2502 fork    \u2502 1023     \u2502 0s     \u2502 0    \u2502 online    \u2502 0%       \u2502 29.3mb   \u2502 thor     \u2502 disabled \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nthor@host01 /opt/the-example-app.nodejs$ ^C\nthor@host01 /opt/the-example-app.nodejs$ pm2 start app.js -i 4\n[PM2][ERROR] Script already launched, add -f option to force re-execution\nthor@host01 /opt/the-example-app.nodejs$ pm2 start app.js -i 4 -f\n[PM2] Starting /opt/the-example-app.nodejs/app.js in cluster_mode (4 instances)\n[PM2] Done.\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2502 name   \u2502 namespace   \u2502 version \u2502 mode    \u2502 pid      \u2502 uptime \u2502 \u21ba    \u2502 status    \u2502 cpu      \u2502 mem      \u2502 user     \u2502 watching \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 0   \u2502 app    \u2502 default     \u2502 0.0.0   \u2502 fork    \u2502 1225     \u2502 1s     \u2502 12   \u2502 online    \u2502 0%       \u2502 50.3mb   \u2502 thor     \u2502 disabled \u2502\n\u2502 1   \u2502 app    \u2502 default     \u2502 0.0.0   \u2502 cluster \u2502 1247     \u2502 0s     \u2502 0    \u2502 online    \u2502 0%       \u2502 41.0mb   \u2502 thor     \u2502 disabled \u2502\n\u2502 2   \u2502 app    \u2502 default     \u2502 0.0.0   \u2502 cluster \u2502 1254     \u2502 0s     \u2502 0    \u2502 online    \u2502 0%       \u2502 38.0mb   \u2502 thor     \u2502 disabled \u2502\n\u2502 3   \u2502 app    \u2502 default     \u2502 0.0.0   \u2502 cluster \u2502 1263     \u2502 0s     \u2502 0    \u2502 online    \u2502 0%       \u2502 35.5mb   \u2502 thor     \u2502 disabled \u2502\n\u2502 4   \u2502 app    \u2502 default     \u2502 0.0.0   \u2502 cluster \u2502 1276     \u2502 0s     \u2502 0    \u2502 online    \u2502 0%       \u2502 29.8mb   \u2502 thor     \u2502 disabled \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"devops/web_server/#ips-et-ports","title":"IPs et Ports","text":"<ul> <li>Quelle adress IP et port utiliser ?</li> <li>localhost vs 127.0.0.1 vs IP adresse.</li> <li>Pourquoi est ce que je ne peux pas me connecter \u00e0 mon serveur ?</li> </ul> <p>Une adresse IP est assign\u00e9e \u00e0 une interface, pas \u00e0 un device. Ainsi on peut tr\u00e8s bien avoir une IP pour se connecter en filaire, et une IP wifi.</p> <pre><code>graph LR\n  subgraph PC\n  A[enp0s3 &lt;br/&gt; 10.0.2.15]\n  B[wlp0s2 &lt;br/&gt; 10.0.2.16]\n  end\n\n  C[switch]\n\n  A -.- C</code></pre> <pre><code>\u276f ip addr show\n\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n\n2: enp3s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether 2c:f0:5d:d4:db:e6 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.1.37/24 brd 192.168.1.255 scope global dynamic noprefixroute enp3s0\n</code></pre> <p>Chacune de ces adresses IP est divis\u00e9e et composantes logiques appel\u00e9es \"ports\", chanque port \u00e9tant un point de communication possible vers l'ext\u00e9rieur.</p> <p>Ainsi, un serveur attendra des requ\u1ebftes d'une certaine interface sur un port bien pr\u00e9cis.</p> <pre><code>graph TD\n  subgraph PC\n  subgraph enp0s3 10.0.2.15\n  A1[0]\n  A2[1]\n  A3[2]\n  A4[...]\n  A5[65535]\n  end\n  subgraph wlp0s2 10.0.2.16\n  B1[0]\n  B2[1]\n  B3[2]\n  B4[...]\n  B5[65535]\n  end\n  end\n\n  C[switch]\n\n  A3 -.-|10.0.2.15:2| C</code></pre> <p>Par exemple, une application python utilisant <code>Flask</code> \u00e9coute par d\u00e9faut sur le port 5000. Mais sur quelle adresse ip ? Pour modifier l'adresse ip, il est n\u00e9cessaire de sp\u00e9cifier sur quelle interface l'application doit \u00e9couter, sur quel <code>host</code>. Pour <code>Flask</code>, cela peut se faire via <code>app.run(port=8000, host=\"10.0.2.15\")</code>. L'application \u00e9coute alors \u00e0 l'adresse suivante <code>http://10.0.2.15:8000</code>.</p> <p>Si l'on souhaite que l'application soit aussi disponible sur toutes les autres interfaces, il suffit alors de remplacer l'adresse de l'h\u00f4te par <code>0.0.0.0</code>.</p> <pre><code>graph TD\n  subgraph PC\n  subgraph enp0s3 10.0.2.15\n  A[8000]\n  end\n  subgraph wlp0s2 10.0.2.16\n  B[8000]\n  end\n  end\n\n  C[app]\n\n  A &amp; B -.- D[0.0.0.0:8000] -.- C</code></pre> <p>Comment faire si nous ne sommes pas encore assez confiant concernant notre application ? On ne veut pas qu'elle soit disponible \u00e0 l'ext\u00e9rieur, mais l'on souhaite quand m\u00eame \u00eatre capable de la tester.</p> <p>Dans ce cas l\u00e0, il ne faut sp\u00e9cifier aucune adresse pour <code>host</code>. Si c'est le cas, par d\u00e9faut l'application \u00e9coute alors sur l'adresse <code>127.0.0.1</code>, qui est l'adresse d\u00e9sign\u00e9e par <code>lo</code> dans la nomenclature et qui est ce que l'on apelle \"l'adresse loopback\".</p> <p>Chaque device poss\u00e8de une interface virtuelle connue comme l'adresse loopback.</p> <pre><code>graph TD\n  subgraph PC\n  subgraph enp0s3 10.0.2.15\n  A[8000]\n  end\n  subgraph wlp0s2 10.0.2.16\n  B[8000]\n  end\n  subgraph lo 127.0.0.1\n  E[8000]\n  end\n  end\n\n  C[app]\n\n  E-.-|127.0.0.1:8000|C</code></pre> <p>Dans ce cas l\u00e0, l'application \u00e9coute \u00e0 l'adresse <code>http://127.0.0.1:8000</code>. Au lieu de taper l'adresse \u00e0 chaque fois, il est possible de taper <code>http://localhost:8000</code>.</p> <p>localhost est le nom assign\u00e9e \u00e0 l'adresse loopback 127.0.0.1, les deux sont interchangeables.</p>"},{"location":"devops/web_server/#lancer-un-application-en-mode-background","title":"Lancer un application en mode background","text":"<p><code>nohup python app.py &amp;</code></p> <p>Note: Nohup stands for no hang up, which can be executed as shown below.</p> <p><code>nohup command-with-options &amp;</code></p> <p>Adding <code>&amp;</code> at the end will move the process to run in background. When you execute a Unix job in the background (using <code>&amp;</code>) and logout from the session, your process will get killed. You can avoid this with nohup.</p>"},{"location":"devops/yaml/","title":"Just enough YAML","text":"<p>Principalement utilis\u00e9 pour \u00e9crire des fichiers de configuration.</p> <p>Paire cl\u00e9/valeur</p> <pre><code>Fruit: Apple\nVegetable: Carrot\nLiquid: Water\nMeat: Chicken\n</code></pre> <p>Array/Lists</p> <pre><code>Fruits:\n- Apple\n- Orange\n- Banana\n\nVegetables:\n- Carrot\n- Cauliflower\n- Tomato\n</code></pre> <p>Dictionary/Map</p> <pre><code>Banana:\n  Calories: 105\n  Fat: 0.4 g\n  Carbs: 27 g\n\nGrapes:\n  Calories: 62\n  Fat: 0.3 g\n  Carbs: 16 g\n</code></pre> <p>Indentation dans YAML.</p> <p>Cl\u00e9/valeur/Dictionnaire/Liste <pre><code>Fruits:\n  - Banana:\n      Calories: 105\n      Fat: 0.4 g\n      Carbs: 27 g\n\n  - Grapes:\n      Calories: 62\n      Fat: 0.3 g\n      Carbs: 16 g\n</code></pre></p> <p>https://www.educative.io/blog/yaml-tutorial</p>"},{"location":"doc_redaction/diagrammes/","title":"Diagram as Code","text":"<p>https://diagrams.mingrammer.com/</p>"},{"location":"doc_redaction/mkdocs/","title":"MkDocs pour r\u00e9diger la documentation Python","text":""},{"location":"doc_redaction/mkdocs/#mkdocs-et-mkdocs-material","title":"Mkdocs et Mkdocs-material","text":""},{"location":"doc_redaction/mkdocs/#mkdocsrtrings","title":"mkdocsrtrings","text":""},{"location":"docker/dev_env/","title":"Dev env","text":"<p>Advanced Docker Security Photo by Ian Taylor on Unsplash</p> <p>In today\u2019s world, developing apps entails much more than just writing code. The use of several languages, frameworks, and architectures, as well as discontinuous interfaces between tools for each stage of the development lifecycle, results in immense complexity. Docker streamlines and accelerates your process while allowing developers the opportunity to create by utilising their preferred tools, application stacks, and deployment environments for each project, according to their needs.</p> <p>Introduction</p> <p>Docker has made it easy for the operations team to directly deploy applications and websites without having to worry about dependencies, configuration settings, or the versions of packages installed on the server. Because of its simplicity in terms of just fetching the image from the registry and executing it with a command (docker run), we frequently fail to recognise that it requires the same level of security as any other entity.</p> <p>We\u2019ve already done a couple of articles on docker security best practices which can be found here and here.</p> <p>In this blog post, we shall cover some more advanced concepts about Docker Security which will definitely prove to be helpful if you are working with multiple containers, want to prevent privilege escalation, etc.</p> <p>Nologin Shell nologin shell</p> <p>Your Docker container can house multiple user accounts. Root account is the highest privilege that a user can gain in the container and once he has the root permission, he can actually execute root commands or can do a lot more in the container. To ensure that a malicious user doesn\u2019t upgrade to the root account, even if he has access to the root account\u2019s password you can change the shell to nologin. You could alternatively also make use of restricted shells for the user. To disable root login (inside the container), all you have to do is modify your Dockerfile and add the following line.</p> <p>RUN chsh \u2013s /usr/bin/nologin root</p> <p>nologin shells are used to deny login access to an account. This line will ensure that even if a a malicious user gets hold of the root user\u2019s account, he\u2019ll not be able to login as it is denied in the nologin file hence no user, even the root can\u2019t login to his account.</p> <p>Disable Privilege Escalation using SUID</p> <p>Before we talk about the mitigation technique, we should understand what SUID is. SUID, which stands for Set owner User ID up on execution is a special type of file permissions given to a file. If a root user gives this permission to an executable, it could be used to escalate privileges to root. All the root user has to do to set a SUID bit is type in</p> <p>chmod +x  <p>These are exploited in the wild to escalate privileges in docker containers. To prevent this you can use a special tag \u2014 security-opt=\u201dno-new-privileges\u201d while spinning up the docker container.</p> <p>This tag ensures that processes and child processes (spawned by the parent process) do not gain any additional privileges using the SUID or SGID bit for that matter.</p> <p>For example,</p> <p>docker run \u2013it  --security-opt=\u201dno-new-privileges\" /bin/bash <p>Note: Here image_id is the Id of the image of your docker container. To find the image id of your container just type in docker image ls docker image ls</p> <p>Create a Read-Only File System</p> <p>Read only file system ensures that the users aren\u2019t able to create any files in the system. This will prevent them from downloading and installing malicious files in the docker container which can further be used for other malicious purposes such as malware, keylogger or a back connect shell. So, it is essential that you protect docker from unauthorized access by creating a read only file system.</p> <p>To create a read only file system we can use this command</p> <p>docker run --read-only \u2013it  /bin/bash <p>It is understandable that a container with no writing permissions could be of some inconvenience for the users. So, what you can do is set up a temporary file system. In this example, I\u2019ve selected my /opt directory for the temporary file system. So, the user will be able to download the files only in the specified directory and they can\u2019t be moved to home or any other directories. It can be achieved by running this command.</p> <p>docker run --read-only --tmps /opt -it  /bin/bash <p>Preventing Inter Container Communication Source</p> <p>It\u2019s normal for you to install multiple containers once you\u2019ve set up Docker in a production environment because they are responsible for distinct activities. For example, you might be utilizing MongoDB in one container while hosting an application in another. However, did you know that it is possible to communicate across containers? The following scenario: A docker container running a vulnerable instance of an application that was exposed via a port is exposed to the outside world. A malevolent user seated on the other docker container has the potential to jeopardize the entire application\u2019s security.</p> <p>Therefore, unless required, you can prevent inter-container communication.</p> <p>You can start off by looking at the default networks that come built in with docker.</p> <p>docker network ls</p> <p>list of networks</p> <p>Here, we\u2019ll be inspecting a network type. For this example, we\u2019ll look at the bridge network type.</p> <p>docker network inspect bridge</p> <p>Inspecting Bridge</p> <p>If you scroll down, you\u2019ll see that it has com.docker.network.bridge.enable-icc set to true.</p> <p>Because multiple devices and containers can be joined to a bridge connection in order for them to communicate, bridge connections are used to interconnect the containers and make them more accessible.</p> <p>Creating a new network and setting it to false is all that we need to do in order to block inter-container communication. Then, when we\u2019re establishing a new container, we can include it in the newly established network that we\u2019ve set up. As a result, the container will be separated and will only be capable of holding one container.</p> <p>Let\u2019s begin by creating a new network</p> <p>docker network create  --driver bridge \u2013o \u201ccom.docker.network.bridge.enable-icc\u201d:\u201dfalse\u201d testnet1</p> <p>Creating a new network</p> <p>Here we\u2019ve created a network of the type bridge and set the inter-container communication as false. The name of the network that we have created is set as testnet1</p> <p>With all this done, what we can do is run a docker container with the network set as testnet1. Listing the networks</p> <p>docker run \u2013it \u2013network testnet1  /bin/bash <p>Note: Here image_id is the Id of the image of your docker container. To find the image id of your container just type in docker image ls Listing Docker Images</p> <p>Conclusion</p> <p>In this blog article, we discussed some of the most critical features of Docker\u2019s safety. Despite the fact that a user has access to the root password, we learned how to block root logins. In the next lessons, we learned how to avoid privilege escalation through the usage of SUID or SGID files. We learned about constructing read-only file systems at the end of the course, as well as how to block inter-container communication by creating a different network. These steps are vital to secure your docker installation. As we learn more about docker security, we\u2019ll be adding more articles to this series.</p> <p>Advanced Docker Security Part II Source Introduction</p> <p>This is part II of the Advanced Docker Series where we\u2019ll be covering some advanced concepts to secure your docker container and ensure that even if your container is compromised, the attacker will not be able to achieve much. In case you\u2019ve missed the first part of the series you can check it out here. Limiting Resources</p> <p>The security of Docker and the factors that influence it may be broken down into two core and crucial categories: Namespaces and cgroups are the terms used to describe them.</p> <p>Namespaces, according to the Docker website, \u201cprovide isolation for running processes (containers), limiting their access to system resources without the running process being aware of the constraints.\u201d Namespaces were not introduced by Docker. It was already present as part of the Linux kernel at the time of writing.</p> <p>The second most crucial thing to know is about cgroups. It is a Linux Kernel feature that lets you restrict access to processes and containers to specific system resources such as CPU, RAM, IOPS, and the network connection. Restricting PIDs</p> <p>PIDs are the number of processes or threads the container has created. The PIDS column contains the total number of processes and kernel threads that were created by the container in the previous step. The Linux kernel refers to this as \u201cthreads.\u201d Alternatively, \u201clightweight process\u201d or \u201ckernel job\u201d are used to describe the same thing. The presence of a large number in the PIDS column combined with a modest number of processes (as reported by ps or top) may indicate that something within the container is generating a significant number of threads. If an attacker gets access to your container, he can cripple it by eating up all the resources of the server and bringing it to a halt. It is therefore recommended to reduce the number of processes that can be spawned on the system.</p> <p>Let\u2019s first start by learning how to check the maximum number of PIDs that a container can spawn.</p> <p>To check the current PIDs, you can find that by typing in docker stats docker stats</p> <p>Let\u2019s run a docker container in detached mode.</p> <p>In detached mode, the container starts up and runs in the background. That means, you start up the container and could use the console after startup for other commands.</p> <p>The opposite of detached mode is foreground mode. That is the default mode, when the -d option is not used. In this mode, the console you are using to execute docker run will be attached to standard input, output and error. That means your console is attached to the container\u2019s process.</p> <p>You can run docker in detached mode by adding an additional flag d along with \u2013it. docker run -itd  <p>As we can see, it provides us with an ID, fa7175306b6ae53d2be39d1e53d47e12d98fbb2cfac9f8d3abf82752f60cca0e</p> <p>This is nothing but a directory for our just spawned container. The contents of this directory would contain important information.</p> <p>find /sys/fs/cgroup -name fa7175306b6ae53d2be39d1e53d47e12d98fbb2cfac9f8d3abf82752f60cca0e</p> <p>List of directories matched</p> <p>If you look at the name convention of the first directory, you\u2019ll notice pids in it. This directory would contain information regarding the PIDs of our just spawned container. Our directory of concern is /sys/fs/cgroup/pids/docker/fa7\u2026.ca0e</p> <p>Looking at the contents, you\u2019ll see a bunch of files stored in the directory. switching to the pids directory of our container</p> <p>The pids.max file contains the maximum number of processes that can be spawned in our docker container. If you look at the contents of the pids.max file, you\u2019ll find that it contains the word max. This means any number of processes can be spawned in our container. pids.max is set to max</p> <p>Depending upon the use case, you can set an upper bound on the number of PIDs that can be spawned.</p> <p>docker run \u2013pids-limit 100 825d55fb6340</p> <p>Note: Here 825d55fb6340 is the image ID. To view your image IDs, just type in docker image ls pids-limit set to 100</p> <p>In the example above, I\u2019ve set a maximum limit on the number of processes that can be spawned, which is effectively 100. Deepening upon your use case, you can set it to a lot lower or higher.</p> <p>Let\u2019s confirm it by visiting the /sys/fs/cgroup/pids/docker/08a1b005840b54bda92116ad810ea228ae32d558a6fc81491b5f6ba042244f7e/pids.max file. pids-limit set to 100</p> <p>As you can verify, the maximum number of pids has been set to 100. Docker Socket Source</p> <p>Let\u2019s understand what socket is first</p> <p>A socket is typically referred to in terms of an IP Address and a port. To interact with a website, or an interface, all you\u2019ll need is a socket which means an IP Address and a port associated with the IP Address on which the service is running.</p> <p>You can then send request(s) to the socket and expect a response. This is commonly referred to as a TCP Socket. There is another socket called the UNIX socket. These sockets are commonly used for Inter Process Communication (IPC) within the same computer/system.</p> <p>The Docker Socket is a UNIX socket. When you type in a docker command like docker pull, run, etc., behind the scenes the docker client is interacting with the docker daemon via the UNIX socket to execute your commands.</p> <p>The docker.sock file is located in the /var/run directory and the owner of the file is the root. It is essential that you don\u2019t tamper with the permissions of this file because this can lead the attacker to gain access to the underlying host system.</p> <p>Also, it is not advisable to mount this file onto a newly spun docker container, as it can be abused by malicious users to gain access to the underlying host system. But why would the /var/run/docker.sock file be mounted by anyone on the container?</p> <p>If you have a couple of docker containers and you want to access/control all of them from a different docker container, then you\u2019ll need to mount this file to the docker container from which you wish to control/access the rest of the docker containers.</p> <p>The process to mount is by adding the \u2013v tag, followed by what you wish to mount and where you wish to mount.</p> <p>Example, -v /opt/important:/tmp</p> <p>In this example, I\u2019ve mounted the /opt/important directory from the host system to the /tmp directory in the docker container.</p> <p>Therefore, if you encounter a DockerFile with a line similar to the one provided below, you can be sure of the fact that any malicious user, with access to the docker container will be able to mount the host file system and read all the sensitive files.</p> <p>docker run \u2013it \u2013name ubuntu \u2013v /var/run/docker.sock:/var/run/docker.sock /bin/bash</p> <p>Conclusion</p> <p>This was a blog post that was more focused on the practical side of things. We gained an understanding of the concept of Process IDs and how we might restrict access to them. Later on, we discovered that mounting a docker.sock file can lead to security vulnerabilities in the Docker container. Security is extremely important in the Docker environment. In addition to being complicated, technological security is also difficult to achieve. As a result, we must make certain that everything is properly corrected and protected prior to the deployment.</p>"},{"location":"ide_vscode/conf/","title":"Configuration de VSCode","text":"<p>On parlera ici de la configuration de vscode, c'est l'un des IDE standards et il offre beaucoup de possibilit\u00e9s :</p> <ul> <li>l'int\u00e9gration de nombreux plugins,</li> <li>la possibilit\u00e9 d'avoir un IDE lors d'une connexion ssh,</li> <li>la possibilit\u00e9 de travailler directement dans un environnement de dev Docker depuis vscode...</li> </ul> <p>D'autres IDE existent et sont tr\u00e8s bien : Atom, PyCharm, Sublime text, le choix de l'IDE pourrait ne pas \u00eatre fix\u00e9.</p>  <p>Question</p> <p>Doit on fixer le choix de l'IDE, afin d'avoir le m\u00eame pour tout le monde ? M\u00eame si je pense que tout le monde utilise d\u00e9j\u00e0 vscode.</p>  <p>La configuration de vscode se fait g\u00e9n\u00e9ralement de fa\u00e7on globale, toutefois pour que les configurations soient li\u00e9es aux projets et non \u00e0 l'utilisateur, il est important que les configurations puissent voyager avec gitlab, donc locale.</p>  <p>Info</p> <p>Chaque projet vscode peut avoir une configuration locale en fournissant un fichier <code>settings.json</code> et en le situant \u00e0 la racine du projet dans le dossier cach\u00e9 <code>.vscode</code>.</p> <pre><code>racine du projet\n|\n|_ .vscode\n|   |\n|   |_ settings.json\n|...\n</code></pre> <p>Ce projet peut ainsi voyager avec gitlab et petmettre d'avoir toujours la m\u00eame configuration peut importe le pc depuis lequel le <code>pull</code> est fait.</p>  <p>Comme dit pr\u00e9c\u00e9demment, le fichier de configuration locale est un json, d\u00e9taillons.</p> <p><pre><code>{\n  // config g\u00e9n\u00e9rale\n  \"editor.suggestSelection\": \"first\",\n  \"workbench.colorTheme\": \"One Dark Pro\",\n  \"editor.fontLigatures\": true,\n  \"editor.fontFamily\": \"JetBrains Mono Medium\",\n  \"editor.fontSize\": 16,\n  \"terminal.integrated.cursorStyle\": \"line\",\n  \"terminal.integrated.cursorWidth\": 2,\n  \"terminal.integrated.fontFamily\": \"MesloLGS NF\",\n  \"git.autofetch\": true,\n  \"trailing-spaces.trimOnSave\": true,\n  // config python\n  // black\n  \"python.formatting.provider\": \"black\",\n  \"python.formatting.blackArgs\": [\n    \"--line-length=88\"\n  ],\n  \"editor.formatOnSave\": true,\n  \"editor.formatOnPaste\": false,\n  \"editor.formatOnType\": false,\n  \"editor.rulers\": [\n    88,\n    120\n  ],\n  \"workbench.colorCustomizations\": {\n    \"editorRuler.foreground\": \"#750917\"\n  },\n  // pylance\n  \"python.languageServer\": \"Pylance\",\n  // isort\n  \"python.sortImports.path\": \"isort\",\n  \"python.sortImports.args\": [\n      \"-sp setup.cfg\"\n  ],\n  \"[python]\": {\n    \"editor.codeActionsOnSave\": {\n        \"source.organizeImports\": true\n    }\n  },\n  // flake8\n  \"python.linting.flake8Path\": \"flake8\",\n  \"python.linting.flake8Enabled\": true,\n  \"python.linting.pylintEnabled\": false,\n  \"python.linting.ignorePatterns\": [\n      \".vscode/*.py\",\n      \"**/site-packages/**/*.py\",\n      \".venv/**/*.py\",\n      \"**.pytest_cache/**/*.py\"\n    ],\n  // mypy\n  \"python.linting.mypyPath\": \"mypy\",\n  \"python.linting.mypyEnabled\": true,\n  // pytest\n  \"python.testing.pytestEnabled\" : true,\n  }\n</code></pre> On va s\u00e9parer les explications en deux parties, la partie g\u00e9n\u00e9rale, et la partie Python.</p>"},{"location":"ide_vscode/conf/#configuration-generale","title":"Configuration g\u00e9n\u00e9rale","text":"<p><pre><code>{\n  // config g\u00e9n\u00e9rale\n  \"editor.suggestSelection\": \"first\",\n  \"workbench.colorTheme\": \"One Dark Pro\",\n  \"editor.fontLigatures\": true,\n  \"editor.fontFamily\": \"JetBrains Mono Medium\",\n  \"editor.fontSize\": 16,\n  \"terminal.integrated.cursorStyle\": \"line\",\n  \"terminal.integrated.cursorWidth\": 2,\n  \"terminal.integrated.fontFamily\": \"MesloLGS NF\",\n  \"git.autofetch\": true,\n  \"trailing-spaces.trimOnSave\": true,\n}\n</code></pre> La configuration est plut\u00f4t classique, \"JetBrains Mono Medium\" et \"MesloLGS NF\" sont des polices d'\u00e9critures avec ligatures \"d\u00e9velopp\u00e9es pour la programmation\".</p>  <p>Note</p> <p>La deuxi\u00e8me n'est vraiment utile que si vous utilisez <code>zsh</code> comme shell avec Oh My Zsh et Powerlevel10k et voulait activer Powerlevel10k dans le terminal int\u00e9gr\u00e9 de vscode.</p>  <p>\"One Dark Pro\" est un th\u00e8me sombre d\u00e9riv\u00e9 du th\u00e8me de base de l'IDE Atom, et <code>\"trailing-spaces.trimOnSave\": true</code> est une option du plugins \"Trailing Spaces\" permettant de supprimer tous les espaces superflux \u00e0 chaque sauvegarde.</p>"},{"location":"ide_vscode/conf/#configuration-avec-python","title":"Configuration avec Python","text":"<p>Pour un projet python, il y a deux endroits que l'on doit configurer :</p> <ol> <li>Le fichier <code>settings.json</code>.</li> <li>Le fichier <code>setup.cfg</code>, situ\u00e9 \u00e0 la racine du dossier.</li> </ol> <p>Le fichier <code>settings.json</code> permet de configurer vscode pour lui dire quel formateur, linter, quelle librairie de tests choisir pour v\u00e9rifier la qualit\u00e9 du code, le fichier <code>setup.cfg</code> lui fixera les param\u00e8tres des ces derniers.</p>  <p>Note</p> <p>D'autres fichiers de configurations des librairies python, comme le fichier <code>pyproject.toml</code> d\u00e9fini dans PEP518, qui est offciellement le fichier de configuration pr\u00e9f\u00e9r\u00e9 par le formateur <code>black</code>, et aussi le fichier de configuration pr\u00e9f\u00e9r\u00e9 pour le manager de d\u00e9pendances et de projet poetry. Sauf que</p> <ul> <li>les d\u00e9veloppeurs de <code>flake8</code> ont d\u00e9cid\u00e9s qu'ils ne supporteraient pas le format <code>pyproject.toml</code>,</li> <li>les d\u00e9veloppeurs de <code>mypy</code> y r\u00e9fl\u00e9chissent...depuis 2018...</li> </ul> <p>Les librairies <code>isort</code>, <code>pytest</code> et <code>coverage</code> sont compatibles avec <code>pyproject.toml</code>, mais dans tous les cas on devrait garder un fichier <code>setup.cfg</code> pour <code>flake8</code> et tous ses plugins. <code>black</code> n'ayant que peu de choix dans ses configurations, autant tout mettre dans un fichier <code>setup.cfg</code>.</p>  <p>Dans ce fichier de configuration, on utilise donc :</p> <ul> <li>black pour le formatage du code.</li> <li>isort pour les triage des imports.</li> <li>flake8 pour le linting.</li> <li>mypy pour le type hinting.</li> <li>pytest pour l'ensemble des tests unitaires, d'int\u00e9grations, etc.</li> </ul>  <p>Note</p> <p>Ca parait \u00e9vident, mais m\u00eame si ces modules sont s\u00e9lectionn\u00e9s dans vscode, rien ne marchera s'ils ne sont pas install\u00e9s dans votre environnement virtuel, environnement docker ou autre.</p>  <p>On reviendra plus en d\u00e9tails sur chacun plus tard, expliquons ce fichier de config.</p> <pre><code>{\n  // config python\n  // black\n  \"python.formatting.provider\": \"black\",\n  \"python.formatting.blackArgs\": [\n    \"--line-length=88\"\n  ],\n  \"editor.formatOnSave\": true,\n  \"editor.formatOnPaste\": false,\n  \"editor.formatOnType\": false,\n  \"editor.rulers\": [\n    88,\n    120\n  ],\n  \"workbench.colorCustomizations\": {\n    \"editorRuler.foreground\": \"#750917\"\n  },\n  // pylance\n  \"python.languageServer\": \"Pylance\",\n  // isort\n  \"python.sortImports.path\": \"isort\",\n  \"python.sortImports.args\": [\n      \"-sp setup.cfg\"\n  ],\n  \"[python]\": {\n    \"editor.codeActionsOnSave\": {\n        \"source.organizeImports\": true\n    }\n  },\n  // flake8\n  \"python.linting.flake8Path\": \"flake8\",\n  \"python.linting.flake8Enabled\": true,\n  \"python.linting.pylintEnabled\": false,\n  \"python.linting.ignorePatterns\": [\n      \".vscode/*.py\",\n      \"**/site-packages/**/*.py\",\n      \".venv/**/*.py\",\n      \"**.pytest_cache/**/*.py\"\n    ],\n  // mypy\n  \"python.linting.mypyPath\": \"mypy\",\n  \"python.linting.mypyEnabled\": true,\n  // pytest\n  \"python.testing.pytestEnabled\" : true,\n  }\n</code></pre>"},{"location":"ide_vscode/conf/#black","title":"black","text":"<p>La premi\u00e8re partie active black comme formateur du code.</p> <ul> <li>On fixe la longueur maximale du code (ie pas de la documentation en docstrings) \u00e0 88 caract\u00e8res avec <code>python.formatting.blackArgs</code>, qui est la longueur maximale accept\u00e9e par black par d\u00e9faut.</li> <li>On fixe le formatage du code en automatique \u00e0 chaque sauvegarde, on d\u00e9sactive le formatage lors d'une copie ou lorsque l'on tape.</li> <li>Afin d'avoir un rep\u00e8re visuel pour la longueur du code, on cr\u00e9e une r\u00e8gle verticale \u00e0 88 caract\u00e8res et une a 120 caract\u00e8res via <code>\"editor.rulers\"</code>. La longueur de 120 caract\u00e8res est un choix personnel pour la longueur maximale des commentaires (de moins en moins utilis\u00e9). On choisit la couleur de ces r\u00e8gles verticales via <code>\"editorRuler.foreground\"</code>, ici rouge.</li> </ul>"},{"location":"ide_vscode/conf/#pylance","title":"pylance","text":"<p>Pylance est un plugin de Microsoft permettant permettant un support am\u00e9lior\u00e9 du plugin python.</p>"},{"location":"ide_vscode/conf/#isort","title":"isort","text":"<p>isort est un module permettant de trier par ordre alphab\u00e9tique et de fa\u00e7on automatique dans un ordre d'importance pr\u00e9cis. Par d\u00e9faut, l'ordre d'importance est le suivant.</p> <ol> <li>FUTURE, <code>from __future__ import ...</code></li> <li>STDLIB, les librairies standards fournies par python de base, par exemple <code>pathlib</code></li> <li>THIRDPARTY, la plupart des autres librairies, numpy, pandas, etc (celles qui ont besoin d'un <code>pip install ...</code> ou <code>conda install ...</code>).</li> <li>FIRSTPARTY, les modules provenant du projet python actuel.</li> <li>LOCALFOLDER, les imports locaux, g\u00e9n\u00e9ralement des imports avec des chemins relatifs.</li> </ol> <p><code>python.sortImports.args</code> permet de dire o\u00f9 se trouve le \"setup path\" (-sp), l'autre action permet d'automatiser le tri \u00e0 chaque sauvegarde, similaire doc \u00e0 celle pour black.</p>"},{"location":"ide_vscode/conf/#flake8","title":"flake8","text":"<p><code>flake8</code> est un linter, il permet de v\u00e9rifier que la code suit bien certaines r\u00e8gles de syntaxes. Pour citer wikip\u00e9dia au lieu de le paraphraser :</p>  <p>Quote</p> <p>lint, or a linter, is a static code analysis tool used to flag programming errors, bugs, stylistic errors, and suspicious constructs. The term originates from a Unix utility that examined C language source code.</p>  <p><code>black</code> formate le code selon certaines r\u00e8gles qui lui sont propres, par exemple les strings sont toutes d\u00e9finies entre doubles apostrophes <code>\"</code>, l'anglais utilisant d\u00e9j\u00e0 les apostrophes simples <code>'</code> pour le possessif ou pour les abbr\u00e9viations, cela \u00e9vite les probl\u00e8mes de conflits.</p> <p><code>flake8</code> peut \u00eatre configur\u00e9 pour s'adapter aux r\u00e8gles de <code>black</code>, et prendre en charge d'autres plugins, comme <code>wemake-python-styleguide</code> qui permet par exemple aussi de v\u00e9rifier la compl\u00e9xit\u00e9 des fonctions.</p> <p>D'autres linter sont disponibles dans vscode, comme <code>pylint</code>, <code>\"python.linting.pylintEnabled\": false</code> le d\u00e9sactive, les lignes de configs suivantes d\u00e9sactivent flake8 pour certains r\u00e9pertoires.</p>"},{"location":"ide_vscode/conf/#mypy","title":"mypy","text":"<p><code>mypy</code> permet de faire du 'type hinting\", en d'autres termes il permet de d\u00e9finir le type de variables que l'on aura en entr\u00e9e d'une fonction, et en d\u00e9duira si cette fonction est coh\u00e9rente.</p> <p>Voici un exemple classique de fonction.</p> <pre><code>def sum(x,y):\n  return x+y\n</code></pre> <p>Maintenant avec du type hinting, mais une erreur.</p> <pre><code>def sum(x: int,y: str):\n  return x+y\n</code></pre> <p><code>mypy</code> retournera une erreur, en disant que cette somme est incoh\u00e9rente.</p> <pre><code>def sum(x: int,y: int):\n  return x+y\n</code></pre> <p>Cette fonction l\u00e0 sera valid\u00e9e.</p> <p><code>mypy</code> est une aide suppl\u00e9mentaire pour la compr\u00e9hension des fonctions, mais aussi pour la r\u00e9daction des docstrings car ces d\u00e9corations seront directement r\u00e9cup\u00e9r\u00e9es par <code>mkdocstrings</code> et le plugin <code>Python Docstring Generator</code> de vscode.</p> <p><code>mypy</code> \u00e9tant compatible avec <code>flake8</code>, <code>\"python.linting.mypyEnabled\": true</code> active cette compatibilit\u00e9 en permettant de recevoir les messages de <code>mypy</code> via des messages <code>flake8</code> dans vscode.</p>"},{"location":"ide_vscode/conf/#pytest","title":"pytest","text":"<p>Librairie pour les tests python.</p>"},{"location":"lectures/gradient_centralization/","title":"Gradient Centralization: A New Optimization Technique for Deep Neural Networks","text":"<ul> <li>ArXiv</li> </ul>"},{"location":"lectures/gradient_centralization/#abstract","title":"Abstract","text":"<p>Optimization techniques are of great importance to effectively and efficiently train a deep neural network (DNN). It has been shown that using the first and second order statistics (e.g., mean and variance) to perform Z-score standardization on network activations or weight vectors, such as batch normalization (BN) and weight standardization (WS), can improve the training performance. Different from these existing methods that mostly operate on activations or weights, we present a new optimization technique, namely gradient centralization (GC), which operates directly on gradients by centralizing the gradient vectors to have zero mean. GC can be viewed as a projected gradient descent method with a constrained loss function. We show that GC can regularize both the weight space and output feature space so that it can boost the generalization performance of DNNs. Moreover, GC improves the Lipschitzness of the loss function and its gradient so that the training process becomes more efficient and stable. GC is very simple to implement and can be easily embedded into existing gradient based DNN optimizers with only one line of code. It can also be directly used to fine-tune the pre-trained DNNs. Our experiments on various applications, including general image classification, fine-grained image classification, detection and segmentation, demonstrate that GC can consistently improve the performance of DNN learning. The code of GC can be found at this https URL.</p>"},{"location":"lectures/gradient_centralization/#notes","title":"Notes","text":"<p>La centralisation du gradient, est une nouvelle m\u00e9thode d'optimisation des r\u00e9seaux de neurones (plut\u00f4t une modification de celles d\u00e9j\u00e0 existantes), qui se concentre sur le gradient lui m\u00eame afin de le centrer, i.e. \u00e9tant donn\u00e9 le vecteur gradient \\(\\overrightarrow{\\mathrm{grad}}\\) obtenu en diff\u00e9rentiant la fonction de perte \\(\\mathcal{L}_{\\vartheta}\\), on le centralise de fa\u00e7on \u00e0 obtenir \\(\\overrightarrow{\\mathrm{grad}}_{CG}\\) v\u00e9rifiant l'\u00e9glite suivante.</p> \\[     \\mathbb{E}(\\overrightarrow{\\mathrm{grad}}_{CG}) =0 \\]  <p>Remarque</p> <p>On le verra par la suite, cette m\u00e9thode peut \u00eatre vu de fa\u00e7on g\u00e9om\u00e9trique comme une projection orthogonale du gradient sur un hyperplan contraignant la fonction de perte \\(\\mathcal{L}_{\\vartheta}\\).</p>"},{"location":"lectures/gradient_centralization/#introduction","title":"Introduction","text":"<p>L'\u00e9fficacit\u00e9 de l'entra\u00eenement des r\u00e9seaux de neurones d\u00e9pend beaucoup des techniques d'optimization employ\u00e9es.</p> <p>Le choix d'un optimiseur (Adam, RMSProp, Nesterov, etc) se fait g\u00e9n\u00e9ralement sur deux crit\u00e8res :</p> <ol> <li>L'acc\u00e9l\u00e9ration de l'entra\u00eenement qui en r\u00e9sulte.</li> <li>L'am\u00e9lioration de la capacit\u00e9 de g\u00e9n\u00e9ralisation du r\u00e9seau.</li> </ol> <p>D'autres techniques d'optimisation existent :</p> <ul> <li>Les m\u00e9thodes d'initialisation des poids (He, Xavierc etc),</li> <li>Le choix des fonctions d'activations (ReLU, mish),</li> <li>Le clipping du gradient,</li> <li>Mettre en place un taux d'apprentissage adaptatif.</li> </ul> <p>Il existe aussi des techniques de normalisation et de standardisation des poids, mais ces derni\u00e8res ne sont pas applicables aux mod\u00e8les pr\u00e9-entrain\u00e9s, dont les poids ne peuvent pas \u00eatre centr\u00e9s, norm\u00e9s (sinon on perd l'int\u00e9r\u00eat du pr\u00e9-entra\u00eenement).</p> <p>L'article apporte deux contributions :</p> <ul> <li>Une nouvelle technique d'optimisation bas\u00e9e sur la centralisation du gradient, qui est facilement impl\u00e9mentable, voir le tutoriel sur keras.io.</li> <li>Les aspects th\u00e9oriques de cette centralisation du gradient. L'article montre en particuliers que la centralisation du gradient :<ol> <li>contraint la fonction de perte en ajoutant une nouvelle contrainte sur le vecteur de poids, ce qui permet de r\u00e9gulariser l'espace des poids ainsi de celui des features de sortie.</li> <li>Am\u00e9liore les propri\u00e9t\u00e9s lipschitzienne de la fonction de perte.</li> </ol> </li> </ul>  <p>D\u00e9finition</p> <ol> <li>Une fonction \\(f\\) entre deux espaces m\u00e9triques \\(f \\, : \\, (X, d_{1}) \\rightarrow (Y, d_{2})\\) est lipschitz s'il existe une constante \\(C &gt;0\\) telle que :</li> </ol> \\[     \\forall x, y \\in X, d_{2}(f(x), f(y)) \\leq C d_{1}(x,y). \\] <p>En particulier, la fonction \\(f\\) est continue.</p> <ol> <li>Une fonction diff\u00e9rentiable \\(f\\) entre deux espaces norm\u00e9s \\(f \\, : \\, (X, d_{1}) \\rightarrow (Y, d_{2})\\) est lipschitz lisse s'il existe une constante \\(C &gt;0\\) telle que :</li> </ol> \\[     \\forall x, y \\in X, \\| \\nabla f(x) - \\nabla f(y) \\| \\leq C \\| x - y \\|. \\] <p>(\\(C\\)-Lipschitz continuous gradient)</p> <ol> <li>La constante \\(C\\) est app\u00e9l\u00e9e la constante de Lipschitz de \\(f\\).</li> </ol>   <p>A voir</p> <ul> <li>Lipschitz Smoothness, Strong Convexity and the Hessian</li> <li>Lipschitz continuous gradient</li> </ul>"},{"location":"lectures/gradient_centralization/#centralisation-du-gradient","title":"Centralisation du gradient","text":"<p>On sait que la Batch Normalization permet de r\u00e9duire la constante de Lipschitz de la fonction de perte \\(\\mathcal{L}_{\\vartheta}\\) (TODO : Sources) et rend les gradients plus lipschitz continus.</p> <p>Cependant, si la batch size est petite, la Group Normalization est plus appropri\u00e9e, on perd alors les propri\u00e9t\u00e9s cit\u00e9es ci-dessus.</p> <p>Peut-on directement op\u00e9rer sur le gradient pour stabiliser l'entra\u00eenement ? Normaliser le gradient n'am\u00e9liore pas la stabilit\u00e9 de l'entra\u00eenement, on va dire le centrer, ie le rendre d'esp\u00e9rance nulle.</p>"},{"location":"lectures/gradient_centralization/#notations-et-conventions","title":"Notations et conventions","text":"<ul> <li> <p>Lorsque l'on parlera qu'une couche dense, on notera sa matrice de poids</p> \\[ W_{fc} \\in \\mathbb{R}^{C_{in} \\times C_{out}}. \\] </li> <li> <p>Lorsque l'on parlera d'une couche convolutive, on notera sa matrice de poids</p> \\[     W_{conv} \\in \\mathbb{R}^{C_{in} \\times C_{out} \\times (k_{1}k_{2})}, \\] <p>\\(k_{1}\\) et \\(k_{2}\\) \u00e9tant les dimensions des noyaux de convolutions.</p> </li> <li> <p>Pour une couche quelconque, on utilisera la notation unifi\u00e9e \\(W \\in \\mathbb{R}^{M \\times N}\\). Pour \\(W_{fc}\\), \\(M = C_{in}\\), et pour \\(W_{fc}\\), \\(M = C_{in}k_{1}k_{2}\\).</p> </li> <li> <p>Pour une couche \\(W \\in \\mathbb{R}^{M \\times N}\\), on notera \\(w_{i} \\in \\mathbb{R}^{M}, i = 1, \\dots, N\\) sa \\(i\\)-i\u00e8me colonne.</p> </li> </ul>  <p>Convention matricielle</p> <p>pour un vecteur d'entr\u00e9e \\(X\\) de la couche \\(W\\), les features en sortie de la couche sont alors donn\u00e9es par la formule suivante.</p> \\[     out_{W}(X) := W^{T} \\cdot X \\qquad W^{T} \\in \\mathbb{R}^{C_{out} \\times C_{in}} \\] <p>Ce qui veut dire que par convention, les vecteurs consid\u00e9r\u00e9s ici sont des vecteurs colonnes.</p> \\[ out_{W}(X) = \\left[ \\begin{array}{ccc}     \\rule[.5ex]{2.5ex}{0.5pt} &amp; w^{T}_{1} &amp; \\rule[.5ex]{2.5ex}{0.5pt} \\\\     \\rule[.5ex]{2.5ex}{0.5pt} &amp; w^{T}_{2} &amp; \\rule[.5ex]{2.5ex}{0.5pt} \\end{array} \\right]^{T} \\cdot \\begin{bmatrix}         x_{1} \\\\         x_{2} \\end{bmatrix} \\] \\[ W^{T} \\cdot X = \\left[ \\begin{array}{ccc}     w_{1,1} &amp; w_{1,2} &amp; w_{1,3} \\\\     w_{2,1} &amp; w_{2,2} &amp; w_{2,3} \\end{array} \\right]^{T} \\cdot \\begin{bmatrix}         x_{1} \\\\         x_{2} \\end{bmatrix} \\]  <ul> <li> <p>Pour la fonction de perte \\(\\mathcal{L}_{\\vartheta}\\), on notera son gradient \\(\\nabla_{W}\\mathcal{L}_{\\vartheta}\\) par rapport \u00e0 la matrice de poids \\(W\\), et \\(\\nabla_{w_{i}}\\mathcal{L}_{\\vartheta}\\) son gradient par rapport au vecteur de poids \\(w_{i}\\).</p> </li> <li> <p>On note \\(\\mathbf{e} := \\frac{1}{\\sqrt{M}}\\cdot \\mathbb{1}_{M}\\), vecteur unitaire de \\(\\mathbb{R}^{M}\\).</p> </li> </ul>"},{"location":"lectures/gradient_centralization/#formulation-de-la-centralisation-du-gradient","title":"Formulation de la centralisation du gradient","text":"<p>Pour une couche dense ou convolutive, supposons que l'on a obtenu le gradient via r\u00e9tropropagation.</p> <p>Pour un vecteur de poids \\(w_{i}\\), de gradient \\(\\nabla_{w_{i}}\\mathcal{L}_{\\vartheta}\\), l'op\u00e9rateur de centralisation du gradient, not\u00e9 \\(\\Phi_{CG}\\), est alors d\u00e9fini comme suit.</p> \\[     \\Phi_{CG}(\\nabla_{w_{i}}\\mathcal{L}_{\\vartheta}) := \\nabla_{w_{i}}\\mathcal{L}_{\\vartheta} - \\mathbb{E}(\\nabla_{w_{i}}\\mathcal{L}_{\\vartheta}) \\] <p>O\u00f9</p> \\[     \\mathbb{E}(\\nabla_{w_{i}}\\mathcal{L}_{\\vartheta}) := \\frac{1}{M}\\sum_{j=1}^{M}\\nabla_{w_{i,j}}\\mathcal{L}_{\\vartheta}. \\] <p>En d'autres termes, pour une matrice de poids, on calcule la moyenne de chaque vecteur colonne de cette matrice et on retire cette moyenne \u00e0 ses colonnes.</p>"},{"location":"lectures/gradient_centralization/#formulation-matricielle-et-representation-geometrique","title":"Formulation matricielle et repr\u00e9sentation g\u00e9om\u00e9trique","text":"<p>D\u00e9finition : Produit de Kronecker</p> <p>Pour \\(x\\) et \\(y\\) deux vecteurs colonnes de dimensions \\(M\\), respectivement \\(N\\), le prodouit de Kronecker de \\(x\\) et \\(y\\), not\u00e9 \\(x \\cdot y^{T}\\) ou \\(x \\otimes y\\) est alors d\u00e9fini de la fa\u00e7on suivante.</p> \\[     x \\cdot y^{T} := \\left[ x_{i} \\cdot y_{j}\\right] \\] <p>O\u00f9 \\(\\left[ x_{i} \\cdot y_{j}\\right] \\in \\mathbb{R}^{M \\times N}\\) est la matrice dont les coefficients en les coordonn\u00e9es \\((i,j)\\) sont donn\u00e9es par le produit \\(x_{i} \\cdot y_{j}\\).</p> <p>Ce produit n'est pas commutatif. C'est un cas particulier de produit tensoriel, voir ici.</p>  <p>Dans le cas qui nous int\u00e9resse ici, on applique le produit de Kronecker au vecteur suivant : \\(\\mathbf{e}\\).</p> \\[     \\begin{align*}     \\mathbf{e} \\otimes \\mathbf{e} &amp; := \\frac{1}{\\sqrt{M}}\\cdot \\mathbb{1} \\otimes \\frac{1}{\\sqrt{M}}\\cdot \\mathbb{1} \\\\                                   &amp; = \\frac{1}{M}\\cdot \\mathbb{1} \\otimes \\mathbb{1} \\\\                                   &amp; = \\frac{1}{M}\\cdot \\mathbb{1} \\otimes \\mathbb{1} \\\\                                   &amp; = \\frac{1}{M}\\cdot\\left[                                         \\begin{array}{ccc}                                             1 &amp; \\cdots &amp; 1 \\\\                                             \\vdots &amp;  &amp; \\vdots \\\\                                             1 &amp; \\cdots &amp; 1                                         \\end{array}                                         \\right]     \\end{align*} \\] <p>Ici, \\(\\mathbb{1} \\otimes \\mathbb{1}\\) est donc une matrice carr\u00e9e de taille \\(M \\times M\\) o\u00f9 tous les coefficients sont \u00e9gaux \u00e0 \\(1\\).</p> <p>Remarquons que dans l'autre sens, le produit scalaire \\(\\mathbf{e}^{T} \\cdot \\mathbf{e}\\) est \u00e9gal \u00e0 \\(1\\), car \\(\\mathbf{e}\\) est par d\u00e9finition un vecteur unitaire.</p> <p>Pour une matrice de poids \\(W\\), de gradient \\(\\nabla_{W}\\mathcal{L}_{\\vartheta}\\), l'op\u00e9rateur de centralisation du gradient, not\u00e9 \\(\\Phi_{CG}\\), est alors d\u00e9fini comme suit.</p> \\[     \\Phi_{CG}(\\nabla_{W}\\mathcal{L}_{\\vartheta}) := \\mathbf{P}(\\nabla_{W}\\mathcal{L}_{\\vartheta}) \\] <p>O\u00f9 \\(\\mathbf{P} := \\mathbb{I}_{M} - \\mathbf{e} \\otimes \\mathbf{e}\\). \\(\\mathbb{I}_{M}\\) \u00e9tant la matrice identit\u00e9 de taille \\(M\\).</p> <p>Avant de voir les propri\u00e9t\u00e9s de l'op\u00e9rateur \\(\\mathbf{P}\\), v\u00e9rifions que l'on obtient bien le m\u00eame r\u00e9sultat avec cette d\u00e9finition que la pr\u00e9c\u00e9dente.</p> <p>On a \\(\\nabla_{W}\\mathcal{L}_{\\vartheta} := \\left[ \\nabla_{w_{1}}\\mathcal{L}_{\\vartheta} , \\cdots, \\nabla_{w_{N}}\\mathcal{L}_{\\vartheta} \\right]\\), d'o\u00f9</p> \\[     \\begin{align*}     \\Phi_{CG}(\\nabla_{W}\\mathcal{L}_{\\vartheta}) &amp; = \\left(\\mathbb{I}_{M} - \\mathbf{e} \\otimes \\mathbf{e} \\right)\\cdot \\left[ \\nabla_{w_{1}}\\mathcal{L}_{\\vartheta} , \\cdots, \\nabla_{w_{N}}\\mathcal{L}_{\\vartheta} \\right] \\\\                                                  &amp; = \\left[ \\nabla_{w_{1}}\\mathcal{L}_{\\vartheta} , \\cdots, \\nabla_{w_{N}}\\mathcal{L}_{\\vartheta} \\right] - \\frac{1}{M}\\cdot \\mathbb{1} \\cdot \\mathbb{1}^{T} \\cdot \\left[ \\nabla_{w_{1}}\\mathcal{L}_{\\vartheta} , \\cdots, \\nabla_{w_{N}}\\mathcal{L}_{\\vartheta} \\right].     \\end{align*} \\] <p>Par lin\u00e9arit\u00e9, il suffit de le v\u00e9rifier sur chaque colonne.</p> \\[     \\begin{align*}     \\nabla_{w_{i}}\\mathcal{L}_{\\vartheta} - \\frac{1}{M}\\cdot \\mathbb{1} \\cdot \\mathbb{1}^{T} \\cdot \\nabla_{w_{i}}\\mathcal{L}_{\\vartheta} &amp; = \\nabla_{w_{i}}\\mathcal{L}_{\\vartheta} - \\frac{1}{M}\\cdot \\mathbb{1} \\cdot \\left( \\mathbb{1}^{T} \\cdot \\nabla_{w_{i}}\\mathcal{L}_{\\vartheta} \\right) \\\\     &amp; = \\nabla_{w_{i}}\\mathcal{L}_{\\vartheta} - \\frac{1}{M}\\cdot \\mathbb{1} \\cdot \\left(\\sum_{j=1}^{M}\\nabla_{w_{i,j}}\\mathcal{L}_{\\vartheta}\\right) \\\\     &amp; = \\nabla_{w_{i}}\\mathcal{L}_{\\vartheta} - \\mathbb{E}(\\nabla_{w_{i}}\\mathcal{L}_{\\vartheta})     \\end{align*} \\] <p>On a donc bien le m\u00eame r\u00e9sultat, peut importe la d\u00e9finition. Passons donc maintenant aux propri\u00e9t\u00e9s de l'op\u00e9rateur \\(\\mathbf{P}\\).</p>  <p>Th\u00e9or\u00e8me</p> <p>L'op\u00e9rateur \\(\\mathbf{P}\\) est idempotent et d\u00e9finie une projection sur l'hyperplan orthogonal au vecteur unitaire \\(\\mathbf{e}^{T}\\).</p>  <p>Montrons premi\u00e8rement que c'est un op\u00e9rateur idempotent. Pour cela il suffit de montrer que \\(P^{2} = P = P^{T}\\). Tout d'abord, on a \\(P^{T} = (\\mathbb{I}_{M} - \\mathbf{e} \\otimes \\mathbf{e})^{T} = \\mathbb{I}_{M}^{T} - (\\mathbf{e} \\cdot \\mathbf{e}^{T})^{T} = \\mathbb{I}_{M} - \\mathbf{e} \\cdot \\mathbf{e}^{T} = P\\).</p> <p>D'o\u00f9,</p> \\[     \\begin{align*}         P^{2} = P^{T}P &amp; = (\\mathbb{I}_{M} - \\mathbf{e} \\otimes \\mathbf{e})^{T} \\cdot (\\mathbb{I}_{M} - \\mathbf{e} \\otimes \\mathbf{e}) \\\\                        &amp; = \\mathbb{I}_{M} - 2 \\mathbf{e}\\cdot \\mathbf{e}^{T} + \\mathbf{e}\\cdot (\\mathbf{e}^{T}\\cdot \\mathbf{e})\\cdot \\mathbf{e}^{T} \\\\                        &amp; = \\mathbb{I}_{M} - 2 \\mathbf{e}\\cdot \\mathbf{e}^{T} + \\mathbf{e}\\cdot\\mathbf{e}^{T} \\\\                        &amp; = \\mathbb{I}_{M} - \\mathbf{e}\\cdot \\mathbf{e}^{T} \\\\                        &amp; = P.     \\end{align*} \\] <p>\\(P\\) \u00e9tant une application lin\u00e9aire de \\(\\mathbb{R}^{M} \\rightarrow \\mathbb{R}^{M}\\), on a la somme directe</p> \\[     \\mathbb{R}^{M} = \\ker P \\oplus \\mathrm{im} \\, P, \\] <p>avec</p> \\[     \\mathrm{im}\\,P = \\ker(\\mathbb{I}_{M}-P), \\] <p>or \\(\\ker(\\mathbb{I}_{M}-P) = \\ker(\\mathbb{I}_{M}-(\\mathbb{I}_{M} - \\mathbf{e} \\otimes \\mathbf{e})) = \\ker (\\mathbf{e} \\otimes \\mathbf{e})\\).</p> \\[     \\ker (\\mathbf{e} \\otimes \\mathbf{e}) = \\left\\{ (x_{1}, \\dots, x_{M}) \\in \\mathbb{R}^{M} \\, : \\, \\sum_{i=1}^{M} x_{i} = 0 \\right\\} \\] <p>Cela d\u00e9finit bien un hyperplan, l'hyperplan des vecteurs centr\u00e9s, ie de moyenne nulle. De plus, on a \\(\\mathbf{e}^{T}\\mathbf{P} = \\mathbf{e}^{T}(\\mathbb{I}_{M} - \\mathbf{e} \\otimes \\mathbf{e}) = \\mathbf{e}^{T} - (\\mathbf{e}^{T} \\cdot \\mathbf{e}) \\cdot \\mathbf{e}^{T} = \\mathbf{e}^{T} -  \\mathbf{e}^{T} = 0\\).</p> <p>Donc \\(\\mathbf{P}\\) est un op\u00e9rateur de projection sur l'hyperplan orthogonal au vecteur unitaire \\(\\mathbf{e}^{T}\\).</p> \\[     P \\, : \\, \\mathbb{R}^{M\\times N} \\longrightarrow \\mathbb{R}^{M\\times N} \\] \\[     \\mathbb{R}^{M\\times N} = \\ker(\\mathbf{e} \\otimes \\mathbf{e})^{M \\times N} \\oplus \\left( \\left\\langle \\mathbb{1}_{M} \\right\\rangle \\oplus \\cdots \\oplus \\left\\langle \\mathbb{1}_{M} \\right\\rangle \\right) \\]"},{"location":"lectures/gradient_centralization/#application-a-la-descente-du-gradient","title":"Application \u00e0 la descente du gradient","text":"<p>Attention</p> <p>On rappelle ici que la couche dense ou convolutive sur laquelle on op\u00e8re est fix\u00e9e.</p> <p>Pour un r\u00e9seau de neurones, on a donc un op\u00e9rateur de centralisation \\(\\Phi_{CG}\\), par couche dense et convolutive.</p>  <p>Pour chaque couche de matrice de poids \\(W \\in \\mathbb{R}^{M \\times N}\\), on a donc :</p> <ul> <li>un vecteur unitaire \\(\\mathbf{e}_{W} = \\frac{1}{\\sqrt{M}}\\cdot \\mathbb{1}_{W}\\),</li> <li>et un op\u00e9rateur de centralisation \\(\\Phi_{CG}\\) projetant sur \\(\\ker (\\mathbf{e}_{W} \\otimes \\mathbf{e}_{W})\\) orthogonalement \u00e0 \\(\\mathbf{e}_{W}^{T}\\).</li> </ul>  <p>Notons \\(W^{t}\\) la matrice des poids \u00e0 l'it\u00e9ration \\(t\\) pour une couche fix\u00e9e. Une \u00e9quation de l'hyperplan sur lequel projette \\(\\Phi_{CG}\\) est la suivante.</p> \\[     \\mathcal{H} := \\left\\{ -w \\in \\mathbb{R}^{M} \\, : \\, \\mathbf{e}_{W}^{T} \\cdot w  = 0 \\right\\} \\]"},{"location":"mlops/DVC/","title":"Le data versioning avec DVC","text":"<ul> <li>La chaine YouTube de DVC</li> <li> <p>La documentation officielle</p> </li> <li> <p>Lien de la premi\u00e8re vid\u00e9o tuto</p> </li> </ul>"},{"location":"mlops/DVC/#recuperer-des-donnees-et-initialisation-du-tracking","title":"R\u00e9cup\u00e9rer des donn\u00e9es et initialisation du tracking","text":"<p>DVC pour Data Versioning Control est un syst\u00e8me de versioning compl\u00e9mentire \u00e0 git, l\u00e0 o\u00f9 git s'occupe de faire un versioning du code \u00e9crit, DVC lui s'occupe du versioning du versant Machine Learning : les datasets et les mod\u00e8les, afin d'avoir une historicisation.</p> <p>DVC fonctionne en parall\u00e8le de git, la premi\u00e8re choise \u00e0 faire est donc d'initialiser les deux lorsque que l'on cr\u00e9e un nouveau repo.</p> <pre><code>git init\n\ndvc init\n</code></pre> <p>Pour r\u00e9cup\u00e9rer un dataset stock\u00e9 en ligne, la chose \u00e0 faire, est d'utiliser la commande</p> <pre><code>dvc get url_de_stockage\n</code></pre> <p>On peut penser \u00e0 <code>dvc get</code> comme une sorte de wrapper sur <code>cURL</code> ou <code>wget</code>.</p> <p>Une fois que l'on a des donn\u00e9es que l'on souhaite ajouter au syst\u00e8me de tracking de dvc, la commande \u00e0 lancer est similaire \u00e0 celle de git :</p> <pre><code>dvc add *adresse_locale_des_donn\u00e9es*\n</code></pre>"},{"location":"mlops/DVC/#push-des-donnees-sur-un-repo-de-donnees-google-drive-amazon-s3","title":"Push des donn\u00e9es sur un repo de donn\u00e9es (Google drive, Amazon S3, ...)","text":"<p>Pour l'instant les donn\u00e9es obtenues par exemple avec <code>dvc get ...</code> ne sont stock\u00e9es qu'en local, voyons comment les stocker de fa\u00e7on historis\u00e9e avec dvc.</p> <p>GitHub, Gitlab, etc ne sont pas faits pour stocker un un grand nombre de donn\u00e9es formant un dataset. Pour cela on peut utiliser des alternatives telles que Google Drive ou Amazon S3 par exemple.</p> <p>Voyons comment cr\u00e9er un \"repo de donn\u00e9es\" via google drive.</p> <ol> <li> <p>Cr\u00e9er un dossier dans votre google drive, l'adresse devrait alors ressembler \u00e0 quelque chose comme cela.</p> <p><code>https://drive.google.com/drive/folders/1KIvq4ibX2iTfBSFoO9PFCzIlHm7BEPHk</code></p> <p>Retenez l'id du dossier qui ce trouve apr\u00e8s le <code>folders/*</code>, ici c'est <code>1KIvq4ibX2iTfBSFoO9PFCzIlHm7BEPHk</code>.</p> </li> <li> <p>Il faut maintenant ajouter ce dossier au syst\u00e8me de tracking de dvc, ce qui se fait similairement \u00e0 git via la commande suivante.</p> <p><code>dvc remote add -d storage gdrive://1KIvq4ibX2iTfBSFoO9PFCzIlHm7BEPHk</code></p> <p><code>-d</code> \u00e9tant pour <code>default</code>.</p> </li> <li> <p>On peut maintenant faire le commit dans git pour synchroniser et dire que ce dossier \u00e0 \u00e9t\u00e9 ajout\u00e9.</p> <p><code>git commit .dvc/config -m \"Configure remote data storage\"</code></p> </li> <li> <p>Pour stocker les donn\u00e9es ur votre gdrive, il suffit maintenant depuis le dossier de stockage local de faire un</p> <p><code>dvc push</code>.</p> <p>Si c'est la premi\u00e8re fois que vous vous connectez \u00e0 gdrive de cette fa\u00e7on, on vous demandera surment de vous identifier.</p> </li> </ol>"},{"location":"mlops/DVC/#pull-des-donnees-depuis-le-cloud","title":"Pull des donn\u00e9es depuis le cloud","text":"<p>Pour r\u00e9cup\u00e9rer les donn\u00e9es stock\u00e9es sur le cloud via DVC, il suffit alors de lancer la commande suivante.</p> <pre><code>dvc pull\n</code></pre>"},{"location":"mlops/DVC/#retour-en-arriere","title":"Retour en arri\u00e8re","text":"<p>Lorsque dvc initialise un repo de donn\u00e9es, il cr\u00e9e aussi un fichier <code>*.dvc</code>, par exemple <code>datas.dvc</code> si <code>datas</code> est le nom du dossier contenant le dataset qui permettra \u00e0 git de prendre en compte les modifications du dataset. L'int\u00e9rieur de ce fichier <code>*.dvc</code> ressemble \u00e0 cela :</p> <pre><code>outs:\n- md5: 8369bfd53fac6b608ea6e88d25d68e44.dir\n  path: datas\n</code></pre>  <p>Remarque</p> <p>Git ne traque pas le dataset en lui m\u00eame, il traque le ficiher <code>.dvc</code> cr\u00e9\u00e9 par dvc pour synchroniser les modifications du dataset avec celle de git.</p>  <p>Pour revenir en arri\u00e8re sur le dataset pr\u00e9c\u00e9dent, cela se fait donc via git, puis ensuite dvc.</p> <ol> <li><code>git log --oneline</code> pour voir les logs des diff\u00e9rents commits.</li> <li><code>git checkout HEAD^1 datas.dvc</code> pour revenir \u00e0 la version pr\u00e9c\u00e9dente de ce fichier, qui est le fichier de logs du dataset dans git.</li> <li><code>dvc checkout</code> permet alors de revenir \u00e0 la version pr\u00e9c\u00e9dente du dataset.</li> </ol>  <p>Attention</p> <p>On peut maintenant faire la remarque suivante : DVC n'est pas un syst\u00e8me de versioning en lui m\u00eame. Le versioning se fait via git, ce que DVC permet est d'\u00e9tendre le versioning \u00e0 des donn\u00e9es qui ne sont pas standards pour git : les datasets (de grandes tailles), les mod\u00e8les, ...</p>  <ul> <li>Lien de la deuxi\u00e8me vid\u00e9o tuto</li> </ul>"},{"location":"mlops/DVC/#partage-des-donnees-et-des-modeles-avec-dvc","title":"Partage des donn\u00e9es et des mod\u00e8les avec DVC","text":"<p>Dans un repo git, il peut \u00eatre utile de savoir quels sont les fichiers qui sont monitor\u00e9s par dvc, cela poeut se faire via la commande <code>dvc list</code>.</p> <pre><code>dvc list https://github.com/user/git_repo nom_du_dossier\n</code></pre> <p>permet de voir l'ensemble des fichiers monitor\u00e9s pas dvc dans le dossier <code>nom_du_dossier</code> du <code>git_repo</code>.</p>  <p>Exemple</p> <p>Par exemple,</p> <pre><code>dvc list https://github.com/iterative/dataset-registry use-cases\n</code></pre> <p>montrera que l'on a un fichier qui est monitor\u00e9 par dvc : le fichier <code>cats-dogs.dvc</code>, qui g\u00e8re quelles sont les donn\u00e9es utilis\u00e9es dans le dataset <code>Cats &amp; Dogs</code>. Pour r\u00e9cup\u00e9rer les images correspondantes, on lance la commande suivante.</p> <pre><code>dvc get https://github.com/iterative/dataset-registry use-cases/cats-dogs\n</code></pre> <p>Les donn\u00e9es (ici des images) images seront alors pull depuis le stockage cloud sur le stockage local.</p>   <p>Attention</p> <p>La commande <code>dvc get</code> ne fait qu'importer les donn\u00e9es, elle ne stocke pas quand ou comment les donn\u00e9es ont \u00e9t\u00e9 recup\u00e9r\u00e9es, ce qui peut \u00eatre probl\u00e9matique.</p>  <p>Pour avoir ces donn\u00e9es de tracking suppl\u00e9mentaires, il faut utiliser le commande <code>dvc import</code>.</p>  <p>Exemple</p> <pre><code>dvc import https://github.com/iterative/dataset-registry get-started/data.xml -o data/data.xml\n</code></pre> <p><code>-o</code> corredspond \u00e0 <code>output</code> et permet de dire o\u00f9 pr\u00e9cis\u00e9ment je souhaite que ces donn\u00e9es soient t\u00e9l\u00e9charg\u00e9es.</p> <p>Utiliser cette commande cr\u00e9era alors deux nouveaux fichiers :</p> <ul> <li>Un fichier <code>.gitignore</code> disant \u00e0 git d'ignorer les donn\u00e9es t\u00e9lm\u00e9charg\u00e9es, qui peuvent \u00eatre volumineuse, et produire des erreurs dans git.</li> <li>Un fichier <code>*.dvc</code>, eg <code>datas.dvc</code> qui permettra de monitorer les donn\u00e9es nouvellements t\u00e9l\u00e9charg\u00e9es.</li> </ul>  <p>La commande <code>dvc update datas.dvc</code> permet elle de v\u00e9rifier les modification du dataset dans le stockage cloud et t\u00e9l\u00e9charger la nouvelle version si n\u00e9c\u00e9ssaire.</p>"},{"location":"mlops/DVC/#utilisation-de-lapi-python-de-dvc","title":"Utilisation de l'API Python de DVC","text":"<p>Python</p> <pre><code>import dvc.api\n\nwith dvc.api.open(\n        'get-started/data.xml',\n        repo='https://github.com/iterative/dataset-registry'\n        ) as fd:\n    # ... fd is a file descriptor that can be processed normally.\n</code></pre>"},{"location":"mlops/DVC/#pipelines","title":"Pipelines","text":"<p>Cr\u00e9er un pipeline de transformation pour avoir toujours les m\u00eames r\u00e9sultats peut se cr\u00e9er de deux fa\u00e7ons :</p> <ol> <li>Via l'interface en ligne de commande,</li> <li>Via la cr\u00e9ation d'un fichier <code>yaml</code>.</li> </ol> <p>Dans le cas o\u00f9 l'on passe par l'interface en ligne de commande, un fichier <code>yaml</code> correspondant \u00e0 la deuxi\u00e8me solution sera de toute fa\u00e7on cr\u00e9\u00e9 automatiquement.</p>  <p>Exemple</p> <p>source</p> <p>Les lignes de commandes suivantes</p> <pre><code>dvc run -n prepare \\\n      -p prepare.seed,prepare.split \\\n      -d src/prepare.py -d data/data.xml \\\n      -o data/prepared \\\n      python src/prepare.py data/data.xml\n</code></pre> <p>correspondent au fichier <code>yaml</code> suivant.</p> <pre><code>stages:\n    prepare:\n        cmd: python src/prepare.py data/data.xml\n        deps:\n        - data/data.xml\n        - src/prepare.py\n        params:\n        - prepare.seed\n        - prepare.split\n        outs:\n        - data/prepared\n</code></pre>"},{"location":"mlops/DVC/#some-qa-from-dvc-website","title":"Some Q&amp;A from DVC website","text":""},{"location":"mlops/DVC/#when-i-run-dvc-repro-on-a-stage-does-it-automatically-push-any-outputs-to-my-remote","title":"When I run <code>dvc repro</code> on a stage, does it automatically push any outputs to my remote ?","text":"<p>The <code>dvc repro</code> command doesn't automatically push any outputs or data to your remote. The outputs are stored in the cache until you run <code>dvc push</code>, which then pushes them from your cache to your remote.</p>"},{"location":"mlops/DVC/#is-there-a-way-to-get-dvc-to-import-from-a-private-repository","title":"Is there a way to get DVC to import from a private repository ?","text":"<p>You can use SSH to handle this and run the following command <code>dvc import git@gitlab.com:&lt;reposiotry location&gt; &lt;data_path&gt;</code>.</p>"},{"location":"mlops/DVC/#what-is-the-difference-between-dvc-pull-and-dvc-checkout","title":"What is the difference between <code>dvc pull</code> and <code>dvc checkout</code> ?","text":"<p>Here are some explanations around how <code>dvc pull</code> and <code>dvc checkout</code> work. They're comparable to <code>git pull</code> and <code>git checkout</code>.</p> <ul> <li><code>dvc pull</code> fetches data from your remote cache to your local cache and syncs it to your workspace,</li> <li><code>dvc checkout</code> syncs data from your local cache to your workspace.</li> </ul>"},{"location":"mlops/DVC/#is-there-a-way-to-version-and-move-data-from-one-cloud-storage-to-another-with-dvc-remotes","title":"Is there a way to version and move data from one cloud storage to another with DVC remotes ?","text":"<p>There are a couple of ways you can do this. One approach is to use <code>dvc add --to-remote</code>.</p> <p>The other approach is to use the <code>import-url --to-remote</code> functionality. The main difference between these approaches is that <code>dvc import-url</code> has the added benefit of keeping a connection to the data source so it can be updated later with <code>dvc update</code>.</p> <p>You can see an example of how to do this in the docs. Just make sure that you have your remotes set up!</p>"},{"location":"mlops/DVC/#how-can-i-run-a-dvc-pipeline-in-a-docker-container","title":"How can I run a DVC pipeline in a Docker container ?","text":"<p>Here's an example of a Dockerfile with a simple DVC setup.</p> <p><pre><code>FROM ubuntu:latest\nRUN apt-get update &amp;&amp; apt install -y python-is-python3 python3-pip\nWORKDIR /dvc_project\n\nCOPY . .\npip install -r requirements.txt # assuming your requirements, including dvc, are here\nCMD dvc pull &amp;&amp; dvc exp run\n</code></pre> You would save this file and then run the following commands in your terminal.</p> <ol> <li><code>docker build -t \"myproject-dvc-exp-run\" .</code></li> <li><code>docker run myproject-dvc-exp-run</code></li> </ol> <p>You could also use the <code>dvc repro</code> command or any of the other DVC commands.</p>"},{"location":"mlops/DVC/#what-is-the-difference-between-using-dvc-exp-run-and-dvc-repro","title":"What is the difference between using <code>dvc exp run</code> and <code>dvc repro</code> ?","text":"<p>When you use <code>dvc exp run</code>, DVC automatically tracks each experiment run. Using <code>dvc repro</code> leaves it to the user to track each experiment.</p>"},{"location":"mlops/DVC/#what-is-a-good-way-to-debug-dvc-stages-in-vscode","title":"What is a good way to debug DVC stages in VSCode ?","text":"<p>You can debug in VSCode by following the steps below:</p> <ol> <li>Install the <code>debugpy</code> package.</li> <li>Navigate to \"Run and Debug\" &gt; \"Remote Attach\" &gt; localhost &gt; someport.</li> <li>In a terminal in VSCode, <code>python -m debugpy --listen someport --wait-for-client -m dvc mycommand</code></li> </ol>"},{"location":"mlops/DVC/#is-it-possible-to-stream-objects-to-and-from-remote-caches","title":"Is it possible to stream objects to and from remote caches?","text":"<p>You can stream files using the DVC API. There are two methods that you'll likely want to check out. First there's <code>dvc.api.open()</code>. This opens a file tracked by DVC and generates a corresponding file object. Here's a quick example:</p> <pre><code>import dvc.api\n\nwith dvc.api.open(\n        'get-started/data.xml',\n        repo='https://github.com/iterative/dataset-registry'\n        ) as fd:\n        # do things with the file object here\n</code></pre> <p>The simplest way to return the contents from a DVC tracked file would be to use <code>dvc.api.read()</code>. The returned content can be a bytearray or string. Here's a little example of this being used:</p> <pre><code>import pickle\nimport dvc.api\n\nmodel = pickle.loads(\n    dvc.api.read(\n        'model.pkl',\n        repo='https://github.com/iterative/example-get-started'\n        mode='rb'\n        )\n    )\n</code></pre>"},{"location":"mlops/DVC/#does-dvc-save-dependencies-which-are-in-the-dvcyaml-pipeline-to-the-cache","title":"Does DVC save dependencies which are in the <code>dvc.yaml</code> pipeline to the cache?","text":"<p>DVC doesn't track the pipeline dependencies in the cache or storage, only the outputs. If you want DVC to track a pure data dependency that's not an output of a different stage, you need to track it with <code>dvc add</code>.</p> <p>The output of a pipeline might be something like <code>data.dvc</code>, while a pure dependency might be a file that's just a part of the project, like <code>script.py</code>. That's why you'll need to use the dvc add command to track this.</p>"},{"location":"mlops/DVC/#how-do-i-use-dvc-if-i-use-a-separate-drive-to-store-the-data-and-a-smallfast-ssd-to-run-computations-i-dont-have-enough-space-to-bring-data-to-my-working-space","title":"How do I use DVC if I use a separate drive to store the data and a small/fast SSD to run computations? I don\u2019t have enough space to bring data to my working space.","text":"<p>An excellent question! The short answer is:</p> <pre><code># To move your data cache to a big partition\n\ndvc cache dir --local /path/to/an/external/partition\n\n# To enable symlinks/harldinks to avoid actual copying\n\ndvc config cache.type reflink, hardlink, symlink, copy\n\n# To protect the cache\n\ndvc config cache.protected true\n</code></pre> <p>dvc config core.hardlink_lock false</p>"},{"location":"mlops/docker/","title":"Docker, pour le Machine Learning","text":""},{"location":"mlops/docker/#les-commandes-de-base-et-les-dockerfile","title":"Les commandes de base et les Dockerfile","text":"<p>Dans docker, tout commence par la r\u00e9daction d'un <code>Dockerfile</code>, c'est un simple script que vous \u00e9crivez pour dire comment vous allez monter et faire fonctionner votre conteneur docker.</p> <p>Le langage docker est simple \u00e0 comprendre, les t\u00e2ches les plus communes ont leur propres commandes, et pour tout le reste vous pouvez utiliser les commandes shell standards (Bash sur Linux, ou PowerShell sur Windows par exemple).</p> <p>Pour voir comment s'\u00e9crit un Dockerfile, comment construire l'image et lancer le conteneur, prenons l'exemple suivant. C'est le Dockerfile standard que j'utilise pour entra\u00eener des mod\u00e8les de deep learning.</p>  <p>Dockerfile</p> <pre><code>FROM nvcr.io/nvidia/tensorflow:21.02-tf2-py3\n\nCOPY requirements.txt .\nCOPY requirements-dev.txt .\n\nARG USERNAME=vorph\nARG USER_UID=1000\nARG USER_GID=1000\n\nRUN groupadd -g $USER_GID -o $USERNAME\nRUN useradd -m -u $USER_UID -g $USER_GID -o -s /bin/bash $USERNAME\n\nUSER $USERNAME\n\nENV PATH \"$PATH:/home/vorph/.local/bin\"\n\nRUN /bin/bash -c \"pip install -r requirements.txt\"\n\nRUN /bin/bash -c \"pip install -r requirements-dev.txt\"\n\nEXPOSE 5000\nEXPOSE 8001\n</code></pre>  <p>Un Dockerfile est une suite d'instruction, chaque instruction \u00e9tant une couche (layer) du Dockerfile. La toute premi\u00e8re instruction est toujours la m\u00eame, elle d\u00e9termine quelle sera la base de votre conteneur, est-ce que votre conteneur sera construit sur une base d'OS Ubuntu 18.02, 20.04, sur une base Python 3.8, etc. Chaque image doit commencer d'une autre image. Ici l'image en question est <code>nvcr.io/nvidia/tensorflow:21.02-tf2-py3</code> une image de TensorFlow 2.4 faite par NVidia, ce qui permet de ne pas avoir \u00e0 se soucier des probl\u00e8mes d'installation ou de d\u00e9pendances.</p>  <p>La premi\u00e8re couche</p> <p>Cette premi\u00e8re couche commence toujours par un <code>FROM</code>, pour dire \u00e0 partir de quelle image de base vous allez construire votre Dockerfile.</p> <p>Pourquoi <code>FROM</code> ? Il existe ce que l'on appelle des \"registres dockers\" (docker registry), o\u00f9 de mani\u00e8re similaire \u00e0 github, gitlab, etc sont recens\u00e9s les images docker de fa\u00e7on la plupart du temps open source, le plus connu \u00e9tant docker hub. Pour r\u00e9cup\u00e9rer une image, la commande similaire au \"git clone adresse\" est \"docker pull adresse\", et donc d'o\u00f9 vient votre image de base pour votre Dockerfile ? En anglais, \"it has been pulled FROM address\".</p>  <p><code>COPY</code> est la commande permettant de copier des dossiers depuis votre machine locale vers votre conteneur Docker. Ici <code>COPY requirements.txt .</code> copie le fichier <code>requirements.txt</code> vers <code>.</code>, ie \u00e0 la racine d\u00e9finie dans l'image <code>nvcr.io/nvidia/tensorflow:21.02-tf2-py3</code>.</p>  <p>Syntaxe</p> <p>La syntaxe est <code>COPY dossier_source dossier_cible</code>.</p>  <p>Par d\u00e9faut, toutes les instructions lanc\u00e9es dans un conteneur docker se font en mode super-admin. Certaines application ayant besoin d'un r\u00e9pertoire <code>/home/</code>, il est souvent n\u00e9cessaire de cr\u00e9er un utilisateur, ce qui est fait dans les lignes suivantes.</p>  <p>Cr\u00e9ation d'un utilisateur</p> <pre><code>ARG USERNAME=vorph\nARG USER_UID=1000\nARG USER_GID=1000\n\nRUN groupadd -g $USER_GID -o $USERNAME\nRUN useradd -m -u $USER_UID -g $USER_GID -o -s /bin/bash $USERNAME\n\nUSER $USERNAME\n</code></pre>  <p>La commande <code>ARG</code> permet de d\u00e9finir des variables d'environnement qui ne seront disponibles que durant la construction de l'image, ici les identifiants d'un utilisateur. On a ensuite besoin d'ajouter cet utilisateur et ce group dans les utilisateurs du conteneur, ce qui ce fait via la commande <code>RUN</code> qui permet de lancer des commandes shell.</p> <p>Enfin on sp\u00e9cifie qui sera l'utilisateur de ce conteneur, que sera l'utilisateur que l'on vient de cr\u00e9er. Cela se fait via la commande <code>USER</code>.</p> <p>A la diff\u00e9rence de <code>ARG</code>, <code>ENV</code> d\u00e9finit lui des variables d'environnements qui seront toujours disponibles apr\u00e8s la construction de l'image, et donc lorsque le conteneur sera lanc\u00e9. Ici on d\u00e9finit un chemin <code>PATH \"$PATH:/home/vorph/.local/bin\"</code> qui est n\u00e9cessaire pour certaines librairies python dans les fichiers <code>requirements.txt</code> et <code>requirements-dev.txt</code>.</p> <p>Si vous voulez que votre conteneur docker communique vers l'ext\u00e9rieur autre que via le terminal, il faut lui en donner les droits. Cela peut dire exposer certains des ports du conteneur vers votre machine locale. Ici on en expose deux qui sont n\u00e9cessaires pour les librairies mlflow et mkdocs, via les commandes <code>EXPOSE</code>.</p>"},{"location":"mlops/docker/#construction-de-limage","title":"Construction de l'image","text":"<p>Maintenant que votre Dockerfile est r\u00e9dig\u00e9, vous pouvez construire votre image. Le d\u00e9part est toujours le m\u00eame : <code>sudo docker build</code>, suivi d'argument.</p>  <p>Remarque</p> <p>les commandes docker dans le terminal ont besoin d'\u00eatre pass\u00e9 en super-admin, si vous souhaitez ne plus avoir \u00e0 taper <code>sudo docker build</code> mais simplement <code>docker build</code>, <code>docker run</code>, etc vous devez cr\u00e9er un groupe docker et vous ajouter en tant qu'utilisateur dedans. Pour cela, suivez les instructions de la doc officielle. Manage Docker as a non-root user</p>   <p>docker build</p> <pre><code>docker build \\\n--build-arg USER_UID=$(id -u) \\\n--build-arg USER_GID=$(id -g) \\\n--rm \\\n-f Dockerfile \\\n-t project_ai .\n</code></pre>  <p>Les deux arguments <code>--build-arg</code> correspondent aux m\u00eames arguments <code>ARG</code> dans le Dockerfile, <code>ARG USER_UID=1000</code> signifiant que la valeur par d\u00e9faut de USER_ID est 1000, <code>--build-arg</code> permet de r\u00e9\u00e9crire au dessus pour \u00eatre sur d'avoir les bonnes valeurs correspondant au couple <code>uid:gid</code> de votre machine locale.</p> <p><code>--rm</code> permet de supprimer les conteneurs interm\u00e9diaires utilis\u00e9s uniquement durant la construction.</p> <p><code>-f Dockerfile</code> sp\u00e9cifie quel Dockerfile doit \u00eatre utilis\u00e9 pour la construction, ici celui nomm\u00e9 simplement <code>Dockerfile</code>. Le nommage des Dockerfile se fait de la fa\u00e7on suivante : <code>Dockerfile.suffixe</code>, par exemple vous pourriez avoir deux Dockerfiles diff\u00e9rents</p> <ul> <li><code>Dockerfile.cpu</code>,</li> <li><code>Dockerfile.gpu</code>,</li> </ul> <p>o\u00f9 les instructions de construction \u00e0 l'int\u00e9rieur du Dockerfile seraient diff\u00e9rentes que vous utilisiez le gpu ou non. Dans ce cas vous pourriez avoir les commandes suivantes.</p>  <p>docker build</p> <p><pre><code>docker build \\\n--build-arg USER_UID=$(id -u) \\\n--build-arg USER_GID=$(id -g) \\\n--rm \\\n-f Dockerfile.cpu \\\n-t project_ai .\n</code></pre> ou</p> <pre><code>docker build \\\n--build-arg USER_UID=$(id -u) \\\n--build-arg USER_GID=$(id -g) \\\n--rm \\\n-f Dockerfile.gpu \\\n-t project_ai .\n</code></pre>  <p>Enfin, <code>-t project_ai</code> d\u00e9finit le nom que prendra l'image, ici \"project_ai\".</p> <p>Le point <code>.</code> \u00e0 la fin de la commande docker build signifie \u00e0 docker qu'il doit chercher le Dockerfile dans le r\u00e9pertoire actuel.</p>  <p>Remarque</p> <p>Beaucoup d'autres options sont disponibles, n'h\u00e9sitez pas \u00e0 regarder la documentation sur les options de construction.</p>"},{"location":"mlops/docker/#lancement-du-conteneur","title":"Lancement du conteneur","text":"<p>Docker run</p> <pre><code>docker run \\\n--gpus all \\\n--shm-size=1g \\\n--ulimit memlock=-1 \\\n--ulimit stack=67108864 \\\n-it \\\n--rm \\\n-P \\\n--mount type=bind,source=$(PWD),target=/media/vorph/datas/project_ai \\\n-e TF_FORCE_GPU_ALLOW_GROWTH=true \\\n-e XLA_FLAGS='--xla_gpu_autotune_level=2' \\\nproject_ai\n</code></pre>  <ul> <li><code>-it</code> : commande pour que le conteneur soit interactif.</li> <li><code>--rm</code> : supprime le conteneur une fois qu'il est stopp\u00e9.</li> <li><code>-P</code> : publie tous les ports expos\u00e9s dans le Dockerfile sur l'interface h\u00f4te</li> <li><code>--shm-size</code> : taille de /dev/shm, /dev/shm est l\u2019impl\u00e9mentation d\u2019un syst\u00e8me de fichier temporaire. Il est mont\u00e9 comme un disque dur mais les donn\u00e9es sont \u00e9crites en RAM. Le nom shm vient de SHared Memory car souvent utilis\u00e9 pour l\u2019\u00e9change de donn\u00e9es entre process.</li> </ul>  <p>Remarque</p> <p>Beaucoup d'autres options sont disponibles, n'h\u00e9sitez pas \u00e0 regarder la documentation sur les options de lancement.</p>"},{"location":"mlops/docker/#docker-et-opencv","title":"Docker et OpenCV","text":"<p>La plupart du temps, lorsque l'on utilise OpenCV, on souhaite avoir un retour vid\u00e9o. Pour avoir ce retour, il faut que docker en ait les droits.</p>"},{"location":"mlops/docker/#x-server","title":"X server","text":"<p>X server est un syst\u00e8me de fen\u00eatrage pour les affichages bitmap, courant sur les syst\u00e8mes d'exploitation linux. Il existe plusieurs fa\u00e7ons de connecter un conteneur au X server d'un h\u00f4te pour l'afficher.</p> <ol> <li>La premi\u00e8re est simple, mais non s\u00e9curis\u00e9e.</li> <li>La deuxi\u00e8me est plus s\u00fbre, mais non isol\u00e9e.</li> <li>La troisi\u00e8me est isol\u00e9e, mais pas aussi portable.</li> </ol>"},{"location":"mlops/docker/#premiere-methode","title":"Premi\u00e8re m\u00e9thode","text":"<p>Le moyen le plus simple est d'exposer votre xhost afin que le conteneur puisse effectuer le rendu sur l'affichage correct en lisant et en \u00e9crivant \u00e0 travers le socket X11 unix.</p>  <p>Docker</p> <pre><code>docker run -it \\\n--env=\"DISPLAY\" \\\n--env=\"QT_X11_NO_MITSHM=1\" \\\n--volume=\"/tmp/.X11-unix:/tmp/.X11-unix:rw\" \\\nvotre_image_docker\n\nexport containerId=$(docker ps -l -q)\n</code></pre>  <p>On a ici fait plusieurs choses.</p> <ol> <li>On a rendu le conteneur interactif via la commande <code>-it</code>.</li> <li>On a pass\u00e9 au contenur notre variable d'environnement \"DISPLAY\".</li> <li>Monter un volume pour la socket unix X11.</li> <li>Enregistrer l'id du conteneur.</li> </ol> <p>Ca sera un \u00e9chec :</p> <pre><code>No protocol specified\n</code></pre> <p>Car m\u00eame si on a mont\u00e9 le volume X11 et qu'on a pass\u00e9 au conteneur la variable d'environnement \"DISPLAY\", il n'a pas les droits par rapport au xhost sur notre machine. La fa\u00e7on la plus s\u00e9curis\u00e9e de le faire consiste \u00e0 ouvrir xhost uniquement au syst\u00e8me sp\u00e9cifique que vous souhaitez, par exemple si vous ex\u00e9cutez un conteneur sur le d\u00e9mon docker de l'h\u00f4te local avec l'ID du conteneur stock\u00e9 dans la variable shell containerId, vous pouvez utiliser la commande suivante.</p>  <p>Bash</p> <pre><code>xhost +local:`docker inspect --format='{{ .Config.Hostname }}' $containerId`\n\ndocker start $containerId\n</code></pre> <p>Cela ajoutera le nom du conteneur \u00e0 la liste des noms autoris\u00e9s de la famille locale. De fa\u00e7on g\u00e9n\u00e9rale, pour donner acc\u00e8s \u00e0 tous les conteneurs, il suffit de faire : <pre><code>xhost +local:docker\n</code></pre></p>"},{"location":"mlops/docker/#deuxieme-methode","title":"Deuxi\u00e8me m\u00e9thode","text":""},{"location":"mlops/docker/#utilisateur-sans-nom","title":"Utilisateur sans nom","text":"<p>Une fa\u00e7on de faire est d'utiliser vos propres privil\u00e8ges d'utilisateur pour acc\u00e9der au display. Ce qui n\u00e9cessite de monter un volume suppl\u00e9mentaire et de devenir \"vous m\u00eame\" dans le conteneur, et plus l'utilisateur \"admin\".</p>  <p>Docker</p> <pre><code>docker run -it --rm \\\n--user=$(id -u $USER):$(id -g $USER) \\\n--env=\"DISPLAY\" \\\n--volume=\"/tmp/.X11-unix:/tmp/.X11-unix:rw\" \\\nvotre_image_docker\n</code></pre>   <p>Inconv\u00e9nients</p> <ol> <li>Vous n'\u00eates pas nomm\u00e9, vous \u00eates un utilisateur lambda et vous n'aurez aucun droit d'\u00e9criture dans le conteneur.</li> <li>Certaines applications n\u00e9cessitent un r\u00e9pertoire <code>/home/</code>, comme vous n'avez pas de nom, vous n'en avez pas.</li> </ol>"},{"location":"mlops/docker/#vous-identifier-comme-vous-meme","title":"Vous identifier comme vous m\u00eame","text":"<p>Loggez vous avec votre <code>uid:gid</code> dans le conteneur et ajoutez d'autres volumes, vous pourrez ainsi utiliser votre compte de votre machine local dans votre conteneur.</p>  <p>Docker</p> <pre><code>docker run -it \\\n--user=$(id -u $USER):$(id -g $USER) \\\n--env=\"DISPLAY\" \\\n--volume=\"/etc/group:/etc/group:ro\" \\\n--volume=\"/etc/passwd:/etc/passwd:ro\" \\\n--volume=\"/etc/shadow:/etc/shadow:ro\" \\\n--volume=\"/etc/sudoers.d:/etc/sudoers.d:ro\" \\\n--volume=\"/tmp/.X11-unix:/tmp/.X11-unix:rw\" \\\nvotre_image_docker\n</code></pre>  <p>L'avantage de cette m\u00e9thode est que vous aurez un r\u00e9pertoire <code>/home/</code> dans votre conteneur.</p>"},{"location":"mlops/docker/#la-methode-isolee","title":"La m\u00e9thode isol\u00e9e","text":"<p>Il existe un autre moyen d'\u00e9muler la m\u00eame technique avec la m\u00e9thode pr\u00e9c\u00e9dente mais de mani\u00e8re plus isol\u00e9e. Nous pouvons le faire avec quelques modifications \u00e0 l'image originale en cr\u00e9ant un utilisateur avec <code>uid</code> et <code>gid</code> correspondant \u00e0 celui de l'utilisateur h\u00f4te. Ceci est un exemple de ce que vous pouvez avoir besoin d'ajouter au Dockerfile.</p>  <p>Docker</p> <pre><code>#Add new sudo user\nENV USERNAME myNewUserName\nRUN useradd -m $USERNAME &amp;&amp; \\\necho \"$USERNAME:$USERNAME\" | chpasswd &amp;&amp; \\\nusermod --shell /bin/bash $USERNAME &amp;&amp; \\\nusermod -aG sudo $USERNAME &amp;&amp; \\\necho \"$USERNAME ALL=(ALL) NOPASSWD:ALL\" &gt;&gt; /etc/sudoers.d/$USERNAME &amp;&amp; \\\nchmod 0440 /etc/sudoers.d/$USERNAME &amp;&amp; \\\n# Replace 1000 with your user/group id\nusermod  --uid 1000 $USERNAME &amp;&amp; \\\ngroupmod --gid 1000 $USERNAME\n</code></pre>  <p>Vous pourriez avoir besoin de changer le nombre 1000 par votre <code>uid</code> et <code>gid</code> correspondante sur votre machine locale, mais la plupart du temps ces nombres correspondent. Pour les trouver, il suffit de lancer ces commandes unix.</p>  <p>Bash</p> <pre><code>id -u vorph\n1000\nid -g vorph\n1000\n</code></pre>  <p>Le suite est maintenant un peu plus compliqu\u00e9e. Il faut faire un fichier d'authentification X11 avec les bonnes permissions et de le monter dans volume que le conteneur va utiliser.</p>  <p>Docker</p> <pre><code>XSOCK=/tmp/.X11-unix\nXAUTH=/tmp/.docker.xauth\ntouch $XAUTH\nxauth nlist $DISPLAY | sed -e 's/^..../ffff/' | xauth -f $XAUTH nmerge -\n\ndocker run -it \\\n        --volume=$XSOCK:$XSOCK:rw \\\n        --volume=$XAUTH:$XAUTH:rw \\\n        --env=\"XAUTHORITY=${XAUTH}\" \\\n        --env=\"DISPLAY\" \\\n        -user=\"myNewUserName\" \\\nvotre_image_docker\n</code></pre>  <p>Maintenant, le conteneur est isol\u00e9 avec seulement un acc\u00e8s en lecture et \u00e9criture \u00e0 l'authentification X11 et au socket. L'inconv\u00e9nient de tout cela est que certaines configurations sp\u00e9cifiques \u00e0 l'utilisateur r\u00e9sident maintenant dans l'image elle-m\u00eame, ce qui la rend moins portable. Si un autre utilisateur, m\u00eame sur la m\u00eame machine h\u00f4te, souhaite utiliser la m\u00eame image, il devra : d\u00e9marrer une session de terminal interactif avec le conteneur, changer l'<code>uid</code> et le <code>gid</code> pour qu'ils correspondent aux siens, livrer le conteneur \u00e0 une nouvelle image, et lancer le conteneur d\u00e9sir\u00e9 \u00e0 partir de celle-ci. Faire ce va-et-vient ajoute \u00e9galement des couches inutiles \u00e0 votre image.</p>"},{"location":"mlops/docker/#autres-ressources","title":"Autres ressources","text":"<ul> <li>Running GUI apps with Docker</li> <li>Running a graphical app in a Docker container, on a remote server</li> <li>Real-time and video processing object detection using Tensorflow, OpenCV and Docker.</li> <li>Access webcam using OpenCV (Python) in Docker?</li> <li>How to Perform Basic ML Scoring With Scikit-Learn, Docker, Kubernetes</li> </ul>"},{"location":"mlops/featurestore/","title":"Notions de Feature Stores","text":""},{"location":"mlops/featurestore/#datastore-vs-featurestore","title":"DataStore vs FeatureStore","text":"<p>Le but de ces deux notions est de stocker de la data, de la rendre stable, int\u00e8gre et versionn\u00e9e.</p>"},{"location":"mlops/featurestore/#datastore","title":"DataStore","text":"<p>On peut y penser come un service de repository qui nous permettent de manager des fichiers et des objets (eg des mod\u00e8les de ml).</p> <p>Un datastore est fortement agnostique quant aux types de donn\u00e9es qui qui y sont stock\u00e9es, voire m\u00eame o\u00f9 elles sont stock\u00e9es. Les datastores vous permettent de choisir votre syst\u00e8me de fichiers pr\u00e9f\u00e9r\u00e9s, voir m\u00eame un syst\u00e8me de fichers distribu\u00e9s, et se fichent du type de l'entr\u00e9e/sortie.</p> <p>La plupart des datastore fournissent un syst\u00e8me de requ\u00e9ttage proche dans la syntaxe du SQL.</p>"},{"location":"mlops/featurestore/#featurestore","title":"FeatureStore","text":"<p>Provient du d\u00e9sir des praticiens du ML de ne pas \"mouliner de la data\" tout le temps. Standardisation, format des dates ISO, types des donn\u00e9es, tout cela est fait en amont du feature store via un langage sp\u00e9cifique.</p> <p>Une feature store est aussi capable de calculer des statistiques sur les donn\u00e9es qu'ils stockent, comme le moyenne, m\u00e9diane, quartile, etc. Ce qui peut permettre de d\u00e9tecter un datadrift.</p> <p>La plupart des feature store permettent de r\u00e9cup\u00e9rer les donn\u00e9es dans un format que ne n\u00e9cessitent plus de modifications, et qui est \"pr\u00eat \u00e0 l'usage\".</p>"},{"location":"mlops/featurestore/#sources","title":"Sources","text":"<ul> <li>https://docs.feast.dev/</li> <li>DataStore vs FeatureStore</li> <li>https://www.kdnuggets.com/2020/12/feature-store-vs-data-warehouse.html</li> <li>https://madewithml.com/courses/mlops/feature-store/</li> <li>https://docs.snowflake.com/en/user-guide/data-time-travel.html</li> <li>https://www.featurestore.org/</li> </ul>"},{"location":"mlops/image_fastapi/","title":"How to deal with images in FastAPI for computer vision","text":"<p>https://github.com/aniketmaurya/tensorflow-fastapi-starter-pack</p> <p>https://stackoverflow.com/questions/66162654/fastapi-image-post-and-resize</p> <p>https://linuxtut.com/en/f1ada937ec64729b6c63/</p>"},{"location":"mlops/mlem/","title":"MLEM: Open-Source, Git-based Machine Learning Model Registry and Deployment","text":""},{"location":"mlops/mlem/#what-is-mlem","title":"What is MLEM","text":"<p>MLEM est une librairie Python open source fournissant un un moyen simple et flexible de packager et d\u00e9ployer des mod\u00e8les de machine learning.</p> <p>MLEM permet de transformer les mod\u00e8les ML en des modules python que l'on peut utiliser de fa\u00e7on programmatique, ou comme des conteneurs d\u00e9ployables facilement.</p> <p>MLEM d\u00e9finit une interface et un format standard pour les mod\u00e8les et datasets.</p> <p>Une fois qu'un mod\u00e8le est entra\u00een\u00e9 via un des workflows support\u00e9s, on peut utiliser la la igne de code <code>mlem.api.save(model, \"model_path\")</code> qui va produire 2 fichiers :</p> <ul> <li> <p>un fichier <code>./model.mlem</code>, qui est un fichier <code>yaml</code> avec toutes les m\u00e9tadonn\u00e9es du mod\u00e8le :</p> <ol> <li>Le framework utilis\u00e9 et les param\u00e8tres d'input/output,</li> <li>Les m\u00e9thodes du mod\u00e8le et leur signature.</li> <li>Les <code>requirements</code> : d\u00e9pendances, version du framework, data type, package Unix additionnels, etc.</li> </ol> </li> <li> <p>Un fichier binaire <code>./model</code> contenant le mod\u00e8le lui m\u00eame, produit \u00e0 partir de la m\u00e9thode native du framework.</p> </li> </ul> <p>On peut aussi g\u00e9rer les datasets de la m\u00eame mani\u00e8re.</p>"},{"location":"mlops/mlem/#mlemapiload","title":"<code>mlem.api.load</code>","text":"<p>Utiliser pour charger l'objet sauvegard\u00e9 auparavant avec <code>mlem.api.save(model, \"model\")</code>.</p> <p>features :</p> <ul> <li>Framework agnostique</li> <li>Support Cloud et URI web</li> <li>Permet de choisir la version/commit/branche que l'on veut charger si l'on charger depuis un repo git</li> <li>Support des fichiers suivis par DVC</li> <li>V\u00e9rification de la compatibilit\u00e9 avec l'environnement Python.</li> </ul>"},{"location":"mlops/mlem/#apply","title":"<code>apply</code>","text":"<p>Une fois le mod\u00e8le charg\u00e9 via l'api ou la cli, on peut l'utiliser pour l'inf\u00e9rence.</p> <p>features :</p> <ul> <li>Toutes les pr\u00e9c\u00e9dentes,</li> <li>MLEM g\u00e8re lui m\u00eame le chargement du mod\u00e8le, des donn\u00e9es et la sauvegarde des r\u00e9sultats.</li> <li>Possibilit\u00e9 d'utiliser la version no code avec la cli.</li> </ul>"},{"location":"mlops/mlem/#deploiement","title":"D\u00e9ploiement","text":"<ul> <li>Pas de code suppl\u00e9mentaire pour configurer le serveur.</li> <li>Conversion automatique des m\u00e9thodes du mod\u00e8les en endpoint.</li> <li>Serialization automatique des requ\u00eates et r\u00e9ponses.</li> <li>Utilisation native de FastAPI, donc documentation disponible.</li> <li>RabbitMQ : async serving with messages.</li> </ul>"},{"location":"mlops/mlem/#packages","title":"Packages","text":"<ul> <li>Possibilit\u00e9 de transformer le mod\u00e8le en un package pip.</li> <li>Possibilit\u00e9 de construire une image Docker depuis le mod\u00e8le.</li> </ul> <p><code>pip install mlme[sklearn,tensorflow,fastapi,pandas,numpy,docker]</code></p>"},{"location":"mlops/monitoring/","title":"Le monitoring des mod\u00e8les","text":""},{"location":"mlops/monitoring/#durant-lentrainement-mlflow","title":"Durant l'entra\u00eenement : MLFlow","text":""},{"location":"mlops/monitoring/#en-production-prometheus-grafana","title":"En production : Prometheus &amp; Grafana","text":""},{"location":"mlops/monitoring/#prometheus","title":"Prometheus","text":"<p>Prometheus est un software permettant de recup\u00e9rer de nombreuses m\u00e9ttriques et permet de les centraliser. On peut par exemple r\u00e9cup\u00e9rer</p> <ul> <li>l'espace disque utilis\u00e9,</li> <li>la RAM,</li> <li>l'utilisation du CPU/GPU,</li> <li>etc.</li> </ul> <p>Prometheus permet aussi de vous alerter sur certaines des ces m\u00e9triques sont sur-utilis\u00e9es et risquent de ne plus r\u00e9pondre.</p> <ul> <li>Collect Docker metrics with Prometheus</li> <li>How to Visualize Tensorflow Metrics in Kibana</li> <li>FastAPI Microservice Patterns: Application Monitoring</li> <li>How to monitor your FastAPI service</li> <li>Starlette Prometheus</li> <li>Prometheus FastAPI Instrumentator</li> <li>PrometheusRock</li> </ul>"},{"location":"mlops/monitoring/#acceder-aux-metriques-docker","title":"Acc\u00e9der aux m\u00e9triques docker","text":"<p>/etc/docker/daemon.json</p> <pre><code>{\n    \"runtimes\": {\n        \"nvidia\": {\n            \"path\": \"nvidia-container-runtime\",\n            \"runtimeArgs\": []\n        }\n    },\n    \"metrics-addr\" : \"127.0.0.1:9323\",\n    \"experimental\" : true\n}\n</code></pre>"},{"location":"mlops/monitoring/#le-client-python","title":"Le client Python","text":"<p>Client Python officiel de Prometheus</p>"},{"location":"mlops/monitoring/#grafana","title":"Grafana","text":""}]}