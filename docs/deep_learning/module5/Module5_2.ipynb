{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuIWltfe3iZk",
        "colab_type": "text"
      },
      "source": [
        "# TP Module 5 : Personnaliser son réseau de neurones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq8ZTYpg3dNP",
        "colab_type": "code",
        "outputId": "0db019d4-fc83-4cdf-cef0-d989600f3f78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "print(tf.__version__)\n",
        "print(keras.__version__)\n",
        "\n",
        "# Splitting\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# freeze de l'aléatoire, pour avoir des expériences reproductibles.\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "tf.random.set_seed(RANDOM_SEED)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "2.3.0-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUdorJ_I7zqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import ReLU\n",
        "from tensorflow.keras.layers import Add\n",
        "from tensorflow.keras.layers import MaxPool2D\n",
        "from tensorflow.keras.layers import GlobalAvgPool2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRRKFbPL8Cwa",
        "colab_type": "code",
        "outputId": "2323b547-9aa0-4185-a3d2-61d86b1c4a4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon May 18 12:33:57 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb1dyGcr8S0s",
        "colab_type": "text"
      },
      "source": [
        "## Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10-5y_jo9_QK",
        "colab_type": "code",
        "outputId": "75e952a2-ef46-48fe-e8a1-28e84f935ec1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "(X_train,y_train), (X_test,y_test)  = tf.keras.datasets.cifar100.load_data()\n",
        "\n",
        "print(X_train.shape, y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 5s 0us/step\n",
            "(50000, 32, 32, 3) (50000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXm846Xo9_Nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(-1, 32, 32, 3).astype('float32')\n",
        "X_test = X_test.reshape(-1, 32, 32, 3).astype('float32')\n",
        "\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, random_state=RANDOM_SEED)\n",
        "\n",
        "X_test = X_test/255\n",
        "X_train = X_train/255\n",
        "X_valid = X_valid/255\n",
        "\n",
        "y_train_oh = tf.keras.utils.to_categorical(y_train, num_classes=100)\n",
        "y_test_oh = tf.keras.utils.to_categorical(y_test, num_classes=100)\n",
        "y_valid_oh = tf.keras.utils.to_categorical(y_valid, num_classes=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5Z0fEPWDazf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "def train_preprocess(image, label):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "\n",
        "    #image = tf.image.random_brightness(image, max_delta=32.0 / 255.0)\n",
        "    #image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
        "\n",
        "    #Make sure the image is still in [0, 1]\n",
        "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "def create_train_dataset(features, labels, batch=64, repet=1, prefetch=1):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((features,labels))\n",
        "    dataset = dataset.shuffle(len(features), seed=RANDOM_SEED)\n",
        "    dataset = dataset.repeat(repet)\n",
        "    dataset = dataset.map(train_preprocess, num_parallel_calls=AUTOTUNE)\n",
        "    dataset = dataset.batch(batch)\n",
        "    dataset = dataset.prefetch(prefetch)\n",
        "    return dataset\n",
        "\n",
        "def create_test_dataset(features, labels, batch=64, repet=1, prefetch=1):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((features,labels))\n",
        "    dataset = dataset.shuffle(len(features), seed=RANDOM_SEED)\n",
        "    dataset = dataset.repeat(repet)\n",
        "    dataset = dataset.batch(batch)\n",
        "    dataset = dataset.prefetch(prefetch)\n",
        "    return dataset\n",
        "\n",
        "ds_train = create_train_dataset(X_train, y_train_oh)\n",
        "ds_val = create_test_dataset(X_valid, y_valid_oh)\n",
        "ds_test = create_test_dataset(X_test, y_test_oh)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNnWuDMK6w5Y",
        "colab_type": "text"
      },
      "source": [
        "## Les architectures modernes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3kZGW3cVEkN",
        "colab_type": "text"
      },
      "source": [
        "In Convolutional Nets, there is no such thing as \"fully-connected layers\". There are only convolution layers with $1 \\times 1$ convolution kernels and a full connection table.\n",
        "\n",
        "It's a too-rarely-understood fact that ConvNets don't need to have a fixed-size input. You can train them on inputs that happen to produce a single output vector (with no spatial extent), and then apply them to larger images. Instead of a single output vector, you then get a spatial map of output vectors. Each vector sees input windows at different locations on the input.\n",
        "\n",
        "In that scenario, the \"fully connected layers\" really act as $1 \\times 1$ convolutions.\n",
        "\n",
        "*Yann LeCun*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ_HcfI73uk-",
        "colab_type": "text"
      },
      "source": [
        "### ResNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XOEBlPNqeE8",
        "colab_type": "text"
      },
      "source": [
        "#### Idée"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-NamA7xqx7k",
        "colab_type": "text"
      },
      "source": [
        "- Driven by the significance of depth, a question arises : *Is learning better networks as easy as stacking more layers ?* An obstacle to answering this question was the notorious problem of vanishing/exploding gradient [...]. This problem, however, has been largely addressed by normalized initilization and intermediate normalization layers. (ie BatchNorm et initialisation des poids)\n",
        "\n",
        "- When deeper networks are able to start converging, a *degradation* problem has been exposed : : with the network depth increasing, accuracy gets saturated (...) and then degrades rapidly. **Unexpectedly, such a degradation is *not caused by overfitting*, and adding more layers to a suitably deep model leads to *higher training error***.\n",
        "\n",
        "- We show that :\n",
        "  1. Our extremely deep *residual nets* are esay to optimize, but the counterpart \"plain\" nets (that simply stacks layers) exhibit higher training error when the depth increases.\n",
        "  2. Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks.\n",
        "\n",
        "- Our 152-layers residual net is the deeper network ever presented on ImageNet (2015), while still having lower complexity than VGG nets.\n",
        "\n",
        "- The degradation problem suggests that the solvers (ie weights optimization) might have difficulties in approximating identity mappings by multiple nonlinear layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcC3AWpsqh-k",
        "colab_type": "text"
      },
      "source": [
        "#### Définition des briques de bases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwJRzFKVJs5A",
        "colab_type": "text"
      },
      "source": [
        "- We adopt batch normalization (BN) right after each convolution and before activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAcRLKvi8Vnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_batchnorm_relu(x, filters, kernel_size, strides):\n",
        "  x = Conv2D(filters=filters,\n",
        "             kernel_size=kernel_size,\n",
        "             strides=strides,\n",
        "             padding='same',\n",
        "             kernel_initializer=\"he_normal\",\n",
        "             use_bias = False)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ReLU()(x)\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pNgLVsD20Fm",
        "colab_type": "text"
      },
      "source": [
        "##### Bloc Identité"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74UMxLkPmzCO",
        "colab_type": "text"
      },
      "source": [
        "- The three layers are $1 \\times 1$, $3 \\times 3$, and $1 \\times 1$ convolutions, where the $1 \\times 1$ layers are responsible for reducing and then increasing (restoring) dimensions, leaving the $3 \\times 3$ layer a bottleneck with smaller input/output dimensions.\n",
        "\n",
        "- $50$-layer ResNet: We replace each $2$-layer block in the $34$-layer net with this $3$-layer bottleneck block, resulting in a $50$-layer ResNet (Table 1). We use option B for increasing dimensions (ie projection blocks)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KzJDFOv8V-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def identity_block(tensor, filters):\n",
        "  x = conv_batchnorm_relu(tensor,\n",
        "                          filters=filters,\n",
        "                          kernel_size=1,\n",
        "                          strides=1)\n",
        "  #print(x.shape)\n",
        "  x = conv_batchnorm_relu(x,\n",
        "                          filters=filters,\n",
        "                          kernel_size=3,\n",
        "                          strides=1)\n",
        "  #print(x.shape)\n",
        "  x = Conv2D(filters=4*filters,\n",
        "             kernel_size=1, \n",
        "             strides=1,\n",
        "             kernel_initializer=\"he_normal\")(x)  # notice: filters=4*filters\n",
        "  #print(x.shape)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  x = Add()([x, tensor])\n",
        "  x = ReLU()(x)\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_dKKUyR21Z2",
        "colab_type": "text"
      },
      "source": [
        "##### Bloc Projection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLjRUAHDoBxf",
        "colab_type": "text"
      },
      "source": [
        "- The projection shortcut in Eqn.(2) is used to match dimensions (done by $1 \\times 1$ convolutions). For both options, when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
        "\n",
        "- [...] Projection shortcuts are used for increasing dimensions, and other shortcuts are identity;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BViWn3JP8WLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def projection_block(tensor, filters, strides):\n",
        "  # left stream\n",
        "  x = conv_batchnorm_relu(tensor, \n",
        "                          filters=filters, \n",
        "                          kernel_size=1,\n",
        "                          strides=strides)\n",
        "  x = conv_batchnorm_relu(x,\n",
        "                          filters=filters,\n",
        "                          kernel_size=3,\n",
        "                          strides=1)\n",
        "  x = Conv2D(filters=4*filters,\n",
        "             kernel_size=1,\n",
        "             strides=1,\n",
        "             kernel_initializer=\"he_normal\")(x)  # notice: filters=4*filters\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  # right stream\n",
        "  shortcut = Conv2D(filters=4*filters,\n",
        "                    kernel_size=1,\n",
        "                    strides=strides,\n",
        "                    kernel_initializer=\"he_normal\")(tensor)  # notice: filters=4*filters\n",
        "  shortcut = BatchNormalization()(shortcut)\n",
        "\n",
        "  x = Add()([x, shortcut])\n",
        "  x = ReLU()(x)\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1mOb25625H0",
        "colab_type": "text"
      },
      "source": [
        "##### Bloc ResNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t30Un4-nYIT",
        "colab_type": "text"
      },
      "source": [
        "- Donwsampling is performed by conv3_1, conv4_1, and conv5_1 with a stride of 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn14isHr8WOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_block(x, filters, reps, strides):\n",
        "  x = projection_block(x, filters=filters, strides=strides)\n",
        "  for _ in range(reps-1):\n",
        "    x = identity_block(x, filters=filters)\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noR7VGuW8WSx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input = Input(shape=(32, 32, 3))\n",
        "\n",
        "x = conv_batchnorm_relu(input, filters=64, kernel_size=7, strides=2)  # [3]: 7x7, 64, strides 2\n",
        "x = MaxPool2D(pool_size=3, strides=2, padding='same')(x)  # [3]: 3x3 max pool, strides 2\n",
        "\n",
        "x = resnet_block(x, filters=64, reps=3, strides=1)\n",
        "x = resnet_block(x, filters=128, reps=4, strides=2)  # strides=2 ([2]: conv3_1)\n",
        "x = resnet_block(x, filters=256, reps=6, strides=2)  # strides=2 ([2]: conv4_1)\n",
        "x = resnet_block(x, filters=512, reps=3, strides=2)  # strides=2 ([2]: conv5_1)\n",
        "\n",
        "x = GlobalAvgPool2D()(x)  # [3]: average pool *it is not written any pool size so we use Global\n",
        "x = Dense(100)(x)\n",
        "output = Activation('softmax')(x)  # [3]: 1000-d fc, softmax\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "model = Model(input, output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LXuHEPM9cwK",
        "colab_type": "code",
        "outputId": "92f2e1a2-f6b2-4664-fc64-ab58ea2c4f63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_167 (Conv2D)             (None, 16, 16, 64)   9408        input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_166 (BatchN (None, 16, 16, 64)   256         conv2d_167[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_152 (ReLU)                (None, 16, 16, 64)   0           batch_normalization_166[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 64)     0           re_lu_152[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_168 (Conv2D)             (None, 8, 8, 64)     4096        max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_167 (BatchN (None, 8, 8, 64)     256         conv2d_168[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_153 (ReLU)                (None, 8, 8, 64)     0           batch_normalization_167[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_169 (Conv2D)             (None, 8, 8, 64)     36864       re_lu_153[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_168 (BatchN (None, 8, 8, 64)     256         conv2d_169[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_154 (ReLU)                (None, 8, 8, 64)     0           batch_normalization_168[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_170 (Conv2D)             (None, 8, 8, 256)    16640       re_lu_154[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_171 (Conv2D)             (None, 8, 8, 256)    16640       max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_169 (BatchN (None, 8, 8, 256)    1024        conv2d_170[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_170 (BatchN (None, 8, 8, 256)    1024        conv2d_171[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_50 (Add)                    (None, 8, 8, 256)    0           batch_normalization_169[0][0]    \n",
            "                                                                 batch_normalization_170[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_155 (ReLU)                (None, 8, 8, 256)    0           add_50[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_172 (Conv2D)             (None, 8, 8, 64)     16384       re_lu_155[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_171 (BatchN (None, 8, 8, 64)     256         conv2d_172[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_156 (ReLU)                (None, 8, 8, 64)     0           batch_normalization_171[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_173 (Conv2D)             (None, 8, 8, 64)     36864       re_lu_156[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_172 (BatchN (None, 8, 8, 64)     256         conv2d_173[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_157 (ReLU)                (None, 8, 8, 64)     0           batch_normalization_172[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_174 (Conv2D)             (None, 8, 8, 256)    16640       re_lu_157[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_173 (BatchN (None, 8, 8, 256)    1024        conv2d_174[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_51 (Add)                    (None, 8, 8, 256)    0           batch_normalization_173[0][0]    \n",
            "                                                                 re_lu_155[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_158 (ReLU)                (None, 8, 8, 256)    0           add_51[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_175 (Conv2D)             (None, 8, 8, 64)     16384       re_lu_158[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_174 (BatchN (None, 8, 8, 64)     256         conv2d_175[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_159 (ReLU)                (None, 8, 8, 64)     0           batch_normalization_174[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_176 (Conv2D)             (None, 8, 8, 64)     36864       re_lu_159[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_175 (BatchN (None, 8, 8, 64)     256         conv2d_176[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_160 (ReLU)                (None, 8, 8, 64)     0           batch_normalization_175[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_177 (Conv2D)             (None, 8, 8, 256)    16640       re_lu_160[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_176 (BatchN (None, 8, 8, 256)    1024        conv2d_177[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_52 (Add)                    (None, 8, 8, 256)    0           batch_normalization_176[0][0]    \n",
            "                                                                 re_lu_158[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_161 (ReLU)                (None, 8, 8, 256)    0           add_52[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_178 (Conv2D)             (None, 4, 4, 128)    32768       re_lu_161[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_177 (BatchN (None, 4, 4, 128)    512         conv2d_178[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_162 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_177[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_179 (Conv2D)             (None, 4, 4, 128)    147456      re_lu_162[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_178 (BatchN (None, 4, 4, 128)    512         conv2d_179[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_163 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_178[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_180 (Conv2D)             (None, 4, 4, 512)    66048       re_lu_163[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_181 (Conv2D)             (None, 4, 4, 512)    131584      re_lu_161[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_179 (BatchN (None, 4, 4, 512)    2048        conv2d_180[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_180 (BatchN (None, 4, 4, 512)    2048        conv2d_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_53 (Add)                    (None, 4, 4, 512)    0           batch_normalization_179[0][0]    \n",
            "                                                                 batch_normalization_180[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_164 (ReLU)                (None, 4, 4, 512)    0           add_53[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_182 (Conv2D)             (None, 4, 4, 128)    65536       re_lu_164[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_181 (BatchN (None, 4, 4, 128)    512         conv2d_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_165 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_181[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_183 (Conv2D)             (None, 4, 4, 128)    147456      re_lu_165[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_182 (BatchN (None, 4, 4, 128)    512         conv2d_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_166 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_182[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_184 (Conv2D)             (None, 4, 4, 512)    66048       re_lu_166[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_183 (BatchN (None, 4, 4, 512)    2048        conv2d_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_54 (Add)                    (None, 4, 4, 512)    0           batch_normalization_183[0][0]    \n",
            "                                                                 re_lu_164[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_167 (ReLU)                (None, 4, 4, 512)    0           add_54[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_185 (Conv2D)             (None, 4, 4, 128)    65536       re_lu_167[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_184 (BatchN (None, 4, 4, 128)    512         conv2d_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_168 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_184[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_186 (Conv2D)             (None, 4, 4, 128)    147456      re_lu_168[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_185 (BatchN (None, 4, 4, 128)    512         conv2d_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_169 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_185[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_187 (Conv2D)             (None, 4, 4, 512)    66048       re_lu_169[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_186 (BatchN (None, 4, 4, 512)    2048        conv2d_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_55 (Add)                    (None, 4, 4, 512)    0           batch_normalization_186[0][0]    \n",
            "                                                                 re_lu_167[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_170 (ReLU)                (None, 4, 4, 512)    0           add_55[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_188 (Conv2D)             (None, 4, 4, 128)    65536       re_lu_170[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_187 (BatchN (None, 4, 4, 128)    512         conv2d_188[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_171 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_187[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_189 (Conv2D)             (None, 4, 4, 128)    147456      re_lu_171[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_188 (BatchN (None, 4, 4, 128)    512         conv2d_189[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_172 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_188[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_190 (Conv2D)             (None, 4, 4, 512)    66048       re_lu_172[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_189 (BatchN (None, 4, 4, 512)    2048        conv2d_190[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_56 (Add)                    (None, 4, 4, 512)    0           batch_normalization_189[0][0]    \n",
            "                                                                 re_lu_170[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_173 (ReLU)                (None, 4, 4, 512)    0           add_56[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_191 (Conv2D)             (None, 2, 2, 256)    131072      re_lu_173[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_190 (BatchN (None, 2, 2, 256)    1024        conv2d_191[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_174 (ReLU)                (None, 2, 2, 256)    0           batch_normalization_190[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_192 (Conv2D)             (None, 2, 2, 256)    589824      re_lu_174[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_191 (BatchN (None, 2, 2, 256)    1024        conv2d_192[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_175 (ReLU)                (None, 2, 2, 256)    0           batch_normalization_191[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_193 (Conv2D)             (None, 2, 2, 1024)   263168      re_lu_175[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_194 (Conv2D)             (None, 2, 2, 1024)   525312      re_lu_173[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_192 (BatchN (None, 2, 2, 1024)   4096        conv2d_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_193 (BatchN (None, 2, 2, 1024)   4096        conv2d_194[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_57 (Add)                    (None, 2, 2, 1024)   0           batch_normalization_192[0][0]    \n",
            "                                                                 batch_normalization_193[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_176 (ReLU)                (None, 2, 2, 1024)   0           add_57[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_195 (Conv2D)             (None, 2, 2, 256)    262144      re_lu_176[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_194 (BatchN (None, 2, 2, 256)    1024        conv2d_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_177 (ReLU)                (None, 2, 2, 256)    0           batch_normalization_194[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_196 (Conv2D)             (None, 2, 2, 256)    589824      re_lu_177[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_195 (BatchN (None, 2, 2, 256)    1024        conv2d_196[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_178 (ReLU)                (None, 2, 2, 256)    0           batch_normalization_195[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_197 (Conv2D)             (None, 2, 2, 1024)   263168      re_lu_178[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_196 (BatchN (None, 2, 2, 1024)   4096        conv2d_197[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_58 (Add)                    (None, 2, 2, 1024)   0           batch_normalization_196[0][0]    \n",
            "                                                                 re_lu_176[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_179 (ReLU)                (None, 2, 2, 1024)   0           add_58[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_198 (Conv2D)             (None, 2, 2, 256)    262144      re_lu_179[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_197 (BatchN (None, 2, 2, 256)    1024        conv2d_198[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_180 (ReLU)                (None, 2, 2, 256)    0           batch_normalization_197[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_199 (Conv2D)             (None, 2, 2, 256)    589824      re_lu_180[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_198 (BatchN (None, 2, 2, 256)    1024        conv2d_199[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_181 (ReLU)                (None, 2, 2, 256)    0           batch_normalization_198[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_200 (Conv2D)             (None, 2, 2, 1024)   263168      re_lu_181[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_199 (BatchN (None, 2, 2, 1024)   4096        conv2d_200[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_59 (Add)                    (None, 2, 2, 1024)   0           batch_normalization_199[0][0]    \n",
            "                                                                 re_lu_179[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_182 (ReLU)                (None, 2, 2, 1024)   0           add_59[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_201 (Conv2D)             (None, 2, 2, 256)    262144      re_lu_182[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_200 (BatchN (None, 2, 2, 256)    1024        conv2d_201[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_183 (ReLU)                (None, 2, 2, 256)    0           batch_normalization_200[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_202 (Conv2D)             (None, 2, 2, 256)    589824      re_lu_183[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_201 (BatchN (None, 2, 2, 256)    1024        conv2d_202[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_184 (ReLU)                (None, 2, 2, 256)    0           batch_normalization_201[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_203 (Conv2D)             (None, 2, 2, 1024)   263168      re_lu_184[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_202 (BatchN (None, 2, 2, 1024)   4096        conv2d_203[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_60 (Add)                    (None, 2, 2, 1024)   0           batch_normalization_202[0][0]    \n",
            "                                                                 re_lu_182[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_185 (ReLU)                (None, 2, 2, 1024)   0           add_60[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_204 (Conv2D)             (None, 2, 2, 256)    262144      re_lu_185[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_203 (BatchN (None, 2, 2, 256)    1024        conv2d_204[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_186 (ReLU)                (None, 2, 2, 256)    0           batch_normalization_203[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_205 (Conv2D)             (None, 2, 2, 256)    589824      re_lu_186[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_204 (BatchN (None, 2, 2, 256)    1024        conv2d_205[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_187 (ReLU)                (None, 2, 2, 256)    0           batch_normalization_204[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_206 (Conv2D)             (None, 2, 2, 1024)   263168      re_lu_187[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_205 (BatchN (None, 2, 2, 1024)   4096        conv2d_206[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_61 (Add)                    (None, 2, 2, 1024)   0           batch_normalization_205[0][0]    \n",
            "                                                                 re_lu_185[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_188 (ReLU)                (None, 2, 2, 1024)   0           add_61[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_207 (Conv2D)             (None, 2, 2, 256)    262144      re_lu_188[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_206 (BatchN (None, 2, 2, 256)    1024        conv2d_207[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_189 (ReLU)                (None, 2, 2, 256)    0           batch_normalization_206[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_208 (Conv2D)             (None, 2, 2, 256)    589824      re_lu_189[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_207 (BatchN (None, 2, 2, 256)    1024        conv2d_208[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_190 (ReLU)                (None, 2, 2, 256)    0           batch_normalization_207[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_209 (Conv2D)             (None, 2, 2, 1024)   263168      re_lu_190[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_208 (BatchN (None, 2, 2, 1024)   4096        conv2d_209[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_62 (Add)                    (None, 2, 2, 1024)   0           batch_normalization_208[0][0]    \n",
            "                                                                 re_lu_188[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_191 (ReLU)                (None, 2, 2, 1024)   0           add_62[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_210 (Conv2D)             (None, 1, 1, 512)    524288      re_lu_191[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_209 (BatchN (None, 1, 1, 512)    2048        conv2d_210[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_192 (ReLU)                (None, 1, 1, 512)    0           batch_normalization_209[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_211 (Conv2D)             (None, 1, 1, 512)    2359296     re_lu_192[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_210 (BatchN (None, 1, 1, 512)    2048        conv2d_211[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_193 (ReLU)                (None, 1, 1, 512)    0           batch_normalization_210[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_212 (Conv2D)             (None, 1, 1, 2048)   1050624     re_lu_193[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_213 (Conv2D)             (None, 1, 1, 2048)   2099200     re_lu_191[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_211 (BatchN (None, 1, 1, 2048)   8192        conv2d_212[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_212 (BatchN (None, 1, 1, 2048)   8192        conv2d_213[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_63 (Add)                    (None, 1, 1, 2048)   0           batch_normalization_211[0][0]    \n",
            "                                                                 batch_normalization_212[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_194 (ReLU)                (None, 1, 1, 2048)   0           add_63[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_214 (Conv2D)             (None, 1, 1, 512)    1048576     re_lu_194[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_213 (BatchN (None, 1, 1, 512)    2048        conv2d_214[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_195 (ReLU)                (None, 1, 1, 512)    0           batch_normalization_213[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_215 (Conv2D)             (None, 1, 1, 512)    2359296     re_lu_195[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_214 (BatchN (None, 1, 1, 512)    2048        conv2d_215[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_196 (ReLU)                (None, 1, 1, 512)    0           batch_normalization_214[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_216 (Conv2D)             (None, 1, 1, 2048)   1050624     re_lu_196[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_215 (BatchN (None, 1, 1, 2048)   8192        conv2d_216[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_64 (Add)                    (None, 1, 1, 2048)   0           batch_normalization_215[0][0]    \n",
            "                                                                 re_lu_194[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_197 (ReLU)                (None, 1, 1, 2048)   0           add_64[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_217 (Conv2D)             (None, 1, 1, 512)    1048576     re_lu_197[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_216 (BatchN (None, 1, 1, 512)    2048        conv2d_217[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_198 (ReLU)                (None, 1, 1, 512)    0           batch_normalization_216[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_218 (Conv2D)             (None, 1, 1, 512)    2359296     re_lu_198[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_217 (BatchN (None, 1, 1, 512)    2048        conv2d_218[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_199 (ReLU)                (None, 1, 1, 512)    0           batch_normalization_217[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_219 (Conv2D)             (None, 1, 1, 2048)   1050624     re_lu_199[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_218 (BatchN (None, 1, 1, 2048)   8192        conv2d_219[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_65 (Add)                    (None, 1, 1, 2048)   0           batch_normalization_218[0][0]    \n",
            "                                                                 re_lu_197[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_200 (ReLU)                (None, 1, 1, 2048)   0           add_65[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_3 (Glo (None, 2048)         0           re_lu_200[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           20490       global_average_pooling2d_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 10)           0           dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 23,600,586\n",
            "Trainable params: 23,547,466\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgXHZ21c9_K7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN16djS-D6P0",
        "colab_type": "code",
        "outputId": "25c2436a-b6de-4012-9b33-c58fe0f63d03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        }
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "history = model.fit(ds_train,\n",
        "                    epochs=20,\n",
        "                    validation_data = ds_val)\n",
        "\n",
        "print(f\"It took {time.time() - start} seconds\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1172/1172 [==============================] - 57s 49ms/step - loss: 0.9570 - accuracy: 0.6647 - val_loss: 1.0660 - val_accuracy: 0.6246\n",
            "Epoch 2/20\n",
            "1172/1172 [==============================] - 57s 48ms/step - loss: 0.9468 - accuracy: 0.6715 - val_loss: 0.9648 - val_accuracy: 0.6563\n",
            "Epoch 3/20\n",
            "1172/1172 [==============================] - 57s 48ms/step - loss: 0.9169 - accuracy: 0.6779 - val_loss: 0.9009 - val_accuracy: 0.6806\n",
            "Epoch 4/20\n",
            "1172/1172 [==============================] - 57s 48ms/step - loss: 0.8700 - accuracy: 0.6936 - val_loss: 0.9810 - val_accuracy: 0.6605\n",
            "Epoch 5/20\n",
            "1172/1172 [==============================] - 58s 49ms/step - loss: 0.8400 - accuracy: 0.7034 - val_loss: 1.0241 - val_accuracy: 0.6444\n",
            "Epoch 6/20\n",
            "1172/1172 [==============================] - 57s 49ms/step - loss: 0.8027 - accuracy: 0.7189 - val_loss: 0.9003 - val_accuracy: 0.6813\n",
            "Epoch 7/20\n",
            "1172/1172 [==============================] - 58s 49ms/step - loss: 0.7728 - accuracy: 0.7295 - val_loss: 1.0182 - val_accuracy: 0.6477\n",
            "Epoch 8/20\n",
            "1172/1172 [==============================] - 57s 49ms/step - loss: 0.7569 - accuracy: 0.7333 - val_loss: 1.1326 - val_accuracy: 0.6317\n",
            "Epoch 9/20\n",
            "1172/1172 [==============================] - 57s 49ms/step - loss: 0.7486 - accuracy: 0.7394 - val_loss: 0.9695 - val_accuracy: 0.6631\n",
            "Epoch 10/20\n",
            "1172/1172 [==============================] - 57s 49ms/step - loss: 0.7180 - accuracy: 0.7495 - val_loss: 0.8269 - val_accuracy: 0.7190\n",
            "Epoch 11/20\n",
            "1172/1172 [==============================] - 57s 49ms/step - loss: 0.6806 - accuracy: 0.7623 - val_loss: 0.8275 - val_accuracy: 0.7151\n",
            "Epoch 12/20\n",
            "1172/1172 [==============================] - 57s 49ms/step - loss: 0.6671 - accuracy: 0.7681 - val_loss: 0.8477 - val_accuracy: 0.7069\n",
            "Epoch 13/20\n",
            "1172/1172 [==============================] - 57s 49ms/step - loss: 0.6447 - accuracy: 0.7735 - val_loss: 0.8543 - val_accuracy: 0.7118\n",
            "Epoch 14/20\n",
            "1172/1172 [==============================] - 57s 48ms/step - loss: 0.6236 - accuracy: 0.7800 - val_loss: 1.1812 - val_accuracy: 0.6317\n",
            "Epoch 15/20\n",
            "1172/1172 [==============================] - 57s 48ms/step - loss: 0.6177 - accuracy: 0.7875 - val_loss: 1.0423 - val_accuracy: 0.6597\n",
            "Epoch 16/20\n",
            "1172/1172 [==============================] - 56s 48ms/step - loss: 0.6007 - accuracy: 0.7906 - val_loss: 0.8287 - val_accuracy: 0.7252\n",
            "Epoch 17/20\n",
            "1172/1172 [==============================] - 56s 48ms/step - loss: 0.5753 - accuracy: 0.7982 - val_loss: 0.7585 - val_accuracy: 0.7417\n",
            "Epoch 18/20\n",
            "1172/1172 [==============================] - 56s 48ms/step - loss: 0.5758 - accuracy: 0.7979 - val_loss: 0.7436 - val_accuracy: 0.7445\n",
            "Epoch 19/20\n",
            "1172/1172 [==============================] - 57s 48ms/step - loss: 0.5455 - accuracy: 0.8107 - val_loss: 0.7485 - val_accuracy: 0.7460\n",
            "Epoch 20/20\n",
            "1172/1172 [==============================] - 57s 48ms/step - loss: 0.5747 - accuracy: 0.7996 - val_loss: 0.7514 - val_accuracy: 0.7444\n",
            "It took 1148.7555301189423 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSVOB5029_Bo",
        "colab_type": "code",
        "outputId": "b8d3be96-6c40-4dda-d361-16e97486aa5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.DataFrame(history.history).plot(figsize=(12,8))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0,2)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYbivUzD3ueq",
        "colab_type": "text"
      },
      "source": [
        "### DenseNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Lu_YBS3fuvN",
        "colab_type": "text"
      },
      "source": [
        "#### Idée"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVP0Vn7UeSnY",
        "colab_type": "text"
      },
      "source": [
        "- As CNNs become increasingly deep, a new research problem emerges : as \n",
        "information about the input of gradient passes through many layers, it can vanish and \"wash out\" by the time it reaches the end (or beginning) of the network.\n",
        "\n",
        "- to ensure maximum information flow between layers in the network, we connect *all layers* (with matching feature maps sizes) directly with each other.\n",
        "\n",
        "- Crucially, in contrast to ResNets, we never combine features through summation before they are passed into a layer, instead, we combine features by concatenating them.\n",
        "\n",
        "- the final classifier makes a decision based on all features maps in the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cswhtp6Ifs4S",
        "colab_type": "text"
      },
      "source": [
        "#### Définition des briques de bases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_T4elTbjKlQ",
        "colab_type": "text"
      },
      "source": [
        "- To facilitate down-sampling in our architecture we divide the network into multiple densely connected *dense blocks*.\n",
        "\n",
        "- We refer to layers between blocks as *transition blocks*. \n",
        "\n",
        "$\\implies$ Le modèle est donc articulé autour d'une architecture qui alterne deux types de blocs :\n",
        "\n",
        "- Les blocs dits \"denses\",\n",
        "- Les blocs dits de \"transitions\". "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfpzAB1Ijt2i",
        "colab_type": "text"
      },
      "source": [
        "##### Composite function & dense blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHJvmJCmgEEP",
        "colab_type": "text"
      },
      "source": [
        "De l'article, nous tirons les indications suivantes.\n",
        "\n",
        "- The network comprises $L$ layers, each of which implements a non-linear transformation $H_{\\ell}(-)$, where $\\ell$ indexes the layer. [...] We denote the output of the $\\ell^{th}$ layer as $x_{\\ell}$.\n",
        "\n",
        "- Consequenttly, the $\\ell^{th}$ layer receives the feature maps of all preceding layers, $x_{0}, \\dots, x_{\\ell-1}$ as input :\n",
        "\n",
        "$$x_{\\ell} = H_{\\ell}([x_{0},x_{1} \\dots, x_{\\ell-1}])$$\n",
        "\n",
        "- We define $H_{\\ell}$ as a compite function of the consecutive operations :\n",
        "  - BN-ReLU-Conv($1 \\times 1$)-BN-ReLU-Conv($3 \\times 3$), \n",
        "  - Each Conv(3 $\\times$ 3) produces $k$ features maps, [...] we let each Conv(1 $\\times$ 1) produce $4k$ feature maps.\n",
        "    - We refer to the hyperparameter $k$ as the *growth rate* of the network.\n",
        "    - The growth rate for all networks is $k=32$\n",
        "  - For convolutionnal layers with kernel size $3 \\times 3$, each side of the inputs is zero-padded  by one pixel to keep the feature-map size fixed.\n",
        "  - We adopt the weight initialization introduced by [10] (ie He)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKj3HbwpeRzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bn_relu_conv(tensor, k, kernel_size):\n",
        "  x = BatchNormalization()(tensor)\n",
        "  x = ReLU()(x)\n",
        "  x = Conv2D(filters=k,\n",
        "             kernel_size=kernel_size,\n",
        "             strides=(1,1),\n",
        "             padding='same',\n",
        "             kernel_initializer='he_normal',\n",
        "             use_bias=False)(x)\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDA2PObXrOIo",
        "colab_type": "text"
      },
      "source": [
        "Les blocs denses ont un schéma répétitif."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J58dSAyrTe7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Concatenate\n",
        "\n",
        "def dense_block(tensor, k, reps):\n",
        "  for _ in range(reps):\n",
        "    x = bn_relu_conv(tensor, 4*k, 1)\n",
        "    x = bn_relu_conv(x, k, 3)\n",
        "\n",
        "    tensor = Concatenate()([x, tensor])  # le tenseur d'input en entrée par définition\n",
        "\n",
        "  return tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkfI8BMRgEBg",
        "colab_type": "text"
      },
      "source": [
        "##### Transition function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_WoqjStgD_l",
        "colab_type": "text"
      },
      "source": [
        "- If a dense blocks contains *m* feature-maps, we let the following transition layer generate $\\lfloor \\theta m \\rfloor$ output feature maps, where $0 < \\theta \\leq 1$ is referred as the compression factor.\n",
        "- We set $\\theta = 0.5$ in our experiment.\n",
        "- We use BN-ReLU-Conv($1 \\times 1$) followed by $2 \\times 2$ average pooling as transition layers between two contiguous dense blocks.\n",
        "\n",
        "\n",
        "Pour avoir accès au nombre de feature maps, on a besoin d'utiliser la commande `tf.keras.backend.int_shape(x)[-1]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZjfPZXDeSBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import AvgPool2D\n",
        "\n",
        "def transition_layer(x, theta):\n",
        "    f = int(tf.keras.backend.int_shape(x)[-1] * theta)\n",
        "    x = bn_relu_conv(x, f, 1)\n",
        "    x = AvgPool2D(pool_size=2, strides=2, padding='same')(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z48wuTrCgD9z",
        "colab_type": "text"
      },
      "source": [
        "- The initial convolution layer comprises $2k$ convolutions of size $7\\times7$ with stride $2$. \n",
        "- At the end of the last dense blocks, a global average pooling is performed and then a softmax classfier is attached."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV0GftsseSEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "\n",
        "def DenseNet(img_shape, k, theta, repetitions, small=True, include_top=True, num_classes=10):\n",
        "  \n",
        "  input = Input(img_shape)\n",
        "\n",
        "  if small:\n",
        "    x = Conv2D(filters = 16,\n",
        "               kernel_size = 7,\n",
        "               strides=2,\n",
        "               padding='same',\n",
        "               kernel_initializer='he_normal')(input)\n",
        "  else:\n",
        "    x = Conv2D(filters = 2*k,\n",
        "               kernel_size = 7,\n",
        "               strides=2,\n",
        "               padding='same',\n",
        "               kernel_initializer='he_normal')(input)\n",
        "    x = MaxPool2D(pool_size = 3,\n",
        "                  strides = 2,\n",
        "                  padding='same')(x)\n",
        "\n",
        "  for reps in repetitions:\n",
        "      d = dense_block(x, k, reps)\n",
        "      x = transition_layer(d, theta)\n",
        "\n",
        "  if include_top:\n",
        "    x = GlobalAvgPool2D()(d)\n",
        "    x = Dense(num_classes)(x)\n",
        "    output = Activation('softmax')(x)\n",
        "  else:\n",
        "    output = GlobalAvgPool2D()(d)\n",
        "\n",
        "  model = Model(input, output)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rauOT00CIdyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_shape = 32, 32, 3\n",
        "k = 32\n",
        "theta = 0.5\n",
        "repetitions = 6, 12, 24, 16\n",
        "num_classes = 100\n",
        "\n",
        "model = DenseNet(img_shape, k, theta, repetitions, num_classes=num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHhgq2pMeSHO",
        "colab_type": "code",
        "outputId": "869a926b-15e7-46dd-be25-a59ceecdc279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_123 (Conv2D)             (None, 16, 16, 16)   2368        input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_120 (BatchN (None, 16, 16, 16)   64          conv2d_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_120 (ReLU)                (None, 16, 16, 16)   0           batch_normalization_120[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_124 (Conv2D)             (None, 16, 16, 128)  2048        re_lu_120[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_121 (BatchN (None, 16, 16, 128)  512         conv2d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_121 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_121[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_125 (Conv2D)             (None, 16, 16, 32)   36864       re_lu_121[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_58 (Concatenate)    (None, 16, 16, 48)   0           conv2d_123[0][0]                 \n",
            "                                                                 conv2d_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_122 (BatchN (None, 16, 16, 48)   192         concatenate_58[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_122 (ReLU)                (None, 16, 16, 48)   0           batch_normalization_122[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_126 (Conv2D)             (None, 16, 16, 128)  6144        re_lu_122[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_123 (BatchN (None, 16, 16, 128)  512         conv2d_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_123 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_123[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_127 (Conv2D)             (None, 16, 16, 32)   36864       re_lu_123[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_59 (Concatenate)    (None, 16, 16, 80)   0           concatenate_58[0][0]             \n",
            "                                                                 conv2d_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_124 (BatchN (None, 16, 16, 80)   320         concatenate_59[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_124 (ReLU)                (None, 16, 16, 80)   0           batch_normalization_124[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_128 (Conv2D)             (None, 16, 16, 128)  10240       re_lu_124[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_125 (BatchN (None, 16, 16, 128)  512         conv2d_128[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_125 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_125[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_129 (Conv2D)             (None, 16, 16, 32)   36864       re_lu_125[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_60 (Concatenate)    (None, 16, 16, 112)  0           concatenate_59[0][0]             \n",
            "                                                                 conv2d_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_126 (BatchN (None, 16, 16, 112)  448         concatenate_60[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_126 (ReLU)                (None, 16, 16, 112)  0           batch_normalization_126[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_130 (Conv2D)             (None, 16, 16, 128)  14336       re_lu_126[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_127 (BatchN (None, 16, 16, 128)  512         conv2d_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_127 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_127[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_131 (Conv2D)             (None, 16, 16, 32)   36864       re_lu_127[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_61 (Concatenate)    (None, 16, 16, 144)  0           concatenate_60[0][0]             \n",
            "                                                                 conv2d_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_128 (BatchN (None, 16, 16, 144)  576         concatenate_61[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_128 (ReLU)                (None, 16, 16, 144)  0           batch_normalization_128[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_132 (Conv2D)             (None, 16, 16, 128)  18432       re_lu_128[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_129 (BatchN (None, 16, 16, 128)  512         conv2d_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_129 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_129[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_133 (Conv2D)             (None, 16, 16, 32)   36864       re_lu_129[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_62 (Concatenate)    (None, 16, 16, 176)  0           concatenate_61[0][0]             \n",
            "                                                                 conv2d_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_130 (BatchN (None, 16, 16, 176)  704         concatenate_62[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_130 (ReLU)                (None, 16, 16, 176)  0           batch_normalization_130[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_134 (Conv2D)             (None, 16, 16, 128)  22528       re_lu_130[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_131 (BatchN (None, 16, 16, 128)  512         conv2d_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_131 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_131[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_135 (Conv2D)             (None, 16, 16, 32)   36864       re_lu_131[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_63 (Concatenate)    (None, 16, 16, 208)  0           concatenate_62[0][0]             \n",
            "                                                                 conv2d_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_132 (BatchN (None, 16, 16, 208)  832         concatenate_63[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_132 (ReLU)                (None, 16, 16, 208)  0           batch_normalization_132[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_136 (Conv2D)             (None, 16, 16, 104)  21632       re_lu_132[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 8, 8, 104)    0           conv2d_136[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_133 (BatchN (None, 8, 8, 104)    416         average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_133 (ReLU)                (None, 8, 8, 104)    0           batch_normalization_133[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_137 (Conv2D)             (None, 8, 8, 128)    13312       re_lu_133[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_134 (BatchN (None, 8, 8, 128)    512         conv2d_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_134 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_134[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_138 (Conv2D)             (None, 8, 8, 32)     36864       re_lu_134[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_64 (Concatenate)    (None, 8, 8, 136)    0           average_pooling2d_4[0][0]        \n",
            "                                                                 conv2d_138[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_135 (BatchN (None, 8, 8, 136)    544         concatenate_64[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_135 (ReLU)                (None, 8, 8, 136)    0           batch_normalization_135[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_139 (Conv2D)             (None, 8, 8, 128)    17408       re_lu_135[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_136 (BatchN (None, 8, 8, 128)    512         conv2d_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_136 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_136[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_140 (Conv2D)             (None, 8, 8, 32)     36864       re_lu_136[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_65 (Concatenate)    (None, 8, 8, 168)    0           concatenate_64[0][0]             \n",
            "                                                                 conv2d_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_137 (BatchN (None, 8, 8, 168)    672         concatenate_65[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_137 (ReLU)                (None, 8, 8, 168)    0           batch_normalization_137[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_141 (Conv2D)             (None, 8, 8, 128)    21504       re_lu_137[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_138 (BatchN (None, 8, 8, 128)    512         conv2d_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_138 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_138[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_142 (Conv2D)             (None, 8, 8, 32)     36864       re_lu_138[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_66 (Concatenate)    (None, 8, 8, 200)    0           concatenate_65[0][0]             \n",
            "                                                                 conv2d_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_139 (BatchN (None, 8, 8, 200)    800         concatenate_66[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_139 (ReLU)                (None, 8, 8, 200)    0           batch_normalization_139[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_143 (Conv2D)             (None, 8, 8, 128)    25600       re_lu_139[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_140 (BatchN (None, 8, 8, 128)    512         conv2d_143[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_140 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_140[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_144 (Conv2D)             (None, 8, 8, 32)     36864       re_lu_140[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_67 (Concatenate)    (None, 8, 8, 232)    0           concatenate_66[0][0]             \n",
            "                                                                 conv2d_144[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_141 (BatchN (None, 8, 8, 232)    928         concatenate_67[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_141 (ReLU)                (None, 8, 8, 232)    0           batch_normalization_141[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_145 (Conv2D)             (None, 8, 8, 128)    29696       re_lu_141[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_142 (BatchN (None, 8, 8, 128)    512         conv2d_145[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_142 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_142[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_146 (Conv2D)             (None, 8, 8, 32)     36864       re_lu_142[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_68 (Concatenate)    (None, 8, 8, 264)    0           concatenate_67[0][0]             \n",
            "                                                                 conv2d_146[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_143 (BatchN (None, 8, 8, 264)    1056        concatenate_68[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_143 (ReLU)                (None, 8, 8, 264)    0           batch_normalization_143[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_147 (Conv2D)             (None, 8, 8, 128)    33792       re_lu_143[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_144 (BatchN (None, 8, 8, 128)    512         conv2d_147[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_144 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_144[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_148 (Conv2D)             (None, 8, 8, 32)     36864       re_lu_144[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_69 (Concatenate)    (None, 8, 8, 296)    0           concatenate_68[0][0]             \n",
            "                                                                 conv2d_148[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_145 (BatchN (None, 8, 8, 296)    1184        concatenate_69[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_145 (ReLU)                (None, 8, 8, 296)    0           batch_normalization_145[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_149 (Conv2D)             (None, 8, 8, 128)    37888       re_lu_145[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_146 (BatchN (None, 8, 8, 128)    512         conv2d_149[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_146 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_146[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_150 (Conv2D)             (None, 8, 8, 32)     36864       re_lu_146[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_70 (Concatenate)    (None, 8, 8, 328)    0           concatenate_69[0][0]             \n",
            "                                                                 conv2d_150[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_147 (BatchN (None, 8, 8, 328)    1312        concatenate_70[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_147 (ReLU)                (None, 8, 8, 328)    0           batch_normalization_147[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_151 (Conv2D)             (None, 8, 8, 128)    41984       re_lu_147[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_148 (BatchN (None, 8, 8, 128)    512         conv2d_151[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_148 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_148[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_152 (Conv2D)             (None, 8, 8, 32)     36864       re_lu_148[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_71 (Concatenate)    (None, 8, 8, 360)    0           concatenate_70[0][0]             \n",
            "                                                                 conv2d_152[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_149 (BatchN (None, 8, 8, 360)    1440        concatenate_71[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_149 (ReLU)                (None, 8, 8, 360)    0           batch_normalization_149[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_153 (Conv2D)             (None, 8, 8, 128)    46080       re_lu_149[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_150 (BatchN (None, 8, 8, 128)    512         conv2d_153[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_150 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_150[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_154 (Conv2D)             (None, 8, 8, 32)     36864       re_lu_150[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_72 (Concatenate)    (None, 8, 8, 392)    0           concatenate_71[0][0]             \n",
            "                                                                 conv2d_154[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_151 (BatchN (None, 8, 8, 392)    1568        concatenate_72[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_151 (ReLU)                (None, 8, 8, 392)    0           batch_normalization_151[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_155 (Conv2D)             (None, 8, 8, 128)    50176       re_lu_151[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_152 (BatchN (None, 8, 8, 128)    512         conv2d_155[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_152 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_152[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_156 (Conv2D)             (None, 8, 8, 32)     36864       re_lu_152[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_73 (Concatenate)    (None, 8, 8, 424)    0           concatenate_72[0][0]             \n",
            "                                                                 conv2d_156[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_153 (BatchN (None, 8, 8, 424)    1696        concatenate_73[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_153 (ReLU)                (None, 8, 8, 424)    0           batch_normalization_153[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_157 (Conv2D)             (None, 8, 8, 128)    54272       re_lu_153[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_154 (BatchN (None, 8, 8, 128)    512         conv2d_157[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_154 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_154[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_158 (Conv2D)             (None, 8, 8, 32)     36864       re_lu_154[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_74 (Concatenate)    (None, 8, 8, 456)    0           concatenate_73[0][0]             \n",
            "                                                                 conv2d_158[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_155 (BatchN (None, 8, 8, 456)    1824        concatenate_74[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_155 (ReLU)                (None, 8, 8, 456)    0           batch_normalization_155[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_159 (Conv2D)             (None, 8, 8, 128)    58368       re_lu_155[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_156 (BatchN (None, 8, 8, 128)    512         conv2d_159[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_156 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_160 (Conv2D)             (None, 8, 8, 32)     36864       re_lu_156[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_75 (Concatenate)    (None, 8, 8, 488)    0           concatenate_74[0][0]             \n",
            "                                                                 conv2d_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_157 (BatchN (None, 8, 8, 488)    1952        concatenate_75[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_157 (ReLU)                (None, 8, 8, 488)    0           batch_normalization_157[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_161 (Conv2D)             (None, 8, 8, 244)    119072      re_lu_157[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 4, 4, 244)    0           conv2d_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_158 (BatchN (None, 4, 4, 244)    976         average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_158 (ReLU)                (None, 4, 4, 244)    0           batch_normalization_158[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_162 (Conv2D)             (None, 4, 4, 128)    31232       re_lu_158[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_159 (BatchN (None, 4, 4, 128)    512         conv2d_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_159 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_159[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_163 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_159[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_76 (Concatenate)    (None, 4, 4, 276)    0           average_pooling2d_5[0][0]        \n",
            "                                                                 conv2d_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_160 (BatchN (None, 4, 4, 276)    1104        concatenate_76[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_160 (ReLU)                (None, 4, 4, 276)    0           batch_normalization_160[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_164 (Conv2D)             (None, 4, 4, 128)    35328       re_lu_160[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_161 (BatchN (None, 4, 4, 128)    512         conv2d_164[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_161 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_161[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_165 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_161[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_77 (Concatenate)    (None, 4, 4, 308)    0           concatenate_76[0][0]             \n",
            "                                                                 conv2d_165[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_162 (BatchN (None, 4, 4, 308)    1232        concatenate_77[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_162 (ReLU)                (None, 4, 4, 308)    0           batch_normalization_162[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_166 (Conv2D)             (None, 4, 4, 128)    39424       re_lu_162[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_163 (BatchN (None, 4, 4, 128)    512         conv2d_166[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_163 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_163[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_167 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_163[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_78 (Concatenate)    (None, 4, 4, 340)    0           concatenate_77[0][0]             \n",
            "                                                                 conv2d_167[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_164 (BatchN (None, 4, 4, 340)    1360        concatenate_78[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_164 (ReLU)                (None, 4, 4, 340)    0           batch_normalization_164[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_168 (Conv2D)             (None, 4, 4, 128)    43520       re_lu_164[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_165 (BatchN (None, 4, 4, 128)    512         conv2d_168[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_165 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_165[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_169 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_165[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_79 (Concatenate)    (None, 4, 4, 372)    0           concatenate_78[0][0]             \n",
            "                                                                 conv2d_169[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_166 (BatchN (None, 4, 4, 372)    1488        concatenate_79[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_166 (ReLU)                (None, 4, 4, 372)    0           batch_normalization_166[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_170 (Conv2D)             (None, 4, 4, 128)    47616       re_lu_166[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_167 (BatchN (None, 4, 4, 128)    512         conv2d_170[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_167 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_167[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_171 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_167[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_80 (Concatenate)    (None, 4, 4, 404)    0           concatenate_79[0][0]             \n",
            "                                                                 conv2d_171[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_168 (BatchN (None, 4, 4, 404)    1616        concatenate_80[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_168 (ReLU)                (None, 4, 4, 404)    0           batch_normalization_168[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_172 (Conv2D)             (None, 4, 4, 128)    51712       re_lu_168[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_169 (BatchN (None, 4, 4, 128)    512         conv2d_172[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_169 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_169[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_173 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_169[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_81 (Concatenate)    (None, 4, 4, 436)    0           concatenate_80[0][0]             \n",
            "                                                                 conv2d_173[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_170 (BatchN (None, 4, 4, 436)    1744        concatenate_81[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_170 (ReLU)                (None, 4, 4, 436)    0           batch_normalization_170[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_174 (Conv2D)             (None, 4, 4, 128)    55808       re_lu_170[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_171 (BatchN (None, 4, 4, 128)    512         conv2d_174[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_171 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_171[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_175 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_171[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_82 (Concatenate)    (None, 4, 4, 468)    0           concatenate_81[0][0]             \n",
            "                                                                 conv2d_175[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_172 (BatchN (None, 4, 4, 468)    1872        concatenate_82[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_172 (ReLU)                (None, 4, 4, 468)    0           batch_normalization_172[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_176 (Conv2D)             (None, 4, 4, 128)    59904       re_lu_172[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_173 (BatchN (None, 4, 4, 128)    512         conv2d_176[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_173 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_173[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_177 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_173[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_83 (Concatenate)    (None, 4, 4, 500)    0           concatenate_82[0][0]             \n",
            "                                                                 conv2d_177[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_174 (BatchN (None, 4, 4, 500)    2000        concatenate_83[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_174 (ReLU)                (None, 4, 4, 500)    0           batch_normalization_174[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_178 (Conv2D)             (None, 4, 4, 128)    64000       re_lu_174[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_175 (BatchN (None, 4, 4, 128)    512         conv2d_178[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_175 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_175[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_179 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_175[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_84 (Concatenate)    (None, 4, 4, 532)    0           concatenate_83[0][0]             \n",
            "                                                                 conv2d_179[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_176 (BatchN (None, 4, 4, 532)    2128        concatenate_84[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_176 (ReLU)                (None, 4, 4, 532)    0           batch_normalization_176[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_180 (Conv2D)             (None, 4, 4, 128)    68096       re_lu_176[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_177 (BatchN (None, 4, 4, 128)    512         conv2d_180[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_177 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_177[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_181 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_177[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_85 (Concatenate)    (None, 4, 4, 564)    0           concatenate_84[0][0]             \n",
            "                                                                 conv2d_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_178 (BatchN (None, 4, 4, 564)    2256        concatenate_85[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_178 (ReLU)                (None, 4, 4, 564)    0           batch_normalization_178[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_182 (Conv2D)             (None, 4, 4, 128)    72192       re_lu_178[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_179 (BatchN (None, 4, 4, 128)    512         conv2d_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_179 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_179[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_183 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_179[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_86 (Concatenate)    (None, 4, 4, 596)    0           concatenate_85[0][0]             \n",
            "                                                                 conv2d_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_180 (BatchN (None, 4, 4, 596)    2384        concatenate_86[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_180 (ReLU)                (None, 4, 4, 596)    0           batch_normalization_180[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_184 (Conv2D)             (None, 4, 4, 128)    76288       re_lu_180[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_181 (BatchN (None, 4, 4, 128)    512         conv2d_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_181 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_181[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_185 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_181[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_87 (Concatenate)    (None, 4, 4, 628)    0           concatenate_86[0][0]             \n",
            "                                                                 conv2d_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_182 (BatchN (None, 4, 4, 628)    2512        concatenate_87[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_182 (ReLU)                (None, 4, 4, 628)    0           batch_normalization_182[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_186 (Conv2D)             (None, 4, 4, 128)    80384       re_lu_182[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_183 (BatchN (None, 4, 4, 128)    512         conv2d_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_183 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_183[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_187 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_183[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_88 (Concatenate)    (None, 4, 4, 660)    0           concatenate_87[0][0]             \n",
            "                                                                 conv2d_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_184 (BatchN (None, 4, 4, 660)    2640        concatenate_88[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_184 (ReLU)                (None, 4, 4, 660)    0           batch_normalization_184[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_188 (Conv2D)             (None, 4, 4, 128)    84480       re_lu_184[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_185 (BatchN (None, 4, 4, 128)    512         conv2d_188[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_185 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_185[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_189 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_185[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_89 (Concatenate)    (None, 4, 4, 692)    0           concatenate_88[0][0]             \n",
            "                                                                 conv2d_189[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_186 (BatchN (None, 4, 4, 692)    2768        concatenate_89[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_186 (ReLU)                (None, 4, 4, 692)    0           batch_normalization_186[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_190 (Conv2D)             (None, 4, 4, 128)    88576       re_lu_186[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_187 (BatchN (None, 4, 4, 128)    512         conv2d_190[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_187 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_187[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_191 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_187[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_90 (Concatenate)    (None, 4, 4, 724)    0           concatenate_89[0][0]             \n",
            "                                                                 conv2d_191[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_188 (BatchN (None, 4, 4, 724)    2896        concatenate_90[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_188 (ReLU)                (None, 4, 4, 724)    0           batch_normalization_188[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_192 (Conv2D)             (None, 4, 4, 128)    92672       re_lu_188[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_189 (BatchN (None, 4, 4, 128)    512         conv2d_192[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_189 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_189[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_193 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_189[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_91 (Concatenate)    (None, 4, 4, 756)    0           concatenate_90[0][0]             \n",
            "                                                                 conv2d_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_190 (BatchN (None, 4, 4, 756)    3024        concatenate_91[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_190 (ReLU)                (None, 4, 4, 756)    0           batch_normalization_190[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_194 (Conv2D)             (None, 4, 4, 128)    96768       re_lu_190[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_191 (BatchN (None, 4, 4, 128)    512         conv2d_194[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_191 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_191[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_195 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_191[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_92 (Concatenate)    (None, 4, 4, 788)    0           concatenate_91[0][0]             \n",
            "                                                                 conv2d_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_192 (BatchN (None, 4, 4, 788)    3152        concatenate_92[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_192 (ReLU)                (None, 4, 4, 788)    0           batch_normalization_192[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_196 (Conv2D)             (None, 4, 4, 128)    100864      re_lu_192[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_193 (BatchN (None, 4, 4, 128)    512         conv2d_196[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_193 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_193[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_197 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_193[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_93 (Concatenate)    (None, 4, 4, 820)    0           concatenate_92[0][0]             \n",
            "                                                                 conv2d_197[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_194 (BatchN (None, 4, 4, 820)    3280        concatenate_93[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_194 (ReLU)                (None, 4, 4, 820)    0           batch_normalization_194[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_198 (Conv2D)             (None, 4, 4, 128)    104960      re_lu_194[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_195 (BatchN (None, 4, 4, 128)    512         conv2d_198[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_195 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_195[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_199 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_195[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_94 (Concatenate)    (None, 4, 4, 852)    0           concatenate_93[0][0]             \n",
            "                                                                 conv2d_199[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_196 (BatchN (None, 4, 4, 852)    3408        concatenate_94[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_196 (ReLU)                (None, 4, 4, 852)    0           batch_normalization_196[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_200 (Conv2D)             (None, 4, 4, 128)    109056      re_lu_196[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_197 (BatchN (None, 4, 4, 128)    512         conv2d_200[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_197 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_197[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_201 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_197[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_95 (Concatenate)    (None, 4, 4, 884)    0           concatenate_94[0][0]             \n",
            "                                                                 conv2d_201[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_198 (BatchN (None, 4, 4, 884)    3536        concatenate_95[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_198 (ReLU)                (None, 4, 4, 884)    0           batch_normalization_198[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_202 (Conv2D)             (None, 4, 4, 128)    113152      re_lu_198[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_199 (BatchN (None, 4, 4, 128)    512         conv2d_202[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_199 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_199[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_203 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_199[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_96 (Concatenate)    (None, 4, 4, 916)    0           concatenate_95[0][0]             \n",
            "                                                                 conv2d_203[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_200 (BatchN (None, 4, 4, 916)    3664        concatenate_96[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_200 (ReLU)                (None, 4, 4, 916)    0           batch_normalization_200[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_204 (Conv2D)             (None, 4, 4, 128)    117248      re_lu_200[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_201 (BatchN (None, 4, 4, 128)    512         conv2d_204[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_201 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_201[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_205 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_201[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_97 (Concatenate)    (None, 4, 4, 948)    0           concatenate_96[0][0]             \n",
            "                                                                 conv2d_205[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_202 (BatchN (None, 4, 4, 948)    3792        concatenate_97[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_202 (ReLU)                (None, 4, 4, 948)    0           batch_normalization_202[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_206 (Conv2D)             (None, 4, 4, 128)    121344      re_lu_202[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_203 (BatchN (None, 4, 4, 128)    512         conv2d_206[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_203 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_203[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_207 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_203[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_98 (Concatenate)    (None, 4, 4, 980)    0           concatenate_97[0][0]             \n",
            "                                                                 conv2d_207[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_204 (BatchN (None, 4, 4, 980)    3920        concatenate_98[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_204 (ReLU)                (None, 4, 4, 980)    0           batch_normalization_204[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_208 (Conv2D)             (None, 4, 4, 128)    125440      re_lu_204[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_205 (BatchN (None, 4, 4, 128)    512         conv2d_208[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_205 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_205[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_209 (Conv2D)             (None, 4, 4, 32)     36864       re_lu_205[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_99 (Concatenate)    (None, 4, 4, 1012)   0           concatenate_98[0][0]             \n",
            "                                                                 conv2d_209[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_206 (BatchN (None, 4, 4, 1012)   4048        concatenate_99[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_206 (ReLU)                (None, 4, 4, 1012)   0           batch_normalization_206[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_210 (Conv2D)             (None, 4, 4, 506)    512072      re_lu_206[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 2, 2, 506)    0           conv2d_210[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_207 (BatchN (None, 2, 2, 506)    2024        average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_207 (ReLU)                (None, 2, 2, 506)    0           batch_normalization_207[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_211 (Conv2D)             (None, 2, 2, 128)    64768       re_lu_207[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_208 (BatchN (None, 2, 2, 128)    512         conv2d_211[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_208 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_208[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_212 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_208[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_100 (Concatenate)   (None, 2, 2, 538)    0           average_pooling2d_6[0][0]        \n",
            "                                                                 conv2d_212[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_209 (BatchN (None, 2, 2, 538)    2152        concatenate_100[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_209 (ReLU)                (None, 2, 2, 538)    0           batch_normalization_209[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_213 (Conv2D)             (None, 2, 2, 128)    68864       re_lu_209[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_210 (BatchN (None, 2, 2, 128)    512         conv2d_213[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_210 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_210[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_214 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_210[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_101 (Concatenate)   (None, 2, 2, 570)    0           concatenate_100[0][0]            \n",
            "                                                                 conv2d_214[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_211 (BatchN (None, 2, 2, 570)    2280        concatenate_101[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_211 (ReLU)                (None, 2, 2, 570)    0           batch_normalization_211[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_215 (Conv2D)             (None, 2, 2, 128)    72960       re_lu_211[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_212 (BatchN (None, 2, 2, 128)    512         conv2d_215[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_212 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_212[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_216 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_212[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_102 (Concatenate)   (None, 2, 2, 602)    0           concatenate_101[0][0]            \n",
            "                                                                 conv2d_216[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_213 (BatchN (None, 2, 2, 602)    2408        concatenate_102[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_213 (ReLU)                (None, 2, 2, 602)    0           batch_normalization_213[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_217 (Conv2D)             (None, 2, 2, 128)    77056       re_lu_213[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_214 (BatchN (None, 2, 2, 128)    512         conv2d_217[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_214 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_214[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_218 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_214[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_103 (Concatenate)   (None, 2, 2, 634)    0           concatenate_102[0][0]            \n",
            "                                                                 conv2d_218[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_215 (BatchN (None, 2, 2, 634)    2536        concatenate_103[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_215 (ReLU)                (None, 2, 2, 634)    0           batch_normalization_215[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_219 (Conv2D)             (None, 2, 2, 128)    81152       re_lu_215[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_216 (BatchN (None, 2, 2, 128)    512         conv2d_219[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_216 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_216[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_220 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_216[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_104 (Concatenate)   (None, 2, 2, 666)    0           concatenate_103[0][0]            \n",
            "                                                                 conv2d_220[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_217 (BatchN (None, 2, 2, 666)    2664        concatenate_104[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_217 (ReLU)                (None, 2, 2, 666)    0           batch_normalization_217[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_221 (Conv2D)             (None, 2, 2, 128)    85248       re_lu_217[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_218 (BatchN (None, 2, 2, 128)    512         conv2d_221[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_218 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_218[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_222 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_218[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_105 (Concatenate)   (None, 2, 2, 698)    0           concatenate_104[0][0]            \n",
            "                                                                 conv2d_222[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_219 (BatchN (None, 2, 2, 698)    2792        concatenate_105[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_219 (ReLU)                (None, 2, 2, 698)    0           batch_normalization_219[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_223 (Conv2D)             (None, 2, 2, 128)    89344       re_lu_219[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_220 (BatchN (None, 2, 2, 128)    512         conv2d_223[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_220 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_220[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_224 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_220[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_106 (Concatenate)   (None, 2, 2, 730)    0           concatenate_105[0][0]            \n",
            "                                                                 conv2d_224[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_221 (BatchN (None, 2, 2, 730)    2920        concatenate_106[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_221 (ReLU)                (None, 2, 2, 730)    0           batch_normalization_221[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_225 (Conv2D)             (None, 2, 2, 128)    93440       re_lu_221[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_222 (BatchN (None, 2, 2, 128)    512         conv2d_225[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_222 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_222[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_226 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_222[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_107 (Concatenate)   (None, 2, 2, 762)    0           concatenate_106[0][0]            \n",
            "                                                                 conv2d_226[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_223 (BatchN (None, 2, 2, 762)    3048        concatenate_107[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_223 (ReLU)                (None, 2, 2, 762)    0           batch_normalization_223[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_227 (Conv2D)             (None, 2, 2, 128)    97536       re_lu_223[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_224 (BatchN (None, 2, 2, 128)    512         conv2d_227[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_224 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_224[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_228 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_224[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_108 (Concatenate)   (None, 2, 2, 794)    0           concatenate_107[0][0]            \n",
            "                                                                 conv2d_228[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_225 (BatchN (None, 2, 2, 794)    3176        concatenate_108[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_225 (ReLU)                (None, 2, 2, 794)    0           batch_normalization_225[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_229 (Conv2D)             (None, 2, 2, 128)    101632      re_lu_225[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_226 (BatchN (None, 2, 2, 128)    512         conv2d_229[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_226 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_226[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_230 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_226[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_109 (Concatenate)   (None, 2, 2, 826)    0           concatenate_108[0][0]            \n",
            "                                                                 conv2d_230[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_227 (BatchN (None, 2, 2, 826)    3304        concatenate_109[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_227 (ReLU)                (None, 2, 2, 826)    0           batch_normalization_227[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_231 (Conv2D)             (None, 2, 2, 128)    105728      re_lu_227[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_228 (BatchN (None, 2, 2, 128)    512         conv2d_231[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_228 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_228[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_232 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_228[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_110 (Concatenate)   (None, 2, 2, 858)    0           concatenate_109[0][0]            \n",
            "                                                                 conv2d_232[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_229 (BatchN (None, 2, 2, 858)    3432        concatenate_110[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_229 (ReLU)                (None, 2, 2, 858)    0           batch_normalization_229[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_233 (Conv2D)             (None, 2, 2, 128)    109824      re_lu_229[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_230 (BatchN (None, 2, 2, 128)    512         conv2d_233[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_230 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_230[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_234 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_230[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_111 (Concatenate)   (None, 2, 2, 890)    0           concatenate_110[0][0]            \n",
            "                                                                 conv2d_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_231 (BatchN (None, 2, 2, 890)    3560        concatenate_111[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_231 (ReLU)                (None, 2, 2, 890)    0           batch_normalization_231[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_235 (Conv2D)             (None, 2, 2, 128)    113920      re_lu_231[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_232 (BatchN (None, 2, 2, 128)    512         conv2d_235[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_232 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_232[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_236 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_232[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_112 (Concatenate)   (None, 2, 2, 922)    0           concatenate_111[0][0]            \n",
            "                                                                 conv2d_236[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_233 (BatchN (None, 2, 2, 922)    3688        concatenate_112[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_233 (ReLU)                (None, 2, 2, 922)    0           batch_normalization_233[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_237 (Conv2D)             (None, 2, 2, 128)    118016      re_lu_233[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_234 (BatchN (None, 2, 2, 128)    512         conv2d_237[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_234 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_234[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_238 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_234[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_113 (Concatenate)   (None, 2, 2, 954)    0           concatenate_112[0][0]            \n",
            "                                                                 conv2d_238[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_235 (BatchN (None, 2, 2, 954)    3816        concatenate_113[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_235 (ReLU)                (None, 2, 2, 954)    0           batch_normalization_235[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_239 (Conv2D)             (None, 2, 2, 128)    122112      re_lu_235[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_236 (BatchN (None, 2, 2, 128)    512         conv2d_239[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_236 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_236[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_240 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_236[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_114 (Concatenate)   (None, 2, 2, 986)    0           concatenate_113[0][0]            \n",
            "                                                                 conv2d_240[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_237 (BatchN (None, 2, 2, 986)    3944        concatenate_114[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_237 (ReLU)                (None, 2, 2, 986)    0           batch_normalization_237[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_241 (Conv2D)             (None, 2, 2, 128)    126208      re_lu_237[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_238 (BatchN (None, 2, 2, 128)    512         conv2d_241[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_238 (ReLU)                (None, 2, 2, 128)    0           batch_normalization_238[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_242 (Conv2D)             (None, 2, 2, 32)     36864       re_lu_238[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_115 (Concatenate)   (None, 2, 2, 1018)   0           concatenate_114[0][0]            \n",
            "                                                                 conv2d_242[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 1018)         0           concatenate_115[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 100)          101900      global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 100)          0           dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 6,965,604\n",
            "Trainable params: 6,886,220\n",
            "Non-trainable params: 79,384\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eOyErCG-t9T0",
        "colab": {}
      },
      "source": [
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8f5e4650-fee1-4546-b016-fba99d7c82d7",
        "id": "iZDa1AQ7t9Ub",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        }
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "history = model.fit(ds_train,\n",
        "                    epochs=20,\n",
        "                    validation_data = ds_val)\n",
        "\n",
        "print(f\"It took {time.time() - start} seconds\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "586/586 [==============================] - 54s 93ms/step - loss: 3.1179 - accuracy: 0.2295 - val_loss: 3.1247 - val_accuracy: 0.2200\n",
            "Epoch 2/20\n",
            "586/586 [==============================] - 55s 93ms/step - loss: 2.7529 - accuracy: 0.2975 - val_loss: 9.7801 - val_accuracy: 0.1771\n",
            "Epoch 3/20\n",
            "586/586 [==============================] - 54s 93ms/step - loss: 2.4882 - accuracy: 0.3499 - val_loss: 2.8652 - val_accuracy: 0.2881\n",
            "Epoch 4/20\n",
            "586/586 [==============================] - 54s 92ms/step - loss: 2.3228 - accuracy: 0.3846 - val_loss: 2.9444 - val_accuracy: 0.2735\n",
            "Epoch 5/20\n",
            "586/586 [==============================] - 54s 92ms/step - loss: 2.1534 - accuracy: 0.4218 - val_loss: 5.3858 - val_accuracy: 0.3058\n",
            "Epoch 6/20\n",
            "586/586 [==============================] - 54s 92ms/step - loss: 1.9807 - accuracy: 0.4586 - val_loss: 2.4927 - val_accuracy: 0.3708\n",
            "Epoch 7/20\n",
            "586/586 [==============================] - 54s 92ms/step - loss: 1.8318 - accuracy: 0.4897 - val_loss: 2.4684 - val_accuracy: 0.3822\n",
            "Epoch 8/20\n",
            "586/586 [==============================] - 54s 92ms/step - loss: 1.7204 - accuracy: 0.5164 - val_loss: 2.2386 - val_accuracy: 0.4179\n",
            "Epoch 9/20\n",
            "586/586 [==============================] - 53s 91ms/step - loss: 1.5877 - accuracy: 0.5470 - val_loss: 2.6878 - val_accuracy: 0.3539\n",
            "Epoch 10/20\n",
            "586/586 [==============================] - 53s 91ms/step - loss: 1.4851 - accuracy: 0.5734 - val_loss: 2.2689 - val_accuracy: 0.4295\n",
            "Epoch 11/20\n",
            "586/586 [==============================] - 53s 91ms/step - loss: 1.3668 - accuracy: 0.6050 - val_loss: 2.2262 - val_accuracy: 0.4541\n",
            "Epoch 12/20\n",
            "586/586 [==============================] - 53s 91ms/step - loss: 1.2730 - accuracy: 0.6249 - val_loss: 2.0790 - val_accuracy: 0.4722\n",
            "Epoch 13/20\n",
            "586/586 [==============================] - 53s 91ms/step - loss: 1.1463 - accuracy: 0.6619 - val_loss: 2.1492 - val_accuracy: 0.4834\n",
            "Epoch 14/20\n",
            "586/586 [==============================] - 53s 91ms/step - loss: 1.0477 - accuracy: 0.6844 - val_loss: 2.5350 - val_accuracy: 0.4299\n",
            "Epoch 15/20\n",
            "586/586 [==============================] - 53s 91ms/step - loss: 0.9707 - accuracy: 0.7059 - val_loss: 2.0789 - val_accuracy: 0.5034\n",
            "Epoch 16/20\n",
            "586/586 [==============================] - 53s 90ms/step - loss: 0.8857 - accuracy: 0.7306 - val_loss: 2.1846 - val_accuracy: 0.4991\n",
            "Epoch 17/20\n",
            "586/586 [==============================] - 53s 91ms/step - loss: 0.7917 - accuracy: 0.7553 - val_loss: 4.4854 - val_accuracy: 0.4355\n",
            "Epoch 18/20\n",
            "586/586 [==============================] - 53s 91ms/step - loss: 0.7192 - accuracy: 0.7772 - val_loss: 2.3988 - val_accuracy: 0.4859\n",
            "Epoch 19/20\n",
            "586/586 [==============================] - 53s 91ms/step - loss: 0.6658 - accuracy: 0.7915 - val_loss: 2.4001 - val_accuracy: 0.4908\n",
            "Epoch 20/20\n",
            "586/586 [==============================] - 54s 92ms/step - loss: 0.6068 - accuracy: 0.8085 - val_loss: 3.2709 - val_accuracy: 0.3686\n",
            "It took 1086.7563781738281 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFfm-pKQeSJv",
        "colab_type": "code",
        "outputId": "040edffb-9dc8-4a6d-8385-54e0124293f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.DataFrame(history.history).plot(figsize=(12,8))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0,2)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3NLNhbSeSQC",
        "colab_type": "code",
        "outputId": "d4c10463-1998-438d-bfe4-fa9c64642399",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "model.evaluate(ds_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 5s 33ms/step - loss: 3.3091 - accuracy: 0.3688\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.3090856075286865, 0.36880001425743103]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoKTsQVE6itm",
        "colab_type": "text"
      },
      "source": [
        "## Customiser ce que se passe dans `fit()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S95X-u7dpA6a",
        "colab_type": "text"
      },
      "source": [
        "### Les étapes cachées dans `.fit()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMeBjfr10hC1",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "class CustomModel(keras.Model):\n",
        "\n",
        "  def train_step(self, data):\n",
        "    x, y = data\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_pred = self(x, training=True)\n",
        "      loss = self.compiled_loss(y, y_pred,\n",
        "                                regularization_losses=self.losses)\n",
        "\n",
        "    trainable_vars = self.trainable_variables\n",
        "    gradients = tape.gradient(loss, trainable_vars)\n",
        "    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "    self.compiled_metrics.update_state(y, y_pred)\n",
        "    return {m.name: m.result() for m in self.metrics}\n",
        "```\n",
        "\n",
        "1. On récupère les données du minibatch :\n",
        "\n",
        "```python \n",
        "  def train_step(self, data):\n",
        "    x, y = data\n",
        "```\n",
        "\n",
        "Le type de données que vous récupérez dépend évidemment du type de modèle que vous entraînez et donc des données que vous passez dans `.fit()`.\n",
        "\n",
        "```python\n",
        "    with tf.GradientTape() as tape:\n",
        "```\n",
        "`tf.GradientTape()` est la méthode de Tensorflow pour différentier les fonctions, i.e. calculer des dérivées et des dérivées partielles. Qui dit dérivées partielles, dit étapes de mises à jours des poids. \n",
        "\n",
        "\n",
        "2. On calcule la prédiction sur le minibatch :\n",
        "$$(\\hat{y}_{1}, \\dots, \\hat{y}_{N}) =( f(x_{1}), \\dots, f(x_{N})) $$\n",
        "```python\n",
        "      y_pred = self(x, training=True)\n",
        "```\n",
        "\n",
        "3. Pour chaque $\\hat{y}_{i}$, on calcule l'erreur faite via la fonction de perte $\\mathcal{L}_{\\vartheta}(y_{i},\\hat{y}_{i})$, et on en déduit l'erreur moyenne sur le minibatch.\n",
        "\n",
        "$$\\mathcal{L}_{\\vartheta} = \\frac{1}{N}\\sum_{i=1}^{N}\\mathcal{L}_{\\vartheta}(y_{i},\\hat{y}_{i})$$\n",
        "\n",
        "```python\n",
        "      loss = self.compiled_loss(y, y_pred,\n",
        "                                regularization_losses=self.losses)\n",
        "```\n",
        "\n",
        "On rappelle que la fonction de perte est définie dans la `.compile()`. \n",
        "\n",
        "4. On calcule alors le gradient pour chaque paramètre dans $\\vartheta$, i.e.\n",
        "\n",
        "$$\\nabla \\mathcal{L}_{\\vartheta}$$\n",
        "```python\n",
        "    trainable_vars = self.trainable_variables\n",
        "    gradients = tape.gradient(loss, trainable_vars)\n",
        "```\n",
        "\n",
        "5. On met alors à jour les paramètres :\n",
        "\n",
        "$$ w_{i} \\leftarrow w_{i} - \\eta \\frac{\\partial \\mathcal{L}_{\\vartheta}}{\\partial w_{i}}(\\vartheta), \\\\\n",
        "b_{i} \\leftarrow b_{i} - \\eta \\frac{\\partial \\mathcal{L}_{\\vartheta}}{\\partial b_{i}}(\\vartheta).$$\n",
        "```python\n",
        "    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "```\n",
        "\n",
        "6. On met alors à jour les métriques (loss, accuracy, ...) et on renvoit un dictionnaire contenant ces mise à jours.\n",
        "\n",
        "```python\n",
        "    self.compiled_metrics.update_state(y, y_pred)\n",
        "    return {m.name: m.result() for m in self.metrics}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsyUy438utTh",
        "colab_type": "text"
      },
      "source": [
        "Voyons un peu comment se comportent ses différentes étapes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0mpQePPycWa",
        "colab_type": "text"
      },
      "source": [
        "Inutile de lancer ça sur un vrai modèle, plusieurs centaines de couches et millions de paramètres. Nous voulons d'abord juste voir comme nt cela fonctionne. Créons donc un dataset et un modèle complètement naïf."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL_EtBU0QvSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomModel(keras.Model):\n",
        "\n",
        "  def train_step(self, data):\n",
        "    print()\n",
        "    print(f\"----Etape: {self.step_counter}\")\n",
        "    self.step_counter += 1\n",
        "    \n",
        "    x, y = data\n",
        "    print(f'Début du train : {x.shape}, {y.shape}')\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      print(f'Start GradientTape step {x.shape}')\n",
        "      y_pred = self(x, training=True)\n",
        "      print(f'Prediction done {y_pred.shape}')\n",
        "      loss = self.compiled_loss(y, y_pred,\n",
        "                                regularization_losses=self.losses)\n",
        "      print(f'loss {loss}')\n",
        "\n",
        "    trainable_vars = self.trainable_variables\n",
        "    gradients = tape.gradient(loss, trainable_vars)\n",
        "    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "    self.compiled_metrics.update_state(y, y_pred)\n",
        "    return {m.name: m.result() for m in self.metrics}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76T8WaZAQkcx",
        "colab_type": "code",
        "outputId": "494ca9b2-7922-41e3-ed98-3c3355eb5c9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "#Créons un dataset dummy\n",
        "t_x = tf.random.uniform([30, 4], dtype=tf.float32)\n",
        "t_y = tf.range(30)\n",
        "\n",
        "ds_x = tf.data.Dataset.from_tensor_slices(t_x)\n",
        "ds_y = tf.data.Dataset.from_tensor_slices(t_y)\n",
        "\n",
        "ds = tf.data.Dataset.zip((ds_x, ds_y))\n",
        "\n",
        "ds = ds.batch(3)\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "# Dummy model\n",
        "input = Input(shape=(4,))\n",
        "\n",
        "x = Dense(32)(input)\n",
        "output = Dense(1)(x)\n",
        "\n",
        "model = CustomModel(input,x)\n",
        "\n",
        "model.compile(loss = 'mean_absolute_error',\n",
        "              optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
        "              run_eagerly=True)\n",
        "model.step_counter = 0\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"custom_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 4)]               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                160       \n",
            "=================================================================\n",
            "Total params: 160\n",
            "Trainable params: 160\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DYxRDruRH-a",
        "colab_type": "code",
        "outputId": "fa4b8abf-07f3-4a8a-bc5c-83150e1a7faf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "model.fit(ds,\n",
        "          epochs=1,\n",
        "          verbose=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----Etape: 0\n",
            "Start train step (None, 4), (None,)\n",
            "Start GradientTape step (None, 4)\n",
            "Prediction done (None, 32)\n",
            "loss Tensor(\"mean_absolute_error/weighted_loss/value:0\", shape=(), dtype=float32)\n",
            "\n",
            "----Etape: 1\n",
            "Start train step (None, 4), (None,)\n",
            "Start GradientTape step (None, 4)\n",
            "Prediction done (None, 32)\n",
            "loss Tensor(\"mean_absolute_error/weighted_loss/value:0\", shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V6eLgRH7eKh",
        "colab_type": "text"
      },
      "source": [
        "Pas très concluant hein ? on ne voit même pas toutes les étapes. C'est parce que Python est un langage lent, la plupart des structure internes de Tensorflow sont codés dans un langage beaucoup plus rapide tel que le C ou le C++.\n",
        "\n",
        "Ce que l'on écrit n'est dont pas toujours ce que l'on obtient vraiment. Pour que `tf.keras` fasse les instructions de façon séquentielle, on complie le modèle en rajoutant l'option :\n",
        "\n",
        "```python\n",
        "run_eagerly=True\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SscQYFd77dfr",
        "colab_type": "code",
        "outputId": "f882685b-1281-4c61-89fc-9827efd293bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.compile(loss = 'mean_absolute_error',\n",
        "              optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
        "              run_eagerly=True)\n",
        "model.step_counter = 0\n",
        "model.summary()\n",
        "\n",
        "model.fit(ds,\n",
        "          epochs=1,\n",
        "          verbose=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"custom_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 4)]               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                160       \n",
            "=================================================================\n",
            "Total params: 160\n",
            "Trainable params: 160\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "----Etape: 0\n",
            "Start train step (3, 4), (3,)\n",
            "Start GradientTape step (3, 4)\n",
            "Prediction done (3, 32)\n",
            "loss 1.0454397201538086\n",
            "\n",
            "----Etape: 1\n",
            "Start train step (3, 4), (3,)\n",
            "Start GradientTape step (3, 4)\n",
            "Prediction done (3, 32)\n",
            "loss 3.949005126953125\n",
            "\n",
            "----Etape: 2\n",
            "Start train step (3, 4), (3,)\n",
            "Start GradientTape step (3, 4)\n",
            "Prediction done (3, 32)\n",
            "loss 6.941404342651367\n",
            "\n",
            "----Etape: 3\n",
            "Start train step (3, 4), (3,)\n",
            "Start GradientTape step (3, 4)\n",
            "Prediction done (3, 32)\n",
            "loss 9.953435897827148\n",
            "\n",
            "----Etape: 4\n",
            "Start train step (3, 4), (3,)\n",
            "Start GradientTape step (3, 4)\n",
            "Prediction done (3, 32)\n",
            "loss 12.954936027526855\n",
            "\n",
            "----Etape: 5\n",
            "Start train step (3, 4), (3,)\n",
            "Start GradientTape step (3, 4)\n",
            "Prediction done (3, 32)\n",
            "loss 15.921150207519531\n",
            "\n",
            "----Etape: 6\n",
            "Start train step (3, 4), (3,)\n",
            "Start GradientTape step (3, 4)\n",
            "Prediction done (3, 32)\n",
            "loss 18.93268394470215\n",
            "\n",
            "----Etape: 7\n",
            "Start train step (3, 4), (3,)\n",
            "Start GradientTape step (3, 4)\n",
            "Prediction done (3, 32)\n",
            "loss 21.93629264831543\n",
            "\n",
            "----Etape: 8\n",
            "Start train step (3, 4), (3,)\n",
            "Start GradientTape step (3, 4)\n",
            "Prediction done (3, 32)\n",
            "loss 24.937978744506836\n",
            "\n",
            "----Etape: 9\n",
            "Start train step (3, 4), (3,)\n",
            "Start GradientTape step (3, 4)\n",
            "Prediction done (3, 32)\n",
            "loss 27.9443302154541\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f258a26f278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBWtOk0TK88y",
        "colab_type": "text"
      },
      "source": [
        "### Prise en main de `GradientTape`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtOZoiOiL3sN",
        "colab_type": "text"
      },
      "source": [
        "`GradientTape` enregistre les opérations qui sont faites dans un graphe (voir rappel du module 1 sur comment une fonction peut se définir comme une graphe), afin de calculer la différentielle de cette fonction.\n",
        "\n",
        "Prenons un exemple simple, la fonction $f(x) = x^{2}$, on souhaite calculer sa dérivée en 3. Les formules classiques d'analyse différentielle nous donnent alors $f'(x)=2x$ et donc $f'(3)=6$.\n",
        "\n",
        "Avec `GradientTape`, on fait comme cela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j8JdDuvLBpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.constant(3.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y = x * x\n",
        "\n",
        "dy_dx = tape.gradient(y, x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mxXfq4MNNdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dy_dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Onv2D2DNmRH",
        "colab_type": "text"
      },
      "source": [
        "Dans le cas de `tf.keras`, la partie `tape.watch(x)` qui nous dit par rapport à quelle variable nous allons dériver n'est pas nécessaire, `tf.keras` sait très bien quels sont les paramètres dans le réseau de neurones que nous entraînons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwG8IxanOB_M",
        "colab_type": "text"
      },
      "source": [
        "On peut évidemment le combiner pour calculer des dérivées secondes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQk4uWZJOHQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.constant(3.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  with tf.GradientTape() as tape2:\n",
        "    tape2.watch(x)\n",
        "    y = x * x\n",
        "  dy_dx = tape2.gradient(y, x)     \n",
        "d2y_dx2 = tape.gradient(dy_dx, x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2sqynq0OVs5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dy_dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OEMbontOXme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d2y_dx2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jpg0PPA-0hAw",
        "colab_type": "text"
      },
      "source": [
        "### We need to go deeper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP32I-SBEzSV",
        "colab_type": "text"
      },
      "source": [
        "Dans les modules suivants nous verrons comment modifier la méthode  `.fit()` pour qu'elle corresponde à l'entraînement que l'on souhaite, par exemple lorque l'on entraînera un autoencodeur variationnel.\n",
        "\n",
        "Il faut aussi noter que l'on peut complètement écrire sa boucle d'entraînement sans passer en aucune façon par la méthode `.fit()`. Ce qui est pratique, voire même nécéssaire si l'on souhaite implémenter certaines bonnes pratiques lors de l'entraînement de certains modèles comme les GAN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__z4NOS9I6iN",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "epochs = ...\n",
        "loss_fn = tf.keras.losses.[..]\n",
        "metric_fn = tf.keras.metrics.[...]\n",
        "optimizer = tf.keras.optimizers.[...]\n",
        "\n",
        "@tf.function\n",
        "def train_step(x, y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    #prédiction sur le minibatch\n",
        "    y_pred = model(x, training=True)\n",
        "    #calcul de la fonction de perte moyenne sur le minibatch\n",
        "    loss_value = loss_fn(y, y_pred)\n",
        "\n",
        "  # calcul des gradients et retropropagation\n",
        "  grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "  # mise à jour des métriques\n",
        "  metric_fn.update_state(y, y_pred)\n",
        "\n",
        "  return loss_value\n",
        "\n",
        "@tf.function\n",
        "def test_step(x, y):\n",
        "  y_pred = model(x, training=False)\n",
        "  loss_value = loss_fn(y, y_pred)\n",
        "  metric_fn.update_state(y, y_pred)\n",
        "\n",
        "  return loss_value\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print(f\"\\nDébut de l'époque {epoch+1},\")\n",
        "  start_time = time.time()\n",
        "\n",
        "  # itération sur les minibatchs du dataset\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(ds):\n",
        "    loss_value = train_step(x_batch_train, y_batch_train)\n",
        "\n",
        "    # Log tous les 10 batches\n",
        "    if step % 10 == 0:\n",
        "      print(f\"Loss sur le batch à l'étape {step} : {float(loss_value):.4f}\")\n",
        "\n",
        "  # Affichage des métriques à la fin de l'époque\n",
        "  metric = metric_fn.result()\n",
        "  print(f\"Métrique pour l'époque : {float(metric):.4f} \\n\")\n",
        "    \n",
        "  # Reset de la métrique à la fin de chaque époque\n",
        "  metric_fn.reset_states()\n",
        "\n",
        "  # validation loop à la fin de chaque époque\n",
        "  for x_batch_val, y_batch_val in ds_val:\n",
        "    val_loss = test_step(x_batch_val, y_batch_val)\n",
        "\n",
        "  val_metric = metric_fn.result()\n",
        "  metric_fn.reset_states()\n",
        "  print()\n",
        "  print(f\"Loss de validation : {float(val_loss):.4f}\")\n",
        "  print(f\"Métrique de validation : {float(val_metric):.4f}\")\n",
        "  print(f\"Durée de l'époque: {time.time() - start_time:.2fs}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYMR6ZwvLl5H",
        "colab_type": "text"
      },
      "source": [
        "Détaillons les parties importantes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCBoqy5jMepy",
        "colab_type": "text"
      },
      "source": [
        "- On lance une boucle `for` pour itérer sur les époques.\n",
        "- Pour chaque époque, on ouvre une autre boucle `for`\n",
        " pour itérer sur les batchs du dataset.\n",
        "- Pour chaque batch, on ouvre un `GradientTape()`, où l'on calcule l'étape de feedforward.\n",
        "- Une fois fini, on calcule le gradient par rapport aux poids du modèle et l'on met à jour ces poids."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIYCNkX3NuLb",
        "colab_type": "text"
      },
      "source": [
        "Détaillons plus. Il est à noter ici que pour cette boucle, nous avons déjà accès aux batchs. Notre dataset est donc déjà sous la forme tensorielle via, l'API `tf.data.Dataset`.\n",
        "\n",
        "\n",
        "Premièrement, on fixe les variables : le nombre d'époques, et les différents fonctions que l'on utilisera. \n",
        "```python\n",
        "epochs = ...\n",
        "loss_fn = tf.keras.losses.[..]\n",
        "metric_fn = tf.keras.metrics.[...]\n",
        "optimizer = tf.keras.optimizers.[...]\n",
        "```\n",
        "\n",
        "On lance alors la boucle principale sur le nombre d'époque.\n",
        "```python\n",
        "for epoch in range(epochs):\n",
        "  print(f\"\\nDébut de l'époque {epoch+1},\")\n",
        "  start_time = time.time()\n",
        "```\n",
        "\n",
        "Pour chaque batch, on lance alors l'étape d'entraînement (on revient dessus plus tard) et on affiche la perte disons par exemple tous les 10 minibatchs.\n",
        "```python\n",
        "for step, (x_batch_train, y_batch_train) in enumerate(ds):\n",
        "    loss_value = train_step(x_batch_train, y_batch_train)\n",
        "\n",
        "    # Log tous les 10 batches\n",
        "    if step % 10 == 0:\n",
        "      print(f\"Loss sur le batch à l'étape {step} : {float(loss_value):.4f}\")\n",
        "```\n",
        "\n",
        "Une fois que tous les minibatchs sont passés, l'époque est finie. On affiche alors la métrique moyenne obtenue à la fin.\n",
        "```python\n",
        "  # Affichage des métriques à la fin de l'époque\n",
        "  metric = metric_fn.result()\n",
        "  print(f\"Métrique pour l'époque : {float(metric):.4f} \\n\")\n",
        "```\n",
        "\n",
        "On remet à zéro la métrique pour le début de la nouvelle époque.\n",
        "```python\n",
        "  # Reset de la métrique à la fin de chaque époque\n",
        "  metric_fn.reset_states()\n",
        "```\n",
        "\n",
        "Si on souhaite  avoir un dataset de validation, c'est ici que ça se passe. Comme pour `train_step`, on y revient bientôt.\n",
        "```python\n",
        "  # validation loop à la fin de chaque époque\n",
        "  for x_batch_val, y_batch_val in ds_val:\n",
        "    val_loss = test_step(x_batch_val, y_batch_val)\n",
        "```\n",
        "\n",
        "On affiche les métriques de validation.\n",
        "```python\n",
        "  val_metric = metric_fn.result()\n",
        "  metric_fn.reset_states()\n",
        "  print()\n",
        "  print(f\"Loss de validation : {float(val_loss):.4f}\")\n",
        "  print(f\"Métrique de validation : {float(val_metric):.4f}\")\n",
        "  print(f\"Durée de l'époque: {time.time() - start_time:.2fs}\")\n",
        "```\n",
        "\n",
        "\n",
        "Comme expliqué plus haut, `tf.keras` a en fait deux modes de fonctionnement, et le fonctionnement de base est celui dit `eager mode`, ce qui fait que les instructions données dans une fonctions définie à la main, comme ici pour `def train_step`, seront exécutées les unes à la suites des autres, ce qui est long.\n",
        "\n",
        "Le décorateur `@tf.function` permet de transformer toute fonction n'yant come variable que des tenseurs en un **graphe statique**. Il n'est pas nécessaire d'en savoir plus sur ces fameux graphes, la seule chose à savoir est que cela **augmente la vitesse à laquelle les opérations sont faites dans la fonction**.\n",
        "\n",
        "En dehors de cela, la fonction `train_step` est **exactement la même que celle définie dans `.fit()`, de la même façon pour la fonction de validation**.\n",
        "\n",
        "```python\n",
        "@tf.function\n",
        "def train_step(x, y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    #prédiction sur le minibatch\n",
        "    y_pred = model(x, training=True)\n",
        "    #calcul de la fonction de perte moyenne sur le minibatch\n",
        "    loss_value = loss_fn(y, y_pred)\n",
        "\n",
        "  # calcul des gradients et retropropagation\n",
        "  grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "  # mise à jour des métriques\n",
        "  metric_fn.update_state(y, y_pred)\n",
        "\n",
        "  return loss_value\n",
        "```\n",
        "\n",
        "```python\n",
        "@tf.function\n",
        "def test_step(x, y):\n",
        "  y_pred = model(x, training=False)\n",
        "  loss_value = loss_fn(y, y_pred)\n",
        "  metric_fn.update_state(y, y_pred)\n",
        "\n",
        "  return loss_value\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY1rCDpS6iw1",
        "colab_type": "text"
      },
      "source": [
        "## Callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_7fihoXLPQ-",
        "colab_type": "text"
      },
      "source": [
        "En plus des arguments classiques tel que `epochs` ou `validation_data`, la méthode `.fit()` accepte aussi l'argument `callbacks`. Les callbacks permettent de fournir une **liste** d'intruction que `tf.keras` à certains moment précis de l'entraînement : \n",
        "\n",
        "- au début (à la fin) de l'époque actuelle,\n",
        "- au début (à la fin de l'étape de minibatch actuelle,\n",
        "- au début (à la fin) de l'entraînement.\n",
        "\n",
        "Nous avons déjà vu le callback `LearningRateScheduler` dans le module suivant lorsque nous parlions du Learninf Rate Decay. L'idée est ici de passer en revue les plus importants, et de voir la structure interne d'un callback pour en écrire un nous même."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BelDLof1NFYU",
        "colab_type": "text"
      },
      "source": [
        "### Les callbacks importants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vnQqUbrNFgG",
        "colab_type": "text"
      },
      "source": [
        "#### `tf.keras.callbacks.EarlyStopping`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSCrnILg53_c",
        "colab_type": "text"
      },
      "source": [
        "`EarlyStopping` permet à `tf.keras` d'aretter de lui même l'entraînement. On lui passe la métrique que l'on souhaite monitorer, et les crières d'arrêts.\n",
        "\n",
        "On l'appelle vec les paramètres suivants.\n",
        "\n",
        "```python\n",
        "keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                              min_delta=1e-2,\n",
        "                              patience=10,\n",
        "                              verbose=1)\n",
        "```\n",
        "\n",
        "Détaillons.\n",
        "\n",
        "```python\n",
        "keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "```\n",
        "Détermine la métrique à surveillez, principalement la `val_loss`, mais on peut aussi monitorer `val_acc`, ou les métriques d'entraînement. Ici, l'entraînement s'arette lorsque `val_loss` ne s'améliore plus.\n",
        "\n",
        "```python\n",
        "                 min_delta=1e-2,\n",
        "```\n",
        "\n",
        "On précise le \"ne s'améliore plus\" : on dit que la val_loss ne s'améliore plus si\n",
        "\n",
        "$$\\mathrm{val \\_loss}(t+1) - \\mathrm{val\\_loss}(t) \\leq 10^{-2}.$$\n",
        "\n",
        "```python    \n",
        "                 patience=10,\n",
        "                 verbose=1)\n",
        "```\n",
        "\n",
        "Si pendant 10 époques ça ne s'améliore pas, on arêtte l'entraînement. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjqnKMwvOSyv",
        "colab_type": "text"
      },
      "source": [
        "#### `tf.keras.callbacks.ModelCheckpoint`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK3uS9F2-LNi",
        "colab_type": "text"
      },
      "source": [
        "`ModelCheckpoint` permet lui de faire des sauvegarde régulière du modèle suivant un critère de métrique. Une nouvelle version du modèle ne sera enrégistrée que si la métrique s'est améliorée."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiyDgMKg9xWh",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "keras.callbacks.ModelCheckpoint('./weights.{epoch:02d}-{val_loss:02d}.h5',\n",
        "                                                 monitor='val_loss',\n",
        "                                                 verbose=1,\n",
        "                                                 save_best_only=True,\n",
        "                                                 save_weights_only=False,\n",
        "                                                 mode='auto',\n",
        "                                                 save_freq='epoch')\n",
        "```                                                 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwIddmhFOSt5",
        "colab_type": "text"
      },
      "source": [
        "#### `tf.keras.callbacks.LearningRateScheduler`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSwQxw98-rwL",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "def exponential_decay(lr0,step):\n",
        "    def exponential_decay_fn(epoch):\n",
        "        return lr0*0.1**(epoch/step)\n",
        "    return exponential_decay_fn\n",
        "\n",
        "exponential_decay_fn = exponential_decay(lr0 = lr0, step = 20)\n",
        "\n",
        "keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSrG6tgoOS_O",
        "colab_type": "text"
      },
      "source": [
        "#### `tf.keras.callbacks.TensorBoard`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzxmVN2tP7oH",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdCLJPdEP_fD",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "%tensorboard --logdir logs\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlyjCkOsQFuu",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "from tensorboard import notebook\n",
        "notebook.list() # View open TensorBoard instances\n",
        "#Control TensorBoard display. If no port is provided, \n",
        "#the most recently launched TensorBoard is used\n",
        "notebook.display(port=6006, height=1000)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmkqMGivPk5E",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "callbacks_fit = [keras.callbacks.EarlyStopping(\n",
        "                 # Stop training when `val_loss` is no longer improving\n",
        "                 monitor='val_loss',\n",
        "                 # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
        "                 min_delta=1e-2,\n",
        "                 # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
        "                 patience=10,\n",
        "                 verbose=1),\n",
        "                 keras.callbacks.LearningRateScheduler(exponential_decay_fn),\n",
        "                 keras.callbacks.ModelCheckpoint('./weights.{epoch:02d}-.hdf5',\n",
        "                                                 monitor='val_loss',\n",
        "                                                 verbose=1,\n",
        "                                                 save_best_only=True,\n",
        "                                                 save_weights_only=False,\n",
        "                                                 mode='auto',\n",
        "                                                 save_freq='epoch')]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyJbpuewNFkY",
        "colab_type": "text"
      },
      "source": [
        "### Structure interne d'un callback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj9OY2q7_szq",
        "colab_type": "text"
      },
      "source": [
        "Ce sont tous des sousclasses de la classe `tf.keras.callbacks.Callback`.\n",
        "\n",
        "On peut les passer en liste lorsque l'on fait appel à l'une des 3 commandes suivantes.\n",
        "\n",
        "```python\n",
        "   model.fit()\n",
        "   model.evaluate()\n",
        "   model.predict()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uEVECFGAQmT",
        "colab_type": "text"
      },
      "source": [
        "Un callback fait une action particulière avec une étape particulière, pour définir cette étape on a les méthodes suivantes.\n",
        "\n",
        "- Méthodes globales\n",
        "```python\n",
        "on_(train|test|predict)_begin(self, logs=None)\n",
        "```\n",
        "```python\n",
        "on_(train|test|predict)_end(self, logs=None)\n",
        "```\n",
        "\n",
        "- Batch-level méthodes\n",
        "```python\n",
        "on_(train|test|predict)_batch_begin(self, batch, logs=None)\n",
        "```\n",
        "```python\n",
        "on_(train|test|predict)_batch_end(self, batch, logs=None)\n",
        "```\n",
        "Ici, logs est un dictionnaire contenant les différentes métriques.\n",
        "\n",
        "- Epoch-level méthodes (uniquement durant lentraînement)\n",
        "```python\n",
        "on_epoch_begin(self, epoch, logs=None)\n",
        "```\n",
        "```python\n",
        "on_epoch_end(self, epoch, logs=None)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x9uLE4_yr1P",
        "colab_type": "code",
        "outputId": "7b2badf6-e770-49ec-9404-1a3b08a33c8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "#Créons un dataset dummy\n",
        "t_x = tf.random.uniform([30, 4], dtype=tf.float32)\n",
        "t_y = tf.range(30)\n",
        "\n",
        "ds_x = tf.data.Dataset.from_tensor_slices(t_x)\n",
        "ds_y = tf.data.Dataset.from_tensor_slices(t_y)\n",
        "\n",
        "ds = tf.data.Dataset.zip((ds_x, ds_y))\n",
        "\n",
        "ds = ds.batch(3)\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "# Dummy model\n",
        "input = Input(shape=(4,))\n",
        "\n",
        "x = Dense(32)(input)\n",
        "output = Dense(1)(x)\n",
        "\n",
        "model = Model(input,x)\n",
        "\n",
        "model.compile(loss = 'mean_absolute_error',\n",
        "              optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 4)]               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                160       \n",
            "=================================================================\n",
            "Total params: 160\n",
            "Trainable params: 160\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulTcxPkhCf_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomCallback(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(f\"Début de l'entraînement, les clés du log sont: {keys}\")\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(f\"Fin de l'entraînement, les clés du log sont: {keys}\")\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(f\"Début de l'époque {epoch}, les clés du log sont: {keys}\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(f\"Fin de l'époque {epoch}, les clés du log sont: {keys}\")\n",
        "\n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(f\"Entraînement : début du batch {batch}, les clés du log sont: {keys}\")\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(f\"Entraînement : fin du batch {batch}, les clés du log sont: {keys}\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EGGZTn93tQ0",
        "colab_type": "code",
        "outputId": "aff213dc-d406-4271-bcbd-a5b9d090c7c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        }
      },
      "source": [
        "history = model.fit(ds,\n",
        "                    epochs=1,\n",
        "                    verbose=0,\n",
        "                    callbacks=[CustomCallback()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Début de l'entraînement, les clés du log sont: []\n",
            "Début de l'époque 0, les clés du log sont: []\n",
            "Entraînement : début du batch 0, les clés du log sont: []\n",
            "Entraînement : fin du batch 0, les clés du log sont: ['loss', 'mean_absolute_error']\n",
            "Entraînement : début du batch 1, les clés du log sont: []\n",
            "Entraînement : fin du batch 1, les clés du log sont: ['loss', 'mean_absolute_error']\n",
            "Entraînement : début du batch 2, les clés du log sont: []\n",
            "Entraînement : fin du batch 2, les clés du log sont: ['loss', 'mean_absolute_error']\n",
            "Entraînement : début du batch 3, les clés du log sont: []\n",
            "Entraînement : fin du batch 3, les clés du log sont: ['loss', 'mean_absolute_error']\n",
            "Entraînement : début du batch 4, les clés du log sont: []\n",
            "Entraînement : fin du batch 4, les clés du log sont: ['loss', 'mean_absolute_error']\n",
            "Entraînement : début du batch 5, les clés du log sont: []\n",
            "Entraînement : fin du batch 5, les clés du log sont: ['loss', 'mean_absolute_error']\n",
            "Entraînement : début du batch 6, les clés du log sont: []\n",
            "Entraînement : fin du batch 6, les clés du log sont: ['loss', 'mean_absolute_error']\n",
            "Entraînement : début du batch 7, les clés du log sont: []\n",
            "Entraînement : fin du batch 7, les clés du log sont: ['loss', 'mean_absolute_error']\n",
            "Entraînement : début du batch 8, les clés du log sont: []\n",
            "Entraînement : fin du batch 8, les clés du log sont: ['loss', 'mean_absolute_error']\n",
            "Entraînement : début du batch 9, les clés du log sont: []\n",
            "Entraînement : fin du batch 9, les clés du log sont: ['loss', 'mean_absolute_error']\n",
            "Fin de l'époque 0, les clés du log sont: ['loss', 'mean_absolute_error']\n",
            "Fin de l'entraînement, les clés du log sont: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE5KV5wOva3s",
        "colab_type": "text"
      },
      "source": [
        "Allons plus dans le détail et voyons par exemple de quelles façons sont calculées les métriques données par `tf.keras`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqEr2lN9DMFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LossCallback(tf.keras.callbacks.Callback):\n",
        "  \n",
        "    def on_train_batch_end(self, batch, logs):\n",
        "        print(f'Batch {batch}, la perte est de {logs[\"loss\"]:.2f}.\\n')\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        print(f'La perte moyenne pour lépoque {epoch} est {logs[\"loss\"]:.2f} \\n')\n",
        "\n",
        "cb = LossCallback()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww8GsL9twhrd",
        "colab_type": "code",
        "outputId": "31d995a7-dcf1-4436-8a51-7107bc42111a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "history = model.fit(ds,\n",
        "                    epochs=1,\n",
        "                    verbose=0,\n",
        "                    callbacks=[cb])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch 0, la perte est de 1.01.\n",
            "\n",
            "Batch 1, la perte est de 2.46.\n",
            "\n",
            "Batch 2, la perte est de 3.93.\n",
            "\n",
            "Batch 3, la perte est de 5.41.\n",
            "\n",
            "Batch 4, la perte est de 6.90.\n",
            "\n",
            "Batch 5, la perte est de 8.40.\n",
            "\n",
            "Batch 6, la perte est de 9.89.\n",
            "\n",
            "Batch 7, la perte est de 11.39.\n",
            "\n",
            "Batch 8, la perte est de 12.89.\n",
            "\n",
            "Batch 9, la perte est de 14.39.\n",
            "\n",
            "La perte moyenne pour lépoque 0 est 14.39 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wr67tEsgrxo",
        "colab_type": "text"
      },
      "source": [
        "La façon dont `tf.keras` calcule la fonction de perte à la fin de de chaque époque est donc en faisant une moyenne : à la fin de l'époque, la métrique de perte donnée est **la perte moyenne sur un minibatch**, et à la fin de chaque batch, la métrique de perte donnée est la **perte moyenne mouvante**.\n",
        "\n",
        "Si l'on souhaite avoir la perte moyenne sur une observation, ou la valeur de la perte sur chaque batch, comment faire ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkyoFFBgiV-W",
        "colab_type": "text"
      },
      "source": [
        "$$\\mathrm{AvgLoss}_{n+1} := \\frac{\\mathrm{Loss}_{n+1}+n\\cdot \\mathrm{AvgLoss}_{n}}{n+1}$$\n",
        "\n",
        "$$(n+1)\\cdot\\mathrm{AvgLoss}_{n+1} -n\\cdot \\mathrm{AvgLoss}_{n} = \\mathrm{Loss}_{n+1}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "madKeNBXyC5N",
        "colab_type": "code",
        "outputId": "5d4e849f-4ff1-46de-90ed-e81ea98a4838",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        }
      },
      "source": [
        "class LossCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self, L = 0):\n",
        "        self.L = L\n",
        "  \n",
        "    def on_train_batch_end(self, batch, logs):\n",
        "      if batch == 0:\n",
        "        print(f'Batch {batch}, loss is {logs[\"loss\"]:.2f}.\\n')\n",
        "        #logs['loss'] gives the running avg mean, not the mean of the minibatch\n",
        "      else:\n",
        "        print(f'Batch {batch}, loss is {(batch+1)*logs[\"loss\"]-self.L:.2f}.\\n')\n",
        "        print(f'Batch {batch}, running loss is {logs[\"loss\"]:.2f}.\\n')\n",
        "      self.L = (batch+1)*logs[\"loss\"]\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        print(f'Avg loss on {epoch} is {logs[\"loss\"]:.2f} \\n')\n",
        "\n",
        "cb = LossCallback()\n",
        "\n",
        "model.fit(ds,\n",
        "          epochs=1,\n",
        "          verbose=0,\n",
        "          callbacks=[cb])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch 0, loss is 0.95.\n",
            "\n",
            "Batch 1, loss is 3.78.\n",
            "\n",
            "Batch 1, running loss is 2.36.\n",
            "\n",
            "Batch 2, loss is 6.73.\n",
            "\n",
            "Batch 2, running loss is 3.82.\n",
            "\n",
            "Batch 3, loss is 9.71.\n",
            "\n",
            "Batch 3, running loss is 5.29.\n",
            "\n",
            "Batch 4, loss is 12.72.\n",
            "\n",
            "Batch 4, running loss is 6.78.\n",
            "\n",
            "Batch 5, loss is 15.75.\n",
            "\n",
            "Batch 5, running loss is 8.27.\n",
            "\n",
            "Batch 6, loss is 18.74.\n",
            "\n",
            "Batch 6, running loss is 9.77.\n",
            "\n",
            "Batch 7, loss is 21.78.\n",
            "\n",
            "Batch 7, running loss is 11.27.\n",
            "\n",
            "Batch 8, loss is 24.76.\n",
            "\n",
            "Batch 8, running loss is 12.77.\n",
            "\n",
            "Batch 9, loss is 27.69.\n",
            "\n",
            "Batch 9, running loss is 14.26.\n",
            "\n",
            "Avg loss on 0 is 14.26 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f258a3c1da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4udAI7X60vM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PrintValTrainRatioCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs):\n",
        "    print(f'\\n Validation-Train Ratio : {logs[\"val_loss\"]/logs[\"loss\"]:.2f}')\n",
        "\n",
        "cb = PrintValTrainRatioCallback()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O48a3I0yiTnR",
        "colab_type": "text"
      },
      "source": [
        "## Annexe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMNKOtPl-N2q",
        "colab_type": "code",
        "outputId": "4b3aab9f-2d9b-4d9b-f32a-d11757525164",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "import random\n",
        "\n",
        "print(tf.__version__)\n",
        "print(keras.__version__)\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "\n",
        "\n",
        "#dummy dataset\n",
        "t_x = tf.random.uniform([30, 4], dtype=tf.float32)\n",
        "t_y = tf.range(30)\n",
        "ds_x = tf.data.Dataset.from_tensor_slices(t_x)\n",
        "ds_y = tf.data.Dataset.from_tensor_slices(t_y)\n",
        "ds = tf.data.Dataset.zip((ds_x, ds_y))\n",
        "ds = ds.batch(3)\n",
        "\n",
        "class LossCallback(tf.keras.callbacks.Callback):\n",
        "  \n",
        "    def on_train_batch_end(self, batch, logs):\n",
        "        print(f'Batch {batch}, loss is {logs[\"loss\"]:.2f}.\\n')\n",
        "        #logs['loss'] gives the running avg mean, not the mean of the minibatch\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        print(f'Avg loss on {epoch} is {logs[\"loss\"]:.2f} \\n')\n",
        "\n",
        "cb = LossCallback()\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "input = Input(shape=(4,))\n",
        "\n",
        "x = Dense(2)(input)\n",
        "X = Dense(1)(x)\n",
        "\n",
        "model = Model(input,x)\n",
        "\n",
        "model.compile(loss = 'mean_absolute_error',\n",
        "              optimizer=tf.keras.optimizers.SGD())\n",
        "\n",
        "history = model.fit(ds,\n",
        "                    epochs=1,\n",
        "                    verbose=0,\n",
        "                    callbacks=[cb])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "2.3.0-tf\n",
            "Batch 0, loss is 0.83.\n",
            "\n",
            "Batch 1, loss is 2.00.\n",
            "\n",
            "Batch 2, loss is 3.40.\n",
            "\n",
            "Batch 3, loss is 4.83.\n",
            "\n",
            "Batch 4, loss is 6.27.\n",
            "\n",
            "Batch 5, loss is 7.75.\n",
            "\n",
            "Batch 6, loss is 9.21.\n",
            "\n",
            "Batch 7, loss is 10.71.\n",
            "\n",
            "Batch 8, loss is 12.20.\n",
            "\n",
            "Batch 9, loss is 13.67.\n",
            "\n",
            "Avg loss on 0 is 13.67 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Module5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
